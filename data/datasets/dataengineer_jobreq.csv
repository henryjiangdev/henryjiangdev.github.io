company,href,html,location,title
AbleTo,/rc/clk?jk=78822a6cd5cb1912&fccid=954e57501f6bca1f&vjs=3,"AbleTo is the leading provider of high quality, technology-enabled behavioral health care. AbleTo believes that everyone deserves access to high-quality care, and offers a suite of technology-enabled services to empower people to lead better lives through better mental health. A proprietary platform connects individuals with AbleTo highly trained licensed providers who deliver weekly sessions by phone or video supported by an integrated digital experience. Members also have access to mental health coaches, and digital support programs. Clinical best practices are leveraged across all services to ensure individuals are getting the care they need. Our outcomes-focused care is proven to improve depression and anxiety by over 50% on average, as demonstrated in several peer-reviewed studies. For patients with high-cost medical conditions, our clinically-tailored treatment is proven to reduce medical costs with care that empowers the patient to address their health needs. AbleTo partners with payers to make high-quality care accessible, affordable and convenient for millions of people to get the treatment they need. | Data is integral to nearly everything that we do at AbleTo, and as a result the data team plays a key role at our company. You should be excited to work with data, have the curiosity to dive deeply into issues, and feel empowered to make a meaningful impact at a mission-driven company. We are a team committed to agile value delivery and solid engineering principles, as well as continuously improving our craft. If you love shipping software that delivers deep and meaningful impact to people's mental and behavioral health, join us! |  | Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, etc.) | Identify code quality issues and implement tests to improve future processes. | Implement data integrity tests to ensure we are ingesting accurate data. | Translate business requirements into actionable data tasks. | Partner with business users to understand their needs, come up with end to end solutions, and communicate the results back to the users. | Implement high-quality test-driven code. |  | Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice. |  | 1+ years experience coding in Python. | Experience working with structured and NOSQL databases. | Familiarity with structuring and writing ETLs. | Experience working with Airflow and Bigquery is a big plus. | Experience working with a multitude of stakeholders is a big plus. |  | Follow AbleTo on LinkedIn and Twitter! |  | AbleTo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. AbleTo is an E-Verify company.",New York NY 10010,Data Engineer I
Flatiron Health,/rc/clk?jk=556a8c38da29549b&fccid=b4df24bc350094d0&vjs=3,"About the job | We are a hyper-growth tech start-up with a mission to bring a smile to people's faces by connecting them to products that bring nourishment, love, memories, connection, nostalgia, and comfort. | We are looking for an experienced Data Engineer to join our team in New York. This role will be responsible for overseeing the design, development, and deployment of our end-to-end production batch and streaming data pipelines and infrastructure. | We currently leverages Snowflake, Kafka, Fivetran, Airflow and GCP in it’s data infrastructure. This role will be involved in guiding the direction of our data team, making decisions around tech stack, and facilitating the evolution of our data infrastructure. |  | Responsibilities | Work alongside the rest of the Data Science, Machine Learning, and software engineering teams to build the data-driven backbone of our business | Implement data infrastructure that best meets the needs of our high performance platform. | Build and maintain production batch and streaming data pipelines to support machine learning, data science, and analytics. | Qualifications | B.S. or M.S. in a quantitative field (e.g., computer science, engineering, etc.) | 4+ years of experience building production data pipelines | Strong programming and debugging skills in Python | Experience with building batch ETL workflows (SQL, Spark, Airflow, etc.) | Experience with streaming workflows like Kafka | Experience with data warehouses (Snowflake, Redshift, etc.) | Experience working in cloud hosted environments (AWS, GCP, etc.) | Bonus: experience building high performance data products in e-commerce",New York NY,Data Engineer
KPMG,/rc/clk?jk=a780166bdad9331d&fccid=7ca1385998532a85&vjs=3,"Condé Nast is a global media company producing the highest quality content with a footprint of more than 1 billion consumers in 32 territories through print, digital, video and social platforms. The company’s portfolio includes many of the world’s most respected and influential media properties including Vogue, Vanity Fair, Glamour, Self, GQ, The New Yorker, Condé Nast Traveler/Traveller, Allure, AD, Bon Appétit and Wired, among others. Condé Nast Entertainment was launched in 2011 to develop film, television and premium digital video programming. | Job Description | Responsibilities include, but are not limited to: | Build batch and streaming data pipelines capable of processing large volumes of data | Build efficient code to transform raw data | Build analytical ready datasets that are appropriately scalable, standardized, and reliable for use by Data Scientists and Data Analysts | Collaborate with other Data Engineers to implement a shared technical vision | Follow agile processes with a focus on delivering production-ready, testable deliverables, and automated code | Participate in the entire software development lifecycle, from concept to release | Minimum Qualifications: | BS, or equivalent industry experience in Computer Science, Software Engineering, or other related Science/Technology/Engineering/Math fields. | 1+ years experience building streaming or batch data pipelines | Experience in writing reusable/efficient code to automate analysis and data processes | Experience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metric providers ranging from web analytics, consumer analytics, user behavior and advertising | Experience implementing scalable, distributed, and highly available systems | Proficiency in Python/PySpark, Scala or Java | Proficiency in SQL | Experience with Spark, AWS Services, Hadoop, Pig, Hive, Flink, or Beam. | Experience with orchestration tools such as Airflow | Experience with Git version control, and other software adjacent tools | What happens next? | If you are interested in this opportunity, please apply below, and we will review your application as soon as possible. You can update your resume or upload a cover letter at any time by accessing your candidate profile. | Condé Nast is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics.",New York NY,Data Engineer I
Apple,/company/Jettison/jobs/Data-Engineer-2dcdff07c4bf7b09?fccid=46e89005b7c970c0&vjs=3,"Job detailsSalary$100,000 - $150,000 a yearJob TypeFull-timeQualificationsBachelor's (Preferred)Python: 5 years (Preferred)Full Job DescriptionGreat opportunity for Data Engineers! Jettison is representing a company that is immediately hiring a Data Engineer experienced with trading systems and strong project management skills in New York. The company is committed to growth and their employees, offering a casual work environment and excellent work-life balance! Highly competitive pay, performance bonuses, great benefits and perks aplenty!**This position is currently fully remote until at least summer and the company determines it is safe to return to the office at a later date, then employees will be expected to be in office full-time.Overview Proprietary trading firm headquartered in New York, is seeking a Data Engineer to join a team that has extensive, global experience in a wide variety of asset classes, risk management, and leading trading technologies. They focus their efforts on hiring extremely talented and motivated individuals to collaborate with each other and be on the forefront of leveraging technology and math to implement sophisticated trading strategies.The Data Engineer will have the opportunity to work in the New York office focusing on building out and supporting our research framework. They are seeking a candidate that is searching for an opportunity to make a real impact, deliver value, and work within an experienced team.ResponsibilitiesCollaborate with quant researchers, and core technologists to build and maintain the central research platformDevelop and optimize existing tools, used by the quantsOngoing testing and optimization of the research pipeline by collecting and analyzing data on the utilization and efficiency of the firms distributed systemsSkills RequiredA Bachelors, Masters, or PhD degree with more than 5 years of experience with an emphasis in Computer Science and Statistics/Math (e.g. Data Science, Physics, etc.)Strong multi-tasking skills, including the ability to prioritize and work on multiple projects at one timeExpert experience in Python and knowledge of DASKAbility to understanding the optimization and profiling of trading systems with proven success of developing models in a low latency trading environmentCompensationCompetitive salaryPerformance bonusesMedical, Dental, Vision, and Life insuranceLong and short-term disabilityRetirement planningPerksCasual dressWeekly catered breakfast and lunchesStocked pantryZen room to relaxJettison provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, national origin, age, disability or genetics.Job Type: Full-timePay: $100,000.00 - $150,000.00 per yearBenefits:401(k)Dental insuranceDisability insuranceHealth insuranceLife insurancePaid time offVision insuranceSchedule:8 hour shiftMonday to FridaySupplemental Pay:Bonus payAbility to Commute/Relocate:New York, NY 10005 (Preferred)Education:Bachelor's (Preferred)Experience:data engineering: 6 years (Preferred)Python: 5 years (Preferred)Trading Systems: 4 years (Preferred)developing models in a low latency trading environment: 4 years (Preferred)quantitative tools development and optimization: 4 years (Preferred)DASK: 2 years (Preferred)Work Location:One locationWork Remotely:Temporarily due to COVID-19",New York NY 10005,Data Engineer
Scholastic,/rc/clk?jk=66868e488a4eae1e&fccid=f8bb8eb40f54f89f&vjs=3,"THE OPPORTUNITY |  | Scholastic is seeking a Data Engineer Associate to provide data-driven insights and execute recommendations as part of the Data Engineering team. The ideal candidate is highly analytical, passionate about hypothesis-driven problem-solving, and excellent at developing data solutions. The role serves a range of functions across Scholastic’s businesses, covering financial analysis, customer analytics, data engineering, web analytics, and analysis of offline activities. |  | YOUR RESPONSIBILITIES | Manage enterprise data across Scholastic's business units | Interface with other developers, product owners and business analysts to understand data needs | Develop and maintain Scholastic's data infrastructure to drive efficient and reliable processes | Identify opportunities to improve Scholastic operations and its supply chain | Utilize data to analyze the effectiveness of Scholastic’s marketing efforts across channels | Generate insights and recommendations that are aligned with business realities and Scholastic’s mission and vision | Develop dashboards and improve reporting measures |  | WHO WE ARE | About Scholastic | Scholastic Corporation (NASDAQ: SCHL) is the world's largest publisher and distributor of children's books, a leading provider of print and digital instructional materials for pre-K to grade 12, and a producer of educational and entertaining children's media. The Company creates quality books and ebooks, print and technology-based learning programs, classroom magazines and other products that, in combination, offer schools customized solutions to support children's learning both at school and at home. The Company also makes quality, affordable books available to all children through school-based book clubs and book fairs. With a 100-year history of service to schools and families, Scholastic continues to carry out its commitment to ""Open a World of Possible"" for all children. Learn more at Scholastic.com/AboutScholastic. |  | About the Associate Program | Scholastic’s Technology Associate Program is designed to identify, train, and promote the next generation of leaders. Each new-hire class receives comprehensive training and is quickly given responsibility to deliver on business goals using industry-leading tools, partners, and technology – all while working in an agile, iterative model that emphasizes collaboration, transparency, and goal-oriented development. | HOW YOU CAN FIT | Graduation date in 2020 or 2021 | Undergraduate/Graduate degree in business, computer science, math, engineering or other quantitative disciplines | Strong SQL skills for purposes of data extraction, transformation, cleaning and analysis | Ability to collaborate with data and software engineers as well as non-technical business owners | Exceptional problem-solving ability, logical reasoning, creative thinking and quantitative aptitude | Strong Excel, SAS, R or Python skills in analysis, statistics and charting |  | Preferred Skills and Knowledge | Familiarity with BI and data visualization tools like Power BI, Tableau, and Looker | Familiarity with web analytics tools such as Google Analytics and Adobe Analytics (a.k.a. Omniture) | Experience in Python, Scala or Java is a plus | Experience using big data technologies like Hadoop, Hive and Spark is a plus",New York NY,Associate Data Engineer
IBM,/company/SHGT/jobs/Data-Engineer-Snowflake-c704ae8379cf43f9?fccid=e665e7e788ab560d&vjs=3,"Job detailsSalary$50 - $65 an hourJob TypeFull-timeTemporaryNumber of hires for this role2 to 4QualificationsExperience:Oracle , 5 years (Preferred)Python , 4 years (Required)Snowflake , 5 years (Required)Work authorization:United States (Required)Full Job DescriptionSeattle WA or New YorkData EngineerContract RoleClient :: InfoGainResponsibilities: In one role the key technologies are Snowflake and Spark. In the second, Oracle Database and Snowflake (they are sun setting Oracle).Strong in Python scripting, minimum 4+ yrs,Must have hands on experience implementing AWS Big data lake using EMR and SparkStrong exp with Snowflake Database Architecture , SQL , Database performance/Optimization .Experience with Airflow tool and DAG's creation, Jobs Orchestration.Good to have Media AdSales experience .Experience leveraging open source big data processing frameworks, such as Apache Spark.Experience developing and deploying data pipelines within a cloud native infrastructure preferably AWSExperience in using CI/CD pipeline (Gitlab)Experience in Code Quality implementation (Used Pep8/Pylint) tools or any other code quality tool.Experience of Python Plugins /operators like FTP Sensor, Oracle Operator etc.Implement Industry Standards /Best Practices.Excellent analytical and problem-solving skillsExcellent verbal and written communication skillsPreferred QualificationsRequired Education  * Bachelor’s degree or better in Computer Science or aPreferred EducationAdditional Information 5+ years of experience working as an Oracle / Snowflake database developer (Oracle 11g or greater)5+ years of working in a data warehousing / big data environmentSQL ETL/ELT development and performance tuningAbility to develop, implement and maintain standards established by the architecture and Development teams.Robust data analysis and root cause analysis skillsSelf-motivated independent thinker and collaborative team member.THE KEY WITH THIS ROLE IS THE ORACLE SKILL SET- AD REPORTING PLATFORM IS IN ORACLE- ORACLE EXPERTISE IS NEEDED- WHOLE STACK IS IN PLSQL -SNOWFLAKE WILL BE THE BRIDGE IN THIS ROLE- THE MODERNIZATION PIECE- ORACLE IS THE LEGACYThis is a remote role - can sit anywhere in states, but must be able to work core hours east/west coast for meetingsContract length: 12 monthsJob Types: Full-time, TemporarySalary: $50.00 - $65.00 per hourSchedule:8 hour shiftExperience:Oracle : 5 years (Preferred)Python : 4 years (Required)Snowflake : 5 years (Required)Work authorization:United States (Required)Work Remotely:Yes",New York NY,Data Engineer Snowflake
Amazon.com Services LLC,/company/Checkmate-Partners/jobs/Data-Engineer-48d070d658896ff5?fccid=7d4fd8e89420ac8a&vjs=3,"Job detailsSalary$160,000 - $250,000 a yearJob TypeFull-timeNumber of hires for this role1QualificationsComputer science: 3 years (Required)Node.js: 3 years (Required)Python: 3 years (Required)SQL: 3 years (Required)US work authorization (Required)Bachelor's (Preferred)Machine learning: 1 year (Preferred)Full Job DescriptionYour Responsibilities: Building highly reliable data services to integrate with dozens of blockchainsCreating ETL pipelines that transform and process petabytes of structured and unstructured data in real-time, ultimately helping financial institutions and governments fight fraud and criminal activityDesigning data models for optimal storage and retrieval to support sub second latency for querying blockchain dataDeploying and monitor large data base clusters that are performant and highly-availableWorking cross-functionally with data scientists, backend engineers and product managers to design and implement, and new data models to support the productDeveloping your skills through exceptional training as well as frequent coaching and mentoring from colleagues*Some of the Traits we value:Bachelor's degree (or equivalent) in Computer Science or related field3+ years of experience building real-time and distributed system architecture, from whiteboard to productionStrong programming skills in Node, Python, and SQL.Versatility. Experience across the entire spectrum of data engineering, including:Data stores (e.g., ClickHouse, ElasticSearch, PostGres, MongoDB, Redis, and Neo4j)Data pipeline and workflow orchestration tools (e.g., Azkaban, Luigi, Airflow, Storm)Data processing technologies (e.g., Spark)Deployment and monitoring large data base clusters in public cloud platforms (e.g., Docker, Terraform, Datadog)Adaptable. Goals can change fast. You anticipate and react quickly.Autonomous. You own what you work on. You move fast and get things done.Excellent communication. You will need communicate complex ideas effectively to both technical and non-technical audiences, and both verbally and in writingCollaborative. You must work collaboratively in a cross-functional team and with people at all levels in an organizationIndustry experience building and productionizing innovative end-to-end Machine Learning systems is a plus.Relevant experience in crypto/blockchain is a plusBenefits:Stock$2,000 yearly coupon for books, conferences, and professional coachingCompetitive salaryPaid time off*Job Type: Full-timePay: $160,000.00 - $250,000.00 per yearSchedule:8 hour shiftEducation:Bachelor's (Preferred)Experience:Computer science: 3 years (Required)Machine learning: 1 year (Preferred)Node.js: 3 years (Required)Python: 3 years (Required)SQL: 3 years (Required)Work Location:One locationVisa Sponsorship Potentially Available:No: Not providing sponsorship for this jobBenefit Conditions:Only full-time employees eligibleWork Remotely:YesCOVID-19 Precaution(s):Remote interview process",New York NY 10005,Data Engineer
PepsiCo,/rc/clk?jk=2b27622e513c8e39&fccid=2973259ddc967948&vjs=3,"Auto req ID: 229747BR |  | Job Description | As a Data Engineer, you will play a critical role in executing the global agenda. You will be tasked with identifying, designing, and implementing data solutions to business problems. You will collaborate with the other DSA teams to create a robust, shared codebase; on which PepsiCo will build automated systems. You will work with internal business stakeholders and strategic partners to identify opportunities for collaborative development and foster a data-driven culture between relevant teams. |  |  | Responsibilities: | Write production-grade code using standard software engineering methodologies | Implement data solutions with software engineering best practices | Own data pipelines end-to-end | Collaborate with business teams to design and implement a solution | Contribute to a data driven culture where everyone is valued | Analyze large data sets and develop custom models to uncover trends, patterns and insights | Work with technologists both inside and outside DSA to identify long-term, sustainable solutions | Provide critical thought leadership to enhance organizational capabilities by utilizing big and small data | Qualifications/Requirements | Motivated to learn and collaborate in a large team setting | Drive to build sustainable and reliable solutions | BS or MS in Computer Science or related field | Knowledge of Python, SQL, GitHub, AWS, Docker, etc. | Knowledge of best-practices in distributed systems and large-scale data processing | Ability to effectively and concisely communicate with both business and technical audiences | Nice to have: | Knowledge of ML pipeline frameworks like kubeflow, MLflow, etc. | ML frameworks (sklearn, Tensorflow, PyTorch, MXNet or similar) | Kubernetes | ETL tools like Airflow, Celery, Luigi, etc. | AWS services like s3, Sagemaker, lambda, SQS, SNS, EMR, ECR, EKS, etc. | Distributed data processing systems like Spark, Hadoop, etc. | MPP cloud data warehouses like Snowflake, Redshift, etc. | We believe that culture should be the cornerstone of everything we do at PepsiCo eCommerce. Operating with a start-up mindset, we are agile, innovative and not afraid of failure. We want our team to come to work every day excited to explore new ways of bringing enjoyment, refreshment and fun to the world. |  | So if you’re looking to solve real-world problems at a company that has the resources to make real change, then we want you at PepsiCo. | All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. |  |  | PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity. |  |  | Our Company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Credit Reporting Act, and all other applicable laws, including but not limited to, San Francisco Police Code Sections 4901 - 4919, commonly referred to as the San Francisco Fair Chance Ordinance; and Chapter XVII, Article 9 of the Los Angeles Municipal Code, commonly referred to as the Fair Chance Initiative for Hiring Ordinance. | Relocation Eligible: Not Eligible for Relocation |  | Job Type: Regular |  |  |  | All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. |  |  | PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity |  |  | Our Company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Credit Reporting Act, and all other applicable laws, including but not limited to, San Francisco Police Code Sections 4901 - 4919, commonly referred to as the San Francisco Fair Chance Ordinance; and Chapter XVII, Article 9 of the Los Angeles Municipal Code, commonly referred to as the Fair Chance Initiative for Hiring Ordinance. |  |  | If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law &amp; EEO is the Law Supplement documents. View PepsiCo EEO Policy |  |  | Please view our Pay Transparency Statement",New York NY,eCom Data Engineer
Dashlane,/rc/clk?jk=f6736d3a3f431ec6&fccid=de71a49b535e21cb&vjs=3,"Introduction | At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk. |  | Your Role and Responsibilities |  | The AI Applications team is seeking an Entry Level Data Engineer. Climate science, weather forecasting and agricultural predictions are among the many leading Big Data, geospatially oriented problems that we work on. Problems that are extremely relevant to all of us in today's changing world. We are looking to add a results oriented individual to join us and help develop and maintain the information/data pipelines that fuel our geospatial analytics platform. A wide variety of data sources, such as satellites and IoT devices, contribute to the over 6 petabytes (and growing daily) of information that we curate for our clients. | As a team member you will be: | Passionate about each client and their success | Building new data ingestion pipelines, and adapting existing ones to changing data sources and changing client data requirements | Working with others to design new ways of ingesting data that makes it faster and easier | Knowledgeable and skilled with one or more languages like Python, Java, Scala and Julia | Knowledgeable and/or willing to learn about different database technologies, Hadoop and Cloud development and delivery | Conscientious about deadlines and deliverables | Excited to continue to learn and develop new skills | Contributing to the governance and curation of the data, and tooling to support these activities |  |  |  | Required Technical and Professional Expertise |  | Basic knowledge in one or more of the following languages: Python, Java, Scala, Julia | Basic knowledge in database technologies | Basic knowledge in Hadoop |  | Preferred Technical and Professional Expertise |  | Previous internship or co-op experience as a Data Engineer |  | About Business Unit | IBM’s Cloud and Cognitive software business is committed to bringing the power of IBM’s Cloud and Watson/AI technologies to life for our clients and ecosystem partners around the world. IBM provides you with the most comprehensive and consistent approach to development, security and operations across hybrid environments—with complete software solutions for business and IT operations, development, data science, security, and management. Our experts and software capabilities help organizations develop applications once and deploy them anywhere, integrate security across the breadth of their IT estate, and automate operations with management visibility. With IBM, you also have access to new skills and methods, governance and management approaches, and a deep ecosystem of industry experts and partners. |  | Your Life @ IBM | What matters to you when you’re looking for your next career challenge? |  | Maybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities – where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust – where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible. |  | Impact. Inclusion. Infinite Experiences. Do your best work ever. |  | About IBM | IBM’s greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries. |  | Location Statement | IBM will not be providing visa sponsorship for this position now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship. |  | IBM intends this job to be performed entirely outside of Colorado. |  | Being You @ IBM | IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.",Littleton MA 01460,Entry Level Data Engineer (AI Applications)
Conde Nast,/rc/clk?jk=83fc29328ec6840b&fccid=c1099851e9794854&vjs=3,"Summary | Posted: Feb 24, 2021 | Weekly Hours: 40 | Role Number:200224461 | At Apple, new ideas have a way of becoming phenomenal products, services, and customer experiences very quickly. You bring passion and dedication to your job and there's no telling what you could accomplish. | We provide the machine learning infrastructure, data products, and algorithms supporting some of Apple's newest and fastest growing services. Going beyond accuracy we are driven to develop fair, explainable algorithms that preserve user privacy. Collaborating with Machine Learning Engineers and iOS Frameworks Engineers, your focus will be to bring rigorous data engineering, systems, and governance practices to the team; we always launch at scale. Imagine what you could do here. | Key Qualifications | Write high quality code. We work mostly in Python, Objective-C, Java, and Swift. However, languages can be learned: we care much more about your general engineering skills than knowledge of any particular language. | Hold yourself and others to a high bar when working with production systems | Enjoy working with a diverse group of people with different training and expertise. | Have extraordinary communication skills, for collaborating across many participating teams | Experience building data pipelines | Experience to quickly grasp many different services and processes and know how to leverage them | Experience with orchestration tools like Airflow or Luigi | Description | We are looking for candidates with broad experience in systems integration, data engineering, and operations. You will be responsible for providing the flexible data infrastructure, tooling, and technical guidance that we need to provide financial intelligence at scale and build great software products. You will work on: | - building flexible and user-friendly infrastructure that powers our data pipelines and centralized feature stores | - developing easy to use data infrastructure for orchestrating ML training jobs | - enabling our ML Engineers to seamlessly transition from experimentation to production | Education &amp; Experience | BS or MS in Computer Science or related technical field, equivalent work experience will be considered",New York NY,Data Infrastructure Engineer
Warner Music Group,/rc/clk?jk=d1abb6375c819f65&fccid=95eedc2455f7362c&vjs=3,"Job detailsJob TypeContractFull Job DescriptionThe Data Engineer performs routine functions in support of the Tech Operations data team. The Data Engineer runs standardized queries, extracts and parses files, loads data into platforms, and ensures the highest levels of quality. |  | Daily Tasks / Average Day: | Run standardized queries to extract data from data lakes and other tools. | Parses files per platform ingestion limitations. | Loads files into platforms (e.g. SFMC, Braze) via UI or other methods. | Completes quality assurance and resolves any discrepancies. | Document completion and audit trail in Jira. | Requirements: | Minimum of 2+ years scripting languages and scheduling jobs (SQL, python) | Experience with data platforms Snowflake/Databricks preferred. | Experience with email/push/inapp platforms (Salesforce, Braze) preferred | Experience with quality assurance and Infosec procedures. | Experience with Agile software development methodology (SCRUM, Kanban). | Experience with Jira preferred. | Flexible and willing to assist other team members | Able to work in a fast paced environment with quick turnaround | Strong problem solving skills | Attention to detail | Ability to manage multiple tasks simultaneously | Excellent interpersonal skills.",New York NY 10036,Data Engineer
Infinity Consulting Solutions Inc.,/company/NYC-Health-+-Hospitals/jobs/Data-Engineer-d78f5e3d2073f433?fccid=8ba13cf21fd7c2de&vjs=3,"Job detailsSalary$54,017 - $100,000 a yearJob TypeFull-timeContractNumber of hires for this role1QualificationsBachelor's (Preferred)SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Full Job DescriptionAbout NYC Health + HospitalsEmpower Every New Yorker — Without Exception — to Live the Healthiest Life PossibleNYC Health + Hospitals is the largest public health care system in the United States. We provide essential outpatient, inpatient and home-based services to more than one million New Yorkers every year across the city’s five boroughs. Our large health system consists of ambulatory centers, acute care centers, post-acute care/long-term care, rehabilitation programs, Home Care, and Correctional Health Services. Our diverse workforce is uniquely focused on empowering New Yorkers, without exception, to live the healthiest life possible.The Test and Trace Corps is looking for a Data Engineer to join the Data, Analytics and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.Summary of Duties and Responsibilities:Reporting to the Senior Data Engineer within the Data and Analytics Unit, the Data Engineer designs, evaluates and tests data structures and will be responsible for:Cleaning large datasets, with responsibility for accuracy and complex analyses through programming and performing statistical analyses on large data sets to ensure integrity of data for analyses and use across the Data, Analytics and Product Development Team to support planning, reporting and development initiatives under the Test and Trace CorpsCompleting complex network, statistical and programming analyses to clean and prepare data, ensuring accuracyServing as the internal engineering consultant to solve structural data issues within the Quality Control (QC) workflowRegulating the cleanliness of data produced from Data Engineers, Data Scientists, and Data Managers across the Data, Analytics and Product Development TeamCollaborating with data staff and other departments to identify and mitigate potential data issuesAuditing and reporting on the QC of data migration and integrations throughout the Test and Trace Corps organizationChecking and preparing data to provide to the Data Scientist and other analysts, as neededMinimum Qualifications1. A Baccalaureate Degree from an accredited college or university with a major in Computer Science, Systems Engineering, applied Mathematics, Business Administration, Economics/Statistics, Telecommunications, Data Communications, or a related field of study; and2. Five (5) years of progressive, responsible experience in the field of data processing, computer systems and applications.Operations Specialty requires supervisory experience (5 years).Network Services requires a telecommunications background and experience.3. Broad knowledge and expertise in the characteristics of computers, peripheral devices, communications systems and hardware capabilities, programming languages, E.D.P. applications, systems analysis methodology, data management and retrieval techniques; or4. A satisfactory equivalent combination of training, education and experienceDepartment PreferencesAbility to work autonomously, think analytically, and anticipate data issues to solve before they ariseExcellent written and verbal communication skills, with the ability to explain data systems to non-technical teamsStrong quality control abilities and exceptional attention to detailGeneral knowledge of SQL, R, Python, Excel and related data analytics toolsets3+ years of experience in a data training, analytics, management, or QC roleNYC residencyJob Types: Full-time, ContractPay: $54,017.00 - $100,000.00 per yearSchedule:8 hour shiftEducation:Bachelor's (Preferred)Experience:SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Work Location:One locationCompany's website:nychealthandhospitals.orgWork Remotely:NoCOVID-19 Precaution(s):Remote interview processPersonal protective equipment provided or requiredSocial distancing guidelines in placeVirtual meetingsSanitizing, disinfecting, or cleaning procedures in place",New York NY 11201,Data Engineer
NYC Health + Hospitals,/rc/clk?jk=916f65fd363d0010&fccid=c48e788a5f55ea2d&vjs=3,"About Spruce | Spruce is the platform enabling modern real estate transactions. We work with forward thinking mortgage lenders, real estate companies and investors, and we believe the future of real estate will be driven by automation, efficiency and digital experiences, and we provide the products and services necessary to make that happen. | About the job | Spruce is looking for an experienced Data Engineer to join its growing Data Science team. The ideal candidate will have an interest in real estate and will thrive in a fast-paced startup environment. | As a part of the Data Science team, you will help build a cutting edge platform that will define the future of real estate transactions. Your ability to help turn data into practical, production-ready features will help us automate some of the manual work of processing real estate transactions |  |  | What you'll do | Work closely with Product, Engineering, and Operations as part of a team building the industry’s first automated title search platform | Use Python to collect and parse real estate records from 3rd party data sources | Using this data, train and implement predictive underwriting models | Convert existing offline prototype models to fully automated, high performance production systems | Requirements | 1-2 years working experience as a data scientist or data engineer | Experience with Python and SQL in a work setting | Working knowledge of web-scraping packages such as Selenium | Strongly Preferred | Advanced degree in a quantitative field or equivalent | Experience developing and implementing statistical models, especially in the ML / AI family | We are proud of the team we’re building. We're committed to equal opportunity employment - and beyond. We believe diverse experiences and perspectives build a stronger team and a better product. We welcome fresh perspectives and challenge our own assumptions to make Spruce better. The more inclusive we are as a company, the better we can serve our customers.",New York NY,Data Engineer
Spruce,/rc/clk?jk=9101159550f44b5b&fccid=10189a63fa03fd40&vjs=3,"Data Engineer for Landing Zone |  | Data Engineering role at InnovateEDU |  | Job type: Full Time |  | Location: Remote |  | About InnovateEDU |  | InnovateEDU is a non-profit whose mission is to eliminate the achievement gap by accelerating innovation in standards-aligned, next generation learning models and tools that serve, inform, and enhance teaching and learning. InnovateEDU is committed to massively disrupting K-12 public education by focusing on the development of scalable tools and practices that leverage innovation, technology, and new human capital systems to improve education for all students and close the achievement gap. |  | About the Project |  | InnovateEDU strives to create real tools and projects that greatly assist a school district in moving toward embracing a data standard and interoperability. Landing Zone, a project at InnovateEDU, provides school districts with a comprehensive data infrastructure through the implementation of an Ed-Fi Operational Data Store (ODS), data mart for analytics in Google BigQuery, and the necessary data workflows in Apache Airflow to connect previously siloed, disparate educational data systems. Landing Zone simplifies the process a district must go through to implement an Ed-Fi ODS, connecting Ed-Fi certified data sources, and consuming non Ed-Fi certified data once has been aligned to the standard. This project has a heavy focus on data engineering, backend work, dev ops, and using data analytics tools to verify data. |  | Who You Are |  | You are a mission-driven individual and believe in working to close the educational achievement gap through the use of data and technical solutions. You are excited about bringing order to disparate data, writing data pipelines, and don’t mind being relentless in the pursuit of data accuracy. You’ve worked as a backend developer or data engineer before and are fluent with SQL and Python. Bonus points for having worked with applications deployed on Kubernetes. |  | You are an optimistic problem-solver. You believe that together we can create real solutions that help the entire education sector move forward despite its complexity. You are excited to join a growing team working on an early-stage product and are looking forward to working on lots of different pieces of that product. You are open to feedback, bring your best every day, and are ready to grow in all areas of your work. You want to join a team of folks who share your vision for mission-driven work at the intersection of education and technology. Finally, you know that sharing often is key to this work and are ready to document everything that you do so that data people in schools everywhere can benefit. |  | Experience and Skills |  | The ideal candidate will have experience in some or all of the following areas: |  |  | 2+ years of experience working as a back end developer or data engineer | Ability to work independently and with teams | Fluency in Python and SQL | Experience developing and deploying solutions using Docker and Kubernetes | Experience working with Apache Airflow | Experience in test-driven development | Experience with Pandas and Jupyter Notebooks a plus | Experience working with data warehouses a plus | Experience with the Google Cloud Platform (especially BigQuery and Kubernetes Engine) a plus | Experience within K12 education a plus |  |  | Responsibilities |  | The Developer’s primary professional responsibilities will include, but not be limited to: |  | Creating, troubleshooting, and maintaining data processing pipelines in Apache Airflow (ETL work) | Writing SQL queries against many different types of databases (Microsoft SQL Server, Oracle, Postgres, Progress Open Edge, BigQuery) to extract and organize data | Maintaining Landing Zone documentation | Deploying code updates across the Landing Zone customer base | Assisting in the design, development, and deployment of infrastructure on the Google Cloud Platform for new customers | Assisting in the design and development of a historical/longitudinal data storage system (data warehouse) | Responding to customer support tickets (this is a shared responsibility on our team) | Working with internal systems such as JIRA, Asana, Slack to stay organized and ensure communication with team members | Other duties as assigned |  |  | Application Instructions |  | Please submit an application on this platform. Applications without both a resume and cover letter will not be considered.",Brooklyn NY 11201,Data Engineer
DataDog,/rc/clk?jk=5dc871b718796978&fccid=ae4c0182fd37ef13&vjs=3,"As a Data Engineer at Hinge, you will create essential data processes and contribute to components of a modern data pipeline that will be the foundation of Hinge’s decision-making ability. The systems you help create, the problems you help solve, and the support of our analytical minds, will be pivotal to the success of Hinge. | This role is key to the success of Hinge. Not only will they help power the love lives of tons of people, but they will play a critical part in the functioning of every team at Hinge, with stakeholders ranging from customer experience to marketing to leadership. | The Data Engineer will be implementing critical ETL pipelines, interpreting fascinating data sets, and improving core functionality that will not only aid the data engineering team, but the rest of the organization. They will be working on an actual big data architecture while concentrating on real-world problems such as privacy concerns. | Responsibilities: | Work with our Engineering teams to ensure data is flowing accurately through data creation to our presentation layers. | Write new and innovative ETL processes. | Improve our Data Engineering stack through containerization, data modeling, developing our ETL pipelines, and more. | Work with stakeholders and translate their needs and expectations into action items and deliverables. | Continue to learn more about the Data Engineering discipline, utilize that knowledge in your deliverables, and identify opportunities to enhance our pipelines. | Contribute meaningful insights and feedback to our team processes. | Requirements: | Proficient in Python, SQL, shell scripting, and databases. | +3 years of professional/industry experience. | Experience delivering data products from conception to delivery and with the infrastructure that supports their underlying processes. | Good communication skills (written/verbal). | Experience modeling data sets for different types of sources and business processes. | Passionate about designing elegant ETL processes. | Nice To Haves: | Used Redshift, Looker, Airflow. | Familiar with AWS technologies, data vault modeling, Agile methodologies, Kubernetes, Docker, Kafka, and data governance. | Our Company: | Hinge is the dating app for people who want to get off dating apps. In today’s digital world, singles are so busy matching that they’re not actually connecting, in person, where it counts. Hinge is on a mission to change that. So we built an app that’s designed to be deleted. On Hinge, there are no rules, timers, or games. Instead, you’ll meet your most compatible matches and you’ll have unique conversations over what you’ve shared on your detailed profile. It’s a natural way to find a great first date. Currently, 3 out of 4 first dates lead to second dates, we’re the #1 mobile-first dating app mentioned in the New York Times wedding section, and we’re the fastest growing dating app in the US, UK, Canada, and Australia. | Our Culture:Authenticity: Share your genuine thoughts and opinions directly.Courage: Invite and deeply consider challenges and criticism.Empathy: Be empathetic, communitarian and trustworthy. | We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",New York NY 10010,Data Engineer
Hinge,/rc/clk?jk=2d83d4bdaed1cd6d&fccid=5c9de1aedf73e3b6&vjs=3,"At Warner Music Group we’re all about our people. Our global company is made up of knowledgeable, passionate, and creative individuals. Our commitment to Diversity, Equity and Inclusion fosters a culture where you can truly belong, contribute, and grow. We believe in each individual’s value and encourage applications from people of any age, gender identity, sexual orientation, race, religion, ethnicity, disability, veteran status, and any other characteristic or identity. | It is the mission of every member of the WMG team around the world to create a nurturing environment for artists, songwriters, and the people behind the music – at every stage of their career. We strive to set WMG apart by embracing innovation – an integral part of our company's DNA. | Consider a career at WMG and be a part of one of the most influential forces in culture today. | Job Title: Data Science Engineer | A little bit about our team: | The Research + Analysis department works to influence WMG’s strategy and optimize day-to-day operations through data and insights. This position sits within the NYC-based global Research + Analysis team, working closely with colleagues across Business Development WEA Distribution, and Label Partners to better understand what makes our global digital partners tick, and how we can use that information to the advantage of our artists and songwriters. | Why this could be your next big break: | The Data Science Engineer will support the newly created Data Science organization within Research + Analysis on tasks at the intersection on Data Engineering, Machine Learning Engineering and Machine Learning Operations. The Data Science Engineer will also serve as a technical liaison with Data Engineers in different technology organizations across Warner Music Group. The position is not entry-level and requires prior experience performing these tasks. The position reports to the Director of Data Science. | Here you’ll get to: | Harden and optimize ETL scripts and data pipelines built by Data Scientists for data exploration, modeling and visualizations, | Deploy model pipelines and build task orchestration, | Ensure computation efficiency in deployed algorithms and data pipelines | Build and maintain tables and APIs (or other endpoints) where model outputs can be read by dashboards or visualization tools | Serve as a technical liaison with Data Engineers in various technology organizations across Warner Music Group | Execute tasks in time and with exceptional attention to detail | It would be music to our ears if you also had: | Graduate Degree (Masters encouraged) in Computer Science, Information Technology, Software Engineering, Data Engineering, Data Science, or related fields | 2 years of experience in a business setting | performing Data Engineering, Machine Learning Engineering or Machine Learning Operations tasks | training, tuning, validating Machine Learning models | scaling and optimizing Machine Learning models for deployment | expertly using SQL and NoSQL databases | operating in cloud environments (e.g. AWS, Azure, GCP) | using tools for large-scale data processing (e.g. Spark, Hadoop) with high proficiency | using scripting languages (e.g. Python, Julia, Scala) at an expert level | Aptitude to learn new technologies, tools and methods as required | Desired Characteristics | Experience with Data Science tasks and knowledge of additional programming languages a plus | Experience using and deploying containers a bonus | About us: | With its broad and diverse roster of new stars and legendary artists, Warner Music Group is home to a collection of the best-known record labels in the music industry including Asylum, Atlantic, East West, Elektra, FFRR, Fueled by Ramen, Nonesuch, Parlophone, Rhino, Roadrunner, Sire, Warner Records, Warner Classics and Warner Chappell Music, one of the world's leading music publishers with a catalogue of more than one million copyrights worldwide. | For more than four decades, WMG has been an industry-leading force in providing a world-class array of services designed to help artists and labels grow their careers and their businesses. Artist &amp; Label Services is the umbrella for WEA (Warner-Elektra-Atlantic) – the pioneering WEA distribution and marketing network – and Alternative Distribution Alliance (ADA) – the ground-breaking global distribution company for independent artists and labels | WMG is committed to inclusion and diversity in all aspects of our business. We are proud to be an equal opportunity workplace and will evaluate qualified applicants without regard to race, religious creed, color, age, sex, sexual orientation, gender, gender identity, gender expression, national origin, ancestry, marital status, medical condition as defined by state law (genetic characteristics or cancer), physical or mental disability, military service or veteran status, pregnancy, childbirth and related medical conditions, genetic information or any other characteristic protected by applicable federal, state or local law.",New York NY,Data Science Engineer
CAPCO,/rc/clk?jk=558159482625ba8f&fccid=f16b975bd9c56fcd&vjs=3,"The Data Engineer at Lazard will work within the Data Analytics Group. The primary responsibility is designing, building, testing, and maintaining large-scale software applications and data pipeline architecture. The Data Engineer will have the opportunity to work closely with data scientists, investment bankers and senior management across the Firm. | Successful candidates should have a strong programming and quantitative background. | Required Qualifications | Advanced degree in Computer Science/Engineering or equivalent work experience. | A thorough understanding of algorithms and data structures, data modeling | Extensive experience with python, Java or other programming languages. | Advanced working SQL knowledge and experience working with relational and NoSQL databases, query authoring (SQL) as well as working familiarity with a variety of databases: MYSQL, PostgreSQL, Mongo DB and ElasticSearch | Experience with software development methodology and release processes (requirements gathering, designing, building, testing and maintenance). | Experience building and optimizing data pipelines, architectures and data sets. Working experience with data orchestration tools such as Airflow, Prefect to create and maintain optimal data pipeline architecture. | Experience with cloud technologies such as AWS, GCP, Azure. | A passion for understanding real world problems through data analysis and presentation | Ability to work both independently and as a team player. |  | Nice to have: | Experience with DevOps, Microservice, and cloud native technologies. | Strong interest or previous experience in finance preferred.",New York NY,Data Engineer
InnovateEDU,/rc/clk?jk=da2836b3b1a80242&fccid=c2a63affe8751868&vjs=3,"About the team: |  | Capco’s Data Team helps our clients transform every aspect of their business. We are highly skilled at formulating data strategy, defining business and technology initiatives across the data management lifecycle, and aligning multi-year strategic roadmaps with client’s business goals. As digital technologies advance and regulations tighten, today’s consumers – and, therefore, today’s businesses – are becoming more aware of the importance of good quality data. We work to establish holistic ways to effectively manage data through the modern data supply chain and facilitate consumption through analytics, modelling, AI, machine learning, dashboarding, and reporting. |  | About the Job: |  | As a member of our Data Team, you will work across Capco’s different domains and solution offerings to help break down large problems, develop approaches and solutions. As a Data Engineer, you will create analytics reporting and provide data-driven strategic insights, trends, and perspective to help drive transformation for our clients. |  | What You’ll Get to Do: |  | Work on hard problems with smart people | Be highly motivated, result-oriented, and take pride in being a problem solver | Work with new technology, focus on using the right tool for the job, rather than any sticky preference for a tool or technology | Learn and share knowledge across our engineering teams, so we can continue to iterate and improve | Write reusable, testable, and efficient code | Design and implement of low-latency, high-availability, and performant applications | Integrate user-facing elements developed by front-end developers with server side logic | Work on implementation of security and data protection projects | Collaborate and work on integration of data storage solutions | Focus on performance tuning, improvement, balancing, usability and automation |  | What You’ll Bring with You: |  | 2+ years demonstrated experience helping organizations design or implement effective Data Management/Governance and/or Analytics programs in the cloud or on premises | Success working in one of several Data Engineering/Management technology areas, including but not limited to: | Metadata management and data lineage utilizing technologies such as Collibra, IBM’s Information Governance Catalog, or Informatica’s Enterprise Data Catalog, or others. | Data Quality tools such as Informatica Data Quality (IDQ), SAS Data Management, IMB InfoSphere Quality stage, or others. | Master Data Management tools, such as Informatica, Oracle, SAP MDG, Profisee, Collibra, or others. | Data prep tools, such as Alteryx Designer, Informatica Big Data, or others. | Demonstrated experience working with Enterprise Data Management frameworks, such as the EDM Council’s DCAM, the Data Management Association’s DMBOK model, or other overall Management frameworks. | Overall knowledge and experience in dealing with Regulatory drivers for Enterprise Data Management , such as CCPA / GDPR for Privacy, BCBS239 / RDA / CCAR for Financial Servicers, or others along with ability to articulate how Management is applied in these cases to address the Compliance or Regulatory needs. | Experience writing and supporting Python code, ETL, Hive and Hadoop | Experience developing Shell scripts and Python programs to automate tasks | Experience developing Spark scripts by using Jupyter Hub or others as required |  | Why Capco? |  | You will join a company that supports and encourages an entrepreneurial outlook and independent thinking. Capco is not about organizational charts and layers – we operate with little hierarchy because we want all employees to feel that Capco is their firm. |  | We offer highly competitive benefits, including medical, dental and vision insurance, a 401(k) plan, tuition reimbursement, and a work culture focused on innovation and creation of lasting value for our clients and employees. |  | Ready to Take the Next Step? |  | If this sounds like you, we would love to hear from you. This is an opportunity to make a difference and contribute to a highly successful company with a significant growth trajectory.",New York NY,Data Engineer
Veeva Systems,/rc/clk?jk=c30c3f753af35a48&fccid=1b50fcfb150b1b48&vjs=3,"The New York Times is seeking inventive and motivated data engineers at all levels of experience to join the Data Engineering group. In this role, you will build critical data infrastructure that surfaces data and insights across the company. | About Us | Our Data Engineering teams are at the intersection of business analytics, data warehousing, and software engineering. As Maxime Beauchemin wrote in “The Rise of Data Engineering”, ETL and data modeling have evolved, and the changes are about distributed systems, stream processing, and computation at scale. They’re about working with data using the same practices that guide software engineering at large. A strong data foundation is essential for The New York Times and we’re responsible for it. We use our data infrastructure to power analytics and data products and to deliver relevant experiences to our customers in real-time. We enable our company to validate strategic decisions, make smarter choices, and react to the fast changing world. We are part of a New York based technology organization with a remote-friendly workplace that includes engineers around the world. We value transparency and openness, learning, community, and continuous improvement. Check out the Times Open blog, which is written by engineers and other technical team members, and follow @nytdevs on Twitter to see what we’re up to. | About the Job | We focus on the software engineering related to data replication, storage, centralized computation, and data API’s. We provide customers and partners with data tools, shared frameworks, and data services. These are the foundational core of our group which enables ourselves and others to work with data from a common underpinning. Our tools and services enable our group to scale and avoid blocking others. We reduce data redundancy by creating systems and datasets that serve as sources of record. We enable discovery and governance of our data. We support key business goals like growing our digital subscriber base, understanding how our customers use our products, and retaining our print subscribers. | As a data engineer, you will: | Run and support a production enterprise data platform | Design and develop data models | Work with languages like Java, Python, Go, Bash, and SQL | Build batch and streaming data pipelines with tools such as Spark, Airflow, and cloud-based data services like Google’s BigQuery, Dataproc, and Pub/Sub | Develop processes for automating, testing, and deploying your work | About You | To thrive in this role, you are excited about data and motivated to learn new technologies. You are comfortable collaborating with engineers from other teams, product owners, business teams, and data analysts and data scientists. You are own and shape your technical domain area and move the related business goals forward. You are eager to resolve upstream data issues at the source instead of applying workarounds. You analyze and test changes to our data architectures and processes, and determine what the possible downstream effects and potential impacts to data consumers will be. | Benefits and Perks: | Make an impact by supporting our original, independent and deeply reported journalism. | We provide competitive health, dental, vision and life insurance for employees and their families | We support responsible retirement planning with a generous 401(k) company match. | We offer a generous parental-leave policy, which we recently expanded in response to employee feedback. Birth mothers receive 16 weeks fully paid; non gestational parents receive 10 weeks, also fully paid. | We are committed to career development, supported by a formal mentoring program and $8,000 annual tuition reimbursement. | We have frequent panel discussions and talks by a wide variety of news makers and industry leaders. | Join a community committed to the richness of diversity, experiences and talents in the world we cover, supported by a variety of employee resource groups. | This role may require limited on-call hours. An on-call schedule will be determined when you join, taking into account team size and other variables. On-call hours are unpaid, unless informed otherwise by your manager. | #LI-AM1 | The New York Times is committed to a diverse and inclusive workforce, one that reflects the varied global community we serve. Our journalism and the products we build in the service of that journalism greatly benefit from a range of perspectives, which can only come from diversity of all types, across our ranks, at all levels of the organization. Achieving true diversity and inclusion is the right thing to do. It is also the smart thing for our business. So we strongly encourage women, veterans, people with disabilities, people of color and gender nonconforming candidates to apply. | The New York Times Company is an Equal Opportunity Employer and does not discriminate on the basis of an individual's sex, age, race, color, creed, national origin, alienage, religion, marital status, pregnancy, sexual orientation or affectional preference, gender identity and expression, disability, genetic trait or predisposition, carrier status, citizenship, veteran or military status and other personal characteristics protected by law. All applications will receive consideration for employment without regard to legally protected characteristics. The New York Times Company will consider qualified applicants, including those with criminal histories, in a manner consistent with the requirements of applicable state and local ""Fair Chance"" laws.",New York NY,Data Engineer
Lazard Ltd.,/rc/clk?jk=f5ceec9a770af192&fccid=353eb997fc901045&vjs=3,"Veeva develops cloud software that helps the world’s largest pharmaceutical companies and emerging biotechs bring critical medicine and therapies to the patients that need them. Our enterprise product suite is ubiquitous in the life sciences industry. | Veeva is a ‘Work Anywhere’ company, so you can connect with teams in our New York City office while also having the flexibility to work from home. And as a Public Benefit Corporation, you will work for a company with purpose and focused on making a positive impact on society. | As a Data Engineer on the New York Analytics engineering team, you will be a core contributor in building out next-generation systems and processes that allow us to ingest advertising data and prepare it for health analytics at an ever-increasing scale. |  | You will be responsible for the design and implementation of major subsystems, and work in collaboration with your peers and the wider engineering organization. | What You'll Do | Handle and provide feedback on ad-hoc data loads for experimental/pre-contract advertising feeds before implementing in our proprietary ingest platform. | Assist other Data Engineers with complex, large, and time-sensitive data tasks across a modern and evolving technology stack. | Work with Software Engineers to migrate custom and one-off implementations into code, collaborating on feature and functionality required with product managers. | Keep up to date on emerging technology solutions that impact the data and cloud computing domains, in particular on AWS. | Provide technical guidance and support to members of other teams across the company in your areas of expertise. | Actively work to develop technical and soft skills through training, event attendance, accreditation, and industry knowledge. | Requirements | 2+ years of hands-on, directly relevant, Data Engineering experience. | Technically proficient in: Python, Relational Databases / SQL, Redshift or another MPP Data Warehouse, Linux | Experience designing and implementing systems using modern frameworks for ELT/ETL such as Apache Airflow. | Experience using AWS Database and Big Data tools – primarily we are interested in Redshift, RDS (MySQL &amp; Postgres), and EMR. | College degree in Computer Science, Math, Systems Engineering or a similar technical field. | Learn More | Engineer Perspective: 3 Reasons to Consider Veeva | Engineering at Veeva | Nice to Have | Proficiency some of the following technologies: Additional Languages: Java, Scala, Containerization: Docker, Kubernetes, NoSQL: MongoDB, ElasticSearch, AWS: S3, RDS, Lambda, EC2, EKS, VPC, SQL, SNS, ELB, CloudFront, Implementation of a Data Lake / Lakehouse design paradigm | AWS Associate Architect or Developer certification. Professional or Specialty is a big advantage. | Perks &amp; Benefits | Office conveniently located in midtown Manhattan and close to several major transportation hubs. | Fully stocked kitchen with snacks and beverages. | Fitness/wellness reimbursement. | Allocation for continuous learning and development. | Private roof deck and flexible working space. | Weekly happy hours and other social activities. | Veeva’s headquarters is located in the San Francisco Bay Area with offices in more than 15 countries around the world. | Veeva is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity or expression, religion, national origin or ancestry, age, disability, marital status, pregnancy, protected veteran status, protected genetic information, political affiliation, or any other characteristics protected by local laws, regulations, or ordinances.",New York NY,Data Engineer - New York Hub
The New York Times,/rc/clk?jk=17cdaccee5bb8884&fccid=fe2d21eef233e94a&vjs=3," | 3+ years of experience as a Data Engineer or in a similar role | Experience with data modeling, data warehousing, and building ETL pipelines | Experience in SQL |  | Are you interested in shaping the future of Advertising and B2B Marketing? We are a growing team with an exciting charter and need your passion, innovative thinking, and creativity to help take our products to new heights. |  | Amazon Advertising operates at the intersection of eCommerce and advertising, offering a rich array of digital advertising solutions with the goal of helping our customers find and discover anything they want to buy. We help advertisers reach Amazon customers on Amazon owned and operated sites and on other high quality sites across the web. We start with the customer and work backwards in everything we do, including advertising. If you’re interested in joining a rapidly growing team working to build a unique, world-class advertising product with a relentless focus on the customer, you’ve come to the right place. |  | The future of our business is compelling and we are focused on building a robust, innovative business, enabling advertisers of all sizes. Our advertising products range from eCommerce ads that integrate the power of familiar shopping features to visually stunning Kindle screensavers. We can reach customers on Amazon properties, both web and mobile, and across our ad platform. A strategic part of our charter is to build a portfolio of self-service, cost-per-click advertising programs to enable both large and small advertisers to engage with customers in relevant ways. Core to these programs is our understanding of our advertisers and marketing programs from a robust analytical perspective. Teams are dedicated to diving deep into telemetry and performance data to drive insights and future marketing strategy to grow our self-service advertising programs among advertisers. |  | As a Data Engineer, you will join the Marketing Science team that is composed of science, engineering, and product functions. Marketing Science empowers Amazon to transform its business-to-business (B2B) marketing and advertiser communication initiatives into smarter programs that are tailored to advertisers’ educative needs. As a contributor to the identification of advertiser insights and recommendations, you will (1) take on projects and make enhancements that improve data processes (e.g., data auditing solutions, management of manually maintained tables, automating, ad-hoc or manual operation steps). (2) build data pipelines to feed machine learning models for real-time and large-scale offline use cases, (3) perform data modeling to support machine learning model training and offline, batch inference workflows, (4) work closely with Data and Applied scientists to scale model training and explore new data sources and model features, and (5) work closely with Software Development Engineers to support offline data needs. |  | Moreover, you will work on project ideas with customers (e.g., analysts, scientists), stakeholders, and engineer peers. You help balance customer requirements with team requirements and help your team evolve by actively participating in the code review process, design discussions, team planning, and ticket/metric reviews. You focus on operational excellence, constructively identifying problems and proposing solutions and you are able to train new peers about how team data solutions are constructed, how they operate, how secure they are, and how they fit into the bigger picture. |  |  | 3+ years of experience as a Data Engineer, BI Engineer, Business/Financial Analyst or Systems Analyst in a company with large, complex data sources. | Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets | Experience working with AWS big data technologies (EMR, Redshift, S3) | Demonstrated strength in data modeling, ETL development, and data warehousing | Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy | Experience providing technical leadership and mentoring other engineers for best practices on data engineering | Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations |  | ",New York NY,Data Engineer
Spotify,/rc/clk?jk=1a7be7d20096d0c8&fccid=9ace5aa4e4fa6ce2&vjs=3,"Job detailsJob TypeContractFull Job DescriptionBasking ridge, New Jersey - United States | Posted - 09/02/20 |  |  | Overview | Opportunity for “Data Engineer” in Basking Ridge, NJ. Kindly go through the requirement and let me know your interest to discuss further about this role |  | Here are the details: | Position: Data Engineer | Location: 100% Remote (Client sits in Basking Ridge, NJ) | Duration: 12 months Contract to Hire |  |  | Job Description: | Project: Behavioural Health Encounter Management Information System Projects |  | Project team supports the ETL processes from various internal claims systems for EDI submissions to State Medicaid systems. Current team size is five with both database and report developers. |  | Day-to-day responsibilities: | 1. Extract data from various internal claims systems for Behavioural Health Medicaid claims, transform to meet the EDI requirements and load to SQL Server environment. | 2. Work with team to ensure data quality | 3. Load Responses from state into SQL Server environment for reporting | 4. Identify and help report data that needs additional research to be valid for EDI submission | 5. Identify rejected responses and report to Encounter research team |  | Software tools/skills are needed to perform these daily responsibilities: | MS SSIS | MS SSRS | SQL Server | Complex SQL querying | ETL (any type is fine) |  | Regards, | Viduth | IT Recruiter | Technosoft Corporation |One Towne Square, Suite 600 Southfield, MI 48076 | | Direct : +1 248 313 4594 | Email: Viduth.Selvan@technosoftcorp.com | www.technosoftcorp.com |",Basking Ridge NJ,Data Engineer
SEMA4,/rc/clk?jk=b905c4a99113f384&fccid=a05d513acd07aa63&vjs=3," | Software Engineer, Data Engineering |  | We ingest thousands of data events per second from over a dozen different device companies (and growing!). This data helps us support the electric grid by providing optimized device control and precise reporting. |  | Since device data is core to our business, we're looking for an engineer to focus on data engineering and make sure the device data we get is accurate, reliable, and timely. The team owns the entire pipeline from ingestion through to publishing data products, so you will also do some full-stack development work. |  | Responsibilities | Improve reliability, recovery, and integrity of our data ingestion pipeline | Develop reporting tools to help our in-house experts and utility clients understand the impact of our services | Work with our in-house Advanced Grid Services/Analytics team to productionize exciting new services for utilities | Requirements | Experience developing automated ETL pipelines with high reliability requirements | At least 2 of years of experience working on a professional web development team | Demonstrated expertise with at least one RDBMS | Demonstrated expertise with MongoDB, DynamoDB or similar document-oriented data store | Nice-to-have experience | Dealing with streaming ingestion | Improving reliability of ETL pipelines | Improving data recovery processes | Improving tooling for data correctness | Python or Java experience | Why work for EnergyHub? | Collaborate with outstanding people: Our employees work hard, do great work, and enjoy collaborating and learning from each other. | Make an immediate impact: New employees can expect to be given real responsibility for bringing new technologies to the marketplace. You are empowered to perform as soon as you join the team! | Gain well rounded experience: EnergyHub offers a diverse and dynamic environment where you will get the chance to work directly with executives and develop expertise across multiple areas of the business. | Work with the latest technologies: You'll gain exposure to a broad spectrum of IoT, SaaS and machine learning challenges, including distributed fault-tolerance, device control optimization, and process modeling to support scalable interaction with disparate downstream APIs. | Be part of something important: Help create the future of how energy is produced and consumed. Make a positive impact on our climate. | Focus on fun: EnergyHub places high value on our team culture. Happy hours and holiday parties are important to us, but what's also important is how our employees feel every single day. | Company Information |  | EnergyHub is a growing enterprise software company that works with the most forward-thinking companies in smart energy. Our platform lets consumers turn their smart thermostats, electric cars, water heaters, and other products into virtual power plants that keep the grid stable and enable higher penetration of solar and wind power. We work on technology that already provides energy and cost savings to millions of people through partnerships with the leading companies in the Internet of Things. |  | Company Benefits |  | EnergyHub offers a generous benefits package including 100% paid medical for employees and a 401(k) with employer match. We offer a casual environment, the flexibility to set your own schedule, a fully stocked fridge and pantry, free Citi Bike membership, secure bike rack, gym subsidy, paid parental leave, and an education assistance program. |  | EnergyHub is an Equal Opportunity Employer |  | In connection with your application, we collect information that identifies, reasonably relates to or describes you (""Personal Information""). The categories of Personal Information that we may collect include your name, government-issued identification number(s), email address, mailing address, other contact information, emergency contact information, employment history, educational history, and demographic information. We collect and use those categories of Personal Information about you for human resources and other business management purposes, including identifying and evaluating you as a candidate for potential or future employment or future positions, recordkeeping in relation to recruiting and hiring, conducting analytics, and ensuring compliance with applicable legal requirements and Company policies.",New York NY,Data Engineer
Pearson,/company/Pinnacle-Alliances/jobs/Data-Engineer-Remote-8ce716bb792c1392?fccid=63b50d442b6377a2&vjs=3,"Job detailsSalary$65 - $70 an hourJob TypeFull-timeContractNumber of hires for this role5 to 10Full Job DescriptionDATA ENGINEERRemoteUSC/GC§ Data engineers build data pipelines (this is what I was referring to as ETL) that transform raw, unstructured data into formats data scientists can use for analysis. They are responsible for creating and maintaining the analytics infrastructure that enables almost every other data function.§ Data engineers build and maintain massive data storage and apply engineering skills: programming languages, ETL techniques, knowledge of different data warehouses, and database languages§ 8 Essential Data Engineer Technical Skills· Database systems (SQL and NoSQL). ...· Data warehousing solutions. ...· ETL tools. ...· Machine learning. ...· Data APIs. ...· Python, Java, and Scala programming languages. ...· Understanding the basics of distributed systems. ...· Knowledge of algorithms and data structures.§ Picking up where the data has been dumped into a data lake (A data lake is a system or repository of data stored in its natural/raw format,Must Haves: Expertise in SQL on the database side (could be a person who has been a DBA)They have to be good at these functions:· Use table functions· CTEs· Query plans· Optimization plansSolid scripting skills:  Python or JavaScript Experience with ETL tools – Talend is preferred but others will suffice. For the right person who has the aptitude to learn they can teach, but the person is going to have to know JavaScript or Python since Talend is written in Java.Strong Database experience - Snowflake is not a must seeing as it’s a relatively new DB system but definitely our most preferred option. Here is simple example as to why.· Oracle, Microsoft SQL Server, MySQL, PostgreSQL; in fact most RDBMs use stored procedures coded using a form of SQL like language that translates to each platform. Snowflake also has stored procedures but they are written in JavaScript versus a flavor of SQL.· This difference although subtle means any person with no exposure to Snowflake has to be educated on the intricate way the system handles things. If they are very knowledgeable on database system they can at least adapt easier but it would add to the learning curve. So although not required, it would be strongly preferred.Contract length: 12 monthsJob Types: Full-time, ContractPay: $65.00 - $70.00 per hourSchedule:8 hour shift",Baltimore MD,Data Engineer Remote
Health Catalyst,/company/Dash-Technologies-Inc/jobs/Junior-Data-Engineer-691404ce74a551e3?fccid=899d5c87949021be&vjs=3,"Job detailsSalary$122,000 - $137,255 a yearJob TypeFull-timeQualificationsBachelor's (Preferred)SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Full Job DescriptionDash Technologies is an industry-leading software solutions provider as well as an IT training provider for over a decade now.Position: Strictly W2 OnlyResponsibilities: Create and maintain optimal data pipeline architecture,Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.Work with stakeholders including the Executive, Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.Job Type: Full-timeSalary: $122,000.00 - $137,255.00 per yearSchedule:Monday to FridayEducation:Bachelor's (Preferred)Experience:SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Work Location:One locationBenefit Conditions:Only full-time employees eligibleWork Remotely:No",California MD,Jr. Data Engineer
Microsoft,/rc/clk?jk=66aecaaab1b1ff87&fccid=734cb5a01ee60f80&vjs=3,"We seek to empower learners around the world to acquire the skills enabling them to get a job, advance in their career and achieve more. Here is an opportunity for you to influence our learning programs and experiences designed to build world class capability for customers, partners, employees, and future generations. | Within Worldwide Learning, we reach millions of learners where they are, anytime, anywhere, and deliver experiences that support exploring, learning, practicing, and proving their skills. On our Data Engineering teams we use Microsoft Azure Data Services to produce quality data that assists our organization in making important decisions around educational tools for Microsoft learners and ensure employee training requirements. | We are looking for talented and motivated data engineers that are interested in helping our organization empower learners through producing valuable data that can be used to understand the needs of our learners and help the organization make the right decisions to produce exceptional learning experiences. | Responsibilities | Design, develop, and maintain data pipelines and backend services for real-time decisioning, reporting, data collecting, and related functions. | Produce high-quality, well-tested, and secure code. | Develop and maintain software designed to improve data governance and security. | Develop processes designed to ensure Data Security and Data Quality. | Work collaboratively with teammates in a fast-paced environment. | Work to help build an inclusive working environment. | Qualifications | Basic Qualifications and Skills: | BS/MS in Computer Science or related technical field or equivalent training &amp; experience. | 3+ years of experience working with SQL Server, Business Intelligence, Data Engineering(ETL/ELT), and/or Reporting | 3+ years building and maintaining end-to-end data systems and supporting services. | 3+ years Incorporating data processing and workflow management tools into pipeline design. | Experienc using SQL to understand data and to investigate data issues or problems and provide solutions. | Experience working with cloud data technologies. | Experience with or knowledge of structured, semi-structured, and unstructured data. | A strong understanding of data warehousing and ETL concepts. | A strong understanding of the value of Data and the benefits of a data-driven organizational culture. | Excellent Problem-solving skills and software development habits. | A passion for understanding data and data exploration using querying languages such as SQL. | Excellent written and verbal communication skills. | Good communication and collaboration skills. | Passion for producing high quality Data products. | Strong intellectual curiosity and passion for learning new technologies. | Preferred Qualifications: | Experience utilizing a variety of data stores, including data warehouses, RDBMSes, in-memory caches, and searchable document Databases. | Experience working with large data sets in SQL/Data Bricks/PySparkSQL/Azure Data Factory or similar. | Experience work with No-SQL storage and databases such as Azure Storage/ Data Lakes/ Or Cosmos DB. | Experience implementing data systems in Python/C#/Scala or similar. | Strong design, implementation and testing skills. | Developing for continuous integration and automated deployments. | Experience developing on cloud platforms (i.e., AWS, Azure) in a continuous delivery environment. | 1+ years software development using languages like JavaScript, Java, or C#. | Ability to provide technical leadership to other developers. | Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form. | Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.",United States,Data Engineer II
Pearson,/rc/clk?jk=29b91dde75316d5e&fccid=f42bbfc261ca3e64&vjs=3,"Data Engineer - 1 | Manhattan, NY 10002 | Only US citizens and Green card holder | Will not consider candidates with 15+ years experience. | The Data Engineer role within the Data Strategy group provides an opportunity to own the build-out and implementation of data solutions to power one of the world’s largest and most respected risk management and reinsurance firms. The Data Strategy group has a “start-up style” mandate and internal consulting role(within a $1.3 billion company) to effectively enhance the acquisition, storage, analysis, fidelity, and monetization of massive amounts of client, internal, and third-party data across the organization. | As a member of the Data Strategy group, the Data Engineer will work with fellow data scientists, product managers, business analysts, and stakeholders from other internal groups to design and improve data-centric projects with the dual mandate of (1) increasing the efficiency of the data collection and analysis process across GC and (2) driving the monetization of data via newly designed and existing products for reinsurance clients. The data engineer will be the lead facilitator on innovative initiatives and will have ownership over the design, development, and delivery of projects which will require direct reporting to senior-level management in both business and technical groups. | Responsibilities: | Develop, implement, and deploy custom data pipelines powering machine learning algorithms, insights generation, client benchmarking tools, business intelligence dashboards, reporting and new data products. | Innovate new ways to leverage enormous amounts of various datasets to drive revenues via the development of new products with the Data Strategy team, as well as the enhanced delivery of existing products | Consume data from a variety of sources (relational DBs, APIs, NetApp and other cloud storage, FTPs) &amp; formats (excel, CSV, XML, parquet, unstructured)) | Construct and maintain data pipelines between GC’s databases, and other sources, with the data lake utilizing modern ETL frameworks | Own the role of data steward for a variety of high value datasets and implement innovative quality assurance practices | Establish and implement metadata management standards and capabilities, including lineage mapping | Enforce strong development standards across the team through code reviews, unit testing, and monitoring | Perform basic data analysis within Jupyter Notebooks to validate the fulfillment of requirements for data pipelines | Evangelize data strategy techniques and best practices throughout global strategic advisory | Keep up-to-date on the latest trends and innovation in data technology and how these trends apply to business and data strategy | Required Knowledge, Skills, and Abilities: (Submission Summary: | 1. 3-5 years of relevant experience as a data engineer or in a similar role | Bachelor’s or master's degree in data science, computer science or related quantitative field such as applied mathematics, statistics, engineering, or operations research? | 2. Extensive experience with Spark, Python, and SQL? | 3. Extensive experience integrating data from semi-structured? | 4. Experience deploying/maintaining cloud resources (AWS, Azure, or GCP)? | 5. Knowledge of various industry-leading SQL and NoSQL database systems | Experience working in an Agile environment to facilitate the quick and effective fulfillment of group goals? | 6. Strong understanding of entity resolution, streaming technologies, and ELT/ETL frameworks? | 7. Ability to articulate the advantages of various cloud and on-premises deployment options? | 8. Experience with Master Data Management? | 9. Experience with web scraping and crowd sourcing technologies? | 10. Familiarity with modern data productivity frameworks and their alternatives such as Databricks, DataRobot, and Alteryx? | 11. Experience with the MS Azure cloud environment, including ARM template deployments? | 12. Strong knowledge of CI/CD principles and practical experience with a CI/CD technology (Azure Devops, GitLab, Travis, Jenkins)? | 13. Please share the link to the candidate's LinkedIn profile. | Powered by JazzHR | zdUYel4VPY",Manhattan NY,Data Engineer - 1
Munich Re America,/rc/clk?jk=1a320eedc32ec26b&fccid=915b1c0ee87e5e8a&vjs=3,"Description | We are the world’s learning company with more than 24,000 employees operating in 70 countries. We combine world-class educational content and assessment, powered by services and technology, to enable more effective teaching and personalized learning at scale. We believe that wherever learning flourishes so do people. | The Entry level Data Engineer is part of a team responsible for supporting Enterprise Data Warehouse (EDW), custom reporting, and the organization's on-demand data needs. Under the direction of the Manager of EDW Systems, you will participate in the planning, designing, developing, installing, testing, and supporting the complete data management solutions to address ongoing business reporting needs and new opportunities. You will have a plenty of opportunities to learn advanced analytics and machine learning with your experienced peers. | Responsibilities | Responsible for maintaining and enhancing our existing data and analytics platform. |  | Building required data pipelines for optimal extraction, transformation, and loading of data from various data sources using Cloud and SQL technologies and supporting them. |  | Triaging the support incidents, investigating, identifying the root cause of the problem, and fixing. |  | Working with stakeholders, including cross-teams and business users, and assisting them with data-related technical issues. |  | Identifying, and implementing internal process improvements, including re-designing the applications for greater scalability, optimizing data delivery, and automating manual processes. |  | Communicate effectively across multiple departments and with stakeholders to review business requirements and propose solutions. |  | Performing other duties as assigned. |  | Qualifications | This is an entry level position and we will consider candidates with: | B.S. degree, or equivalent, in Computer Science, Information Systems or related field |  | 0-2 years of project work experience |  | Basic knowledge of Python and/or Java programming skills |  | The ability to learn new technologies quickly |  | Strong analytical and problem-solving skills |  | Strong oral and written communication skills (English) |  | It will be helpful if you have some knowledge or exposure in some of the following areas: | Data warehousing concepts, BI environment |  | Dimensional modeling, database structures, and query optimization |  | GCP Big Data products, including BigQuery, Pub/Sub, Cloud Composer, Dataflow, Cloud Storage, and related cloud technologies |  | Data pipelines (ETL, ELT) and data wrangling procedures using Python and SQL |  | Batch and stream processing (including GCP Dataflow/Kafka Streams) |  | Required for the interview: You should be able to demonstrate work experience by providing report examples and source code. | Due to this position's nature, the applicant will need the ability to work from home or during off-hours as necessary. | NOTE: This position is REMOTE due to the Pandemic. Once it is considered safe to re-open our Durham office, it will be required to work on site. | #LI-POST | Primary Location : US-NC-Durham | Job : Technology | Organization : Technology &amp; Operations | Employee Status : Regular Employee | Job Type : Standard | Job Level : Individual Contributor | Shift : Day Job | Job Posting : Feb 24, 2021 | Job Unposting : Ongoing | Schedule: : Full-time Regular | Req ID: 2012413",Durham NC,Data Engineer - Data Warehouse - Entry Level
Facebook,/company/Nota-Bene-Global-Services-Inc./jobs/Big-Data-Engineer-ea2f932730a0b99c?fccid=62c483a06a8150ec&vjs=3,"Job detailsSalary$65 - $75 an hourJob TypePart-timeNumber of hires for this role2 to 4QualificationsExperience:Google Cloud Platform , 3 years (Required)Full Job DescriptionHi All,Hope you are doing great…!!!Please go through below job description and let me know your interest ASAP.Role: Data Engineer with GCP, Apache Beam experienceLocation: Anywhere in US / Canada || RemoteJob Type: ContractDescription: Must have experience building data pipelines in beam.Must have strong experience with Apache Beam.Good experience with Dataflow &amp; Big Query.At least 3+ years of experience working in &amp; out with GCP Data services.Job Type: Part-timeSalary: $65.00 - $75.00 per hourSchedule:8 hour shiftExperience:Google Cloud Platform : 3 years (Required)Work Remotely:Yes",New York NY,Big Data Engineer
Madewell,/rc/clk?jk=0a393ec7e9c56774&fccid=bcedbc68e7bf6c93&vjs=3,"Data Engineer | (Reports to Head of Data and Analytics) | You’ve got to… | Get us – a passion for the brand that shows up in everything you do, everyday. | Be a strategic thinker – spend time and energy on what drives the greatest results. | Look under rocks, be curious, ask questions and use your smarts to think boldly and do the right thing. | Be a team player – cultivate productive relationships with cross-functional business partners. | Communicate consistently, with purpose and an understanding of your audience. | Be a multi-task master – make quick decisions under tight timelines. | Be nimble and comfortable with change. | Work independently and take the lead, even when all of the pieces are not in place. | Articulate your point of view and have the courage and conviction to stand up for your beliefs. | Have a great fashion esthetic and be all over what’s happening in the industry. | Always be on, up for anything and ready to have fun along the way. | We want you to… | Translate business requirements into effective and resilient data pipelines and data models | Implement best practices to ensure data quality throughout our data pipelines and integrations | Recommend improvements and modifications to existing data integrations and ETL/ELT | Handle various data formats including structured (databases) and semi-structured (XML, JSON, csv) | Design and develop scalable, high performing data models for a wide range of business groups, including Store Operations, Financial Planning, Allocation, Merchandising, Web and Marketing | Forge a strong partnership with our business stakeholders to ensure success of data initiatives | Evangelize a data-driven culture and processes using modern tools and methodologies | Administer Snowflake database and data orchestration tools | Strive to implement data governance throughout the enterprise | Identify operational data issues and recommend strategies to resolve data problems. | Resolve critical data issues as they arise | Have experience designing and developing enterprise data warehouses | Be able to work with minimum supervision and proactively prioritize tasks | Oh, and by the way, you… | Have a bachelor’s degree. Preferred bachelor’s degree in a Technical Field; | 4+ years of experience in the data warehouse space in hands-on roles | 4+ years of experience in writing complex SQL and ETL/ELT processes, preferably in a retail, brand manufacturing or consumer-facing company | 4+ years of experience with schema design and dimensional data modeling | 1+ years of experience with object-oriented programming languages, like Java or Python | Preferred experience in a retail, CPG or e-commerce company | We are committed to affirmatively providing equal opportunity to all associates and qualified applicants without regard to race, color, ancestry, national origin, religion, sex, marital status, age, sexual orientation, gender identity or expression, legally protected physical or mental disability or any other basis protected under applicable law.",Long Island City NY 11101,Madewell Data Engineer
Essence,/rc/clk?jk=5a03807b849b4d9b&fccid=26865d82c27c8d42&vjs=3,"Job detailsJob TypeContractFull Job DescriptionAbout Essence |  | Essence, part of GroupM, is a global data and measurement-driven media agency whose mission is to make advertising more valuable to the world. Clients include Google, Flipkart, NBCUniversal, L'Oreal and the Financial Times. The agency is more than 1,800 people strong, manages $4B in annualized media spend and deploys campaigns in 106 markets via 20 offices in APAC, EMEA and North America. |  | Visit essenceglobal.com for more information and follow us on Twitter at @essenceglobal |  | About the role |  | This role forms part of a globally focused business intelligence team, whose objective is to ensure that one of our most important global accounts have access to the right data in order to inform their marketing decisions. |  | The team's primary responsibility is to create and maintain a global Google Cloud based reporting solution, which automates the collection and transformation of disparate marketing data into a single source of truth. Not only does that mean running and maintaining the solution, but also continually improving it to incorporate new data sources, and to derive new methods to support ever-evolving business demands. |  | As a Data Engineer your primary responsibility will be to support a Business Intelligence Director to create and maintain underlying data infrastructure that provides the client team with the data they need to provide timely, accurate and meaningful data that fuels marketing analytics. In doing so you will apply your understanding of cloud technology and key data engineering skills and knowledge to help you build a robust solution that ensures high client satisfaction |  | Some of the things we'd like you to do: |  | Develop the technical solutions, in line with specifications, that collect, store and transform disparate data sources | Develop and maintain automated jobs that ensure required data is made available in an efficient and scalable way as possible | Assist the development and maintenance of data quality checks and procedures, helping account teams overcome data issues before they impact the quality of the reporting solution | Support the translation of user requirements and business needs into technical specifications | Utilize your proficiency of the Google Cloud Platform and associated technologies, ensuring your work incorporates industry best practice | Monitor automated jobs, troubleshooting data issues as-and-when they arise | Support other members of the team responsible for ""last mile"" transformation and distribution of data | Attend internal stakeholder meetings, presenting your solutions and providing updates on your work. | Support the development strong working relationships with third-party data providers that we rely on for access to necessary data |  | A bit about yourself: |  | Required |  | 3-5 years experience building underlying data pipelines and ETL, particularly useful if done using Google Cloud Platform, Airflow, DBT etc. | Proven track record of querying data and developing effective reporting solutions using programming and/or statistical languages (e.g. SQL, Python, R). | Analytically minded, enabling you to understand and overcome technically complex challenges | Strong organizational skills and attention to detail, including the ability to manage multiple tasks in a fairly autonomous way | Strong spoken and written communication skills, ensuring your thoughts and needs are heard and understood | Delivers best results when working in a team environment, and an ability to partner effectively with people of varying degrees of technical capability |  | Desirable |  | Experience with marketing platforms and the data they generate, in particular Google Marketing Platform, Nielsen and Rentrak. Knowledge of their API's a plus. | An understanding of how data is tracked and exchanged in the process of digital advertising (e.g. role of ad servers and other third-party tech vendors) | Work experience within a marketing organization, preferably at a media agency or related company (e.g. publisher, ad tech, client marketing org) |  | What you can expect from Essence |  | Essence's mission is to make advertising more valuable to the world. We do this by employing the world's very best talent to solve some of the toughest challenges of today's digital marketing landscape. It's important that we hire people whose values reflect those of our own: genuine, results-focused, daring and insightful. As an Essence employee, we promise you a workplace that invests in your career, cares for you and is fun and engaging. We believe these factors create a workplace where you can be yourself and do amazing work.",New York NY,Data Engineer (Contract)
Intone Networks,/company/Applicantz/jobs/Data-Engineer-810b6e0d607ed500?fccid=d8f047e3c7d32348&vjs=3,"Job detailsSalary$62 - $70 an hourJob TypeFull-timeContractNumber of hires for this role1Full Job DescriptionQualifications Skills and ExperienceAt least 5+ years of experience as a Data Engineer in analyzing product, customer, marketing related data and building data pipelines.Strong Communication skillsStrong background in quantitative data analysis as well as prediction and regression techniques.Experience performing A/B testing on very large, multi-dimension datasetsProficient in SQL and Visualization (Tablueau/Qlikview)Strong Proficiency in Microsoft Excel and standard analytic programsStrong business acumen and ability to manage conversations at multiple levels of the organizationAbility to present data insights concisely to various stakeholders, especially business teamsEntrepreneurial spirit and a passion for data, customer focused mindset, team playerResponsibilities:Demonstrate up-to-date expertise in handling data extractions from multiple data sources and capable of analyzing large volumes of data to address key business questions to enable growthManage stakeholder relationships and expectations by developing a communication process to keep others up-to-date on project results and timeline expectationsLead or participate in multiple projects by completing and updating project documentation; managing project scope; adjusting schedules when necessary; determining daily priorities; ensuring efficient and on-time delivery of project tasks and milestones; following proper escalation paths; and managing customer and supplier relationshipsProvide decision makers and influencers with logical data-driven insights translating into hypotheses for lean testing, rapid prototyping and effective implementationConstruction of critical business application tools to be used by managers and consultants to be tuned to the pulse of the businessIdentify new technical requirements, and test/troubleshoot data capture to ensure reporting and analytics systems conform to data quality standardsContract length: 12 monthsJob Types: Full-time, ContractSalary: $62.00 - $70.00 per hourBenefits:401(k)Dental insuranceHealth insurancePaid time offVision insuranceSchedule:8 hour shiftWork Remotely:Temporarily due to COVID-19COVID-19 Precaution(s):Remote interview process",Menlo Park CA,Data Engineer
FanDuel,/company/Software-technologies-inc/jobs/Data-Engineer-46df3807ccbf5664?fccid=ca0f101be19c2c49&vjs=3,"Job detailsJob TypeFull-timeContractNumber of hires for this role2 to 4QualificationsETL: 4 years (Preferred)Python: 4 years (Preferred)SQL: 4 years (Preferred)MongoDB: 4 years (Preferred)Elasticsearch: 3 years (Preferred)Full Job DescriptionRole : Data EngineerLoccation : Reston, VADuration : Long TermVisa : Need only USC, GC &amp; EAD'SRequired Skills: Python, AWS Glue, SQL, MongoDB, ElasticSearch, and NiFiWe are looking for an Experienced Data Engineer who will be responsible for standardizing our data ingestion pipelines and improving performance of data storage and retrieval. Our projects ingest structured and unstructured data from multiple sources, that eventually ends up in a unified Data Lake. This data is available for reporting and analytics by various other teams. This candidate should be comfortable working with SQL as well as NoSQL databases. Cloud experience is a bonus.Your primary focus will be the continuous development/improvement of our data ingestion pipeline, and data parsing. Major technologies involved include Python, AWS Glue, SQL, MongoDB, ElasticSearch, and NiFi.Skills and ResponsibilitiesDesign and develop stream and batch processing data pipelines for ingesting healthcare data (CSVs, JSON, Common Data Formats like OMOP, ACT etc.,) into MongoDB and Elastic Search.Ability to write intermediate to advanced SQL/Python for data ingestion and processing.Support existing and develop new data flows as needed by developing processes that verify, standardize, and scale data input, transformation and storage.Collaborate with product managers and other engineers to implement and document complex and evolving requirementsResearch and implement cutting edge solutions to solve challenges related to ETL, data processing, and analytics.Support efforts by the Data Science and Data Analytics teams.Ability to debug complex data issues without frequent guidance from senior team members.Occasional Linux server management including the review or management of log files, crontab, security configuration, etc.Developing techniques to work with both tabular and hierarchical data.Strong attention to detail, good work ethic, ability to work on multiple projects simultaneously, and good communication skillsRequired SkillsMust have strong programming &amp; ETL skills in Python &amp; SQL.Experience working with Healthcare Common Data formats.3 to 6 years of experience working in Data Engineering or related roles.A degree in Computer Science or an equivalent major.Experience working with various database systems: MongoDB/SQL Server/PostgreSQL etc.Experience with agile methodologies and short release cyclesEnjoys collaborating with other engineers on architecture and sharing designs with the teamDesired ExperienceExperience working with NiFi.Experience with cloud technologies (AWS)Experience working with PHI/Healthcare data.AI / ML experience.Job Types: Full-time, ContractSchedule:Monday to FridayExperience:Data Engineer: 8 years (Preferred)ETL: 4 years (Preferred)Python: 4 years (Preferred)SQL: 4 years (Preferred)MongoDB: 4 years (Preferred)Elasticsearch: 3 years (Preferred)Work Location:One locationWork Remotely:Temporarily due to COVID-19",Remote,Data Engineer
Capgemini,/rc/clk?jk=83ef13daad97cb74&fccid=545bc5c78669ff5f&vjs=3,"Responsibilities |  | Ingestion of data from multiple sources using Looker | Migrating legacy visualizations to Looker Looks and Dashboards | Implementing Looker Best Practices | Designing Looker based solutions for both external and internal stakeholders |  | Qualifications and Skills |  | 2+ years of relevant professional experience | Experience with multiple visualization tools | Experience with BigQuery required | Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle, Snowflake) | Good understanding of SQL Engine and able to conduct query performance tuning | Experience with at least one of the following Visualization Tools (PowerBI, Microstrategy, or Tableau) and Looker | Implementation of a Looker Environment | Business Analysis Skills |  | About Myers-Holum |  | Myers-Holum is a technology and management consulting firm founded in 1981 and based in New York, New York. | HQ in Manhattan | 80 consultants working across the US focused on Information Mgmt, Enterprise Data Warehousing, Google BigQuery and NetSuite ERP implementations | Website: www.myersholum.com |  | Benefits |  | MHI offers competitive base salary + incentive pay as well as training and certification in a variety of products and professional skill sets | In US: MHI offers a company health insurance policy that covers 100% of premiums for the individual | Remote working opportunity when not traveling for client requirements with full access to the team through technology",New York NY 10016,Looker Data Engineer
Facebook,/company/Match-Inc/jobs/Data-Engineer-5ea3b5a196c58b8a?fccid=9dbaeb5de04679f7&vjs=3,"Job detailsSalary$60 - $68 an hourJob TypeFull-timeContractNumber of hires for this role2 to 4Full Job DescriptionJob Title : Data EngineerLocation : San Francisco ,CADuration : 10 MonthsData Engineer will leverage Airbnb data tools like SQL, Hive, Spark, and Python to facilitate data migrations in our data warehouse. They must be able to efficiently deal with ambiguity and communicate well with data stakeholders to facilitate these migrations. They will work across a variety of business and data domains.Skills:SQL expertise is requiredData validation, comparison, and lightweight reporting is requiredHiveQL, Spark, and Python skills are preferredDimensional Data Modeling knowledge is preferredContract length: 12 monthsJob Types: Full-time, ContractSalary: $60.00 - $68.00 per hourSchedule:Monday to Friday",San Francisco CA,Data Engineer
ACME SMOKED FISH CORP,/rc/clk?jk=eeff48484a688d0f&fccid=86e9be6ce380173e&vjs=3,"Role | Tesla's mission is to accelerate the world's transition to sustainable energy. We are committed to hiring the world's best and brightest people to help make this future a reality. Every Tesla is designed to be the safest, quickest car in its class—with industry-leading safety, range and performance. Quality Engineering team plays a key role in ensuring safety of our customers by providing world class Quality of our products. Quality Systems and Analytics team is looking for a key player on the team who can help drive Quality data analytics and help cross-functional engineering organizations to provide opportunities in product quality improvements. Candidate should have experience working with large data sets, finding best ways to engineer the data to help create critical KPI metrics, building innovative visualizations and dashboards all the while keeping in mind what improvements can be driven in underlying data systems. | Responsibilities: | Analyze manufacturing, equipment and vehicle data and extract useful statistics and insights about failures in order to drive meaningful improvements to production quality and customer experience | Work effectively with engineers and conducting end-to-end analyses, from data requirement gathering, to data processing and modeling | Interpret data, analyze results using statistical techniques and provide ongoing reports | Monitoring key product metrics, understanding root causes of changes in metrics | Identify, analyze, and interpret trends or patterns in complex data sets and depict the story via dashboards and reports | Acquire data from primary or secondary data sources and maintain databases/data systems to empower operational and exploratory analysis | Maintaining existing data visualizations, data pipelines and dashboard enhancement requests | Automating analyses and authoring pipelines using SQL, Python, Airflow, Kubernetes based ETL framework | Drive underlying data systems improvement by working with key cross-functional stakeholders | Perform data quality validations to ensure data creation is as per the business needs and expectations | Work with management to prioritize business and information needs | Requirements: | BS/MS in Management Information Systems, Computer Science, Math, Physics, Engineering, Statistics or other technical field | 3+ years of work experience in data analytics or engineering related field | Strong knowledge of SQL and experience with multiple data architecture paradigms (MySQL, MicrosoftSQL, Vertica, Oracle, kafka, Spark) | Proficient at ad-hoc analysis using SQL queries, python data analysis packages (e.g.Pandas, Numpy) , report writing and presenting findings | Strong knowledge of data visualization techniques and tools using Tableau, Power BI, Superset, Matplotlib, Plotly etc. | Strong understanding of various statistical techniques to effectively summarize data findings | Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy | Strong knowledge of data warehousing concepts, data mining tools and techniques | Knowledge of manufacturing and service operations preferred | Able to work under pressure while collaborating and managing competing demands with tight deadlines | Excellent communication skill and ability to work with different business stakeholders to understand, identify and translate business challenges into data projects | A passion and curiosity for data and data-driven decision making",Fremont CA,Data Engineer
CBOE,/rc/clk?jk=e1986758fad3e9d5&fccid=86e9be6ce380173e&vjs=3,"The Role | We are looking for a Data Engineer to be part of our Applications Engineering team. This person will design, develop, maintain and support our Enterprise Data Warehouse &amp; BI platform within Tesla using various data &amp; BI tools, this position offers unique opportunity to make significant impact to the entire organization in developing data tools and driving data driven culture. | Responsibilities: | Work in a time constrained environment to analyze, design, develop and deliver Enterprise Data Warehouse solutions for Tesla’s Finance and Accounting teams | Work on ETL SSIS, Business Intelligence &amp; Reporting tools like SSRS, SSAS (Multidimensional and Tabular) and Tableau | Work with systems that handle sensitive data with strict SOX controls and change management processes | Develop collaborative relationships with key business sponsors and IT resources for the efficient resolution of work requests. | Provide timely and accurate estimates for newly proposed functionality enhancements | critical situation | Communicate technical and business topics, as appropriate, in a 360-degree fashion, when required; communicate using written, verbal and/or presentation materials as necessary. | Develop, enforce, and recommend enhancements to Applications in the area of standards, methodologies, compliance, and quality assurance practices; participate in design and code walkthroughs. | Utilize technical and domain knowledge to develop and implement effective solutions; provide hands on mentoring to team members through all phases of the Systems Development Life Cycle (SDLC) using Agile practices. | Qualifications: | Minimum Qualifications: | 5+ years of experience in SQL database engine, tunning, modeling and querying is necessary | 5+ years of experience in SSIS in large/medium scale implementations | Must have strong experience in Data Warehouse ETL design and development, methodologies, tools, processes, and best practices | Kimball multidimensional modeling methodologies | Strong experience in stellar dashboards and reports creation for C-level executives | Preferred Qualifications: | Experience in Finance functional areas like planning and budgeting, accounting, business intelligence, procure-to-pay, order-to-cash | Understanding of SOX controls and audits procedures | DAX, MDX | Python",Fremont CA,SQL Data Engineer
Tesla,/rc/clk?jk=34ba4ac8930bd903&fccid=1639254ea84748b5&vjs=3,"Every month, billions of people leverage Facebook products to connect with friends and loved ones from across the world. On the Data Engineering Team, our mission is to support these products both internally and externally by delivering the best data foundation that drives impact through informed decision making. As a highly collaborative organization, our data engineers work cross-functionally with software engineering, data science, and product management to optimize growth, strategy, and experience for over three billion users, as well as our internal employee community. In this role, you will see a direct correlation between your work, company growth, and user satisfaction. Beyond this, you will work with some of the brightest minds in the industry, and you'll have a unique opportunity to solve some of the most interesting data challenges around efficiency and integrity, at a scale few companies can match. As we continue to expand and create, we have a lot of exciting work ahead of us! | Manage data warehouse plans for a product or a group of products. | Collaborate with engineers, product managers and data scientists to understand data needs. | Build data expertise and own data quality for allocated areas of ownership. | Design, build and launch new data models in production. | Design, build and launch new data extraction, transformation and loading processes in production. | Support existing processes running in production. | Define and manage SLA for all data sets in allocated areas of ownership. | Work with data infrastructure to triage infra issues and drive to resolution. | Improve frameworks, and systems to facilitate easier development of data artifacts. | Influence product and cross-functional teams to identify data opportunities to drive impact. | Mentor team members by giving/receiving actionable feedback. | 4+ years experience in the data warehouse space. | 4+ years experience in custom ETL design, implementation and maintenance. | 4+ years experience with object-oriented programming languages. | 4+ years experience with schema design and dimensional data modeling. | 4+ years experience in writing SQL statements. | Experience analyzing data to identify gaps and inconsistencies. | Experience managing and communicating data warehouse plans to internal clients. | BS/BA in Technical Field, Computer Science or Mathematics. | Experience working with either a MapReduce or an MPP system. | Knowledge and practical application of Python. | Experience working autonomously in global teams. | Experience influencing product decisions with data. | Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started. | Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",New York NY 10003,Data Engineer Analytics Team
Tesla,/rc/clk?jk=200aa7e08feff8a0&fccid=1639254ea84748b5&vjs=3,"Every month, billions of people leverage Facebook products to connect with friends and loved ones from across the world. On the Data Engineering Team, our mission is to support these products both internally and externally by delivering the best data foundation that drives impact through informed decision making. As a highly collaborative organization, our data engineers work cross-functionally with software engineering, data science, and product management to optimize growth, strategy, and experience for over three billion users, as well as our internal employee community. In this role, you will see a direct correlation between your work, company growth, and user satisfaction. Beyond this, you will work with some of the brightest minds in the industry, and you'll have a unique opportunity to solve some of the most interesting data challenges around efficiency and integrity, at a scale few companies can match. As we continue to expand and create, we have a lot of exciting work ahead of us! | Manage and execute data warehouse plans for a product or a group of products to solve well-scoped problems. | Identify the data needed for a business problem and implement logging required to ensure availability of data, while working with data infrastructure to triage issues and resolve. | Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way. | Build data expertise and leverage data controls to ensure privacy, security, compliance, data quality, and operations for allocated areas of ownership. | Design, build and launch new data models and visualizations in production, leveraging common development toolkits. | Independently design, build and launch new data extraction, transformation and loading processes in production, mentoring others around efficient queries. | Support existing processes running in production and implement optimized solutions with limited guidance. | Define and manage SLA for data sets in allocated areas of ownership. | Experience in the data warehouse space. | Experience in custom ETL design, implementation and maintenance. | Experience with object-oriented programming languages. | Experience with schema design and dimensional data modeling. | Experience in writing SQL statements. | Experience analyzing data to identify deliverables, gaps and inconsistencies. | Experience managing and communicating data warehouse plans to internal clients. | BS/BA in Technical Field, Computer Science or Mathematics. | Experience working with either a MapReduce or an MPP system. | Knowledge and practical application of Python. | Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started. | Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",New York NY 10003,Data Engineer Product Analytics
USDM Life Sciences,/rc/clk?jk=170b24860fff2992&fccid=2e5e13c3edec255d&vjs=3,"About USDM |  |  | USDM Life Sciences is a premier consulting company with 20+ years of experience assisting heavily regulated biotech, medical device, and pharmaceutical companies with their GxP technologies to accelerate growth. Our deep domain knowledge and technology expertise in life sciences business processes are what sets us apart. From strategy to implementation and adoption, we have delivered thousands of GxP projects globally. |  |  | As part of the USDM team, you have the opportunity to work with cutting edge technologies through our many partnerships with companies like Microsoft, Google, Oracle, DocuSign, Box, and many more. From molecule to market, you will help connect technology, people, and data in new ways to generate real-time insights to improve business outcomes for USDM’s clients. Are you ready to make an impact and drive real digital transformation in life sciences? |  |  | Founded in Santa Barbara in 1999, USDM has grown to a progressive, global company with 300+ remote employees and offices throughout the US, Canada, and Germany. |  |  | Data Engineer |  Duties | Develop image processing algorithms using Python | Support development of Machine Learning/Deep Learning (ML/DL) algorithms and pipelines using the image data | Work with scientists, contractors, and IT staff to curate, organize, and document clinical images from Genentech and Roche trials | Support data integration efforts across diverse data sources | Continually search for opportunities to automate workflows and streamline processes | Direct communication with outside vendors and in-house staff to reconcile data transfer problems | Reporting on imaging assets to business stakeholders | Coordination of transfers of clinical imaging data from outside sources |  Skills | Experience with medical/clinical image data analysis techniques | Proficient in scripting languages such as Python, Matlab, etc. | Experience with medical image formats (e.g., DICOM, nifti) is required | Experience with ML/DL a strong plus | Experience with clinical imaging in multiple therapeutic areas such as Oncology, Ophthalmology and Neurology a plus | Understanding of clinical trials a plus | Excellent communication and presentation skills |  Education Bachelor’s degree in Computer Science, Biomedical Engineering, or a similar discipline; Master of Science preferred |  | Disclaimer: This job description is intended to describe the general nature and the level of the work being performed by the people assigned to this position. It is not intended to include every job duty and responsibility specific to the position. USDM Life Sciences reserves the right to amend and change responsibilities to meet business and organizational needs as necessary. |  | USDM Life Sciences is an equal opportunity employer. All qualified applicants are encouraged to apply and will receive consideration for employment without regard to their protected veteran, disabled, or any other protected status.",South San Francisco CA,Data Engineer
CCS Global Tech,/rc/clk?jk=85479ee7e09dc6d8&fccid=55ca72cb2faa6d78&vjs=3,"Rockerbox is building a marketing analytics platform that processes Terabytes of data per day for some of today’s most well-known e-commerce and digital brands. Come join our growing engineering team that is solving complex challenges around data and analytics! |  | We are looking for an engineer who is comfortable managing complex data pipelines and architecting the services behind them. You would be responsible for developing and packaging our applications, as well as maintaining and optimizing our infrastructure. | What we’re looking for | Expertise in big data technologies, e.g. Elasticsearch, SMACK stack (Spark, Mesos, Akka, Cassandra/Scylla, Kafka)Experience with functional programming in Scala, Python, GoDeep knowledge of containerization and service orchestration technologies (Docker, Mesos, DC/OS, Kubernetes) and web servers technologies (OpenResty, Nginx, Lua)Familiarity with monitoring tools like Grafana, Graphite, Prometheus | You’re a great fit if you... | Know your way around tech stacks for big data ingestion, storage, processingUnderstand the lifecycle of developing and shipping applications and databases from staging to productionHave a track record of experimenting with bleeding-edge technology and frameworksCan comfortably transfer ad-hoc analysis to a production environmentGet joy out of setting stuff up and making it run smoothly | Benefits | Work in a small, dynamic teamHealthcare Insurance: health, dental, visionPaid time off: choose your own schedule!Competitive salariesOpportunities to give back to the open-source developer community |  |  | About Rockerbox |  | Rockerbox is the single source of truth for e-commerce brands. Rockerbox works with leading DTC brands such as Rothy's, Figs, Burton and Blue Apron to help them better understand and optimize the marketing levers that grow their companies. Rockerbox enables this through our SaaS platform that centralizes all of our customer's marketing spend and campaigns (paid, organic, digital and offline). Rockerbox has been honored with many prestigious awards. We were named the New York Times &amp; 212 NYC Adtech Startup of the Year, winner of the ad:tech Startp Spotlight and ranked on the Inc. Magazine 5000 fastest growing companies in America for 2 years in a row.",New York NY 10003,Data Pipeline Engineer
LT Technology Services,/rc/clk?jk=e23ae18f0e7b2f79&fccid=3ed0572c448b2368&vjs=3,"3+ years experience working on data engineering problems, especially user-facing data products You want to tackle hard technical challenges—while fighting for the users Very strong SQL skills Solid knowledge of at least one modern web framework (Ruby on Rails preferred) Knowledge of Elasticsearch is a plus, as is familiarity with Spark and Kafka Work alongside Cased founders and our engineering team on the design, scalability, and implementation of data-focused products and our data infrastructure Some examples: you'll help us build anomaly detection, apply machine learning algorithms to analyzing security events, and help optimize our search infrastructure Learn from customer feedback and weigh in on product decisions Help to build a diverse and inclusive company culture — as an early employee you'll have a significant impact on the type of company Cased becomes.",San Francisco CA,Data Engineer(Remote)
Flatiron Health,/rc/clk?jk=cd25709e17098024&fccid=966804bce9d82eb0&vjs=3,"We're looking for a Data Reporting Engineer to join the Data Products initiative team at Flatiron. Here's what you need to know about the role, our team and why Flatiron Health is the right next step in your career. |  | What you'll do |  | In this role, you will be a data expert for OncoEMR, Flatiron's electronic health platform. You will work directly with customers to enable data-driven decision making at hundreds of community cancer clinics across the country, leading to better care for their patients. You will help design and build a next-generation analytics platform to simplify analysis and unlock impactful insights in the future. In addition, you'll also: |  | Write and edit custom SQL queries to generate customer-facing dashboards and reports for community oncology clinics designed to meet their clinical and operational needs | Analyze reporting requests to develop solutions addressing common needs across oncology practices | Build and maintain data pipelines that power parts of our analytics product | Collaborate with customers and synthesize feedback to design and prototype new reporting products | Work with platform software engineers to improve the data infrastructure that powers clinical analytics | Maintain cross-functional relationships with customer-facing teams and continually enhance team efficiency |  | Who you are |  | You are an analytical problem solver who is fluent in SQL and has experience building data reports and visualizations. You are excited to learn about the cancer patient journey and how community oncology practices operate. You communicate effectively and empathetically with clients to understand the problem they are solving, gather requirements, and design and build an analytic solution. |  | You have experience with: | Database performance and interpretation of query execution plans | Database scripting and Extract, Transform and Load processes | You are organized with strong prioritization and communication skills | You love working with engineering teams to develop analytics content that complements new application features and workflows | You thrive in a cross-functional environment |  | Extra Credit: |  | You have healthcare industry knowledge/context (especially oncology-specific knowledge) | You have experience with T-SQL and building reports in SQL Server Reporting Services | You have experience visualizing data in Looker dashboards | You have experience working with Visual Studio, C#, .Net, python, or spark |  | Why You Should Join Our Team |  | A career at Flatiron is a chance to work with everyone involved in the future of cancer care and research—all under one roof. Researchers, data scientists, designers, clinicians, technologists and many more all work together to improve cancer care and accelerate research. |  | At Flatiron, we strive to build and maintain an environment where employees from all backgrounds are valued, respected and have the opportunity to succeed. You'll also find a culture of continuous learning, broad and inclusive employee support offerings, and a commitment to supporting our team members in all aspects of their lives—at home, at work and everywhere in between. We offer: |  | Flatiron University training curriculum which includes presentation skills, meeting mastery, coding languages and more | Career coaching opportunities | Hackathons for all employees (not just our engineers!) | Professional development benefit for attending conferences, industry events and external courses | Work/life autonomy via flexible work hours and flexible paid time off | Employee Resource Groups (ERGs) that encourage our employees to share their unique experiences and perspectives | Generous parental leave (16 weeks for either parent) | Back-up child care | Flatiron-sponsored fitness classes |  |  | Flatiron Health is proud to be an Equal Employment Opportunity employer. |  | We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.",New York NY 10007,Data Reporting Engineer Data Products
Intone Networks,/rc/clk?jk=af90df186bcbe5a7&fccid=3ed0572c448b2368&vjs=3,"Job detailsJob TypeContractFull Job DescriptionJob Description: We are looking for a Senior Associate Data Engineer to be part of our team of top-notch technologists. You will lead and deliver technical solutions for large-scale digital transformation projects. Working with the latest data technologies in the industry, you will be instrumental in helping our clients evolve for a more digital future. Your Impact: · Combine your technical expertise and problem-solving passion to work closely with clients, turning complex ideas into end-to-end solutions that transform our clients’ business · Translate client requirements to system design and develop a solution that delivers business value · Lead, design, develop and deliver large-scale data systems, data processing and data transformation projects · Automate data platform operations and manage the post-production system and processes · Conduct technical feasibility assessments and provide estimates for design and development of the solution Skills &amp; Experience: · Demonstrable experience in data platforms involving implementation of end-to-end data pipelines · Ability to Apply Snowflake best practices to Snowflake Data Warehouse. Strong development background on snowflake stored procedures using JavaScript, SQL, data modeling, Snowflake DB setup, configuration and deployment · Hands-on experience with Azure cloud data services · Implementation experience with column-oriented database technologies, NoSQL database technologies (i.e. Cosmos DB) and traditional database systems (i.e. SQL Server, MySQL) · Experience in implementing data pipelines for both streaming and batch integrations using tools/frameworks like Azure Data Factory, Spark, Spark Streaming and python scripting etc. · Ability to handle module or track level responsibilities and contributing to tasks “hands-on” experience in data modeling, warehouse design and fact/dimension implementations · Experience working with code repositories and continuous integration",Remote,Data Engineer
Blink Health,/rc/clk?jk=05f8655e1deb3de8&fccid=eb9a38460cbcb6a7&vjs=3,"Main areas of focus: Data modeling, data architecture, ETL / pipelines, data transformation, data analytics, data science, data governance, BI, business intelligence, Looker | Location: This opportunity is remote and open to any applicant eligible to work in the U.S. or with an existing U.S. work permit. |  | JOB HIGHLIGHT | You’ll implement data models, architectures and ETL pipelines enabling the organization to drive business solutions and improvements and derive insights through data. |  | JOB SUMMARY | As a Data Engineer at charity: water, you’ll play a crucial role in democratizing data, enhancing internal transparency, and improving the reliability of our product and revenue data. You will report to, and work with the Data Product Manager to implement new models and improve existing data architecture, build ETL pipelines, advocate for data governance and accessibility with a goal of driving product improvements and deriving business solutions and insights. You will lay the groundwork to develop analytics and data science as a practice. You will write tests and documentation and develop systems to support the maintenance of data models. You will work cross-departmentally to identify reporting needs and help create reports and dashboards that tell data stories of our revenue and product performance. |  | YOU’LL BE RESPONSIBLE FOR… | Developing and maintaining data architecture to support revenue data and operations | Refactoring, reorganizing, and cleaning up legacy content and architecture, ensuring a healthy BI environment and codebase | Supporting the data team through the application of best practices, such as code-linting, data tests, and documentation | Helping identify trends in data sets, and developing models that make raw data more useful to the organization | Enhancing the data infrastructure and tech stack to support automation and continuous deployment of predictive models, scalability, and optimized data delivery | Supporting revenue reporting needs organization-wide, identifying ways to improve data reliability, efficiency and quality | Pioneering the democratization of data and self-serving via training sessions, with the support of the Looker administrative team |  | YOU MUST HAVE… | 2+ years of experience in data engineering or data development | A degree in computer science, information systems, engineering, math/statistics, or related | Experience with relational databases such as Postgres or MySQL etc. | Experience writing complex SQL queries from scratch | Implementation experience in data governance, data quality data modeling | Strong written and oral communication skills | Strong business intuition and ability to understand complex business systems |  | IT’S AN ADDED PLUS IF YOU HAVE… | Experience with cloud-based data warehouses such as Redshift, BigQuery, or Snowflake | Experience with analytical engineering transformational tools such as dbt | Experience working with data sets with millions of rows | Beginner to intermediate level experience or familiarity with BI tools such as Looker, Tableau, Power BI, etc. | Data science experience, e.g.: building predictive models, monitoring model performance, etc. |  | YOU’LL BE SUCCESSFUL IF... | You get things done | You follow through on every request, no matter how big or small. You keep your cool under pressure and know how to prioritize your responsibilities. And when you need help, you're not afraid to ask. |  | You take initiative | You see something that needs improvement and you take action. You propose solutions to problems and research subjects that will help you do your job better. You’re proactive about asking questions and seeking clarity about requirements. You thrive in fast-paced environments with people that are constantly looking to innovate, enjoy challenges, and demand excellence. |  | You're passionate about accuracy | You double and triple-check your work because you know that accuracy is crucial when reporting on revenue and product data. You pay strong attention to detail, have excellent math, grammar, spelling, and proof-reading skills. |  | You are organized and detail-oriented | You prioritize well and are able to manage important projects, schedules, and communications in an orderly fashion. |  | TEAM OVERVIEW | The Subscription team is a sophisticated group of product managers, designers, storytellers, engineers, and marketers that power our unique monthly giving platform, The Spring. They focus on developing a first in class experience for our Spring monthly donors while building a product that invites transparency, generosity, and community. |  | ORGANIZATIONAL OVERVIEW | At charity: water, we believe that water changes everything. We work with local implementing partners to fund sustainable solutions that provide clean and safe drinking water in developing countries. Since 2006, with a diverse team of world-changers and an unstoppable community of supporters, we’ve brought clean water to more than 11 million people worldwide. With the makings of a mid-size tech startup, we’re reinventing charity through endless innovation, contagious passion, beautifully-crafted stories, and a powerful brand that inspires a new kind of generosity.",New York NY 10013,Data Engineer
Intone Networks,/rc/clk?jk=21cf2b8827944aa2&fccid=6d7040765331fd1d&vjs=3,"Blink Health is a well-funded healthcare technology company on a mission to make prescription drugs more accessible and affordable for everyone. We're scaling up in a highly complex vertical to change the way Americans access the prescription drugs they need. |  | Our proprietary platform and supply chain allows us to offer everyone — whether they have insurance or not — amazingly inexpensive prices on over 15,000 medications. With the addition of telemedicine and home delivery for prescriptions, Blink is providing a life-changing experience for people all over the country and fixing how opaque, unfair and overpriced healthcare has become. We are a highly collaborative team of builders and operators who invent new ways of working in an industry that historically has resisted innovation. Join us! |  | About The Team |  | Blink Engineering strives to build trusted, highly observable, data-driven products to bring affordable, accessible healthcare to all Americans. We understand healthcare is the most complex system most of us will ever fix. We believe in solving this complexity through the use of simple, well-known technologies. We are a highly collaborative team that believes in owning outcomes over owning code and putting patients at the center of everything we do. |  | The Blink Health Data Engineering and Analytics team is a small team responsible for building infrastructure, frameworks and tooling to enable data-driven decisions; building and maintaining our data warehouse for security and scale. This role is central to building and executing on a robust and forward-looking data strategy for the company, and the successful candidate blends top-tier software engineering expertise with the ability to look ahead at what we need to build for the future. |  | About the Role |  | As Data Engineer, you will be a helping building our next generation of data tools and frameworks, in addition to developing and maintaining data products and infrastructure. You will proactively assess production DW support trends to determine and implement short- and long-term solutions, and be able to design for data integrity, reliability, and performance. |  | Required Experience |  | You have 4+ years hands-on experience and demonstrated strength with: | Python software development. You will be coding. | Building and maintaining robust and scalable data integration (ETL) pipelines using SQL, EMR, Python and Spark. | Writing complex, highly-optimized SQL queries across large data sets. | Designing and maintaining columnar databases (e.g., Redshift, Snowflake) | Distributed data processing (Hadoop, Spark, Hive) | ETL with batch (AWS Data Pipeline, Airflow) and streaming (Kinesis) | Integration and design for Business Intelligence tools (e.g., Looker, QuickSight) | Creating scalable data models for analytics. | You have experience designing and refactoring large enterprise data warehouses and associated ETLs, with continuous improvement examples for automation and simplification across all aspects of the DW environment, inclusive of both engineering and business reporting. | Proven success with communicating effectively across diverse disciplines (including product engineering, infrastructure, analytics, data science, finance, marketing, customer support, etc.) to collect requirements and describe data engineering strategy and decisions. | Undergraduate or graduate degree in Computer Science",New York NY,Data Engineer
Balyasny,/rc/clk?jk=89517088722543dc&fccid=3187d6d05b329ebf&vjs=3,"Balyasny Asset Management is seeking an experienced Commodities Data Engineer to work in our industry-leading Data Intelligence Group. As a Commodities Data Engineer you will work directly with commodity Portfolio Managers, Analysts and Traders, helping them integrate and synthesize data. The ideal candidate will have a minimum of 4 years’ hands-on development experience with an excellent technical background. You should also have experience in building scalable, performant data delivery systems along with familiarity with commodities assets. Strong communication skills are a must as well as the ability to multi-task in a fast-paced trading environment. | As a Commodities Data Engineer, you will be responsible for the following: | Develop and enhance data delivery pipelines, leveraging the latest cloud-based data processing technologies | Integrate data from a variety of vendors, across the full landscape of market data | Collaborate with Portfolio Management teams to deliver data solutions to them. This will include delivery, cleansing, pre-processing and general wrangling of data. | Work alongside Portfolio Management teams to support their day-to-day processing and operations. | Work as a part of the overall Data organization to help engineer and support robust, scalable, cloud-based data delivery pipelines. | Liaise with data vendors to help provide a high-quality data product to the financial professionals at the firm. | Own the full scope of the assigned projects, including understanding and gathering requirements, designing the solution, and building and delivering completed functionality to production | Produce comprehensive written documentation | Perform with minimum supervision, exercising sound judgment |  | QUALIFICATIONS &amp; REQUIREMENTS: |  | In order to effectively represent the Company and communicate with clients, the employee must be someone who has: | Strong familiarity with the commodities space and the assets traded within it | Degree in Computer Science or closely related field | 4+ years development background | Broad knowledge of electronic markets and investment products | Strong knowledge of Python | Experience building and supporting complex technical systems and pipelines | Experience with AWS development a plus | Unix (Linux) and database experience (SQL, PostgreSQL) | Familiarity with object oriented development approach | Analytical skills – Ability to troubleshoot and logically assess problems and determine solutions | Documentation skills – ability to represent ideas, requirements, and problems in clear and concise documents",New York NY,Commodities Data Engineer
Luxoft,/rc/clk?jk=61d6a162d7bbb868&fccid=4b841d912d46e4fd&vjs=3,"Project Description: | We are seeking a highly qualified and talented technologist to join the Data Platform team. The team is re-envisioning client's platform include the data pipeline, research infrastructure in the cloud and back testing. |  | Application is a multi-manager platform that invests its capital with Internal and Partner portfolio managers, primarily on an exclusive or semi-exclusive basis, across quantitative, fundamental equity and tactical trading strategies. We have created a unique structure to provide global portfolio managers with autonomy, flexibility and support to best enable them to maximize the value of their businesses. |  | Over the last 30 years, our client has successfully capitalized on inefficiencies and opportunities within the equity markets. We have developed and invested heavily in proprietary technology, infrastructure and risk analytics. Our portfolio exposure has expanded across the Americas, Europe and Asia as well as multiple asset classes and products. |  | Responsibilities: | The Data Engineer will be leading, architecting and developing optimized, high throughput applications for scalable research and back testing in the cloud. The infrastructure will provide PMs with infrastructure to analyze data, back test their strategies and deploy them to production trading seamlessly. |  | Skills Required: | ",New York NY 10006,Data Engineer
Capital One - US,/rc/clk?jk=e90abdf9e59c657c&fccid=b85c5070c3d3d8c8&vjs=3,"11 West 19th Street (22008), United States of America, New York, New York | Data Engineer | Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who love to solve real problems and meet real customer needs. | We are seeking Data Engineers who will be a part of a team that’s building new analytical and machine learning tools and frameworks to exploit advantages in the latest developments in cloud computing. As aCapital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One. Learn more about#lifeatcapitalone and our commitment todiversity &amp; inclusion by jumping to slides 76-91 on our Corporate Social Responsibility Report. | What You’ll Do: | Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies | Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems | Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake | Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal &amp; external technology communities, and mentoring other members of the engineering community | Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment | Perform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance | Basic Qualifications: | Bachelor’s Degree | At least 2 years of experience in application development | At least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper) | Preferred Qualifications: | Master's Degree | 3+ years of experience in application development | 1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink | 1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud) | 1+ years of experience with Ansible / Terraform | 2+ years of experience with Agile engineering practices | 2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase) | 2+ years of experience with NoSQL implementation (Mongo, Cassandra) | 2+ years of experience developing Java based software solutions | 2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell) | 2+ years of experience developing software solutions to solve complex business problems | 2+ years of experience with UNIX/Linux including basic commands and shell scripting | At this time, Capital One will not sponsor a new applicant for employment authorization for this position.",New York NY 10011,Data Engineer
Microsoft,/rc/clk?jk=206d6d20632bd4c3&fccid=dd616958bd9ddc12&vjs=3,"United Talent Agency (UTA) and IQ, UTA’s strategic insights group, is looking for a rising Data Engineer to join our small but mighty team where you’ll have the opportunity to expand your knowledge through working with our customers and our senior technical resources. UTA IQ is dedicated to analyzing third-party data and leveraging its own proprietary tools and predictive analytics to provide actionable insights to our clients. You will be responsible for staying on top of industry trends, adopting advanced best practices and technologies and fostering a data-driven culture throughout the company. | What You’ll Do |  | Support full software development lifecycle | Work with team to establish system performance metrics to drive development priorities | Create, improve, and update systems to improve scalability performance and capacity | Be a resource and guide to less experienced staff or to those with other specialties | What you Need |  | BS in Computer Science or related field strongly preferred | Minimum 3+ years of professional work experience | 2 years of Linux experience | 2 years relevant experience with RESTful service development |  Experience with multiple types of data stores (NoSQL, SQL, Search Indexes) |  | Knowledge of existing cloud provider solutions (AWS or Azure). | Familiarity with Serverless Architectures | Comfortable working in a fast-paced, continuous delivery environment | Experience participating in cross-functional development teams | Experience in distributed systems design and architecture | Experience in troubleshooting and tuning systems | Good knowledge of C#, Java, or C++ and OOP principles | Ability to articulate ideas to non-technical audience | Passionate about scaling software | What You’ll Get | The unique and exciting opportunity to work at one of the leading global entertainment companies. | The opportunity to innovate and do the best work of your career as part of collaborative and cross-functional team | Access to the tools, leadership and resources you’ll need to create and drive a center of excellence | Competitive benefits and programs to support your well-being | About UTA | UTA is one of the world's leading talent and entertainment companies. We help the world’s most inspiring people make the world a more inspiring place. Many of our colleagues spend their careers here. It’s one of the reasons UTA is considered among the entertainment industry’s best places to work. |  | At UTA, the belief in the client relationship is paramount and marked by respect, a sense of stewardship, and a commitment to an artist’s entire creative life. Our commitment to every employee is the same. |  | UTA seeks the innovators, the entrepreneurs, the talented, the creative, the thoughtful and, the passionate, who share our love for the work all of us are privileged to do. For more information: https://www.unitedtalent.com/about/ |  | UTA and its Affiliated Companies are Equal Employment Opportunity employers and welcome all job seekers including individuals with disabilities and veterans with disabilities.",New York NY,Data Engineer
Cedar Inc,/company/DriveCentric/jobs/Data-Engineer-998a05929f509acd?fccid=e84c49577ce6e9ea&vjs=3,"Job detailsSalary$80,000 - $110,000 a yearJob TypeFull-timeFull Job DescriptionAre you tired of not being challenged, not having a voice, or having to work with outdated technologies? Do you want to be a direct contributor in a company that is an innovation leader and has the awards to prove it? Do you want your fair share of the profits from a fast-growing company that’s doubling its customer base year-over-year?A Data Engineer I/II develops and performs data migration / ETL processes for store launches, investigates and fixes data issues, monitors and optimizes database performance, and assists in the administration of databases and data warehouses.Responsibilities:Write, execute, and validate DML, DDL, and DCL scripts to meet business and customer needs.Coordinate and perform data imports, data modifications, database maintenance, etc. outside of business hours, as needed.Assist Customer Support and Development by analyzing data to troubleshoot application issues.Manage SSIS packages for customer onboarding ETL processes.Onboard new customers by scrubbing data and importing from multiple data sources.Requirements:2+ years of experience writing DML and DDL, with 1+ years of hands-on experience with T-SQL.1+ years of hands-on Microsoft SQL Server 2012+ experience.Ability to balance business and technical objectives when making decisions.Ability to balance multiple assignments in a fast-paced environment.Exceptional communication, problem-solving, and analytical skills are a must.Have a positive, can-do attitude.Pluses:1+ year of scripting and managing ETL packages, with SSIS or other tools.Hands-on experience with Database Administration (SQL Server)Hands-on experience with PostgreSQL.Benefits:Competitive salaryHealth, Vision, and Dental Insurance (eligible on day 1)401K with matching up to 4%9 company holidays + 12 vacation days in first yearAmple professional growth opportunitiesJob Type: Full-timePay: $80,000.00 - $110,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceFlexible scheduleHealth insurancePaid time offVision insuranceSchedule:Monday to FridayWork Location:Fully RemoteVisa Sponsorship Potentially Available:No: Not providing sponsorship for this jobThis Job Is:Open to applicants who do not have a college diplomaCompany's website:https://drivecentric.com/Company's Facebook page:https://www.facebook.com/DriveCentric/",Remote,Data Engineer I/II
Rackspace,/company/Career-Solution-Inc/jobs/Data-Engineer-a37cfc9f8d9cde06?fccid=712e24e21b5e2d95&vjs=3,"Job detailsSalary$85,079 - $169,749 a yearJob TypeFull-timePart-timeContractNumber of hires for this role2 to 4Full Job DescriptionPerforming data integration related work for which includes Ad stack Tech integration, BI continuity and other data integration required for running the business ""Top Skills:PythonCloud experience preferably GCP (airflow experience)Required Qualifications:Bachelor's Degree in Computer Science or a related discipline5+ years of applicable engineering experience.Strong proficiency in Python with an emphasis in building data pipelinesAbility to write complex SQL to perform common types of analysis and aggregationsExperience with Apache Airflow or Google ComposerDetail-oriented and document all the workAbility to work with others from diverse skill-sets and backgroundsNice to have: Experience with version control systems (Git and Bitbucket)Experience with Atlassian products Jira and ConfluenceExperience with Docker containerizationknowledge of Application Programming InterfacesJob Types: Full-time, Part-time, ContractSalary: $85,079.00 - $169,749.00 per yearSchedule:Day shiftCOVID-19 Precaution(s):Remote interview process",Philadelphia PA,Data Engineer
Indeed,/company/Solvenyle/jobs/Data-Engineer-8ec0c761426e8665?fccid=31a27e3bd6590f2f&vjs=3,"Job detailsSalary$60,000 - $75,000 a yearJob TypeFull-timeNumber of hires for this role5 to 10QualificationsMaster's (Preferred)SQL: 5 years (Preferred)Data Warehouse: 4 years (Preferred)Full Job DescriptionHi ,Hope you doing Good!!This is Lokesh from Ibrain, Here Please find the below opportunity which fits your profile. Please let me know your Interest. References are highly appreciated.Role: Data EngineerType of Hire:  Direct hireWork Auth: US Citizen or GC holderLocation: RemoteType of Hire: Direct ClientWe are looking for a Jr. Data Engineer to join our team in the ongoing development and support of enterprise solutions for data warehousing, enterprise analytics, and integrations across our enterprise, and support the ongoing buildout of our data management and analytics environment on Microsoft Azure platform.A Data Engineer will also be responsible for:Recreating existing application logic and functionality in the Azure and SQL Database environment.Working with external partners to bring data into the firm's environment.Designing and maintaining database schemas.Working with the Data Team to translate functional specifications into technical specifications.Additional Responsibilities:Work with external resources on large projects.Work with other internal technical personnel to troubleshoot issues and propose solutions.Support compliance with data stewardship standards and data security procedures.Apply proven communication and problem-solving skills to resolve support issues as they arise.QualificationsBachelor’s degree or equivalent experience.1-3+ years of experience in a Database Developer or Data Engineer role.Understanding of data management (e. g. permissions, security, and monitoring).Experience with scripting languages such as R and Python.Knowledge of software development best practices.Excellent analytical and organization skills.Effective working in a team as well as working independently.Strong written and verbal communication skills.Preferred applicants will also have:Expertise in database development projects and ETL processes.Experience in an agile SDLC environment.Experience planning and implementing QA and testing, and data warehousing.Experience with Microsoft Azure, Data Factory, and SQL DatabaseThank You !Lokesh I IT RecruiterM: 636-486-4431Job Type: Full-timePay: $60,000.00 - $75,000.00 per yearSchedule:8 hour shiftEducation:Master's (Preferred)Experience:SQL: 5 years (Preferred)Data Warehouse: 4 years (Preferred)Work Location:Fully RemoteCOVID-19 Precaution(s):Remote interview process",Remote,Data Engineer
HASH,/rc/clk?jk=fb4aad91f038d3fd&fccid=c70aa11f82f1796c&vjs=3,"About this role |  | We're hiring an experienced Data Engineer to join our team in order to help lead the build-out of our data integration and pipeline processes and tools. |  | HASH is a remote-first company, with offices in London and New York. This position is open to applicants globally. |  | Requirements | At minimum a Bachelor's Degree in Computer Science, Applied Mathematics, Operations Research, Statistics, or similar. | 3+ years of experience as a Data Engineer or in a similar role (BI Engineer, Business/Financial Analyst or Systems Analyst) | Direct experience with SQL, data modeling, data warehousing, and building ELT/ETL pipelines | 4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field | Preferred Qualifications | Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets | Experience working with AWS big data technologies (EMR, Redshift, S3) | Experience providing technical leadership and mentoring other engineers for best practices on data engineering | Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations | Pay &amp; Benefits | Competitive salary and equity: commensurate with experience and incentive-aligned ✅ | 29 days annual holiday including company holidays | A range of location-specific benefits, including access to a 401(k) and fully comprehensive health insurance and a fully funded Health Savings Account (HSA) for US-based employees, as well as free annual NYC MetroCards and (truly) infinite caffeine for those working from our Manhattan office. |  | Our Mission |  | Our mission is to enable everybody to make the right decisions. We’re making raw information and expert knowledge understandable and usable by all. |  | The Company |  | HASH is a venture-backed startup based in New York and London. We're backed by venture capital firms as well as the founders of successful startups like Kaggle (the world’s largest data science community) and Stack Overflow (the world’s most-trafficked developer site).",Remote,Data Engineer
LOCKHEED MARTIN CORPORATION,/company/Trigent-Software-Inc/jobs/Data-Engineer-e22046bdf6402058?fccid=2da0776ec527bb70&vjs=3,"Job detailsSalary$50 - $55 an hourJob TypeFull-timeContractNumber of hires for this role1QualificationsBachelor's (Preferred)SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Full Job DescriptionJob Title: Cloud Data EngineerLocation: Remote FULL TIMEDescription: A cloud Data Engineer is needed to work collaboratively with a team dedicated to the modernization of legacy systemswith respect to data and platforms. Demonstrated experience in designing and building AWS data processing cloud-basedinfrastructures is required. Performs testing, maintenance, construction and development of large-scale data processingarchitectures, processing systems and databases. The data engineer will work with users to define existing or new systemscope and objectives. The data engineer will provide analytical support and technical advice during the conceptualization,development, and implementation phases of the project. Have the ability to assemble large, complex sets of data that meetnon-functional and functional business requirements. The data engineer should have exposure to dimensional and relationaldata modeling concept in order to better understand modernization solutions to efficiently design and build infrastructurefor optimal extraction, transformation and loading of data from various data sources using AWS, as well as, SQL and NoSQLdata technologies. Roles &amp; Responsibilities: - Provide subject matter expertise and hands on delivery of data capture,curation and consumption pipelines on AWS. - Expert in writing SQL and some experience in Python and shell scripting.- Ability to build cloud data solutions and provide domain perspective on storage, big data platform services, serverlessarchitectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security. - Creates solutions while leveraging performanceengineering and relational techniques - Participate in deep architectural discussions to build confidence and ensure customersuccess when building new solutions and migrating existing data applications on the AWS platform. - Build full technology stackof services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service),operations, management and automation. - Manage small teams of delivery engineers successfully delivering work efforts(if in an independent contributor role) at a client or within Accenture. - Quality control of data assets pipeline and data servicedevelopment.- Collaborate with cross-functional teams to understand data flows and process to enable design and creation of efficientand flexible solutions to each engineering challenge. - Flexible approach to analyzing technical issues and clearly communicaterecommendations/solutions with stakeholders - Experience working with CI/CD processes and automated testingPlease fill the details for Submission: Legal name:Contact no:Email id:Current location:Relocation:Availability:Linkedin:Visa Copy:DL Copy:Salary:Reference: Name:Company :Title:Contact:Email:Thanks&amp;Regards,VenkatTrigent Software Inc2 Willow Street, Suite #201 Southborough, MA 01745Contact: 508-490-6016venkat_k AT trigent.comJob Types: Full-time, ContractPay: $50.00 - $55.00 per hourSchedule:8 hour shiftEducation:Bachelor's (Preferred)Experience:SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Contract Length:More than 1 yearWork Location:Fully Remote",Remote,Data Engineer
Conde Nast,/rc/clk?jk=65407e0825ca8682&fccid=aeb15e43a6800b9d&vjs=3,"This position is for a Data Engineer within the Lockheed Martin Chief Data and Analytics Office (CDAO). This is a great opportunity for someone who is looking to work in a fast-paced Information Technology organization where the latest technologies will be developed and applied. |  | We are looking for someone with strong hands on experience in all layers of data integration and analytics! The ideal candidate has experience and a solid understanding of delivering full-stack data solutions across the entire data processing pipeline. |  | The work location can be virtual or at any major Lockheed Martin facility. |  | Duties and responsibilities include, but are not limited to: |  | Exhibits a degree of ingenuity, creativity, and resourcefulness when collecting, and processing data for analysis while working with an Agile scrum teamDevelop and automate workflows that clean, transform, and aggregate unorganized data into databases or data sourcesWork through all stages of a data solution lifecycle, e.g., analyze/profile data, create conceptual, logical and physical data model designs, architect and design ETL, testing, reporting and analyticsResponsible for collecting, ingesting, processing, storing, and virtualizing large datasets from a wide variety of data sources and stakeholdersMaintain data systems performance by identifying and resolving production and application development problems; calculating optimum values for parameters; evaluating, integrating, and installing new releasesParticipate in on-call rotations to monitor and resolve production issues during off-hoursProvides ongoing support, monitoring, and maintenance of deployed solutionsCollaborate with data architects, data scientists, data analysts, and system engineers to develop a solution across the entire data processing pipelineUS Citizenship | Basic Qualifications: | Experienced in design or development of enterprise data solutions, applications, and integrations | Knowledge of modern enterprise data architectures, design patterns, and data toolsets and the ability to apply them | Has software engineering experience | Strong problem solving, conceptualization, and communication skills | Degree in Computer Science, Systems Engineering, or related field | Desired Skills: | Data modelingExtraction, Transformation and Load (ETL) tools (i.e. Data Services, Informatica)Data warehousing solutions (HANA modeling, SDI/SLT HANA replication) | Database systems (SQL and NO SQL) – HANA, Oracle, SQL ServerDistributed data systems (e.g., Hadoop, HBase, Cassandra, Spark) | Languages: Java Script, SQL, Python, XML, Java, Shell, Python | BASIC QUALIFICATIONS: | job.Qualifications |  | Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status. | Join us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They’re dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about. |  | As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories. | EXPERIENCE LEVEL: | Experienced Professional",Littleton CO 80127,Data Engineer Asc
Braintrust,/company/Equitus-Corporation/jobs/Data-Analyst-Engineer-978a81f7e9fedc47?fccid=b6400bab4a7b0893&vjs=3,"Job detailsSalary$27 - $59 an hourJob TypeFull-timeQualificationsTop Secret (Preferred)Full Job DescriptionEquitus Corporation is seeking an energetic, positive individual to fill the role of Data Analyst or Data Engineer at our small office located in Clearwater, FL. This individual will support a broad range of technical research topics. As part of a small, high intensity team, the applicant must have an inquisitive mind, and a passion for discovery and learning. We are seeking someone who has high attention to detail, is a fast learner, and can adapt to a fast-paced work environment. If you feel as if this position speaks to you, please respond with your resume.Responsibilities: · Understand and integrate various data types and sources· Develop and implement new techniques for insight generation from previously disconnected data types· Learn and use Equitus advanced intelligence platform· Demonstrate knowledge of the Equitus intelligence platform by incorporating various disconnected data typesQualifications (Required): · Excellent writing skills· Must be a self-starter with strong critical thinking skills· Excels at problem solving· Ability to review large amounts of data to determine trends· Ability to use a variety of Microsoft products (Excel, Word, etc.)· Strong analytics and research skillsQualifications (Preferred): · 1-2 years of applicable experience· Knowledge of advanced internet-based research including: Boolean logic, advanced research techniques, search engine and database resources, social networking tools, commercial and industry based databasesEquitus Corporation is an equal opportunity employer and does not discriminate against persons based on race, religion, national origin, sexual orientation, gender, gender identity and expression, marital status, age, disability, pregnancy, medical condition, or covered veteran status.Job Type: Full-timePay: $27.00 - $59.00 per hourBenefits:Paid time offSchedule:Monday to FridaySecurity Clearance:Top Secret (Preferred)Work Location:One locationCompany's website:www.equitus.usWork Remotely:No",Clearwater FL 33755,Data Analyst/Engineer
DriveCentric,/rc/clk?jk=a813e0d7a4866c25&fccid=734cb5a01ee60f80&vjs=3,"Are you looking for an opportunity to be part of a fast growing and dynamic team? Do you want to broaden and deepen your understanding of key business drivers, challenges, risks and opportunities? Are you passionate about business intelligence and the impact it can have within a large business? If so, this role may be right for you. | The Finance Data and Experiences team builds, curates, and consumes data for reporting, deep data analysis as well as advanced data science. We develop insights that are used in scenarios spanning increasing customer satisfaction, defining and tracking key business metrics and informing the future roadmap for the Data business. | As a Data Engineer on the Finance Data and Experiences team, you are an authority, familiar with all data warehousing technical components, infrastructure, and their integration. You are an expert in both high level design and experience with Star Schemas, Data Cubes and Dimensional Models as well as deeply technical in building durable data pipelines with the ability to scale elegantly with data volume growth. You will be responsible for designing, developing, and maintaining data pipelines and back-end services for real-time decisioning, reporting, optimization, data collection, and related functions. The code you write will enable our users to get data in a timely manner. You'll work on a variety of tools and systems, most of which are Cloud-based applications or data-platform components (e.g., data pipelines, processing, Visualization, reporting, etc.). In this role, you will be working in Agile development method for faster, quick-wins with the quality of design, development and operating the BI components you develop. This team enables thousands of Finance, Sales, Marketing, and Engineering users (from senior executives to field leaders) to effectively monitor, analyze, and make data-driven decisions on their business | We are looking for a Data Engineer who has a proven record of successfully building and supporting BI and data warehouse solutions and platforms to join us in our evolution, taking a leading role in designing, implementing and evolving innovative capabilities tailored to solving complex scenarios. In this role, you will also leverage Microsoft’s latest BI and cloud technologies for delivering BI applications to drive analytics and insights. | Responsibilities | The role’s core responsibilities include: | Design, architect, implement, and support key datasets that provide structured and timely access to actionable business information with the needs of the end customer always in view | Build ETLs/ELTs to take data from various telemetry streams, data lakes and supporting data sources and craft a unified dimensional or star schema data model for analytics and reporting | Develop a deep understanding of vast data sources (existing on the cloud) and know exactly how, when, and which data to use to tackle particular business problems | Design efficient and standardized, data structures, and database schemas | Develop high performing and functional code | Collaborate on delivering technical excellence across the organization | Ability to lead a team of BI developers to ensure timely, accurate and within scope delivery of all sprint items | Work with Business Engagement and Business stakeholders to understand requirement, translate them into technical requirements, and partner with our PMs to plan delivery commitments | Thought leadership, creation, and execution on new platform capabilities | Continuously evaluate our processes and reporting to identify opportunities to improve, enhance, and automate existing and new deliverables | Collaborate with various functions at Microsoft to develop sound business tools | Establish and operate data quality assurance processes | Qualifications | The ideal candidate will have the following qualifications: | Bachelor’s degree or equivalent experience | 1 to 5 years of experience working in BI projects including analytics, data warehousing applications. | Experience in Business Intelligence technologies | SQL Server and Azure SQL Services | SSAS, DAX, SQL, SSIS, PowerShell | Cosmos, Azure Data Lake, Spark, Data Bricks | Excel, Power BI, Power Query | Experience managing technical BI/database development resources | Knowledge of Microsoft data assets/systems preferred | Ability to work independently and collaboratively with cross-functional teams | Ability to manage multiple and competing projects/priorities | Strong communication, cross group collaboration and interpersonal skills | Passion for using data and analytics to drive business value | Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form. | Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.",Redmond WA 98052,BI Data Engineer
Seated,/rc/clk?jk=b14844a3f505f19d&fccid=e76096926e6cd257&vjs=3,"EXOS is a leader in the field of human performance, a category it created more than 20 years ago. Today, EXOS employs more than 4,500 people in over 600 locations worldwide. With award-winning facilities, sports medicine clinics, technology, and services, EXOS connects people to the solutions they need and provides comprehensive game plans, regardless of skill and sport, based on time-tested fundamentals and research in order to help people take control of their health and performance. At EXOS, we have a passion for performance through continuous self improvement. This persona is inspired by helping others in all of our EXOS Communities improve their personal performance. Our dedication to promoting diversity, multiculturalism, and inclusion is more than a commitment at EXOS - it’s the foundation of every great performance team. We are fully focused on equality and believe deeply in diversity of race, gender, sexual orientation, religion, ethnicity, national origin and all the other fascinating characteristics that make us different. EXOS is trusted by hundreds of clients including leaders in business, health care, community organizations, and world champions in sports. | Job Description: | DAY-TO-DAY RESPONSIBILITIES | Build large-scale batch and real-time data pipelines with data processing frameworks like Spark and AWS managed services. | Use best practices in continuous integration and delivery. | Help drive optimization, testing and tooling to improve data quality and our ability to use data to make product decisions. | Collaborate with other software engineers, ML engineers and stakeholders, taking learning and leadership opportunities that will arise every single day. | Collaborate with the analytics team to support their BI tools and initiatives to deliver the reliability, speed, and scalability of a data platform they’ll love working with. | Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product and infrastructure objectives. | Optimize the existing data warehouse which will create a single version of the truth and standardize data into coherent formats so that it can be queried by users by leveraging query languages, as well as generating API calls to gather third-party vendor data. | Create and maintain pertinent datasets aligned to business needs, including gathering client report requirements to execute to highest possible standards. | Ensure data structures, reports and any other data-related item meets standards and guidelines outlined by EXOS Data Governance. | Find trends in data sets and develop algorithms to help make raw data more useful to the enterprise. | Identify, design and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. | Collaborate with data analysts to understand data requirements necessary for building new data pipelines, and optimize existing data streams, so that analysts can seamlessly connect to data through Tableau and/or Google Data Studio, among other tools. | Work within multidisciplinary teams to identify client needs, define critical success indicators, identify and maintain data flows that fuel operations, and supply data. | Keep abreast of new data storage, delivery, analysis, visualization, reporting techniques and software to develop more powerful data infrastructure. | Be a trusted technical advisor to customers and solve complex data challenges. | Inspire and lead others with your work ethic, business results, intrapersonal skills and willingness to see success based on team accomplishments vs. your individual achievements. | KNOWLEDGE, SKILLS, AND ABILITIES | Know how to work with high volume heterogeneous data, preferably with distributed systems on the AWS platform. | Know how to write distributed, high-volume services in Go, Java, Scala, or Python leveraging AWS managed services. | Knowledgeable about data modeling, data access, and data storage techniques. | Appreciate agile software processes, data-driven development, reliability, and responsible experimentation. | Want to own the software you write in production. | Understand the value of partnership within and across teams. | Care a lot about fostering a diverse culture that includes everyone and supports them being their authentic self. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our customers. | Experience building the infrastructure required for optimal ETL of data from a wide variety of data sources. | A successful history of manipulating, processing and extracting value from large disconnected datasets. | Superior understanding of database query languages and substantial knowledge in analytical approaches. | Expertise with data mining and visualization techniques, ability to apply context to data, and strong ability to communicate the data (verbal and written). | REQUIRED QUALIFICATIONS | Bachelor’s degree in a technology-related field (data science, computer science, software engineering, etc.), with 5+ years’ data engineering experience. | Experience with data processing software and algorithms. | Experience in writing software in one or more languages: Java, Python, Go. Experience in SQL. | Experience managing client-facing projects, troubleshooting technical issues, working with cross-functional stakeholders. | Experience in working with/on data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures. | Excellent verbal and written communication skills. | PREFERRED QUALIFICATIONS | Hands-on experience in big data, information retrieval, data mining or machine learning. | Experience architecting, developing software, or internet scale data solutions in virtualized environments. | Reflective, independent and eager learner (e.g., learns from mistakes, asks good questions, able to generate creative solutions to problems with minimal guidance). | Detail-oriented with an ability to effectively multi-task in a deadline-driven atmosphere. | High levels of integrity, autonomy, self-motivation and ability to work well in a team. | Experience in health, fitness, wellness, etc. | If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us ! | We are an equal opportunity employer |  | EXOS is proud to be an affirmative action / equal opportunity employer. All qualified applicants will receive consideration without regard to race, creed, gender, marital status, sexual orientation, citizenship status, color, religion, national origin, age, disability, veteran status, or any other status protected under local, state, or federal laws. EXOS provides reasonable accommodation to employees and applicants for employment who have disabilities. You may request reasonable accommodation, in writing, by reaching out to our People Operations department at: | Attention: People Operations, Accommodations | 2629 E. Rose Garden Ln. | Phoenix, AZ 85050",Remote,Jr. Data Engineer
F. Schumacher & Co.,/rc/clk?jk=c07f9839a40044a7&fccid=0927f6330c6efafa&vjs=3,"Schumacher &amp; Co., America’s leading name for manufacturing and distributing fabric, wallcovering, trim and furnishings is seeking a Jr. Data Analytics Engineer to pioneer with us into some new business ideas and areas. |  | F. Schumacher &amp; Co.’s mission is to be the premier resource of branded decorative interior furnishings to consumers by offering the highest quality in product and service. Being a highly design and customer-centric company, we pride ourselves on always evolving and pioneering in order to take advantage of changes in customer behavior and environment. As a risk- seeking and innovative company we are constantly trying new ideas and evolving our leading market position. |  | We are looking for a Jr. Data Analytics to add to our team. This role requires the ability to work quickly and wear many hats while managing multiple projects and work across different teams. The best candidate will come with enthusiasm, will be a self-starter, have a fine eye for detail and have experience with data analytics. | YOU WILL: | Fulfill data requests from all internal organizations | Develop data queries and integrate them into reports and tools | Gain a thorough understanding of the company’s data and how that data aligns with business needs | Work with stakeholders to clearly define business requirements in developing analytics tools | Think of creative solutions on how to best fulfill data requests through presentation, user-interface, and functionality | Manage stakeholder expectations of projects | Be a critical point-of-contact for data requests | YOU HAVE/ARE: | 1 year of experience in SQL, with the opportunity and expectation to develop skill | 1 year of experience and advanced understanding of Excel, i.e. formulas, tables, named ranges, conditional formatting, etc. Experience with macros is a plus | Must be a self-motivator and a self-learner | Must be a people person; collaborating with other departments is key, specifically in clearly defining business requirements for project requests | Basic project management is a plus |  | ABOUT SCHUMACHER: | At F. Schumacher &amp; Co we love style, taste and innovation. We believe that design matters and great design matters more, as it can transform space, lives and careers. | While we celebrate our storied heritage, we are rather looking ahead to the next 129 years. Our talented team is innovative and dynamic, and our culture is progressive and fun. In order continue our success; we are always looking for talented people that fit. | Culture: In our company, you’re judged by your ideas and results, not by your experience or title. So, we encourage you to be thoughtful, casual, and to speak your mind. | Development: We strive every day to develop you and your colleagues for what we believe is a challenging and supportive business environment. | Teamwork: We believe that success and efficiency can only be a product of collaboration. At FSCO, working together is the rule, not the exception. | Innovation: We have achieved success and longevity through innovation. And we encourage experimentation and rule breaking. Mistakes? Those are things we embrace, talk about and learn from.",Remote,Jr. Data Analytics Engineer
Splunk,/rc/clk?jk=d5183b1de093c473&fccid=1f1b30bb311b8b1e&vjs=3,"We are hiring experienced senior-level data engineer who is versatile in skillset and are passionate about bringing value to our clients. We look for technology and data analytic leaders who are experts in their fields who also want to make a meaningful impact for our clients in the Bay Area. |  | The successful candidate will be a strong leader with the ability to execute specialized modern data solutions in the Snowflake Cloud data warehouse. They will possess skills in design, development, implementation, documentation, and management of data solutions, and be expected to clearly articulate and present technical information to clients and stakeholders. |  | Responsibilities: | Drive the design, building a metadata driven generic and scalable data pipeline framework for the Snowflake Cloud Data Warehouse; | Understanding of how to model data for scalability and optimized performance | Collaborate with business users to understand the need / problem | Collaborate with BI developers, Data Scientists, Product Managers, Software Engineers, Data Modelers and/or Platform owners to define the scope, design and implement the correct solution | Produces detailed and high-quality documentation detailing design and behaviors of data pipelines | Participate in architectural evolution of data engineering patterns, frameworks, systems, and platforms including defining best practices, standards, principles, and policies | Working knowledge of data quality approaches and techniques | Familiarity implementing Role Base Access Security (RBAC) for sensitive data | Bring a positive energy every day and work within a team to deliver the best possible solutions | Learn the business and the data that supports the business | Required Qualifications: | 5+ years experience in data integration and data warehousing (preferably Snowflake Cloud data warehouse) | 5+ years experience with relational database technologies (SQL Server, MySQL, Oracle, or similar) | 5+ years experience with ETL/ELT tools such as Fivetran, Talend, Matillion, Apache Airflow or similar | 3+ years experience Experience with custom ETL/ELT and programming/scripting language experience: Pyspark, Python, Scala (Python required) | Experience with developing dynamic data pipeline frameworks (with Fivetran, Talend, Matillion, Apache Airflow or similar) | Current experience with developing on Cloud Environments (AWS, Azure, GCP for instance) | Experience working in an Agile team and delivery capability | Experience performance tuning and optimization | BS Degree or MS Degree in Computer Science or equivalent area of study is required",Remote,Data Engineer
CrossFit,/rc/clk?jk=f14029161b82ef35&fccid=fdc9c901e79ff4b7&vjs=3,"Job detailsSalary$130,000 - $150,000 a yearFull Job DescriptionCrossFit is looking to hire a Data Engineer to join our technology department. You will focus on developing tools and systems for CrossFit's Business Intelligence and Analytics needs. This role will work on complex issues where analysis of situations or data requires an in-depth evaluation of business and technical needs. | RESPONSIBILITIES: | Ability to contribute to various areas of the technology org including SQL queries, data structures, storage formats, data integrations across a variety of systems, and developing the tools necessary to automate data migrations. |  Identifies problems and provides solutions to solve business and technology problems within the team. Contributes to the development of business tools and systems in accordance with the org’s architectural standards.Develops automated tests, contributes to build and deployment tools, and generally improves the quality, build, and maintenance infrastructure around our tools.Communicates with non-technical stakeholders to understand business context and create appropriate documentation for various internal partners. Thinks beyond individual requests to provide larger architectural direction to help meet future business needs and simplify larger integrations. Is able to provide estimates for milestones, deliverables and standards for performance. |  | KNOWLEDGE AND SKILL:Proficiency in one or more Software Development languages and frameworks including but not limited to: Python, PHP, SQL, LookMLProficiency with AWS capabilities including RDS (MySql), DynamoDB, Redshift, S3, SNS, EC2 and LambdaUnderstanding of structured data sets, data pipelines, ETL tools, data reduction, transformation and aggregation techniques. Knowledge of tools such as FiveTran, DBT or DataRobot a plus but not required.Familiarity with all aspects of the Software Development Lifecycle, including planning, object and system design, development, automated testing, deployment via CI/CD tools, logging, and monitoring.Experience collaborating with a team of programmers and analysts made up of varying skill setsExcellent verbal, interpersonal and written communication skills.Passion for CrossFit and our mission to be the world's leading platform for health, happiness and performance |  | EDUCATION/EXPERIENCE :BS degree in Computer Science, or related field.5+ years relevant work experience |  | Compensation range - $130k-150k DOE | CrossFit is a proud equal employment opportunity employer. We seek to recruit, develop, and retain qualified applicants from a variety of backgrounds, skills, and perspectives. All qualified applicants will receive consideration for employment without regard to race, color, sex, religion, national origin, age, pregnancy, sexual orientation, gender identity, gender expression, past or present military service, disability, genetic information, or any other basis protected by applicable federal, state, or local laws.",Remote,Data Engineer 3
EXOS,/rc/clk?jk=a6cf3887cf83adfe&fccid=aef928e89977f7f0&vjs=3,"Ready to shake things up? Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and strive to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun, and most significantly to each other’s success. We continue to be on a tear while enjoying incredible growth year over year. | As the Senior Data Engineer within the Data Technologies and engineering organization, you will own and be part of business analytics projects from Inception through Hypercare. As a data and reporting engineer you will drive transformational data and analytics initiatives for improving the effectiveness and efficiency for Enterprise business functions. Having the right blend of technological depth, Sales, Marketing, Finance process and systems expertise along with analytics skills is key to the success of the role. You will be expected to manage end to end delivery of small to medium scale projects in the data &amp; analytics area through active leadership and partner management, both at strategic and tactical levels. Very Strong presentation skills and executive presence, with effective communication to create impact and influence both at the executive and mid-level management. | What you'll do: Yeah, I want to and can do that. | Deep understanding of Cloud Data Warehouse methodologies, Data Architecture, Data Modeling, and metadata. | Ability to support the creation of single source of truth for Business Metrics leveraging agreed upon data definitions, create a common data foundation, and semantic layers to be consumed by business. | Deliver highly available, reliable, innovative large-scale data warehousing solutions to facilitate data ingestion, build optimized aggregates and building reporting solutions. | Deep knowledge and hands-on experience working with Cloud database technologies like Redshift, Snowflake and multiple BI platforms such as Tableau, Splunk and scripting language like Python. | Solid grasp of operational processes, systems and data in multiple areas of Sales &amp; operations; Forecasting, Quote to Cash, Pipeline management. | Understanding of SaaS business applications and processes as in Salesforce, SAP, Eloqua, Anaplan, etc. | Identify data patterns, attribute hierarchies and data relationships and organize into data dictionaries, create standardized definitions for metrics and critical metrics. | Analytical and problem-solving experience, exposure to large-scale systems and some experience writing code. | Strong business analysis skills - capturing and documenting requirements, understanding business impacts and tradeoffs, conducting interviews and workshops, proposing solutions, documentation and training, etc. | Requirements: I’ve already done that or have that! | 8+ years of experience as a Data Warehouse Analyst, Data Engineer and/or Data Scientist | Savviness with complex SQL queries and knowledge of database technologies including window and analytical functions | Experience with Python analytic libraries and Business Intelligence tools such as Tableau. | An ability to provide technical guidance, direction and problem solving to data engineering team members. | Confidence to offer consultation to business partners and team members within Sales, Partner, Marketing Operations. | A familiarity working with an AGILE/SCRUM process management. | Preferred knowledge and experience: These are a huge plus. | Knowledge of Splunk products | Agile certifications | Education: Got it! | Bachelor’s degree preferably in Computer Science, Information Technology, Management Information Systems, or equivalent years of industry experience. | What We Offer You: Wow, I want that. | A constant stream of new things for you to learn. We're always expanding into new areas, bringing in open source projects and contributing back, and exploring new technologies. | A set of extraordinarily hardworking, innovative, open, fun and dedicated peers, all the way from engineering and QA to product management and customer support. | Growth and mentorship. We believe in growing engineers through ownership and leadership opportunities. We also believe mentors help both sides of the equation. | A stable, collaborative and supportive work environment. | We don't expect people to work 12 hour days. We want you to have a successful time outside of work too. Want to work from home sometimes? No problem. We trust our colleagues to be responsible with their time and dedication and believe that balance helps cultivate an extraordinary environment | This isn’t a job – it’s a life changer – are you ready? | Splunk has been named one of San Francisco Bay Area’s “Best Places to Work” by the San Francisco Business Times, ten years in a row. We offer a highly competitive compensation package and a plethora of benefits. | Splunk is proud to be an equal opportunity workplace and is an affirmative action employer. We value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which the candidate is applying. For job positions in San Francisco, CA, and other locations where required, we will consider for employment qualified applicants with arrest and conviction records.",New York NY 10001,Senior Data Engineer
Dow Jones,/rc/clk?jk=eb84dc9ff2ddec9b&fccid=d2621f51dde471d3&vjs=3,"Job Description: | The Wall Street Journal is looking for a data engineer who will develop tools and infrastructure to empower the newsroom with insights and data. This role is an important force in bolstering the newsroom's data capacities as we increase focus on our audience and what's known about them. | We are looking for a full-stack data engineer who will build, enhance, and maintain new and current data products which drive audience analytics and insights. This work may include: (1) unlocking new datasets and data assets, (2) creating and maintaining data pipelines, (3) deploying data and insights to editors in the newsroom, and (4) working closely with data scientists to enable the training, deployment, monitoring, and maintenance of machine learning models. | As a member of the News Insight team, you will work closely with our data scientists, editors, and strategy leads as we build internal data products for the newsroom. You will collaborate daily with other data engineers within News Insights and Dow Jones’ DataSET team. Importantly, you will work directly with other engineers, product managers, and program managers who collectively work on our internal data products. | Required experience, knowledge, and skills: | Strong familiarity and experience with ingestion, streaming and batch processing, data infrastructure design, and data analytics. | Top-notch understanding of basic statistics and issues surrounding data quality | Experience building infrastructure required for optimal extraction, transformation and loading of data from diverse data resources | Experience running and supporting production of enterprise data platforms. | Experience with relational and non-relational databases. | Experience building data pipelines in AWS (preferred) or GCP. | Proficiency in Scala, Python, Bash, Git, Spark, and SQL. | 2 or more years of data engineering experience | Excellent written and verbal communication skills, as well as top organizational, time management, and documentation skills. | Preferred: | Experience working with digital media content tracking across websites and mobile applications. | Exposure to, or direct experience with, implementing machine learning driven product features or tools. | You will report to the Chief of Data Science, who leads our News Insights team. | LI-JA1-WSJ | Dow Jones , Making Careers Newsworthy | All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, or disability status. EEO/AA/M/F/Disabled/Vets . | Dow Jones is committed to providing reasonable accommodation for qualified individuals with disabilities, in our job application and/or interview process. If you need assistance or accommodation in completing your application, due to a disability, please reach out to us at TalentResourceTeam@dowjones.com. Please put “Reasonable Accommodation"" in the subject line. | Business Area: NEWS/WSJ | Job Category: IT Development Group | About Us | The Wall Street Journal is a global news organization that provides leading news, information, commentary and analysis. The Wall Street Journal engages readers across print, digital, mobile, social, and video. Building on its heritage as the preeminent source of global business and financial news, the Journal includes coverage of U.S. and world news, politics, arts, culture, lifestyle, sports, and health. It holds 38 Pulitzer Prizes for outstanding journalism. The Wall Street Journal is published by Dow Jones, a division of News Corp (NASDAQ: NWS, NWSA; ASX: NWS, NWSLV). | If you are a current employee at Dow Jones, do not apply here. Please go to the Career section on your Workday homepage and view ""Find Jobs - Dow Jones."" Thank you. | Req ID: 22578",New York NY 10176,Data Engineer
Twitch,/rc/clk?jk=aa81351c7fe79006&fccid=e21f6affbb4bb0b2&vjs=3,"About Us |  | Launched in 2011, Twitch is a global community that comes together each day to create multiplayer entertainment: unique, live, unpredictable experiences created by the interactions of millions. We bring the joy of co-op to everything, from casual gaming to world-class esports to anime marathons, music, and art streams. Twitch also hosts TwitchCon, where we bring everyone together to celebrate, learn, and grow their personal interests and passions. We're always live at Twitch. Stay up to date on all things Twitch on LinkedIn, Twitter and on our Blog. |  | About the Role |  | Data is central to Twitch's decision-making process, and data engineers operate at the forefront of this by creating datasets that drive decisions across all of Twitch. You will shape the way that performance is measured, establish how we transform our data, and scale analytics methods and tools to support our growing business, leading the way for high-quality, high velocity decisions. |  | We're looking for an experienced data engineer to join our Monetization Business and BI team, which is focused on making trusted, reusable data assets for data analysts, data scientists, and applied scientists in Ads, Commerce, and throughout Twitch to use. Your responsibilities will include developing and enhancing our data warehouse and datamarts which act as sources of truth across the company, working with data producers to ensure data coverage partnering with other data tool creators driving data quality, and connecting into data interfaces that allow everyone at Twitch to use the data. In the process, you will work with technical and non-technical staff members throughout Twitch. |  | You Will: | Delight data consumers throughout Twitch by ensuring they have the data they need to inform decisions, where and when they need it. | Define and own organization-level data architecture for a trusted, governed, dimensionally-modeled repository of data that enables Twitch staff to quickly and reliably answer their questions. | Prioritize projects from a diverse set of partners | Protect data sources against data quality issues: work with data producers to ensure data passes acceptance tests; design, develop and maintain data quality monitoring and assurance framework; and continuously improve the processes for developing new ones, raising the level of quality expected from our work. | Improve data discovery: create data exploration processes and promote adoption of data sources across the company. | Optimize business, engineering, and data processes via data architecture, engineering, testing, and operational excellence best practices. | You Have: | 3+ years of experience in data engineering, software engineering, or other related roles | 3+ years using relational database concepts with a working knowledge of SQL, SQL Tuning, data modeling best principles, OLAP, Big Data technologies | 3+ years of experience generating data pipelines from multiple data sources, in collaboration with diverse team members | Experience with development best practices, including query optimization, version control, code reviews, and documentation | Experience with Amazon Web Services: Redshift, S3, Glue, EMR, or Athena | Experience with Python | Perks | Medical, Dental, Vision &amp; Disability Insurance | 401(k), Maternity &amp; Parental Leave | Flexible PTO | Commuter Benefits | Amazon Employee Discount | Monthly Contribution and Discounts for Wellness Related Activities &amp; Programs (e.g., gym memberships, off-site massages), | Breakfast, Lunch &amp; Dinner Served Daily | Free Snacks and Beverages |  | Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. |  | We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",San Francisco CA,Data Engineer
MassMutual,/rc/clk?jk=94e3f617231cdc18&fccid=430cffbf1c607717&vjs=3,"What great looks like for this role: |  | Our ideal Advanced Data Engineer is a collaborative leader skilled in designing and constructing highly scalable data management systems. Youâ€™re also committed to data integrity, are highly analytical, and can work on multiple projects at once. Youâ€™ll use your skills to develop, monitor, and manage data systems across our platform. Additionally, you will act as a mentor to junior team members and coach them on best practices and engineering standards. The team culture of working collaboratively, cross-functionally, using new technologies combined with the work/life balance provided by MassMutual are core reasons people enjoy working on the Data Engineering team at MassMutual. |  | Objectives of this role: |  | Design, construct, install, test and maintain highly scalable data management systems. | Ensure systems meet business requirements and industry practices. | Design, build, and maintain a high-performance streaming and messaging platforms to support the enterprise. | Daily and Monthly Responsibilities: |  | Design, build, and maintain a high-performance streaming and messaging platforms to support the enterprise. | Create messaging standards for our messaging platforms. | Develop tools and processes to support the platform. | Create custom software components and analytics applications. | Work across departments and business units to define patterns and data needs. | Translate high-level business requirements into technical specs. | Basic Qualifications: |  | Bachelorâ€™s degree in computer science or engineering. | 2+ years of experience with designing and building data platforms. | 2+ years of experience with streaming/messaging platforms (Kafka, MQ, RabbitMQ, etc) | 2+ years of experience creating producer and consumer applications with Kafka | 2+ years of coding and scripting (Python, Java, Scala) and design experience. | Expertise in tuning and troubleshooting streaming/messaging platforms. | Experience with ELT methodologies and tools. | Strong data integrity, analytical and multitasking skills. | Excellent communication, problem solving, organizational and analytical skills. | Able to work independently. | Preferred Qualifications: |  | 3+ years of experience creating producer and consumer applications with Kafka | Experience with KStreams and KSQL | Experience with designing an automating deployment process (CI/CD) | Basic knowledge of database technologies (Vertica, Redshift, etc)",Boston MA,Data Engineer
BICP,/rc/clk?jk=b129ae5aee70f8a5&fccid=cffd065f9ff9e672&vjs=3,"Job detailsSalary$45 - $75 an hourJob TypeFull-timeContractFull Job DescriptionJOB TYPE: Freelance, Contract Position - No agencies (See notes below) | LOCATION: Remote (TimeZone: BST, CST, EST) | HOURLY RANGE: Our client is looking to pay $45 - $75 USD / HR | ESTIMATED DURATION: 40Hrs/Week - Long Term, ongoing project | ABOUT US: | Braintrust (usebraintrust.com) is a user-controlled talent network, where you keep 100% of what you earn and actually get to own the platform. We've been onboarding some big clients and specifically need a Data Engineer for our client. | About the Project &amp; Role | Have the opportunity to work with a massive worldwide company that is based in Switzerland. As data engineer you will be working on a consumer data platform called Treasured Data. This is a new huge market, and could provide exciting opportunities in the future. | You will be working directly with an established team consisting of Fermi.xyz leadership and in-house client resources. | Must Haves: |  |  Proficiency in SQL and Python |  SRE best practices | Experience with Consumer Data Platforms | Experience with SQL Presto, DigDag, Azure DevOps | Cyber reliability engineering | Excellent English | Teamplayer | Be able to make a call at 11:30 CT | ABOUT THE HIRING PROCESS: | Qualified candidates will be invited to do a screening interview with the Braintrust staff. We will answer your questions about the project, and our platform. If we determine it is the right fit for both parties, we'll invite you to join the platform and create a profile to apply directly for this project. | C2C Candidates: This role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application. | Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status. |  | This is a remote position.",San Francisco CA 94147,Data Engineer
Comcast,/company/INTOLON/jobs/Big-Data-Engineer-2386b994fe59be92?fccid=a7d19fe94642d6fb&vjs=3,"Job detailsSalary$92,471 - $182,709 a yearJob TypeFull-timeContractNumber of hires for this role1Full Job DescriptionA Big Data Engineer will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company.ResponsibilitiesSelecting and integrating any Big Data tools and frameworks required to provide requested capabilitiesImplementing ETL process {{if importing data from existing data sources is relevant}}Monitoring performance and advising any necessary infrastructure changesDefining data retention policies{{Add any other responsibility that is relevant}}Job Types: Full-time, ContractPay: $92,471.00 - $182,709.00 per yearSchedule:Monday to FridayWork Remotely:Temporarily due to COVID-19",Austin TX,Big Data Engineer
2U,/rc/clk?jk=e766207dae095be4&fccid=8f236bae1b36b1dc&vjs=3,"What We're Looking For: |  | The data engineering team is responsible for 2U's analytical data warehouse and supporting data pipelines. We're cloud based (AWS &amp; Snowflake) and code mostly in Python. The team provides services for multiple stakeholders within the organization including senior management, and delivers multiple projects throughout the year. |  | We're a fun bunch, everyone has a seat at the table and an ability to impact how we perform. |  | We are looking for a smart and collaborative data engineer with an avid interest in data and programming. Experience in system management &amp; data pipelines is required, automated testing is a plus. As an engineer II, you'll be a team member and an implementer of our tech stack (Python, AWS, Snowflake to mention a few). There is plenty of room for experimentation on how our stack can continue to evolve and it is highly encouraged. |  | Responsibilities Include, But Are Not Limited To: |  | Write maintainable, high-performance code | Conduct exploratory and automated testing | Debug complex problems under time constraints | Implement technical work according to product / design specifications | Refactoring, to keep code maintainable | Provide technical guidance and feedback to other team-members | Planning and estimating development tasks and short-term projects | Running deployment software to put code on production | Participate in application level technical design | Participate in technical data architecture |  | Things That Should Be In Your Background: |  | One or more programming languages, preferably Python and SQL, 2-5 years experience | 1-3 years of experience working within an AWS (or equivalent) environment | Team work, and ability to collaborate with a variety of roles, such as Business Analysts, Product Managers and peer engineers | Ability to work effectively in a fast-paced team environment | Problem-solving and decision-making skills | Eager to learn and grow as an engineer | Attention to detail |  | About 2U Inc. (NASDAQ: TWOU) |  | 2U is comprised of 3 lines of business: Graduate Degree Programs, Short Course, and Boot Camps. Going beyond traditional learning management systems, we use tech, people, and data to help top universities and enterprise organizations transform in the digital era—and eliminate the back row in higher ed. We support lifelong learning which means thinking beyond a single degree. It means finding ways for students to gain the skills they need to change careers, evolve their expertise, and meet the challenges of the changing world head-on. We help our partners fill those needs—developing new digital education technologies and offerings capable of supporting students at different points in their lives. Whether they need a simple refresher, to learn something new, or to change their career trajectories completely, our partners are there to help them succeed. Together with our partners, 2U has positively transformed the lives of more than 275,000 students and lifelong learners. |  | 2U Diversity and Inclusion Statement |  | At 2U, we are committed to creating and sustaining a culture that embodies diverse walks of life, ideas, genders, ages, races, cultures, sexual orientations, abilities and other unique qualities of our employees. We strive to offer a workplace where every employee feels empowered by the ways in which we are different, as well as the ways in which we are the same. |  | Benefits &amp; Culture |  | Working at 2U means working with individuals that are passionate and mission driven. We collaborate on tough problems to deliver the best outcomes for our partners, students, and each other. You will find team members working together in our open office spaces, gathered in the kitchen grabbing a snack, or taking a break in our game rooms. |  | 2U offers a comprehensive benefits package: |  | Medical, dental, and vision coverage | Life insurance, disability and 401(k) | Unlimited snacks and drinks | Tuition reimbursement program | Generous paid leave policies including unlimited PTO | Additional time off benefits include: | time off to volunteer for non-profit organizations | parental leave after 9 months of employment | holidays that include a winter break from Christmas through New Years! | To learn more, visit 2U.com. #NoBackRow |  | Note: The above statements are intended to describe the general nature and level of work performed by individuals assigned to this position, and are not intended to be construed as an exhaustive list of all responsibilities, duties and skills required. All employees may be required to perform duties outside of their normal responsibilities from time to time, as needed. |  | 2U is an equal opportunity employer that does not discriminate against applicants or employees and ensures equal employment opportunity for all persons regardless of their race, creed, color, religion, sex, sexual orientation, pregnancy, national origin, age, marital status, disability, citizenship, military or veterans' status, or any other classifications protected by applicable federal, state or local laws. 2U's equal opportunity policy applies to all terms and conditions of employment, including but not limited to recruiting, hiring, training, promotion, job benefits, pay and dismissal.",Brooklyn NY,Data Engineer II
Apex Global Solutions,/rc/clk?jk=daa2e0a9db913c15&fccid=01b91641951e8886&vjs=3,"Data is at the core of everything we do here at HRT. We excel at deriving deep insights from all types of data, allowing us to achieve consistent success in a competitive market. As we think about exciting new avenues for growth, we look forward to simultaneously expanding the scope of data at HRT to new frontiers. |  | Being a data engineer at HRT means being a pioneer. In this role, you'll have the opportunity to work on problems that are pretty brand new to us. You'll help design tools, processes, and an entirely new system for managing, storing, accessing, and exploring data at HRT. We're looking for someone who loves data (maybe even dreams about data) and wants to be a critical part of bringing a world-class data warehouse to life. |  | The Skills: |  | Strong programming experience in Python | Proficient in a strongly-typed language like Java, C, or C++ | Demonstrated ability to work with data | Data infrastructure experience - you know how to really store data; and no, we don't mean saving an Excel file on your desktop (everyone knows that's messy and it should be in a nicely named folder) | Track record of working successfully in a collaborative environment | Top-notch communication skills |  | The Profile: |  | You have a minimum of 2-3 years of experience working in data infrastructure | Bachelor's degree in computer science, math or a related field | You enjoy being part of an amazing team but don't mind working alone on a difficult problem | You can analyze and fix problems quickly | You really like to work with people who motivate you and make you better | In your spare time you: code, tinker, read, explore, break things, and have an insatiable curiosity for all things computer related |  | Culture: |  | Hudson River Trading (HRT) brings a scientific approach to trading financial products. We have built one of the world's most sophisticated computing environments for research and development. Our researchers are at the forefront of innovation in the world of algorithmic trading. |  | At HRT we come from all sorts of backgrounds: mathematics, computer science, statistics, physics, and engineering. We're a community of self-starters who are motivated by the excitement of being at the cutting edge of automated trading. Our culture celebrates great ideas whether they come from HRT veterans or new hires. At HRT we're friends and colleagues, whether we are sharing a meal, playing the latest board game, or writing elegant code. We embrace a culture of togetherness that extends far beyond the walls of our office. |  | Seem like something you might be interested in? Our goal is to find the best people and bring them together to do great work in a place where everyone is valued. HRT is proud of our diverse staff; we have offices all over the globe and benefit from our varied and unique perspectives. HRT is an equal opportunity employer; so whoever you are we'd love to get to know you.",New York NY 10005,Data Engineer
Takeda Pharmaceuticals,/company/V-Projects/jobs/Python-Developer-5a077e69485df4ad?fccid=339277b3279285d6&vjs=3,"Job detailsSalary$55 - $75 an hourJob TypeFull-timePart-timeContractNumber of hires for this role2 to 4QualificationsBachelor's (Preferred)Full Job DescriptionBackend / Data Engineer Immediate interviews within 24 Hrs Remote. Backend / Data Engineer- Django Rest framework- Python-Pyspark- NoSQL database : Cassandra/ HBaseFrontendAngular/ ReactInfraa. AWSb. AWS-S3Job Types: Full-time, Part-time, ContractPay: $55.00 - $75.00 per hourSchedule:8 hour shiftEducation:Bachelor's (Preferred)Work Location:Fully Remote",Remote,Python Developer/ Data Engineer
Novartis,/rc/clk?jk=8415e417a1146249&fccid=ea25315ee9da22e5&vjs=3,"Job detailsJob TypeCommissionFull Job DescriptionJob Summary | Responsible for planning and designing new software and web applications. Analyzes, tests and assists with the integration of new applications. Documents all development activity. Assists with training non-technical personnel. Has in-depth experience, knowledge and skills in own discipline. Usually determines own work priorities. Acts as a resource for colleagues with less experience. | Job Description | Core Responsibilities | Collaborates with project stakeholders to identify product and technical requirements. Conducts analysis to determine integration needs. | Designs new software and web applications, supports applications under development and customizes current applications. Assists with the software update process for existing applications and roll-outs of software releases. | Participates in training representatives and operations staff on internally developed software applications. | Researches, writes and edits documentation and technical requirements, including software designs, evaluation plans, test results, technical manuals and formal recommendations and reports. | Monitors and evaluates competitive applications and products. Reviews literature, patents and current practices relevant to the solution of assigned projects. | Provides technical leadership throughout the design process and guidance with regards to practices, procedures and techniques. Serves as a guide and mentor for junior-level Software Development Engineers. | Works with Quality Assurance team to determine if applications fit specification and technical requirements. | Displays in-depth knowledge of engineering methodologies, concepts, skills and their application in the area of specified engineering specialty. | Displays in-depth knowledge of and ability to apply, process design and redesign skills. Presents and defends architectural, design and technical choices to internal audiences. | Displays knowledge of and ability to apply, project management skills. | Consistent exercise of independent judgment and discretion in matters of significance. | Regular, consistent and punctual attendance. Must be able to work nights and weekends, variable schedule(s) and overtime as necessary. | Other duties and responsibilities as assigned. | Employees at all levels are expected to: | Understand our Operating Principles; make them the guidelines for how you do your job. | Own the customer experience - think and act in ways that put our customers first, give them seamless digital options at every touchpoint, and make them promoters of our products and services. | Know your stuff - be enthusiastic learners, users and advocates of our game-changing technology, products and services, especially our digital tools and experiences. | Win as a team - make big things happen by working together and being open to new ideas. | Be an active part of the Net Promoter System - a way of working that brings more employee and customer feedback into the company - by joining huddles, making call backs and helping us elevate opportunities to do better for our customers. | Drive results and growth. | Respect and promote inclusion &amp; diversity. | Do what's right for each other, our customers, investors and our communities. | Disclaimer: | This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications. | Comcast is an EOE/Veterans/Disabled/LGBT employer. | Education | Bachelor's Degree | Relevant Work Experience | 5-7 Years | Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That’s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality – to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.",New York NY,Software Engineer (Data)
Invitae,/company/Vyne-Group/jobs/Data-Engineer-3add3f32fbce8bbb?fccid=687426a3fd978325&vjs=3,"Job detailsSalary$108,000 - $138,000 a yearJob TypeFull-timeQualificationsUS work authorization (Required)Associate (Preferred)Full Job DescriptionVyne Group is a Staffing and Recruitment Agency. We have a small and rapidly growing Healthcare Analytics Startup looking for a Data Engineer. This role is fully remote (even after Covid-19) and will have the chance to work with a young and exciting new company.Please only Green Card Holders and US Citizens.Company Overview:Our client is a company that is small but growing organization dedicated to delivering high-value healthcare analytics across the nation.Their mission is to put patients, providers, and their local communities in the driver’s seat of healthcare evidence generation by giving them the tools to participate more proactively in research with medical device and pharmaceutical companies. They operate a clinical data registry with 100+ leading hospital participants nationwide and are working to expand the impact of our platform in research.We offer equity and compensation as part of our compensation package.The Role:As a key early hire, you will report to our Director of Operations and be expected to assume a growing leadership role within our product organization.Your core responsibility will be leading the development and maintenance of our analytic product offerings. You will be responsible for creating and managing our Tableau reporting platform which delivers intuitive reports translating point-of-care data into actionable information that improves patient outcomes. You will prototype reports, solicit feedback from multiple stakeholders, monitor report utilization, and support educational needs related to interpreting the reports. In addition to driving value through our analytic products, you will support our data engineers to implement best in class data governance technologies and processes..Job Duties:Key Responsibility #1 – Lead healthcare analytics activities (50%)Lead all analytic activities with support from data architectures, data engineers, key business analysts, and project stakeholdersLead prototyping of reports and dashboards that support scorecards, predictive modeling, forecasting, data quality, statistical analysis, geo-mapping, and multi-dimensional analyticsConduct data exploration, solve data problems, execute algorithmic data processing using statistical programming tools or BI platformsDevelop and implement data quality metrics, administrative dashboards, and tools to monitor platform performanceResearch and prototype new visualization methodologies and toolsKey Responsibility #2 – Support data processing and data warehousing activities (20%)Support data mapping and program development to support the loading of clinical, claims, registry data, and other healthcare data into our data environmentsSupport the QAing of other team member’s work related to data activitiesKey Responsibility #3 – Maintain our Tableau environments (20%)Maintain our production, development, and staging environments including system upgrades, patches, and APIsManage users, roles, and permissionsKey Responsibility #4 – Support our customers report interpretation and utilization (10%)Support customer calls to explain reports and support custom report developmentDevelop report training materials and conduct report training sessionsRequired For This Role: 5+ years in an analytics role; experience with healthcare data strongly preferredIntermediate to advanced experience with Tableau Dashboard and Report Development (will need to become proficient in Tableau if not already)Intermediate to advanced experience in Tableau Administration, Security, and LoggingExperience with ETL/ELT developmentAdvanced SQL experienceAbility to operationalize and build repeatable workflows through scripting, streamlining, and automating data-related processesAbility to work in a fast-paced environment with shifting prioritiesDemonstrated impact in building data analytic capabilities within an organizationDemonstrated leadership experience needed to lead innovation initiatives and contribute to growth within a small companyFamiliarity with AWS, S3, EC2, RDSExperience working with Git, or other version control systemExperience with architecting reliable and scalable data structuresExperience designing Data Profiling and Data Governance measuresDesired, but not Required:Knowledge of HIPAA and PHI regulationsExperience with real-world healthcare data terminologies (ICD-10, LOINC, SNOMED, CPT, etc) and clinical, IT, and financial systemsJob Type: Full-timePay: $108,000.00 - $138,000.00 per yearBenefits:Paid time offParental leaveSchedule:Monday to FridayCOVID-19 considerations:This company is entirely remote and the interview process will be as well.Education:Associate (Preferred)Work Location:Fully RemoteVisa Sponsorship Potentially Available:No: Not providing sponsorship for this jobThis Job Is Ideal for Someone Who Is:Dependable -- more reliable than spontaneousPeople-oriented -- enjoys interacting with people and working on group projectsAchievement-oriented -- enjoys taking on challenges, even if they might failThis Company Describes Its Culture as:Innovative -- innovative and risk-takingTeam-oriented -- cooperative and collaborativeCOVID-19 Precaution(s):Remote interview processVirtual meetings",New York NY,Data Engineer - Healthcare Industry & Remote
Baker Hughes,/rc/clk?jk=015fdc18ee832843&fccid=4819693c833d5b6e&vjs=3,"Job Description | Millennium Pharmaceuticals, Inc. is seeking a Data Engineer in Cambridge, MA with the following requirements: Bachelors degree in Computer Science or related field or foreign equivalent degree. Masters level project/coursework in the Computer Science field covering the below required skills. Required skills: author and implement Python, and Java languages to achieve a programmatic or analytic result; author and Implement Spark at scale to achieve a programmatic or analytic result; use Application of Machine Learning and / or Deep Learning to find hidden information/patterns in data; Apply data wrangling, manipulation and management of technologies to create structured datasets accessible through databases for further analysis using Deep and Machine Learning techniques. | Apply on-line at www.takedajobs.com and search for Req #R0027567. | Locations | Boston, MA | Worker Type | Employee | Worker Sub-Type | Regular | Time Type | Full time |  |  | Job ID R0027567",Boston MA,Data Engineer
Hudson River Trading,/rc/clk?jk=6bf49ddd76de01a4&fccid=402f23c7b01ca527&vjs=3,"Job Summary: | The primary purpose of this role is to assist with the translation of business requirements and functional specifications into modules and Data or Platform solutions. This includes assisting with the implementation and maintenance of business data solutions to ensure successful deployment of released applications. This role participates in all software development lifecycle phases and is critical to supporting software testing activities. |  |  | Key Responsibilities: | Assists with translating business requirements and specifications into modules and Data or Platform solutions with guidance from senior colleagues; provides insight into recommendations for technical solutions that meet design and functional needs | Supports the development, configuration, or modification of integrated business and/or enterprise application solutions within various computing environments by leveraging various software development methodologies and programming languages | Assists in the implementation and maintenance of business data solutions to ensure successful deployment of released applications with guidance from senior colleagues as appropriate | Supports systems integration testing (SIT) and user acceptance testing (UAT) with guidance from senior colleagues to ensure quality software deployment | Supports in all software development end-to-end product lifecycle phases by applying an understanding of company methodologies, policies, standards, and controls | Understands Computer Science and/or Computer Engineering fundamentals. Learning software architecture; actively seeks knowledge and applies to data solutions or platform applications | Drives the adoption of new technologies by researching innovative technical trends and developments | Solves technical problems; solutions may need refinement and/or feedback from more senior level engineers |  | Minimum Qualifications: | Bachelor\'s degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field) |  | Preferred Qualifications: | In most cases Lowe’s will not be able to provide sponsorship for roles located in the Tech Hub | Master\'s degree in Computer Science, CIS, or related field | 1 year of experience in software development or a related field | 1 year of experience developing and implementing business systems within an organization | 1 year of experience working with defect or incident tracking software | 1 year of experience writing technical documentation in a software development environment | 1 year of experience with Web Services | Experience with application and integration middleware | Experience with database technologies | 1 year of experience in Hadoop, NO-SQL, RDBMS, Teradata, MicroStrategy or any Cloud Bigdata components |  | About Lowe’s: | Lowe’s Companies, Inc. (NYSE: LOW) is a FORTUNE® 50 home improvement company serving approximately 18 million customers a week in the United States and Canada. With fiscal year 2019 sales of $72.1 billion, Lowe’s and its related businesses operate or service more than 2,200 home improvement and hardware stores and employ approximately 300,000 associates. Based in Mooresville, N.C., Lowe’s supports its hometown Charlotte region and all communities it serves through programs focused on creating safe, affordable housing and helping to develop the next generation of skilled trade experts. For more information, visit Lowes.com. |  | About Lowe’s in the Community: | As a FORTUNE® 50 home improvement company, Lowe’s is committed to creating safe, affordable housing and helping to develop the next generation of skilled trade experts through nonprofit partnerships. Across every community we serve, Lowe’s associates donate their time and expertise through the Lowe’s Heroes volunteer program. For the latest news, visit Newsroom.Lowes.com or follow @LowesMedia on Twitter. |  | Lowe’s is an equal opportunity employer and administers all personnel practices without regard to race, color, religious creed, sex, gender, age, ancestry, national origin, mental or physical disability or medical condition, sexual orientation, gender identity or expression, marital status, military or veteran status, genetic information, or any other category protected under federal, state, or local law.",Charlotte NC 28202,Associate Data Engineer
Associated Bank,/rc/clk?jk=39be6b14acd7d0a8&fccid=03925754e1730576&vjs=3,"Invitae is a healthcare technology company that leverages genetic information to empower doctors and patients to make informed medical decisions. Our software engineers work on a variety of projects ranging from innovations in healthcare systems to taming the chaos of biology. We're constantly improving our tools and technologies to deliver the highest quality actionable information for the patient. |  | Our Data Infrastructure Team develops the data ingestion pipelines and data platform architecture that supports the analytical and reporting needs of internal business stakeholders, data scientists, and our machine learning team. This is a hands-on role. |  | What you will do: |  | Understand our complex data ecosystem and build ETL solutions | Develop a real-time streaming infrastructure that supports critical business functions | Design and develop tools to enable teams to consume and understand data faster | Collaborate with multiple teams and own solutions from end-to-end |  | We look for engineers who: |  | Are self-starters and can work towards a larger goal with minimal guidance | Prior experience utilizing data warehousing or building out data warehouses | Have at least 3 years of hands-on experience working with large datasets, pipelines, and warehouses | Have a focus on high-quality code, including automated testing and coding best practices | Have experience with messaging/queuing systems or stream processing systems | Have architected distributed systems with infrastructure automation, monitoring and alerting |  | At Invitae, we value diversity and provide equal employment opportunities (EEO) to all employees and applicants without regard to race, color, religion, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance.",New York NY,Data Engineer
Lowe's,/rc/clk?jk=1456c2013aa44fd8&fccid=c0386215ca040126&vjs=3,"Associated Bank is an equal opportunity employer committed to creating a diverse workforce. We support a work environment where colleagues are respected and given the opportunity to perform to their fullest potential. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Applicants with a disability who need assistance applying for a position with Associated Bank are asked to email: careers@associatedbank.com | Job Summary |  | The Data Engineer I is responsible for ingesting, transforming, and preparing data for consumption by our various lines of business. This will include designing basic data solutions, working with business lines to understand needs, documenting, building, and testing the solutions. This will require ETL (Extract, Transform, Load) development and data analysis skills using SQL and/or scripting tools. Incumbent will collaborate with other teams in Operations &amp; Technology, including Software Engineering, Enterprise Architecture, and Infrastructure teams as well as our support teams. Solutions developed will adhere to best practices and standards provided by the Data Architects on our team. The role will ensure proper handoffs to the DataOps team to ensure proper support and maintainability, as well as be available for support remediation and troubleshooting. Position will also work with the lines of business on reporting and data visualizations in addition to collaborating and learning more advanced analytics capabilities of Artificial Intelligence (AI) and Machine Learning. | Job Accountabilities | Ingest, transform, and prepare data for consumption by our various lines of business. | Work with the lines of business on reporting, data visualizations, and overall data consumption. | Collaborate and learn more advanced analytics capabilities of Artificial Intelligence (AI) and Machine Learning. | Collaborate with other teams in Operations &amp; Technology, including Software Engineering, Enterprise Architecture, and Infrastructure teams as well as our support teams. | Coordinate handoffs to DataOps team to ensure proper support and maintainability, as well as be available for support remediation and troubleshooting. | Data Engineer I | Education | Bachelor's Degree Mathematics, Computer Science, MIS, Analytics, or related. Preferred | Experience is Required | Less than 2 years ETL/ELT experience (SSIS, Informatica, DataStage, etc). Experience working with basic structured, semi-structured, and unstructured data. Basic SQL knowledge proficiency and analytical skills. Required | Less than 2 years Exposure to modern data/analytics architecture (Big Data, Cloud, etc.). Exposure to advanced analytics tools (Python, R, etc) Preferred | Data Engineer II | Education | Bachelor's Degree Mathematics, Computer Science, MIS, Analytics, or related. Required | Experience is Required | 2 – 5 Years - ETL/ELT experience (SSIS, Informatica, DataStage, etc). Experience working with basic structured, semi-structured, and unstructured data. Basic SQL knowledge proficiency and analytical skills. Exposure to modern data/analytics architecture (Big Data, Cloud, etc.). Exposure to advanced analytics tools (Python, R, etc) | Compliance Statement |  | Fully complies with all applicable enterprise policies and procedures. Acts in compliance with all applicable laws and regulations as outlined in training materials, including but not limited to Bank Secrecy Act. Responsible for reporting suspicious activity to Financial Intelligence. Responsible to report all customer complaints as prescribed and procedure violations to management or HR. Responsible to report ethical concerns as needed to Associated’s anonymous Ethics Hotline. | Associated Bank is committed to working diligently with any colleague who needs an accommodation perform the essential functions of the job. Please contact the Leaves &amp; Accommodations office to request an accommodation.",Milwaukee WI,Data Engineer I or II - Can Be Remote Based On Home State
1010data,/rc/clk?jk=db7c25558b6f7f9d&fccid=76670ee1307f6d9c&vjs=3,"Job detailsSalary$110,000 - $115,000 a yearFull Job DescriptionThe Position: | Do you dream about creating a more sustainable future? At Uplight, we are motivating energy users and providers to accelerate the clean energy ecosystem. Working with over 75 of the world’s leading electric and gas utilities, Uplight provides an end-to-end customer energy experience. Uplight delivers personalized experiences that customers have now come to expect–improving satisfaction, increasing revenue, reducing the cost to serve, and contributing to carbon reduction goals. We are now B Corp certified, enabling us to put our values into action by not only making decisions for the benefit of our shareholders, but also for our customers, environment, employees, and community. | We are seeking a Data Engineer to join our team and help us achieve our ambitious goals for our business and for the planet. |  What you get to do: | Work as an Engineer on our analytics engineering team, primarily developing in Python and leveraging a wide range of technologies, notably: AWS and GCP, Docker, Apache Airflow, Apache Spark, and PostgreSQL | Take problems from inception all the way to completion - own the building, testing, deployment, and maintenance of the code that you work on | Tackle complex problems that span a wide range of technical abilities, including: | Developing data pipelines to transform and process data between systems | Productionize machine learning pipelines leveraging billions of rows of data | Scaling our software to handle the ever-growing customer data | Work effectively on an Agile team and collaborate well with your other team members. | Build robust and well documented processes to facilitate data triage and associated fixes | Save the planet | Skills and experience are necessary, but we hire on value alignment first, so if you feel you would be a good fit with us, still consider applying. | What you bring to Uplight: | A minimum of 3 years of professional experience developing in a modern programming language (Python preferred) | Solid knowledge of ETL and data integration | A value for testing and developing quality software | Strong critical thinking skills and a desire to work with ambiguous challenges | Experience working in an Agile environment and a strong understanding of the full SDLC | Strong troubleshooting skills that span the full-stack (front-end clients, APIs, networking, DNS, Linux, containers, databases, distributed systems, etc.) | Experience deploying production applications on at least one major cloud provider (AWS, GCP, Azure) | Experience writing and maintaining data pipelines and ETLs leveraging Spark or similar tooling | Bonus Points: | Experience in the utility industry | Experience working cross-functionally with design, product, customer success, sales, etc. | Deep technical knowledge of Python, AWS/GCP, Docker, and/or PostgreSQL. | What makes working at Uplight amazing: | In addition to all the standard medical and dental benefits, that kick in Day 1, we: | Are proud to be over 400+ rebels with an important cause by helping to create a more sustainable planet. | Are committed to the environment, our employees, and our communities. | Are focused on career growth by following defined career ladders | Take our work and mission seriously and….we love to laugh! | We also: | Provide a 401k Match | Have an innovative flexible time off policy | Keep you energized with plenty of food and drink | Salary range 110 - 115K/Yr. | Uplight provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",Boston MA,Data Engineer
Spotify,/rc/clk?jk=c8b705530a468ff4&fccid=b44c7339866c4605&vjs=3,"Working with William Hill, you will be at the heart of the technological revolution with one of the world's most trusted betting and gaming companies. William Hill deals with projects ranging from desktop or mobile casinos and betting sites, to name a few. We process 500 online Sportsbook bets per second each Saturday, that's the same as the number of orders processed by Amazon UK, on its busiest day of the year. We deal with more than 20 million users daily; impressed? You can be sure there are many more challenges waiting for you. |  | When we say cutting edge, we mean it. Here, you can work on highly reliable systems with low latency, much like the transactional systems of the best financial institutions, but…with the fun included. |  | You will have access to development opportunities, including IT conferences, internal training, and lunch and learn sessions. You will be part of a great working atmosphere, performing complex work in a collaborative team of amazing people, with forward-thinking managers. You will have the opportunity to make an impact. |  | What You Will Do: |  | Assist in building a world-class Big Data platform which will give us power to process streams of data , as well as, enable machine learning and advanced analytics capabilities. Everything cloud-based for scalability and speed to market. | Handle large volumes of data and integrate our platform with a range of internal and external systems. | Understand new tech and how it can be applied to data management. | Work with an agile team alongside business, testers, architects and project managers. | Focus on the development of complex logic integrations | Maintain and evaluate quality of documentation, code, and business logic and non-functional. | Keep NFRs as priority by maintaining code, supporting, restoring, monitoring and performance for any delivery. |  | If you join us as a Senior, you will be a mentor to a team, and will need to bring some previous experience of this. |  | What You Need: |  | Highly experienced in writing well designed, testable, efficient code which follows good coding standards | You're an expert in development and data quality | You're a strong developer, and you have expertise in SQL to handle large data sets and complex data transformations. | You have experience with Python | It's essential that you've got experience of the full SDLC in an equivalent environment. | You have worked in small focused scrum teams delivering events driven integrations across multiple teams. | You're experienced in working within an integration environment with testers to ensure end to end performance and resilience SLA's can be achieved. | Agile mindset and practice in software development process e.g. Scrum, Kanban, TDD, BDD | Experience with Big data platforms: Apache Spark / Hadoop and Scala is a plus | Experience with cloud solutions for Big Data (Snowflake, GCP BigQuery, AWS Redshift) | Experience with data pipelines (e.g. Airflow) and streaming processing (Kafka, Kinesis, Spark Streaming, Flink) is a plus | Experience with supporting Data Scientist (Machine Learning) is a plus | You have experience mentoring other engineers |  |  | William Hill provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, creed, national origin, ancestry, sex, age, physical or mental disability, pregnancy, veteran or military status, genetic information, sexual orientation, gender identity or expression, marital status, civil union/domestic partnership status, familial status, domestic violence victim status, or any other legally recognized protected basis under federal, state or local laws. William Hill complies with applicable federal, state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. |  |  | Essential Functions/Exposures: |  | Must be able to sit for extended periods of time | Must be able to type and talk on the phone for extended periods of time |  |  | #LI - OG1",Jersey City NJ,Data Engineer
DEPT OF INFO TECH & TELECOMM,/rc/clk?jk=2f23570b42059be6&fccid=0efe3e25ee20de25&vjs=3,"Job detailsSalary$54,281 - $75,000 a yearFull Job DescriptionDEPT OF INFO TECH &amp; TELECOMM |  | Job Posting Notice | Job ID | 445669 | Business Title | Cyber Associate Data Engineer | Civil Service Title | Title Classification | Job Category | Technology, Data &amp; Innovation | Career Level | Experienced (non-manager) | Work Location | 80 Maiden Lane | Division/Work Unit | # of Positions | 2 | Title Code No | 13633 | Level | 01 | Proposed Salary Range | $ 54,281.00 - $ 75,000.00 (Annual) |  | Job Description |  | About New York City Cyber Command | New York City Cyber Command (NYC3) is committed to protecting City systems that provide vital services to New Yorkers from cyber threats, and helping residents become safer in their digital lives. |  |  | As the organization defending the largest municipality in the country, NYC3 is charged with directing citywide incident response, setting citywide cybersecurity policies and standards and working with city agencies to strengthen their cyber defenses. |  |  | Job Description | We are seeking a motivated, data focused, entry level Associate Data Engineer to join our Data Science Team. Our Data Science Team strives to make security data a strategic asset by providing a platform to structure, manage, integrate, control, analyze, and support threat management activities. As an entry level Software Engineer, you will help build a secure, scalable, and cloud native data processing frameworks that will support NYC3’s cybersecurity mission. Your duties will include: |  | Developing and maintaining our data pipeline using Apache Beam, Java, Python and other data processing technologies;Identifying and implementing performance improvements across all pipelines;Engaging with data consumers and producers in order to design appropriate models to suit all needs;Maintaining information exchanges through publish, subscribe, and alert functions that enable users to send and receive critical information as required;Supporting incident management, service-level management, change management, release management, continuity management, and availability management for databases and data management systems;Administering databases and/or data management systems that allow for the secure storage, query, protection, and utilization of data. | Minimum Qual Requirements |  | 1. A baccalaureate degree, from an accredited college including or supplemented by twenty-four (24) semester credits in cyber security, network security, computer science, computer programming, computer engineering, information technology, information science, information systems management, network administration, or a pertinent scientific, technical or related area; or |  |  | 2. A four-year high school diploma or its equivalent approved by a State’s department of education or a recognized accrediting organization and three years of satisfactory experience in any of the areas described in “1” above; or |  | 3. Education and/or experience equivalent to “1” or “2”, above. College education may be substituted for up to two years of the required experience in “2” above on the basis that sixty (60) semester credits from an accredited college is equated to one year of experience. In addition, twenty-four (24) credits from an accredited college or graduate school in cyber security, network security, computer science, computer programming, computer engineering, information technology, information science, information systems management, network administration, or a pertinent scientific, technical or related area; or a certificate of at least 625 hours in computer programming from an accredited technical school (post high school), may be substituted for one year of experience. | Preferred Skills |  | The preferred candidate should possess the following: | A bachelor’s degree in computer science or information systems with a specialization in mathematics, number; theory, applied cryptography, or statistics or relevant experience;Experience with the Agile Development Methodology;Expert knowledge in both Java and Python; | Familiarity with Unix scripting, Web development, and automated testing;Familiarity with machine learning techniques and machine learning toolkits such as R, scikit-learn, etc;Experience working with Terraform;Familiarity with the CI/CD process,At least one year professional, academic, or personal experience with software development or data engineering experience (includes internship experience);At least 1 year professional, academic, or personal experience with object-oriented/object function scripting languages; preferably java or python;Familiarity with or exposure to cloud application development;Familiarity with distributed data processing frameworks. | To Apply |  | Special Note: Taking and passing civil service exams are necessary to maintain employment with the City of New York. Please check the Department of Citywide Administrative Services (DCAS) website (http://www.nyc.gov/html/dcas/html/work/exam_monthly.shtml) for important exam filing information. Please ensure that you are either a permanent employee in the civil service title listed on this posting, or, that you file for the examination when there is an open filing period. For more information regarding the civil service process, please visit the DCAS website at: http://www.nyc.gov/html/dcas/html/work/work.shtml |  |  | Interested applicants with other civil service titles who meet the preferred requirements should also submit a resume for consideration | For City employees, please go to Employee Self Service (ESS), click on Recruiting Activities &gt; Careers, and search for Job ID #445669 | For all other applicants, please go to www.nyc.gov/jobs/search and search for Job ID #445669 |  | SUBMISSION OF A RESUME IS NOT A GUARANTEE THAT YOU WILL RECEIVE AN INTERVIEW | APPOINTMENTS ARE SUBJECT TO OVERSIGHT APPROVAL |  | Department of Information Technology &amp; Telecommunications and the City of New York are equal opportunity employers. |  | DoITT participates in E-Verify | Hours/Shift |  | Day - Due to the necessary technical support duties of this position in a 24/7 operation, candidate may be required to work various shifts such as weekends and/or nights/evenings. | Work Location |  | New York, NY | Residency Requirement |  | New York City residency is generally required within 90 days of appointment. However, City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau, Suffolk, Putnam, Westchester, Rockland, or Orange County. To determine if the residency requirement applies to you, please discuss with the agency representative at the time of interview. | POSTING DATE | 09/22/2020 | POST UNTIL | Until Filled |  | The City of New York is an Equal Opportunity Employer",Manhattan NY 10005,Cyber Associate Data Engineer
Google,/rc/clk?jk=6c63c5b51bac3c03&fccid=7caa90d0c4a3687e&vjs=3,"Job detailsJob TypeFull-timeTemporaryFull Job DescriptionOTC Markets Group Inc., operator of premier US financial marketplaces, is seeking a passionate and dedicated Spark/Scala/Kafka Data Engineer with three to five years of experience to join our Cloud Data Reporting team based in our New York, NY office. Our team is responsible for building the next generation real-time applications which include but are not limited to internet and internal websites, real-time application tools and various internal processes. We utilize a home-grown Spark based platform to execute monitoring, surveillance analytics and data mining on the data sets. We are looking for a talented Data Engineer to work in the Data Reporting Team building and scaling the next generation data platform. As a member of the team you will work in an intimate, collaborative environment along with other Engineers to create systems that organize, analyze, and maintain the data for the firm. | You will participate in the continuous advancement of the data infrastructure in our company and support the large data sets required to expand our product lines to provide the best (real-time) information possible. In addition, you will collaborate with a stellar engineering team to ensure our support process is more efficient, get projects done in a timely manner and work as a team to be an integral part in the success of the company. If you are up to the challenge and are comfortable multi-tasking while building relationships across the organization then this is an exciting and amazing opportunity a for you to be part of a team which will continue to embark on both new technologies and our cloud initiative. Our clients include domestic and international companies, both public and private. | OTC Markets Group’s newly redesigned headquarters foster an open and collaborative environment for employees and clients, utilizing more than 33,000 square feet of space. Our NY office is located in Brookfield Place, in the heart of the financial district, surrounded by Hudson River Park and home to luxury retail and dining establishments, featuring extensive public space and world-class amenities. | Our Core Values are incorporated in each aspect of our Company. We encourage autonomy, professionally passionate discussions of opposing viewpoints, creativity and transparency. We are Open, Transparent, and Connected. We are OTC strong. At OTC Markets, we all win together. | We invest heavily in employee satisfaction and offer all our employees a highly competitive compensation package. As a dynamic, growing company that fosters an open culture, we emphasize autonomy, responsibility, innovation and self-discipline. We are looking for someone who wants to make an impressive impact at a company known for its reputation on quality and achievement. | The Data Engineer will be provided with the tools needed for success including a team of passionate individuals who are there to support your achievements and celebrate the wins! | Please note, due to COVID-19, OTC Markets is currently operating in a remote work environment with the voluntary option to work in-office a few days a week. This is a temporary adjustment to the start date work location with phase-in updates provided by the Human Resources team. | Our successful candidate will: | Code and test reliable and resilient real-time Spark Applications. | Learn our architecture and start to contribute immediately as we value and encourage brainstorming and input from everyone on the team. | Participate in all phases of the SDLC including but not limited to architecture, technical design and documentation, testing, implementation and product launch. | Write unit/integration tests as part of the development initiative providing great test coverage which will enable continuous delivery of code. | Implement readable, maintainable, and highly performant Scala/Python code. | Use many of the available AWS services to build applications. | Create and maintain technical documentation and architecture diagrams. | Support projects through their entire lifecycle from analysis to production rollout. | Plan and coordinate project schedules, goals, and milestones. | Collaborate with the business stake holders on the feature set specifications. | Collaborate with team members on the implementation and planning. | If you are excited to code in Scala/Python with Spark, Kafka and AWS services, deliver projects from start to finish, and most of all work in a fantastic work environment, then review the requirements below, and see if you are a good fit. We will provide any required training and mentoring to bring you to the next level. We are passionate and use the best of breed new technology. We are looking for engineers who are not afraid to ask questions and reach out to the internal teams and external message boards for help. You will write and own the microservices to support both the business lines and the technical implementation. As you grow in the company you will mentor new team members. | Requirements: | Data Engineering - 3-5 years’ experience with AWS, Scala, Spark, Kafka, S3, Python | Spark streaming and optimization | You have experience with continuous integration/deployment and Scala build tool. | Good Debugging Skills, excellent troubleshooting skills | Must be self-motivated and willing to learn. You love to stay on top of new technology and share with others. | Good knowledge of SQL. The ability to recognize when you require input from the DBA team. | Excellent verbal and written communication skills with employees both onsite and in remote locations. | Proven ability to work with individuals at all organization levels. | Ability to work on concurrent projects, when required. | Ability to develop a detailed project plan when working on a project. | Support production issues when required. | Experience in agile project development. | Must know how to use GIT. | Strong Linux knowledge. | Nice to haves: | You have experience with key AWS services, such as EC2, RDS (Postgres, Aurora), Lambda, Athena | Experience working with equity financial data (Quotes, Trades, Company Information) | You have worked with large data sets. | You have worked in the Financial Markets. | Please note, we will neither sponsor nor relocate for this position. | What OTC Markets offers its Team Members (why you should choose us): | Generous vacation policy in addition to 9 annual holidays observed and Summer Fridays. | Snacks and sodas and a very chic coffee bar. | Annual bonus and stock incentive program. | Office refreshments and company happy hours. | Monday Bagels and Friday Pizza. | Life and disability insurance, including paid parental leave. | Health insurance plans designed to meet the various coverage needs and preferences (Medical, Dental, Vision). | Flexible Spending Accounts for health, transit, parking and dependent care, as well as Healthcare Savings. | Accounts for qualifying plans. | Come as you are and just be you. We are an equal opportunity and e-verify employer and prohibit discrimination and harassment of any kind. All employment decisions are based on business needs, job requirements, and individual qualifications, without regard to race, creed, color, religion, gender, national origin, age, marital status, political belief, physical or mental disability, sexual orientation, military or veteran status, genetic information, family or parental status, gender identity, pregnancy, including childbirth or related medical condition, or any other characteristic protected by federal, state, or local law. We encourage applicants of all ages and backgrounds. | OTC Markets Group Inc. (OTCQX: OTCM) operates Open, Transparent and Connected financial markets for 10,000 U.S. and global securities. Through our OTC Link® ATS, we directly link a diverse network of broker-dealers that provide liquidity and execution services for a wide spectrum of securities. We organize these securities into markets to inform investors of opportunities and risks: the OTCQX® Best Market; the OTCQB® Venture Market; and the OTC Pink® Open Market. Our data-driven platform enables investors to easily trade through the broker of their choice at the best possible price and empowers a broad range of companies to improve the quality and availability of information for their investors. To learn more about how we create better informed and more efficient financial markets, visit www.otcmarkets.com. | OTC Link ATS is operated by OTC Link LLC, member FINRA/SIPC and SEC regulated ATS. | Applicants have rights under the federal law: | Equal Employment Opportunity is the Law | Polygraph Protection Act | FMLA | Please no calls or 3rd party recruiter submissions. | Powered by JazzHR | dJemj99LQb",New York NY 10282,Data Engineer
Mount Sinai,/company/iShift-LLC/jobs/Senior-Data-Engineer-c07ae1fa3026a544?fccid=49fa710de2b8290d&vjs=3,"Job detailsSalary$60 - $79 an hourJob TypeFull-timeContractQualificationsData Engineering, Warehousing, or Data Science: 6 years (Required)Python: 2 years (Required)Azure DevOps: 2 years (Required)US work authorization (Required)Bachelor's (Preferred)Apache Spark: 2 years (Preferred)Apache Airflow: 2 years (Preferred)Apache Kafka or Hadoop: 2 years (Preferred)Full Job DescriptionPython DeveloperAbout iShift: iShift is a Consulting and Managed Services firm offering clients advice and solutions in the area of Network, Data Center, and Cloud technologies. iShift is recruiting for multiple full-time contract-to-hire Sr. Data Engineers with one of our financial services clients.We are seeking a Senior Data Engineer to help build data infrastructure and data pipelines that power our client's business data. The ideal candidate is expected to operate within a distributed, agile, cross-functional environment. This is an opportunity to create an enterprise-wide impact by providing normalized data to all stakeholders and downstream systems. The candidate is responsible for the ETL/ELT processes, following architecture guidelines, reliability, accuracy, monitoring, and infrastructure surrounding internal data processing.Roles and Responsibilities: Design implement and champion the tooling required for data ingestion, transformation, and orchestration.Collaborate with Applications and Security TeamsWork with Data Analysts, Business SME’s in data modeling effortsWork with Security/Infrastructure Architects and Data Engineers to implement security policiesCollaborate with Application teams to ingest change data streamsPerform code review and debug in case of process failures or data discrepanciesMonitor application performance and operationsQualifications: 6+ years of experience in Data Engineering, Data Warehousing and/or Data scienceHands-on experience building big data pipelines using Python, Apache Spark, Apache AirflowCommand in big data tech stack – Apache Kafka, HadoopExperience working in Azure environmentExperience with Azure DevOps is requiredExperience with delta lake is a plusExperience with SQL Server is a plusStrong communication and analytical and problem-solving skillsContract: 6-month Contract-to-hire opportunityLocation: Phoenix preferred, Remote is an option.Job Types: Full-time, ContractPay: $60.00 - $79.00 per hourEducation:Bachelor's (Preferred)Experience:Data Engineering, Warehousing, or Data Science: 6 years (Required)Python: 2 years (Required)Apache Spark: 2 years (Preferred)Apache Airflow: 2 years (Preferred)Apache Kafka or Hadoop: 2 years (Preferred)Azure DevOps: 2 years (Required)Contract Renewal:PossibleFull Time Opportunity:YesWork Location:One locationCompany's website:www.ishift.netBenefit Conditions:Only full-time employees eligibleWork Remotely:YesCOVID-19 Precaution(s):Remote interview processVirtual meetings",Phoenix AZ 85012,Sr. Data Engineer
Oracle,/rc/clk?jk=907689a62487ca55&fccid=fe404d18bb9eef1e&vjs=3,"Engineering | Backend | We are looking for a Senior Full-stack Engineer to join a band within the organization responsible for business critical data and infrastructure for financial modeling. Our mission is to use data and analytics engineering to turn billions of data points into insights and datasets that power key business decisions at Spotify. | Location | New York, NY | Job type | Permanent | As a Senior Fullstack Engineer in D&amp;M (Data &amp; Metrics product area) you will join a team of data engineers responsible for maintaining and evolving some of the most critical datasets and financial data infrastructure in the company and much more. As a full-stack engineer will design and implement visualization and full-stack applications that will empower Business Users and Data Analysts to perform financial modeling and planning at scale with requirements for insights, availability and quality. | Data and information has become an invaluable asset for technology companies that focus on innovation and are dedicated to providing a great user experience. Spotify is taking this concept to heart. The volume and breadth of data at Spotify is staggering – trillions of records of different interactions flow through our platform on a daily basis. This squad and its product area are at the center of this initiative. This will enable us to partner with business and engineering teams to prioritize requests and consistently deliver value against time-sensitive priorities. | Are you excited to grow your technical skills and work directly with stakeholders, product managers, and engineers to gain impact? Then this is a great opportunity to join the band! | What you'll do | Be an impactful contributor to the team by planning, designing and building elegant solutions for both Business Users, Data Scientists and other Engineers | Work on our web and backend services | Work in a transparent environment that supports individual growth | Getting insights and briefly work on Data related challenges using Google Cloud Platform | Work hand-in-hand with stakeholders to understand and visualize our business performance and ultimately help shape the future of Spotify’s on a global scale. | Innovate our data products to create a single coherent platform to serve our stakeholders | Work in a supportive team that offers engineers the flexibility to be creative | Work closely with others to understand, document, troubleshoot and analyze requirements for complex fullstack solutions | Lead and mentor engineers as we grow the bigger team | Who you are | Know how to build user-friendly, data-rich JavaScript/HTML/CSS/Typescript applications | A degree in Computer Science or a similar area are bonuses but not mandatory | Have 5+ years of experience in the development of high-quality fullstack and interface solutions | Have excellent analytical and problem-solving skills and can communicate ideas | Value team success over personal success | Previously worked with Business Users and Data Scientists is a plus | Capable of tackling very loosely defined problems and thrive when working on a team which has autonomy in their day to day decisions | A self-motivated individual contributor and great teammate with the ability to multitask, prioritize and communicate progress in a rapidly changing environment. | Like to build skills to further enhance the t-shape within analytics and data engineering | A communicative person that values building strong relationships with colleagues and multiple stakeholders, and have the ability to explain complex topics in simple terms | Perks of being in the band | Extensive learning opportunities, through our dedicated team, GreenHouse. | Flexible share incentives letting you choose how you share in our success. | Global parental leave, six months off - fully paid - for all new parents. | All The Feels, our employee assistance program and self-care hub. | Flexible public holidays, swap days off according to your values and beliefs. | Spotify On Tour, join your colleagues on trips to industry festivals and events. | Learn about life at Spotify | You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens. | Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service with a community of more than 345 million users.",New York NY,Senior Fullstack Engineer - Data & Metrics
Goldman Sachs,/rc/clk?jk=8fa2ecd380a39253&fccid=b5399d79c48f2808&vjs=3,"Data Engineer | INTURN is the first marketplace to help brands efficiently sell their excess inventory to retailers. We are a venture backed NYC start-up addressing a complex section of the retail industry. |  Job summary | You have the opportunity to collaborate with application engineers, data analysts, and product managers to store, understand, and allow reporting on INTURN’s data. We seek to manage a collection of data and warehousing. Successful candidates will help INTURN manage the brokerage of millions of dollars of merchandise and complexities of a rapidly growing business. INTURN’s analytics team is looking for someone to support data visualization and reporting tools for internal users. | You may be the one to join an awesome team and be part of iterative development, from compiling, vetting specifications, schema design through integration, testing and deployment If you are passionate about building a product that is solving a real-world problem that buyers and sellers will use every day. | INTURN is looking for a self-motivated, detail oriented, individual with strong problem solving skills who can think outside of the box. |  Key Qualifications | Strong CS skills with 3+ years in professional data-oriented engineering experience. | Expert in at least of one or more of the following languages PHP/Ruby/Python/Java | Proven SQL and scripting skills. | Experience with AWS, EC2, Linux command-line. | Willingness to learn and try to technologies | Experience with non-relational and/or column-oriented data stores | Comfort working with large data sets. | Familiarity with Amazon Web Services (AWS) | Love data! Charting, recording, exploring, and finding the underlying causes |  Bonus | BA in Computer Science or related field | Experience with CSS Preprocessors (SaSS or less) | Familiarity with R | Previous startup experience |  Perks | Benefits include Premium health benefits (including health,vision, dental), competitive salaries, equity, and a chance to do cutting-edge work with a great team",New York NY 10011,Data Engineer
Uplight,/rc/clk?jk=9c3e58427f0a8299&fccid=a5b4499d9e91a5c6&vjs=3,"Note: By applying to this position your application is automatically submitted to the following locations: New York, NY, USA; Cambridge, MA, USA; Chicago, IL, USA | Minimum qualifications: |  | Bachelor's degree or equivalent practical experience. | Experience with traditional Analytic Warehouse solutions, Big Data technologies, Real Time Streaming, performance, and scalability optimizations. | Experience in implementing analytics systems architecture (e.g. data migration, streaming, security, metadata management, visualization tools). | Experience with cloud computing including cloud market and competitive dynamics. Experience delivering technical presentations. |  | Preferred qualifications: |  | Technical sales experience or professional consulting experience in the fields of cloud computing, data, information lifecycle management, and Big Data. | Experience with developing data warehousing, data lakes, batch or real-time event processing and ETL workflows solutions (i.e. Informatica, Talend, Alooma, SAP, Data Services) which could include architecture design, implementing, tuning, schema design, and query optimization of scalable and distributed systems. | Experience in writing code in a common development language such as Java, Python, JavaScript, C++, Scala, R, or Go. | Understanding of DNS, TCP, Firewalls, Proxy Servers, Load Balancing, VPN and VPC, and working knowledge of Linux. | About the job | The Google Cloud Platform team helps customers transform and build what's next for their business — all with technology built in the cloud. Our products are engineered for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. Our teams are dedicated to helping our customers — developers, small and large businesses, educational institutions and government agencies — see the benefits of our technology come to life. As part of an entrepreneurial team in this rapidly growing business, you will play a key role in understanding the needs of our customers and help shape the future of businesses of all sizes use technology to connect with customers, employees and partners. |  | As a Sales Engineer for Financial Services, you will engage with accounts and channel partners to recognize and assess customer requirements, prepare and present demonstrations of Google’s Cloud technologies, and address and overcome technical objections that arise throughout the process. | You’ll develop new relationships with business unit leaders to more deeply understand their unique company and industry challenges to influence their perspective of Google Cloud solutions. You will do this with deep industry knowledge so that you will be able to engage in communicating business value, while ultimately driving shareholder value. You will showcase the innovative nature of our products and solutions to make organizations more productive, collaborative, and mobile. | Google Cloud provides organizations with leading infrastructure, platform capabilities and industry solutions. We deliver enterprise-grade cloud solutions that leverage Google’s cutting-edge technology to help companies operate more efficiently and adapt to changing needs, giving customers a foundation for the future. Customers in more than 150 countries turn to Google Cloud as their trusted partner to solve their most critical business problems. | Responsibilities | Support local teams in recognizing key business opportunities, engage customers to address aspects of the data lifecycle. | Identify business and technical requirements, conduct full technical discovery, and architect customer solutions to meet gathered requirements. | Work hands-on with Google Cloud Platform products to demonstrate and prototype integrations in customer/partner environments. Travel as needed for meetings, technical reviews, and onsite delivery activities. | Prepare and deliver product messaging in an effort to highlight the Google Cloud Platform value proposition, using techniques that include whiteboard and slide presentations, product demonstrations, white papers, and RFI response documents. | Be responsible for leading technical projects, including technology advocacy, product and solution briefings, proof-of-concept work, and the coordination of additional technical resources. | Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form.",New York NY,Sales Engineer Data Analytics Financial Services Google Cloud
William Hill US,/rc/clk?jk=15fa664d17748c67&fccid=c007936ceb766fe5&vjs=3,"Strength Through Diversity | Ground breaking science. Advancing medicine. Healing made personal. | Clinical Innovation - Req 2676412 |  | Data Engineer |  | The Mount Sinai Health System | Do you have what it takes to wear the badge? | The Mount Sinai Health System’s commitment to excellence extends beyond delivering world-class health care. The System’s ongoing success is dependent upon our highly motivated, nonclinical professionals working to improve business operations. Our leadership team is driven to provide exceptional service by cultivating a workforce that is dedicated to upholding Mount Sinai’s mission of delivering innovative, breakthrough medicine with compassion and integrity. | Are You Ready? | Are you ready to discover the limitless possibilities of big data analytics and machine learning on improving the patient’s care? Are you interested in learning and developing machine learning data product? | Duties and Responsibilities: | The Data Engineer will work on learning and creating batch and streaming machine learning pipeline for accelerating translational research and improving clinical care. Day to day responsibilities include: | Create machine learning pipeline using big-data technology stacks | Work on Python, Apache Spark, Kafka, MongoDB, and Machine Learning | Additional responsibilities include developing prototypes and proof of concepts for the selected use cases, and implementing complex machine learning pipeline | Basic Qualification: | Bachelor's degree in Engineering (e.g. Computer Science , Biomedical Informatics, Physics, Statistics) | Minimum of 2 years job experience in related field | Proficient with at least 1 programming languages among Scala/Python/Java/C/C++. Must be flexible and fast to pick up new languages. | Proficient on Big Data Technology Stacks like Apache Spark, Apache Kafka, NoSQL, and Machine Learning. | Knowledge of Supervised and Un-supervised Learning | Knowledge of deploying and maintain the Machine Learning Software in production environment. | Preferred Qualification: | Hands-on experiences on Apache Spark, Kafka, MongoDB, Apache Nifi and other big data technology stacks and streaming tools. | Familiarity with and the ability to leverage a wide variety of open source technologies and tools. | Knowledge of cloud architecture and implementation on Azure or AWS is a big plus. | Advanced Knowledge of Machine Learning | Strength Through Diversity |  |  | The Mount Sinai Health System believes that diversity and inclusion is a driver for excellence. We share a common devotion to delivering exceptional patient care. Yet we’re as diverse as the city we call home- culturally, ethically, in outlook and lifestyle. When you join us, you become a part of Mount Sinai’s unrivaled record of achievement, education and advancement as we revolutionize healthcare delivery together. | We work hard to recruit and retain the best people, and to create a welcoming, nurturing work environment where you have the opportunity and support to develop professionally. We share the belief that all employees, regardless of job title or expertise, have an impact on quality patient care. | Explore more about this opportunity and how you can help us write a new chapter in our story! | Who We Are | Over 38,000 employees strong, the mission of the Mount Sinai Health System is to provide compassionate patient care with seamless coordination and to advance medicine through unrivaled education, research, and outreach in the many diverse communities we serve. | Formed in September 2013, The Mount Sinai Health System combines the excellence of the Icahn School of Medicine at Mount Sinai with seven premier hospitals, including Mount Sinai Beth Israel, Mount Sinai Brooklyn, The Mount Sinai Hospital, Mount Sinai Queens, Mount Sinai West (formerly Mount Sinai Roosevelt), Mount Sinai St. Luke’s, and New York Eye and Ear Infirmary of Mount Sinai. | The Mount Sinai Health System is an equal opportunity employer. We comply with applicable Federal civil rights laws and does not discriminate, exclude, or treat people differently on the basis of race, color, national origin, age, religion, disability, sex, sexual orientation, gender identity, or gender expression. | EOE Minorities/Women/Disabled/Veterans",New York NY 10029,Data Engineer
Disney Streaming Services,/company/SageBeans-RPO/jobs/Data-Engineer-1c62bfbfb4321ae2?fccid=19baeba96d94a9fc&vjs=3,"Job detailsSalary$110,000 - $120,000 a yearJob TypeFull-timeContractNumber of hires for this role5 to 10QualificationsExperience:designing and deploying ETL solutions on Azure, 2 years (Required)data engineer, software engineer, systems engineer, 5 years (Required)Python, 2 years (Required)RDBMS, 3 years (Required)Location:Bellevue, WA (Required)Full Job DescriptionLocal Candidates will be preferred.In this role, you will:Design, develop, and deploy infrastructure in support of data operations relating to advanced analytics systems.Work with business stakeholders to distill data requirements and to resolve data related issues.Work with data scientist to engineer data pipelines in support of machine learning features.Creation and maintenance of large, complex datasets and/or queries that meet business requirements.Support QA testing of data pipelines as well as comprehensive solutions.Responsible for leading project tasks in support of advancing pilot systems into a production environment.Strive to automate software deployments and maintenance through CI/CD and IaC.Research and incorporate new technologies to ensure the solutions are meeting or exceeding modern standards.Build effective monitoring systems to keep track of system health.Develop strong collaborative working relationships with our teams and our key stakeholders.Document Processes, Services and Environments.We’d love to hear from you if you have:5+ years’ experience as a data engineer, software engineer, systems engineer, or equivalent3+ years’ experience with RDBMS2+ years’ experience with PythonExperience designing and deploying ETL solutions on AzureExperience with Spark and/or Databricks and associated distributed analytics software packagesStrong written and verbal communication skills and experience in working effectively in cross functional teamsCritical decision making and communication skillsJob Types: Full-time, ContractSalary: $110,000.00 - $120,000.00 per yearExperience:designing and deploying ETL solutions on Azure: 2 years (Required)data engineer, software engineer, systems engineer: 5 years (Required)Python: 2 years (Required)RDBMS: 3 years (Required)Location:Bellevue, WA (Required)",Bellevue WA,Data Engineer
MarketAxess,/rc/clk?jk=961ba1f9385b63e4&fccid=8887e5b75bc5ade9&vjs=3,"Company Description | Over the last twenty years, MarketAxess has completely revolutionized the fixed-income market by creating a single platform using proprietary technology that connects the world’s most influential financial institutions. Today, more than 1,800 institutional investors and broker-dealer firms rely on us to bring them accurate market data to assist with their trading decisions, connectivity solutions that facilitate straight-through processing and technology services to optimize trading environments. | MarktetAxess is looking to bring on a skilled Data Engineer to be part of the Data Architecture team and a driving force behind the data platform build-out. | As part of the enterprise architecture modernization, the data platform is one of the key foundational pillars as it connects data with technology to help identifying opportunities and insights. It enables the various business teams within MarketAxess to make faster and better decisions, and ultimately help completely streamline traditional data processing. | What You’ll Do | Work closely with our various scrum teams on their specific data processing needs for building business applications | Constantly interact with volume (large history), velocity (data streaming), and variety (many sources) of data | Focus heavily on data quality measures | Encompass both on-prem and AWS to provide seamless app access | Who You Are | Success with designing, building, and scaling infrastructure across AWS and analytics applications using big data and machine learning technologies | 2+ years of working experience on AWS big data products, such as EMR/Spark | Experience working with terabytes of data and solid understanding of the challenges of transforming and enriching large datasets | Experience measuring data ingestion quality | Comfortability of using tools in AWS to monitor and support cloud based solutions, such as Datadog | 5+ years of experience programming, ideally in Java and Python | Well versed in infrastructure as code technologies, such as Terraform, Helm Charts and CloudFormation | What’s in it for You | In addition to working on cutting-edge technology and in our culture of innovation, collaboration, and openness, we offer: | Healthcare and wellness options | 401(k) match | Employee stock purchase plan | Training and tuition assistance | Other perks include generous time-off policies, stipends for gym memberships, exercise equipment and similar, and access to on-site healthcare | MarketAxess Corporation and its affiliates provide equal employment opportunities to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, veteran status or any other characteristic prohibited by federal and state law. |  | Additional Information | All your information will be kept confidential according to EEO guidelines.",New York NY 10001,Data Engineer
Takeda Pharmaceuticals,/rc/clk?jk=51e00ff5f8d74263&fccid=105ecfd0283f415f&vjs=3,"Job detailsJob TypeContractFull Job DescriptionDuration: 4+ months | Requirement: | Advanced SQL: Candidate Must Have Advanced SQL skills and experience. Experience with Snowflake SQL is a plus.Strong Communicator: Candidate Must Have ability to clearly and confidently communicate with business client, IT client and 3rd party partners.Python Programming Language: The candidate Should Have Python skills and experience. Or, they should have some Python training and at least have experience with other procedural language of similar or higher complexity such as Java, Scala, etc.Snowflake DB: Experience with the Snowflake DB is a plus.Apache Airflow: The candidate Should Have experience with an ETL Orchestration tool. Airflow experience is preferred, but experience with another ETL orchestration tool of similar or equal complexity such as NiFi would work as well.AWS: Candidate Should Have experience with AWS (and products such as Spark).Github: Experience with Github is a plus- or, experience could be with a similar version control tool such as Bitbucket.Jira: Experience with Jira is a plus- or, experience could be with a similar Agile development life cycle too such as VersionOne or Rally |  | The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job. |  | A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.",New York NY,Data Engineer
OTC Markets Group Inc,/rc/clk?jk=c29698fe25fbb84f&fccid=4819693c833d5b6e&vjs=3,"Job detailsSalary$101,500 - $195,800 a yearFull Job DescriptionBy clicking the “Apply” button, I understand that my employment application process with Takeda will commence and that the information I provide in my application will be processed in line with Takeda’sPrivacy Noticeand Terms of Use. I further attest that all information I submit in my employment application is true to the best of my knowledge. | Job Description | Title: Data Science Engineer - Pharma Manufacturing Science | Are you looking for a patient-focused, innovation-driven company that will inspire you and empower you to shine? Join us in our Lexington office. | At Takeda, we are transforming the pharmaceutical industry through our R&amp;D-driven market leadership and being a values-led company. To do this, we empower our people to realize their potential through life-changing work. Certified as a Global Top Employer, we offer stimulating careers, encourage innovation, and strive for excellence in everything we do. We foster an inclusive, collaborative workplace, in which our global teams are united by an unwavering commitment to deliver Better Health and a Brighter Futureto people around the world. | Here, you will be a vital contributor to our inspiring, bold mission. In this role, a typical day will include: | OBJECTIVES/PURPOSE | Develop and implement capabilities that improve knowledge management in a highly matrixed environment. | Create and implement processes for the Lifecycle Management function that efficiently collect business intelligence from publicly available sources (such as 10-Ks) through effective use of visualization and reporting | Collect and disseminate actionable manufacturing business intelligence to inform the broader manufacturing strategy within Global Manufacturing and Supply. | Extend the data science capabilities within Global Manufacturing Sciences through the use of programming tools such as Python | Create software solutions to complex operational issues and improvement of knowledge management | ACCOUNTABILITIES | Develop tools to optimize analysis (such a SWOT) of publicly available business intelligence (10-Ks, journals, etc) to identify business trends and opportuntities in commercial manufacturing. Communicate results on a routine basis. | Collabrate with other GMS functions such as Strategy &amp; Business Excellence on developing software tools to optimize and automate LCM activities. | Expand current knowledge management capabilities | DIMENSIONS AND ASPECTS | Technical/Functional (Line) Expertise | Analyze unstructured problems, identify root causes and align team objectives to drive divisional goals | Use agile principles to reduce process complexity and increase organizational capacity | Develop and apply custom strategies for processing and visualizing multidimensional data | Experience capturing and translating business requirements into technical solutions | Leadership | Ability to inspire others to share their vision and support them to reach a common goal | Decision-making and Autonomy | Creates functional strategies and goals that are closely aligned with Takeda’s objectives and develops metrics to track and assess performance. | Develops concise, strategic plans based on a clear understanding of the organization’s strengths, weaknesses, opportunities and threats. | Interaction | Creates a clear and unifying vision that inspires the team to excel. | Achieves results by managing the right combination of diverse people, resources, and processes. | Builds teams across functions and geographies with individuals who have the right skills and experience to deliver on key organizational initiatives. | Strong interpersonal and active problem-solving skills | Engage partners to deliver agile and predictive results within a complex and evolving business landscape | Interface with each Operating Unit and Region supporting lifecycle management of development and commercially approved products | Innovation | Leverage business analytics (such as SWOT) to identify emerging opportunities and capability needs for GMSci. | Ability to apply first principles in chemistry, physics, and engineering to manufacturing problems and create new solution paths to manufacturing challenges. | Translate prior knowledge into new understanding of the development pipeline and expand the working knowledge of the essential activities for all key functional areas | Identify opportunities and anticipate changes in the business landscape through an understanding and ongoing assessment of the environment affecting the business. | Complexity | Understands differences in practices across organizations or countries, and balances local demands and perspectives with global strategies. | Creates an environment that promotes information exchange and the open and honest expression of opinions, thoughts and beliefs. | EDUCATION, BEHAVIOURAL COMPETENCIES AND SKILLS | Education / experience | Minimum requirement of | Bachelors degree in computer science, chemistry, biology or related area. | 0-1 year experience in biopharma or related field | Skills | Experience working with automated data extraction | Experience with Python SciPy tools (numpy, pandas) | Ability to define and propose solutions to business problems | Application of Open Source resources for creating mobile platforms to communicate manufacturing intelligence | Ability to work with ERP and QMS systems, such as SAP and Trackwise | Expert knowledge of Microsoft tools (Excel, SharePoint, etc) | Strong communication skills | Ability to articulate complex issues and ideas with clarity to enable understanding | Behaviors | Create an environment that fosters a patient-focused entrepreneurial mindset | Strategic enterprise thinking to deliver innovative patient-focused solutions that build patient trust and Takeda’s reputation | Self-directed, accomplishment-driven individual with a strong sense of passion and urgency who can work both independently and in a cross-functional team environment | Location and Salary Information: | Greater Boston Area | Base Salary Range: $ 101,500 to 195,800 based on candidate professional experience level. | Employee may also be eligible for Short Term and/or Long Term incentive benefits. | Employees are eligible to participate in Medical, Dental. Vision, Life Insurance, 401(k), Charitable Contribution Match, Company Holidays, Personal &amp; Vacation Days, Student Loan Repayment Program and Paid Volunteer Time Off | This posting is made in compliance with Colorado’s Equal Pay for Equal Work Act, C.R.S. § 8-5-101 et seq. | WHAT TAKEDA CAN OFFER YOU: | 401(k) with company match and Annual Retirement Contribution Plan | Tuition reimbursement Company match of charitable contributions | Health &amp; Wellness programs including onsite flu shots and health screenings | Generous time off for vacation and the option to purchase additional vacation days | Community Outreach Programs | Empowering Our People to Shine | Discover more at takedajobs.com | No Phone Calls or Recruiters Please. | Locations | Boston, MA | Worker Type | Employee | Worker Sub-Type | Regular | Time Type | Full time |  |  | Job ID R0029340",Boston MA,Data Science Engineer - Pharma Manufacturig Sci
Capgemini,/rc/clk?jk=eb676d9b8b2b6c6b&fccid=16a97ed26c75bf2d&vjs=3,"MORE ABOUT THIS JOB: |  | Make data a strategic asset for the enterprise by providing a platform that enables the structuring, management, integration, control, discovery, usage, and governance of our Data Assets. | YOUR IMPACT Data Lake is the Firm’s strategic repository for enterprise data. Technology teams across the Firm are clients, participating in providing and consuming data to &amp; from the lake. You will be part of the data lake machine learning and analytics team that leverages machine learning alongside big data engineering developers to continuously turn data into action. Your role would be to apply machine learning and statistical techniques to predict, analyze and improve the performance of this highly complex big-data platform. You will be rapidly applying analytics/ML to predict trends of various components of the lake to drive strategical improvements of significant business value. You will also be designing these high impact models to run on big data and put them in production at scale. You will be collaborating daily with big data developers in the lake to understand their requirements, and convert them into real-world applied ML/analytics problems to drive major business impact and cost savings. As part of understanding the problems of lake developers, you will work with latest technologies like Apache Spark, Flink, Elastic Search, HDFS, AWS to understand complex distributed applications which handle large data sets. The Data Lake is being adopted by technology teams across the Firm at a very high rate. As a result the platform is growing and evolving, and with it, its analytical/ML needs to monitor, predict and correct its behavior automatically from vast sources of data. | RESPONSIBILITIES AND QUALIFICATIONS: |  | HOW YOU WILL FULFILL YOUR POTENTIAL | Your design and development work will contribute to a world-class client experience and optimize business performance. |  | SKILLS AND EXPERIENCE WE ARE LOOKING FOR | 1-2 years of experience of applied machine learning and statistics on real world problem statements. (max 2-3 years).Good working knowledge of pyspark, jupyterhub and machine learning techniques.Working knowledge of scripting languages, linux, networking and file systems.Strong technical skills, analytical mindset, self-motivated, independent, creative, can solve interesting and sometimes difficult technical problems under time pressure and resource constraints.Ability to stay commercially focused and to always push for quantifiable commercial impact.Ability to collaborate effectively across global teams and communicate complex ideas in a simple manner. |  | Preferred Qualifications | Masters in Data Science or Machine learning educational background.Knowledge of big data technical area. | ABOUT GOLDMAN SACHS: |  | The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world. |  |  | Â© The Goldman Sachs Group, Inc., 2020. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet.",Jersey City NJ 07302,Data Lake Machine Learning/Analytics Engineer - Data Lake Engineering
Oscar Health,/company/CTS/jobs/Data-Engineer-Gcp-256616170999e2d3?fccid=6e2e629a64e98650&vjs=3,"Job detailsSalary$87,737 - $200,265 a yearJob TypeContractNumber of hires for this roleOn-going need to fill this roleFull Job DescriptionTitle – Data Engineer with GCPLocation - REMOTEDuration : 6 months, extended to 1 yearStart date : ImmediateRequirementsMinimum 8-10 yrs of experience in IT experienceExperience using Python and SQL for data filtering, transformational and loadingExperience developing ETL/ELT using tools such as airflow/cloud composerAbility to set up and monitor real-time streaming data solutions (Kafka)Experience working with relational and MPP databases such as Postgres, HIVE, and BigqueryAbility to leverage software development lifecycle capabilities including Git version control, unit testing, and CI/CD pipelines.Projects they would work onETLs to make shared data sources (digital thread).write ETL job to get data lake into postgres for more robust applicationsWork on setting up streaming data sources and consumption for manufacturing and other data sources. Help current applications scale through optimization.Create data engineering pipelines and templates for data sources in GCPResponsibilitiesCreate and automate ETL mappings to consume and aggregate data from multiple different data sourcesMonitor performance, troubleshoot and tune ETL processes as appropriateExecution of end to end implementation of underlying data ingestion workflow.Solve complex data problems to deliver insights that helps our business to achieve their goalsContract length: 12 monthsJob Type: ContractSalary: $87,737.00 - $200,265.00 per yearSchedule:8 hour shiftWork Remotely:YesCOVID-19 Precaution(s):Remote interview process",Remote,Data Engineer with GCP
EXL Services,/rc/clk?jk=cb6316daece026d4&fccid=570de3d15b9bbb97&vjs=3,"RISIRISA is looking to hire a Data Engineer to join its team in New York. You will collaborate with Data Scientists and Design Technologists to build custom, data-driven tools to help our clients in the commercial, public, and social sectors solve a wide range of complex human-scale problems spanning global development, music, health, cybersecurity, etc. |  | Requirements |  | Ability to wrangle and process large data files into usable formats and databases | Experience designing algorithms to perform analysis and aggregation on data | Experience building and deploying production-level web services | Enthusiasm for learning new techniques and technologies to solve hard problems |  | Technical skills we look for: |  | Python | SQL | Java | Hadoop Ecosystem (Hive, Pig, etc.) | Web/API building | Familiarity with Javascript and NodeJS | Familiarity with Git for version control |  | Bonus: |  | PHP | Familiarity with production deployment and administration in Linux, Amazon AWS, Heroku, etc. | Django | NoSQL (Mongo, etc.) | PostGRES | Redis | C++ | iOS/Android development |  | Apply to ris@risirisa.com",New York NY,Data Engineer
Take-Two Interactive Software Inc.,/rc/clk?jk=d2e8a5eaac52a039&fccid=5bcd1ef0a7f4fb99&vjs=3,"Citi Consumer Technology is seeking dozens of talented entry-level engineers for its Data &amp; Analytics organization, based in Dallas, Texas, and Jacksonville, Fla. | We work with a variety of technologies and are hiring across multiple teams, including Visualization, Data Engineering &amp; Machine Learning Engineering. You’ll find more information below on our roles, our interview process, and our commitment to your growth and development. | Applications are accepted year-round, with multiple start dates organized around orientation trainings. | Our Organization | The best way to think about Citi Consumer Technology is that we’re a tech company inside a bank. We develop software that has a variety of end users: mobile apps and websites that help our customers manage their money, desktop software that allows our customer service agents to provide help and advice, and internal databases, microservices or tools that teams at Citi rely upon every day. | Our Data &amp; Analytics team makes all of that possible, whether it’s getting data from Point A to B securely and efficiently; creating customized visualization tools for evaluating performance or forecasting outcomes; building and maintaining our Big Data pipelines; or scaling our data science models to production. | No matter which team you’re part of here, you’ll be solving complex challenges alongside great people. | Your Role | You’d be placed into our Data &amp; Analytics organization into one of the following roles (experience with one or more of the listed technologies is required): | Data Engineer: Hadoop/Big Data Technologies, Spark, Google or AWS Cloud Development Data Solutions, Scala, Python, SQL. In this role, you will build/manage our Big Data pipelines and manage data migrations. | Machine Learning Engineer: Data Engineer skillset plus experience with Machine Learning concepts, frameworks and infrastructure. Here, you will scale our data science models into production-level applications. | Visualization Engineer: Angular/React, Java, Tableau. Create customized dashboards for reporting or forecasting models. This can be through full-stack web applications, Tableau or a combination of the two. | During the interview process, we will work with you to determine which of these roles best suits your skills and interests. Job offers are made with a specific role in mind, so you would know your specific position well in advance of joining the company. | Training | Soon after joining, you’ll go through several weeks of training to better understand our applications, team structure and processes. You’ll also have a chance to dive deeper into the technical aspects of your role so that you can hit the ground running. Our goal is to equip you with the tools you need to get off to a bright start at Citi. | But the learning can’t stop there. As technology evolves, you must adapt. Through access to additional instructor-led trainings and free access to thousands of hours of content via Udemy, Pluralsight and other online platforms, you’ll never stop growing your skillset. | Communication/Leadership | While much of your time will be spent coding or on other highly technical work, this is a highly collaborative environment that will also make use of your communication and influencing skills. Whether you’re working with a project manager to set a realistic deadline, helping demo a product to a business partner, or working with internal stakeholders like Architecture or Security, we need engineers who can clearly communicate technical concepts, be great teammates, and influence without authority. You’ll be entrusted with meaningful work that impacts our customers, and you’ll be empowered to suggest new ideas. | The Most Important Thing | We believe bright people can learn new things, so while the qualifications below are important, there is one that we absolutely will not compromise on: attitude. | We’re looking for great teammates who are excited to learn new things, motivated to contribute, and eager to collaborate — not compete — with others. We’ve worked hard to build a supportive family environment, and we’re proud to say that candidates and new hires consistently notice it. | This is a place where people look out for one other, build relationships, and help each other succeed. | If that sounds like you, we’d love to have you here! | Qualifications | Degree, certification or commensurate skills in Computer Science, Computer Engineering, Statistics, Mathematics or a related field. | Hands-on experience with one or more of Hadoop/Big Data Technologies, Spark, Google or AWS Cloud Development Data Solutions, Scala, Python, Machine Learning concepts/frameworks/infrastructure, Angular/React, Java, Tableau, or SQL. | Ability to pass technical interviews consisting of basic algorithmic programming exercises. | Must be collaborative and adaptable, with good communication skills. | Experience designing and interacting with databases. | Knowledge of the principles of software engineering and data analytics. | System level understanding - Data structures, algorithms, distributed storage &amp; compute. | Experience with containerization and related technologies (e.g. Docker, Kubernetes) is a plus. | Knowledge of agile(scrum) development methodology is a plus. | Knowledge of SAS is a plus. | Experience in the Financial industry is a plus. | Locations: Dallas, TX; Jacksonville, FL | Majors: Computer Science, Computer Engineering, Software Development or similar. | Start Dates: Rolling start dates available throughout 2021. | - | Job Family Group: | Technology | - | Job Family: | Digital Software Engineering | - | Time Type: | - | Citi is an equal opportunity and affirmative action employer. | Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. | Citigroup Inc. and its subsidiaries (""Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi. | View the ""EEO is the Law"" poster. View the EEO is the Law Supplement. | View the EEO Policy Statement. | View the Pay Transparency Posting",Irving TX 75062,Entry-Level Data Positions (Data Engineer Machine Learning & Visualization)
Citi,/rc/clk?jk=a6b7905692550af1&fccid=ea149e1e335c2860&vjs=3,"SUMMARY: | The Data Engineer 2 will join a fun, dynamic team to help solve integration and data problems relating to sports and entertainment. This role will integrate data from various systems and also improve and expand the existing data processes as needed. The position will be responsible for performing ETL and ELT from many disparate systems into the data warehouse and will have the opportunity to complete projects from beginning to end. The right candidate will be motivated to learn, contribute to the team and organization, and grow along with the business. |  | DUTIES AND RESPONSIBILITIES | Data Integration | Using cloud technology, combine data from various sources, cloud and on-premise, based on requirements | Perform data cleansing and standardization | Load data into a cloud data warehouse as projects dictate | Using the enterprise ETL tool, create modify, and improve integration pipelines | Translate business requirements into data warehouse pipelines using ETL/ELT methodologies | Extract and load many disparate systems into a centralized data warehouse | Business Intelligence &amp; Data Analysis | Assist with preparing and loading data for Analysis and BI reports and dashboards | Identify opportunities for new data sources | Collaborate with analysts to come up with creative solutions to data challenges | Ongoing Responsibilities | Import and integrate new data sources based on business need | Proactively identify potential data problems, but react as needed to unexpected issues | Improve existing processes to streamline efforts | Handle multiple projects and meet deadlines | Monitor, schedule, and maintain existing integrations | SUPERVISORY RESPONSIBILITIES | This position has no supervisory responsibilities. |  | SKILLS AND QUALIFICATIONS | Bachelors degree in Information Systems, Computer Science, or related field | 2-3 years of experience working with data using SQL or similar technology | Very high attention to detail | Familiarity with a data integration platform, such as Snaplogic, SSIS, or Informatica | Familiarity with BI Visualization tools, such as Tableau | Ability to work on multiple projects in a fast-paced environment | Strong communication skills to all levels of technical expertise | PHYSICAL DEMANDS | Sitting for extended periods of time | Dexterity of hands and fingers to operate a computer keyboard, mouse, and other computing equipment | The employee frequently is required to talk or hear | The employee is occasionally required to reach with hands and arms | Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus | Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions | WORK ENVIRONMENT | The noise level in the work environment is usually moderate | Fast paced office environment | Ability to work nights and weekends as business dictates | CERTIFICATES, LICENSES, REGISTRATIONS | None required |  | OTHER DUTIES | Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice. |  | This company is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics.",Foxborough MA 02035,Data Engineer 2
Two Sigma Investments LLC.,/rc/clk?jk=d83f64a292fe85db&fccid=4b5d257051285786&vjs=3,"Job Summary: | At Disney Streaming Services, data is central to measuring all aspects of delivering streaming video. The data engineering team is responsible for collecting, preparing, and processing extremely large volumes of data using public cloud and open source technologies. The data engineering team offers transparency into system operation, and guidance for performance optimizations. |  | If you are interested in joining Disney Streaming Services in the pursuit of not only crafting new media products but enjoying the products you build, we are interested in hearing from you. | Responsibilities: | Develop high performance Spark ETLs in Scala and Python | Develop unit/integration tests and data validations to ensure the quality of code and data | Drive and maintain a culture of quality, innovation and experimentation | Work in an Agile environment that focuses on collaboration and teamwork | Coach data engineers best practices and technical concepts of building large scale data platforms | Basic Qualifications: | 5+ years of relevant professional experience | Experience with terabyte or petabyte scale data systems | Proficiency in Scala (or willingness to learn) | Technology Skills &amp; Experience: | Proficiency with Scala or Python (preferably both) | Traditional relational databases and/or distributed systems such as Hadoop/Hive, BigQuery, Redshift, Snowflake | Streaming platforms such as Flink, Spark | Workflow management tools such as Airflow | Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase) | Data exploration and data visualization tools such as Looker, Tableau, Chartio, Apache Superset, Plotly / Dash |  | Required Education | Bachelor’s degree in Computer Science or related field or equivalent work experience | Preferred Education | Bonus for advanced degree in an analytical field such as economics, mathematics, or computer science | Additional Information: | New York City preferred | Remote possible for the right candidate who is willing to work EST hours from within the USA",New York NY,Sr Data Engineer - Core Data team
Maritz Inc.,/company/Lnfoblox/jobs/Associate-Data-Engineer-5157d44a5fe6c208?fccid=9d00d89e557aef6a&vjs=3,"Job detailsSalary$57,190 - $273,904 a yearJob TypeFull-timeNumber of hires for this role2 to 4Full Job DescriptionJob descriptionResponsibilities:Curate very large-scale data from a multitude of sources into appropriate sets for research and development for the data science, threat analysts, and developers across the companyDesign, test, and implement storage solutions for various consumers of the dataDesign and implement mechanisms to monitor data sources over time for changes using summarization, monitoring, and statistical methodsLeverage computer science algorithms and constructs, including probabilistic data structures, to distill large data into sources of insight and enable future analyticsConvert prototypes into production data engineering solutions through disciplined software engineering practices, Spark optimizations, and modern deployment pipelinesCollaborate on design, implementation, and deployment of applications with the rest of software engineeringSupport data scientists and threat analysts in building, debugging and deploying Spark applications that best leverage dataBuild and maintain tools for automation, deployment, monitoring, and operationsCreate test plans, test cases, and run tests with automated toolsRequirements:0-2 years Acute Knowledge with Python3, and 2 years experience with Spark. SPSS Modeler knowledge is helpfulKnowledge in data engineering, data science, and related data-centric fields using large-scale data environmentsPractical Knowledge in using SQL and working with modern relational databases, including MySQL or PostgreSQLKnowledge with NoSQL Non-Relational databases (AWS DynamoDB), virtualization, containers and orchestration (Docker, Kubernetes, XEN), cloud deployments and CI/CD is preferred0-1 years of experience with developing ETL pipelines and data manipulation scriptsProficient in Object-Oriented Design and S.O.L.I.D principlesStrong emphasis on unit testing and code qualityProficient with AWS products (EMR S3, Lambda, VPC, EC2, API Gateway, etc)Education:MS or BS in Computer Science or a related field, or equivalent work experience required.Job Type: Full-timeSalary: $57,190.00 - $273,904.00 per yearBenefits:401(k)Dental insuranceEmployee discountHealth insuranceHealth savings accountPaid time offProfessional development assistanceReferral programRetirement planSchedule:10 hour shift12 hour shift8 hour shiftDay shiftMonday to FridayNight shiftOvertimeWeekendsSupplemental Pay:Bonus paySigning bonusWork Remotely:Temporarily due to COVID-19COVID-19 Precaution(s):Remote interview processVirtual meetingsSpeak with the employer+91 4088098413",Santa Clara CA,Associate Data Engineer
Homesite Insurance,/rc/clk?jk=690a3e2077452dc0&fccid=a6a761b598113fc6&vjs=3,"Certilytics, Inc. provides sophisticated predictive analytics solutions to major healthcare organizations by integrating financial, clinical, and behavioral insights. |  |  | Data Engineer |  | We are seeking a data engineer to join our team of data experts. The data engineer is responsible for expanding, optimizing, and troubleshooting our data pipelines that receive, ingest, process, and extract data, which serve both internal and external needs. The data engineer supports our operational and business objectives, working with data analysts on existing and new data initiatives. The data engineer is self-directed and comfortable supporting the data needs of multiple teams, systems, and products. |  |  | Responsibilities/Accountabilities |  | Create and maintain data pipelines that support internal and external clients | Optimize and troubleshoot complex joins across massive data sets (billions of records) | Identify, design, implement, and own internal process improvements | Automate manual processes, and optimize data delivery and ingestion | Respond to operational data pipeline failures with a sense of ownership and an eye toward continuous improvement | Assemble accurate large, complex data sets that meet business requirements | Evaluate logs across a wide range of complex systems | Work with data experts to strive for greater functionality in our data platform | Generate accurate and effective documentation | Other duties as assigned |  | Required Skills | Requirements (Knowledge, Skills, Abilities) |  | Provable SQL knowledge and experience working with a variety of databases | Successful history of manipulating, transforming, and extracting value from large data sets | Strong project management and organizational skills | Knowledge of programming languages (Python and Java) | Ability to work with various functional teams in a dynamic environment | Excellent written and verbal communication | Strong collaborative drive | Experience designing, building, and maintaining ‘big data’ pipelines | Ability to function autonomously and with partners and teammates across several time zones | Technical mindset and desire to learn new tools and approaches | Experience working with healthcare data desired | Experience with big data tools: Hadoop, Hive, Spark, Yarn desired |  |  | Physical Demands &amp; Work Conditions |  | Ability to work remotely without interruptions | Sedentary work that primarily involves sitting/standing in-front of a computer |  | #TTR | Required Experience",United States,Data Engineer
Catalyte,/rc/clk?jk=307079e2d615b14f&fccid=6d3b3db8f9b2d20f&vjs=3,"Introduction: |  | For more than 45 years, East West Bank has served as a pathway to success. With over 120 locations across the U.S. and Greater China, we are the premier financial bridge between the East and West. Our teams of experienced, multi-cultural professionals help guide businesses and community members on both sides of the Pacific looking to explore new markets and create new opportunities, and our sustained growth and expertise in industries like real estate, entertainment and media, private equity and venture capital, and high-tech help build sustainable businesses and expand our associates’ potential for career advancement. | Headquartered in California, East West Bank (Nasdaq: EWBC) is a top performing commercial bank with an exclusive focus on the U.S. and Greater China markets. With total assets over $52.2 billion, we’re ranked among the top 25 largest banks by asset size in the United States. And since 2010, we have been recognized by Forbes magazine as one of “America’s Best Banks.” With a strong foundation, and enterprising spirit and a commitment to absolute integrity, East West Bank gives people the confidence to reach further. | Overview: |  | East West Bank is seeking a Data Engineer. The Data Engineer works with banking data and business units to propose, design, and deliver business solutions. The data engineer will collect and analyze data wherever it resides, including in spreadsheets, files, databases, and APIs. The data engineer will create reports, dashboards, and other visualizations and they will develop applications that collect data from users to enrich enterprise data sets to improve their completeness and accuracy. | Responsibilities: | Collaborate with team members and business units to Deliver Data-Oriented solutions to the enterprise. | Design, implement, deploy and maintain Data solutions. These solutions are written in T-SQL, Python, C#, Java and R. | Build reports in SSRS, Power BI, Tableau, Excel and other visualization tools. | Build high-performance algorithms, predictive models, and prototypes. | Translate, cleanse and normalize large datasets. | Collect and document business requirements. Maintain functional and technical artifacts including design documents, data mappings, architecture, data models, and dictionaries. | Qualifications: | Bachelor’s degree required in Computer Science, Information Technology, Management Information Systems or Business Management. | Master’s degree in Mathematics, Statistics, Computer Science, Data Science or relevant. | Strong computing skills with at least one of the following: C#, Java, Python, R, T-SQL with a strong interest to learn. | Quantitative, analytical, process oriented and troubleshooting skills | Proficiency with Excel. | Analytical and problem solving skills including troubleshooting. | Able to work under pressure while managing competing demands and tight deadlines. | Well organized with meticulous attention to detail. | Can-do attitude, self-motivated and strong work ethic. | Self-driven to identify areas of improvement. | Must be team-oriented with experience working on interdepartmental team projects. | Extract and collect large data sets from various sources and formats | Interpret and analyze results from data extractions to identifying patterns and trends | Evaluate internal controls and identify deficiencies through the testing of data source, systems, and processes | Define new data collection and analysis processes | Create reports, processes and tools to monitor key risk indicators for business units across the organization. | Test and validate that key assumptions, data sources, and procedures utilized in measuring and monitoring risk and internal controls can be relied upon on an ongoing basis; and, in the case of transaction testing, to assess that controls are working as intended. | Test adherence with Bank’s policies and controls, as well as regulatory requirements | Maintain an understanding of business operations, operational risks and regulatory requirements | Anticipate changes in the internal and external environment and adapt the testing program accordingly | Assuring the integrity of data, including data extraction, storage, manipulation, processing and analysis | Provides recommendations to streamline tasks and create a more efficient working environment | Report results back to management and relevant team members | Product knowledge in | Loans | Deposits | Banking operations",Pasadena CA 91101,Data Engineer
Syneos Health Commercial Solutions,/rc/clk?jk=cc5fe44b6b9d4aff&fccid=7c835f31abef9deb&vjs=3,"Job Description: |  |  | The team has been building our data warehouse and we are looking for expertise in completing this project. There are many improvement-based projects to complete such as bringing in new data sources to the warehouse, creating reports through tableau, refining existing reports, and extracting new insights. |  |  | We are looking for a savvy Data Engineer to work with our Data and technology team. The hire will be responsible for expanding and optimizing our data warehouse and data pipeline architecture, as well as optimizing data flow and collection. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple systems. The right candidate will be excited by the prospect of optimizing and expanding our team’s data architecture to build complete our data warehouse to the pave way for advanced analytics. |  |  | Responsibilities: |  |  |  | Create and maintain optimal data pipeline architecture, |  |  | Assemble large, complex data sets that meet functional / non-functional business requirements. |  |  | Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. |  |  | Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ |  |  | Connect analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. |  |  | Work with data and analytics experts to strive for greater functionality in our data systems. |  |  | Support the analytics team in defining the technology roadmap to underpin advanced analytics and data science. |  |  |  |  |  | KEY SKILLS/EXPERIENCES: |  |  |  | Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. |  |  | Experience building and optimizing data pipelines, architectures and data sets. |  |  | Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. |  |  | Strong analytic skills related to working with unstructured datasets. |  |  | Build processes supporting data transformation, data structures, metadata, dependency and workload management. |  |  | A successful history of manipulating, processing and extracting value from large disconnected datasets. |  |  | Working knowledge of message queuing, stream processing, and highly scalable data stores. |  |  | Strong project management and organizational skills. |  |  | Experience supporting and working with cross-functional teams in a dynamic environment |  |  |  | You should also have experience using the following software/Tools: |  |  |  | 4+ years of experience in a Data Engineer role, |  |  | Experience with Azure Data Factory, Azure Analysis Services, PowerBi, or Alteryx |  |  | Experience with relational SQL and NoSQL databases. |  |  | Experience setting up Tableau dashboard preferred |  |  | Experience with Analytics tools such as Adobe Analytics or Google Analytics. |  |  | Experience with Python, R and C# would be advantageous. |  |  | Knowledge and experience of architectures to support advanced analytics and data science would be advantageous.",New York NY 10001,Data Engineer
Bloomberg,/rc/clk?jk=fd164c5d9c23ef34&fccid=8e547279469474b7&vjs=3,"Company Details: |  | We started in early 2019 as a small group of technologists with a passion for making insurance better. Today we are working with a team of industry experts who run five different insurance brands and collectively control $1 billion in annual premiums. | We believe in an idea and execution meritocracy. In other words, a place where the best ideas win and the people who deliver the most value get the most opportunities. | As we grow our team, we are looking for inquisitive, entrepreneurial people who are excited to reimagine the insurance industry. | Insurance is too complex. Help us make it better. | What we’re looking for | As a Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within our organization. You will be responsible for expanding and optimizing data, the data pipeline architecture, the data flow and collection for cross functional teams. As a crucial part of the business, you will guide and support our software developers, database architects, data analysts, and data scientists on business initiatives while ensuring optimal data delivery architecture is consistent. Whether it’s working on a solo project or with the team, you are self-directed and comfortable supporting the data needs of multiple teams, systems, and products. | What we’ll bring | An engaged and supportive leadership team that will invest in you | Talented engineering teams to build products with | A broad group of industry experts who work closely with us on everything we do | A budget for continual improvement | Generous retirement plan | Excellent medical and dental insurance (and other health benefits) | Responsibilities: | Create and maintain optimal data pipeline architecture. | Assemble large, complex data sets that meet functional/non-functional business requirements. | Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery/ dataOps. | Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL. | Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. | Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. | Qualifications: |  | Skills You’ll Need | Bachelor’s Degree | 3+ years of experience in a Data Engineer role | 3+ years of experience with relational SQL databases. | 1+ year of experience with object-oriented/object function scripting languages like Python. | 1+ year of experience working with or understanding formal ETL tools | Knowledge of cloud based data warehousing products such as Snowflake",Manassas VA 20110,Data Engineer
Berkley,/rc/clk?jk=6684d27a98ce48ac&fccid=be3b11aa573faee7&vjs=3,"Looking for opportunities to use cutting edge technologies to construct data pipelines to analyze petabytes of data? Interested in working with data scientists to generate analytical insights that help members make the best decisions for the individual care needs? |  | Aetna data engineers work with data scientists side by side to optimize our ability to engage with members to help them make better healthcare decisions via campaigns. A common campaign for us would include a number of predictive models that identify specific members to message at specific moments, a suite of creative tactics with varying behavioral economics principles to deliver content, and the use of a large number of channels and apps in a highly coordinated and journey based fashion - all deployed by our own team through the experimentation platform that we’ve developed. |  |  | If you join our organization, you’ll learn about cutting-edge machine learning techniques, develop the experiment platform to expand the ability to engage, and launch campaigns at a large scale and high velocity. |  | Required Qualifications | Participates in the design, build and management of large-scale data ETL (Extract / Transform / Load) workflows for real-time and offline analytic processing.Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.Collaborates with data scientists to integrate algorithms and models into automated processes.Design and implement scalable, configurable, and self-learning marketing campaign platformUses expertise, judgment, and precedents to contribute to the resolution of moderately complex problems.Leads portions of initiatives of limited scope, with guidance and direction. |  | Preferred Qualifications | Proficient in Python, Java, Scala, or C++; Experience in shell scriptsProficient in SQL and experience in one of the databasesExperience in Spark is preferred but not requiredExperience with Hadoop and Hive is a plusHealthcare experience is a plusGood software engineering fundamentals; Strong problem-solving skills and critical thinking ability.Strong collaboration and communication skills within and across teams. |  | Education | Bachelor’s degree in Computer Science, Engineering, Math, or related disciplines; Master’s degree is preferred |  | Business Overview | At Aetna, a CVS Health company, we are joined in a common purpose: helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart. | We are committed to maintaining a diverse and inclusive workplace. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, gender, gender identity, age, disability or protected veteran status. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.",New York NY,Data Engineer
East West Bank,/rc/clk?jk=f1d1bf2c1536dd7e&fccid=1b18bc32950d637c&vjs=3,"As a GCP Data Engineer at Caserta, you will work in teams to deliver innovative solutions on Google Cloud using core cloud data warehouse tools like Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you will be working with some of the most forward-thinking organizations in data and analytics. |  | Responsibilities: | Build and Deploy Data Pipelines on Google Cloud to enable AI &amp; ML capabilities. |  | Drive the development of cloud-based and hybrid data warehouses &amp; business intelligence platforms. |  | Build Data Pipelines to ingest structured and Unstructured Data. |  | Gain hands-on experience with new data platforms and programming languages. |  | Qualifications: | 5+ years of experience consulting in Data Engineering or Data Warehousing. |  | Hands-on experience with Google Cloud Platform. |  | Experience leading data warehousing, data ingestion, and data profiling activities. |  | Advanced SQL &amp; Python skills. |  | Hands-on experience with Google cloud platform technologies: Google Cloud Platform Pub/Sub, Cloud Functions, DataFlow, DataProc (Hadoop, Spark, Hive), Cloud Machine Learning, Cloud Data Store and BigTable, BigQuery, DataLab, and DataStudio. |  | Migrating Data Pipelines to Google Cloud Platform (GCP). |  | Strong aptitude for learning new technologies and analytics techniques. |  | Highly self-motivated and able to work independently as well as in a team environment.",New York NY,Google Cloud Data Engineer
CVS Health,/rc/clk?jk=789dbaed8b47a50b&fccid=b9ca7ec1464dfa21&vjs=3,"Job detailsSalaryUp to $124,000 a yearFull Job DescriptionHomesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations. | One thing that's stayed the same since our founding: our commitment to our customers, partners and employees. | Join us on our journey as we continue to grow into a powerful contender in the field of insurance. | Compensation may vary based on the job level and your geographic work location. | Compensation Minimum:$74,500 | Compensation Maximum:$124,000 | We seek an independent contributor to streamline the access to data by developing new automated data structures that empower non-engineers to rapidly leverage the wealth of Homesite data. The data consolidation and simplification will include leveraging API to access data, cross system communication, data governance and resilient automation. | Responsibilities | Lead efforts to design, develop and implement databases enhancement to improve efficiency and streamline the use for analytics, business analysis and data governance. | Develop a mastery of the corporate data structure to be able to perform ETL operations from data tables existing in the company. | Test, design, and implement process automation techniques to support efficiencies. | Clearly communicate complex findings to colleagues influencing outcomes and communicating the value add of the data engineering work being completed. | Foster relationships with colleagues to identify and explore new sources of data. | Knowledge, Skills and Competencies | A mastery of best practices in coding, code organization, data transformation, ETL, process automation, and APIs. | Expertise in relational databases (such as MS SQL Server, MySQL and Aurora) concepts. | Ability to comprehensively understand data sources, elements and relationships in both business and technical terms. | Expertise with Microsoft SQL Server (including SSIS, SSRS) including 2-5 years of experience. | Knowledge of one version control system, preferably Git is required. | Bachelor’s degree in Computer Science, Computer Engineering, Information Technology/Systems, related field, or equivalent experience. | Motivated individual with strong analytic, problem solving, and troubleshooting skills. | Other Job Experiences Desired | Insurance industry experience a plus, but not required. | Experience with cloud computing APIs (Amazon Web Services preferred). | Experience with cloud computing services (Amazon Web Services like Lambda, S3, CloudWatch, ECS, and RDS preferred) | When you work at Homesite you can expect benefits that support your physical, emotional, and financial wellbeing. You will have access to comprehensive medical, dental, vision and wellbeing benefits that enable you to take care of your health. We also offer a competitive 401(k) contribution, a pension plan, an annual incentive, and a paid-time off program. In addition, our student loan repayment program and paid-family leave are available to support our employees and their families. Interns at Homesite are eligible for the paid time off program. Contingent workers are not eligible for American Family Enterprise benefits. | Stay connected: Join Our Enterprise Talent Community !",Remote,Data Engineer
Arcadia,/company/Anant-Corporation/jobs/Data-Platform-Engineer-4dc1b60351169c1d?fccid=2667f00e232f23a7&vjs=3,"Job detailsSalary$90,000 - $180,000 a yearJob TypeFull-timePart-timeContractNumber of hires for this role1Full Job DescriptionOverviewThe Data Engineer participates in the design and build of modern data products that comprise raw data stores (data lakes) and cleansed data repositories in relational and non-relational databases such as PostgreSQL, Cassandra/Datastax, populated by batch (Spark) or streaming data pipelines (Spark, Kafka Streams). The Data Engineer works with a team to create a robust, sustainable and flexible design and leads the technical delivery.ResponsibilitiesDeveloping and managing data processes to ensure that data is available and usableCreation and automation of data pipelines and platformsManaging and monitoring data quality via automated testing frameworksWorking closely with Architects, Engineers, Data Scientists, and DevOps to design, build, test, deliver, and maintain sustainable and highly scalable data solutionsResearching data acquisition and evaluating suitabilityIntegration of data management solutions into client environmentActively managing risks to data and ensuring there is a data recovery planBuilding data repositories such as: data warehouses, data lakes, and operational data stores, etc.Qualifications3+ years relevant professional work experience.Experience and expertise in the following:Creating robust and extensible data pipelines for production systemsUse of cloud platforms, preferably AWS, Google, and AzureCreating secure, performant, and well-modeled data storesData lake design patterns and technology options (schema on read, metadata capture, search framework)Familiarity with NoSQL databases with a strong preference for Cassandra/DatastaxSource code version control management using gitUsing data catalogs and data pipeline tools for Data OperationsProfessionalism; to include written and oral communication. An ability to understand your audience and adjust your communication style to fitAptitude and desire for learning new technologies.About AnantAnant is working to become the authoritative market leader in business platforms. Most technology leaders have a hard time retaining the experts to help them build and manage global data platforms because of the high costs of specialized talent so we created a training program for their teams and a network of trained specialists on our framework that are available on a full, part, or on a project by project basis.Anant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law.Job Types: Full-time, Part-time, ContractPay: $90,000.00 - $180,000.00 per yearSchedule:Monday to FridayContract Length:VariesContract Renewal:PossibleFull Time Opportunity:YesWork Location:Fully RemoteHours per week:20-29Typical start time:9AMTypical end time:6PMCompany's website:Anant.usCompany's Facebook page:https://www.linkedin.com/company/anant/COVID-19 Precaution(s):Remote interview processVirtual meetings",Remote,Data Platform Engineer
Stevens Institute of Technology,/rc/clk?jk=2bc85490cf66bdef&fccid=ced94c2be57b2133&vjs=3,"Who we are: |  | New Fortress Energy (NFE) is a fully integrated energy company that produces liquefied natural gas (LNG) and delivers clean, reliable energy to customers around the world. We believe that everyone should have access to affordable energy; however, billions of people around the planet still lack this critical element for quality of life and economic growth. Founded in 2014, we have embarked on an international expansion, driven by our vision of a world in which electricity is no longer a luxury good. |  | Today, we deliver a completely integrated and customized energy solution for our customers to help them benefit from natural gas. Our world-class team has expertise across power, infrastructure, transportation and LNG, and we have developed a proven track record building and operating liquefaction facilities, onshore and offshore regasification terminals, pipelines, power plants, and various small scale solutions. |  | What you will do: |  | This role will be responsible for delivering insightful visualizations and analysis to help us in understanding and optimizing our business and system operations. Reporting to the Chief Technology Officer, this individual will be bridging the gap between information technology and business users by partnering with various business functions across the enterprise. You will be utilizing a variety of data sources, from databases to real-time data streams, to implement collaborative solutions that touch upon accounting, operations, sales, marketing, and other areas. |  | Design and implement complex business visualizations, dashboards, and reports to deliver insights. | Analysis of complicated data sets and presenting meaningful outcomes to a variety of stakeholders | Act as a translator between business and technical teams, by consulting on the data available or feasibility of acquiring data, keeping cost versus benefit in mind | Use data visualization, reports and data-driven presentations to present digestible conclusions and recommendations to top decision makers, so that business strategies can be designed around them | Develop and communicate goals, strategies, tactics, analytical plans, timelines, and key performance metrics to reach goals | Manage a priority list of action items that are needed for stakeholders | Work with internal teams to implement analytics platforms based on business rules and strategy | Partner with business functions to achieve automation goals for reporting and analytics | Manage and maintain the company's data warehouse | Pull data from disparate data sources into the company's data warehouse | Tell a story through data and draw insights to tell a story |  | What you bring: |  | A Masters degree in Data Science, Computer Science, Mathematics, Statistics, Operations Research or other quantitative fields | 4-6 years of hands-on BI development experience with major commercial BI tool | Hands-on experience with R, Python, or other related analytical/programming tools or software | Understanding of RDBMs and SQL programming skills, such as PostgreSQL, MySQL, Microsoft SQL Server | Experience with web services and ability to connect data using SOAP API, REST API, Plumber | Knowledge of data visualization and reporting tools, such as D3, Tableau, Microsoft PowerBI | Ability to communicate complex analytical insights in a simple, concise manner | Experience with ArcGIS a plus | Strong communication skills | Excellent organization and prioritization skills |  | What we offer: |  | We're honored to be named one of Crain's top 100 places to work in NYC! We offer our employees a generous vacation policy, healthy snacks, team events, medical, dental + vision coverage, commuter benefits, 401K, healthcare concierge, free short term, long term disability and life insurance. We have passionately invested in your physical health and schedule with a high end onsite gym and showers! |  | Our commitment to diversity + inclusion: |  | New Fortress Energy is an equal opportunity employer and promotes a diverse and inclusive workplace. NFE considers all applicants without regards to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, gender identity, or sexual orientation.",New York NY,Data Visualization Engineer
Ferguson,/rc/clk?jk=3e0e1cdbde8461f9&fccid=7f5135ad5b2f5b3a&vjs=3,"ABOUT VICE MEDIA GROUP |  | VICE Media Group is the world's largest independent youth media company. Launched in 1994, VICE has offices in 35 cities across the globe with a focus on five key businesses: VICE.com, an award-winning international network of digital content; VICE STUDIOS, a feature film and television production studio; VICE TV, an Emmy-winning international television network; a Peabody award-winning NEWS division with the most Emmy-awarded nightly news broadcast; and VIRTUE, a global, full-service creative agency with 25 offices around the world. VICE Media Group's portfolio includes Refinery29, the leading global media and entertainment company focused on women; PULSE Films, a London-based next-generation production studio with outposts in Los Angeles, New York, Paris and Berlin; i-D, a global digital and bimonthly magazine defining fashion and contemporary culture; and Garage, a digital platform and biannual publication converging the worlds of art and design. |  | VICE Media Group's portfolio includes Refinery29, the leading global media and entertainment company focused on women; PULSE Films, a London-based next-generation production studio with outposts in Los Angeles, New York, Paris and Berlin; i-D, a global digital and bimonthly magazine defining fashion and contemporary culture; and Garage, a digital platform and biannual publication converging the worlds of art and design. |  | VICE x Team |  | Data Services provides data and the infrastructure for working with it to analysts and business stakeholders across the company. We are responsible for making sure that data and reports are delivered reliably and accurately. |  | We use S3 to store raw data and use Snowflake and Spark for processing, with end user reporting delivered through Domo. Orchestration is handled with Apache Airflow |  | Role x You |  | You are an experienced python engineer with experience working in data pipelines and batch processing. |  | You have experience working within a team and communicating with technical and non-technical stakeholders. You write code that is easy to read, defensive and stable. |  | You are always looking for a better solution and believe in self directed learning. |  | Qualifications |  | 3+ years experience in Python or other scripting language | Experience with any of Airflow, Luigi, Prefect, Oozie etc. | Strong SQL skills, especially in Snowflake, BigQuery, Redshift etc. | Experience consuming APIs. | Experience working within an AWS environment. | Comfortable on the command line |  | We'd love if you also had these: |  | Media, Advertising or Consulting experience. | Exposure to Salesforce, Google Analytics, Netsuite, Google Ad Manager. | Experience with CI/CD pipelines | Experience with analysis and deriving insights from data |  | We want to find people who believe in our goals and feel inspired enough to grow while they're here to fill the role, rather than someone who checks the boxes but isn't invested. We encourage you to apply and show us what you've got. |  | If you require reasonable accommodation during the application and selection process, please let us know. We will work together to best meet your needs. |  | Working at VICE |  | VICE prioritizes the ideas and people that other media companies miss. We believe that innovation is a direct result of diverse, inclusive cultures so we don't just ""tolerate"" differences, we celebrate it and see it as essential to our staff, culture, and business. To learn more read the VICE Guide to VICE |  | Agencies: VICE Media Group is not partnering with agencies nor accepts unsolicited resumes and will not be responsible for any fees or expenses related to such unsolicited resumes and/or applicants.",Brooklyn NY,Junior Data Engineer
Dascena,/rc/clk?jk=0b66789c8466b2a8&fccid=d46039b952140fd4&vjs=3,"Guy Carpenter is seeking candidates for the following position based in the New York office: |  | Data Engineer | What can you expect? | The Data Engineer role within the Data Strategy group at Guy Carpenter (“GC”) provides an opportunity to own the build-out and implementation of data solutions to power one of the world’s largest and most respected risk management and reinsurance firms. The Data Strategy group has a “start-up style” mandate and internal consulting role (within a $1.3 billion company) to effectively enhance the acquisition, storage, analysis, fidelity, and monetization of massive amounts of client, internal, and third-party data across the GC organization. |  | As a member of the Data Strategy group, the Data Engineer will work with fellow data scientists, product managers, business analysts, and stakeholders from other internal groups to design and improve data-centric projects with the dual mandate of (1) increasing the efficiency of the data collection and analysis process across GC and (2) driving the monetization of data via newly designed and existing products for GC’s reinsurance clients. The data engineer will be the lead facilitator on innovative initiatives and will have ownership over the design, development, and delivery of projects which will require direct reporting to senior-level management in both business and technical groups. | We will count on you to: | Develop, implement, and deploy custom data pipelines powering machine learning algorithms, insights generation, client benchmarking tools, business intelligence dashboards, reporting and new data products. | Innovate new ways to leverage enormous amounts of various datasets to drive revenues via the development of new products with the Data Strategy team, as well as the enhanced delivery of existing products | Consume data from a variety of sources (relational DBs, APIs, NetApp and other cloud storage, FTPs) &amp; formats (excel, CSV, XML, parquet, unstructured)) | Construct and maintain data pipelines between GC’s databases, and other sources, with the data lake utilizing modern ETL frameworks | Own the role of data steward for a variety of high value datasets and implement innovative quality assurance practices | Establish and implement metadata management standards and capabilities, including lineage mapping | Enforce strong development standards across the team through code reviews, unit testing, and monitoring | Perform basic data analysis within Jupyter Notebooks to validate the fulfillment of requirements for data pipelines | Evangelize data strategy techniques and best practices throughout global strategic advisory | Keep up-to-date on the latest trends and innovation in data technology and how these trends apply to GC's business and data strategy |  | What You Need to Have: | 3-5 years of relevant experience as a data engineer or in a similar role | Bachelor’s or master's degree in data science, computer science or related quantitative field such as applied mathematics, statistics, engineering, or operations research | Extensive experience with Spark, Python, and SQL | Extensive experience integrating data from semi-structured | Experience deploying/maintaining cloud resources (AWS, Azure, or GCP) | Knowledge of various industry-leading SQL and NoSQL database systems | Experience working in an Agile environment to facilitate the quick and effective fulfillment of group goals | Good interpersonal skills for establishing and maintaining good internal relationships, working well as part of a team and for presentations and discussions | Strong analytical skills and intellectual curiosity as demonstrated through academic experience or work assignments | Good ability to prioritize workload according to volume, urgency, etc. and to deliver on required projects in a timely fashion |  | What makes you stand out? | Strong understanding of entity resolution, streaming technologies, and ELT/ETL frameworks | Ability to articulate the advantages of various cloud and on-premises deployment options | Experience with Master Data Management | Experience with web scraping and crowd sourcing technologies | Familiarity with modern data productivity frameworks and their alternatives such as Databricks, DataRobot, and Alteryx | Experience with the MS Azure cloud environment, including ARM template deployments | Strong knowledge of CI/CD principles and practical experience with a CI/CD technology (Azure Devops, GitLab, Travis, Jenkins) | Guy Carpenter &amp; Company, LLC is a leading global risk and reinsurance specialist with more than 3,100 professionals in over 60 offices around the world. Guy Carpenter delivers a powerful combination of broking expertise, trusted strategic advisory services and industry-leading analytics to help clients adapt to emerging opportunities and achieve profitable growth. Guy Carpenter is a business of Marsh &amp; McLennan Companies (NYSE: MMC), the world’s leading professional services firm in the areas of risk, strategy and people. The company’s 75,000 colleagues advise clients in over 130 countries. With annualized revenue approaching $17 billion, Marsh &amp; McLennan helps clients navigate an increasingly dynamic and complex environment through four market-leading companies including Marsh, Mercer and Oliver Wyman. For more information, visit www.guycarp.com and follow Guy Carpenter on LinkedIn and Twitter @GuyCarpenter | Marsh &amp; McLennan Companies and its Affiliates are EOE Minority/Female/Disability/Vet/Sexual Orientation/Gender Identity employers.",New York NY 10036,Data Engineer
New Fortress Energy,/rc/clk?jk=ea1d00011c7abd25&fccid=5527fa86119a4ea6&vjs=3,"Job detailsJob TypeInternshipFull Job DescriptionPosition Overview: | The Climate Corporation’s mission is to help the world’s farmers sustainably increase their productivity with digital tools. The Data and Analytics team is focused on creating competitive advantage for Climate and our customers through novel data infrastructure, metrics, insights and data services. We are a small but rapidly growing analysis and engineering team that builds and leverages state-of-the-art analytics systems. Our work informs decisions and direction for our business, while also impacting our products. We are looking for a Data Engineer to not only build data pipelines to efficiently and reliably move data across systems, but also to build the next generation of data tools to enable us to take full advantage of this data. In this role, your work will broadly influence the company's products, data consumers and analysts. We are looking for a candidate with knowledge of data warehousing and experience with ETL tools. | What You Will Do: | Help design and build a Multidimensional Data Warehouse. | Build and maintain the core data model, data pipelines, core data metrics and data quality. | Work directly with stakeholders across multiple functions (Science, Marketing, Sales, Risk, Finance, Product) to define needs/requirements. | Champion data warehousing best practices | Develop and build infrastructure in an AWS cloud environment | Build data expertise and own data quality for the data pipelines | Design and develop new systems and tools to enable folks to consume and understand data faster | Provide expert advice and education in the usage and interpretation of data systems to end consumers of the data | Basic Qualifications: | Actively pursuing a BS, MS or Ph.D. in Computer Science or related technical discipline | Working knowledge of at least one programming language (preferably Java, Scala, Python or Javascript) | Strong curiosity, ownership, problem-solving skills and ability to adapt and learn quickly | Preferred Qualifications: | Experience working with data warehouse projects at large to massive scale. | A solid foundation in computer science, with strong competencies in data structures, and data models. | Strong knowledge in SQL | Experience working with high performance, distributed computing environments | Experience working with AWS | Experience with massive scale relational databases (MPP) is a plus (Vertica/Redshift/Teradata) | What We Offer: | Our environment is extremely engaging and fast-paced, with a diverse set of top engineers, agronomists, and statisticians working together to provide the best possible products and experiences for our customers. | We offer competitive pay and perks. | We provide meals and a large assortment of snacks and drinks to get you through the day. | You’ll get 1:1 mentoring from a talented and motivated employee on your team. | You’ll have the opportunity to work on real projects with experienced employees and have a direct impact on the company. (Our internships are not summer camp!) | You’ll have the amazing opportunity to participate in one of our Biotechnology Tours where you get out of the office and into the field (literally) with agents and farmers. | You’ll work in an award winning office with smart designs, large common areas and open floor plans that fuel collaboration. | You’ll attend various company sponsored events, activities and off-sites - we make sure interns are having fun! | We regularly host meet-up groups and tech-talks and encourage participation in relevant workshops and conferences | You’ll have the opportunity to interaction with key executives and leadership. | We also hinge our cultural DNA on these five values: | Inspire one another | Innovate in all we do | Leave a mark on the world | Find the possible in the impossible | Be direct and transparent | Learn more about our team and our mission: | The Climate Corporation - The Technology Behind Making A Difference | https://youtu.be/c5TgbpE9UBI or visit https://climate.com/careers | Climate aims to create a welcoming and collaborative environment for our employees in which a diverse set of perspectives and voices are represented and celebrated. | As part of our dedication to the diversity of our workforce, The Climate Corporation is committed to Equal Employment Opportunity and does not discriminate based on race, religion, color, national origin, ethnicity, gender, sex (including pregnancy), protected veteran status, age, disability, sexual orientation, gender identity, gender expression, or any unlawful criterion existing under applicable federal, state, or local laws. If you need assistance or an accommodation due to a disability, you may contact us at accommodations@climate.com.",San Francisco CA 94103,Intern: Data Engineer
Spotify,/rc/clk?jk=1f21e2cb6e5252de&fccid=4a4c5788c8139bce&vjs=3,"Join a mashup of energy enthusiasts and creative tech wizards who are taking the fight to climate change. Disrupt and reimagine the energy experience using modern technologies. |  | Arcadia is a technology company revolutionizing the energy industry. We make choosing clean energy easy for everyone, no matter where you live or who your utility provider is. Founded in 2015, Arcadia set out with one purpose that continues to drive us today: a 100% clean energy future. |  | What we're looking for: |  | We are seeking a Data Engineer to design, develop, and maintain large-scale data applications and pipelines, working closely with the Analytics &amp; Data Science team to improve Arcadia's business through smarter use of data. The role will be hands-on and cross-functional, collaborating across the company to establish and execute the data engineering roadmap. |  | This is an exceptional opportunity for someone who relishes the chance to engage with cutting-edge technology, influence how our team builds and stays relevant, and work in a fast-paced environment with engineers on a high-morale, tightly knit team. Our engineering values are deeply ingrained in our culture- you can read more about them here. |  | This role is based in Washington, D.C., or New York City, NY, though we are open to considering a remote candidate (we are all working from home at the moment). |  | Our core data stack makes heavy use of the AWS ecosystem, with pipelines built using both Lambdas and ECS tasks running Python on top of Redshift. Our customer-facing application stack also includes several Ruby on Rails apps, a GraphQL service layer, and a number of React clients. In your application, please include a link to GitHub or another place where your code is published, though we understand that not everyone has public code online. |  | What you'll do: | Manage data pipelines from disparate sources, standardizing and feeding them into our centralized data warehouse | Work with both the Engineering and Analytics &amp; Data Science teams to optimize data flow and queries for large data sets to improve scalability | Sync data across internal and external systems, such as marketing and sales automation tools, to enable key stakeholders to build best-in-class experiences | Support ongoing efforts to establish and enforce best practices on data quality, use, and security across the company | What will help you succeed: | Must-haves: | 3+ years combined programming and/or DevOps experience | Significant experience with and a strong understanding of languages/tools relevant to engineering &amp; data teams' work | Experience in one or more of the following languages: Python, Java, Ruby, Javascript | Advanced knowledge of algorithms, data structures, and relational algebra | Database management experience with PostgreSQL, RDS, or Redshift | Data extraction experience with a strong understanding of thread-based and event-based paradigms | Extensive experience in managing data pipelines, schemas, and storage for multiple systems for multiple teams | Strong communication skills | Nice-to-haves: | Undergraduate and/or graduate degree in math, statistics, engineering, computer science, or related technical field | Experience in predictive modeling and statistical analysis | Experience with enterprise database interfaces and messaging APIs | Experience with Amazon Web Services (AWS) or other cloud infrastructure platforms | Experience with entity resolution at scale | Experience in the energy sector | Benefits: | Market-based compensation (salary + equity) | Healthcare, dental, vision, 401(k) and commuter benefits | Paid Time Off (holidays, vacation, professional development, volunteer, parental leave) | A supportive engineering culture that values diversity, empathy, teamwork, trust, and efficiency | Professional development opportunities | All-company lunches | Free clean energy | A chance to decarbonize and disrupt the energy sector |  | Eliminating carbon footprints, eliminating carbon copies. |  | Here at Arcadia, we cultivate diversity, celebrate individuality, and believe unique perspectives are key to our collective success in creating a clean energy future. Arcadia is committed to equal employment opportunity regardless of race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, protected veteran status, or any status protected by applicable federal, state, or local law. While we are currently unable to consider candidates who will require visa sponsorship, we welcome applications from all qualified candidates eligible to work in the United States.",New York NY,Data Engineer
ELLKAY LLC,/rc/clk?jk=ecf469b943ae9ccc&fccid=1603a30593aeda56&vjs=3,"Company Description | ELLKAY is a nationwide leader in healthcare connectivity, providing innovative, customizable solutions and unparalleled services for over a decade. We empower diagnostic laboratories, PM/EMR vendors, ACO and HIE companies, hospitals, and other healthcare organizations with cutting-edge technologies and solutions that improve their bottom lines. | Our 'Client-first' focus has made ELLKAY one of the most respected healthcare IT companies in the nation. We value our clients and believe that strong relationships are the foundation for a strong company, and we're dedicated to providing connectivity to the healthcare industry. | Company Culture: We deal with medical data and we take our work very seriously, but not ourselves. If you’re a smart, hard-working, dedicated individual who thrives in a laidback, friendly work environment, ELLKAY may be the place for you. We’re committed to attracting good people who are passionate about the work they do. | ELLKAY was founded over a decade ago on the values of innovation, efficiency, and service created in a collaborative work culture. As we have grown, we are proud to still possess the same energy and passion for what we do. We strive to provide exceptional customer experiences to our clients, which begins with first employing amazing people. ELLKAY is proud to maintain a high-quality, innovative, and diverse workforce. |  | Job Description | ELLKAY has been rated the #1 data migration and transformation company in the U.S. healthcare market, and we’re looking to expand our EMR Data Migration! | We’re looking for a detail-oriented, problem-solving team players with a strong technical background to fill our EMR Data Migration Specialist position. | You will be responsible for performing data extraction of discrete and non-discrete data from various EMR Systems and migrate these datasets to other EMR systems and Archive. Qualified candidate will serve as the primary point of contact for EMR data migration process for assigned projects and will be responsible to meet project timelines. | Essential Duties and Responsibilities: | Assess client’s desired scope, analyze client data and design project plan. | Proactively identifies issues and works on resolution plan to alleviate impact on project. | Set up environment for EMR data Migration and performs ETL for desired databases | Performs data validation and testing to ensure accuracy | Evaluate and identify opportunities to drive continuous process improvements | Manage multiple projects simultaneously and prioritize tasks to ensure timely delivery | Coordinate deliverables with various internal departments. | Set and manage appropriate expectations for successful project execution. |  | Qualifications |  | Bachelor’s degree Computer Science, Data Analytics or related field | 2+ years Experience with data analysis and SQL | Knowledge of healthcare data &amp; workflow preferred | Strong problem solving and analytical skills | Ability to exercise effective decision-making capabilities in a fast-paced environment. | Excellent communication and organizational skills | Additional Information | This is an onsite position at our Elmwood Park HQ. Remote work is may be available. | For more information on our company, visit www.ELLKAY.com. | Interested applicants should submit a letter of interest with salary requirements and resume. | ELLKAY LLC is a Smoke-Free Workplace. | ELLKAY is an Equal Opportunity Employer.",Elmwood Park NJ 07407,Data Migration Engineer
Marsh and McLennan,/rc/clk?jk=05b0cece226869c9&fccid=033b673c9c5dc788&vjs=3,"Job Description: | We realize that our greatest assets are our best-in-class associates, which is why we’re dedicated to offering limitless opportunities for growth and advancement. We want to help you build a long-lasting career with Ferguson, we can continue to lead the industry and help build our nation’s infrastructure from the ground up. Join our team today. | Ferguson is currently seeking the right individual to fill an immediate need for a Data Engineer. | What is the opportunity for you? | The Data Engineer will have end-to-end responsibility for maintaining our data flows into the data warehouse. This includes ETL, designing &amp; building data models, managing the business intelligence development cycle, data validation, and managing uptime. They will work closely alongside our business analysts to ensure alignment among both the technical and business aspects of business intelligence development. | What are you going to do? | Design and build data models using SQL | Maintain, modify, and build upon the current model within the platform | Manage data refresh processes and uptime | Manage the BI development lifecycle from development to production | Translate business requirements into dimensional models | Maintain and build upon the semantic layer and measure/attribute definitions | Perform extensive data validation | Communicate effectively with analytics colleagues and business stakeholders | What are you going to bring to the table? | Proven experience building and validating data models | Experience with Python scripting | Self-motivated, team player, take initiative, quick learner, systematic, adaptable, etc. | Strong SQL skills required for building tables, views, and implementing complex logic | Ability to learn and use database software (SQL) | Outstanding organizational skills | Solid problem solving, numerical fluency, and analytical skills | BA or BS degree preferred | What do we bring to the table? | Competitive Compensation Packages | Great 401(k), PTO and benefits package which includes Medical, Dental, Life and Vision | Amazing work environment that makes you want to be at work everyday | Team building and company-wide events throughout the year (sometimes too many to count) | Internal Training programs suited to what you are wanting to develop in | A wide variety of local discounts plus all of Ferguson Inc. employee perks | The Company is an equal opportunity employer as well as a government contractor that shall abide by the requirements of 41 CFR 60-300.5(a), which prohibits discrimination against qualified protected Veterans and the requirements of 41 CFR 60-741.5(A), which prohibits discrimination against qualified individuals on the basis of disability.",Newport News VA,Data Engineer - Remote
The Climate Corporation,/rc/clk?jk=60b8155b63448587&fccid=353eb997fc901045&vjs=3,"Crossix is a health-focused technology company dedicated to advancing healthcare marketing with analytics and innovative planning, targeting, measurement, and optimization solutions. Positioned at the center of big data, innovative technology, and multichannel media, Crossix, a Veeva Company, provides our clients with insights to help make strategic business decisions and drive improved patient outcomes. Crossix knows that our employees are integral to our success, which is why we have created an inclusive culture where everyone can thrive. Crossix is headquartered in midtown Manhattan with opportunities to work in our NYC office and other locations around the country. Along with competitive salaries and benefits, we invest in opportunities for professional development and career growth, and provide other amenities like a beautiful rooftop, team bonding activities, etc. |  | Veeva is seeking an intellectually curious, resourceful, and collaborative Data Engineer to join Veeva’s Advanced Analytics team. At Advanced Analytics we build the technology and data science products that power Veeva’s targeting, measurement, and optimization solutions. This is a great opportunity to be at the forefront of innovation and help us build the necessary infraestructure to solve some of the most challenging problems in healthcare data and analytics. |  |  | What You'll Do | Work architecting and building scalable and reliable data pipelines to support development of Machine learning and data science products | Collaborate closely with Data Scientists, Product, and Engineers to improve processes, and implement methods and features | Rapidly build prototype solutions, communicate findings, iterate | Use technologies like AWS, Spark, Python, R, SQL, and Git in addition to proprietary data mining software on a daily basis | What You've Done | 5+ years of hands-on experience working as a Data Engineer | Degree in a relevant technical field (Machine Learning, Computer Science, Engineering, Operations Research, etc) | Experience working closely with Analytics/Data Science teams, building machine learning pipelines | Highly proficient in Python and SQL | Proficient in Git | Experience working with AWS environment and distributed execution frameworks like Spark | Strong communication skills |  |  | Nice to Have | Familiar with workflow management tools (Airflow, Luigi, etc) | Background in data warehouse concept and design |  |  | Perks &amp; Benefits | Flexible PTO | Allocations for continuous learning and development | Health &amp; wellness programs |  |  | If this role and our exciting company culture seem appealing to you, please apply! We want to continue to grow our diverse team of hardworking and humble people who are passionate about their work. We hope that’s you!",New York NY,Senior Data Engineer
Veeva Systems,/rc/clk?jk=29ff5308c60ceb99&fccid=fe404d18bb9eef1e&vjs=3,"Engineering | Data | The Platform department builds the technology ecosystem that enables Spotify to learn and deliver quickly, while safely and easily scaling to billion customers, and enabling our rapid employee growth around the globe. Our team consists of six organisational units that reflect the needs and groupings of our internal customers. We support the craft and practice of designers, engineers, researchers, and data scientists by developing the frameworks, capabilities and tools they need to do their work optimally, quickly, and safely. We are an amplifier for efficiency, quality, and innovation across all of Spotify! | Location | New York, NY | Job type | Permanent | We are looking for a Senior Data Engineer with a strong infrastructure background to join our team that builds libraries, tooling, and the infrastructure that supports most of the data processing done at Spotify. We research and develop state of the art solutions, creating a robust and reliable platform that enables developers to solve complex technical challenges with the best user experience possible, enforcing standards and ensuring reliability and job efficiency. We do this by thinking open source first, contributing to large open-source code bases while building up the community around the projects that enable our platform. | What you'll do | Work on scio, our open-source Scala API for Apache Beam and Google Dataflow, influencing it's API, providing support for additional IOs, improving support for different runners (Flink, Spark), and optimizing for job efficiency | Help build out a new infrastructure offering to support various execution engines running on Kubernetes (GKE) | Take an active part in the operational responsibilities for running our infrastructure | You will create and update standards and documentation that improve the experience for people working with data at Spotify | You will contribute features, build up the community and the exposure of several of our open source projects | Who you are | You have experience working with data engineering and a deep understanding of that problem space | You have experience with JVM-based data processing frameworks such as Beam, Spark, and Flink. You understand their APIs and can debug their internals | You have experience working with containerization technologies such as Kubernetes (GKE) and Docker | You know and care about sound engineering practices like continuous delivery, defensive programming, and automated testing | You are comfortable with asynchronous communication, being able to work independently while always sharing context with your team members | You think of open source first, understanding the value of building up a community and contributing back | Perks of being in the band | Extensive learning opportunities, through our dedicated team, GreenHouse. | Flexible share incentives letting you choose how you share in our success. | Global parental leave, six months off - fully paid - for all new parents. | All The Feels, our employee assistance program and self-care hub. | Flexible public holidays, swap days off according to your values and beliefs. | Spotify On Tour, join your colleagues on trips to industry festivals and events. | Learn about life at Spotify | You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens. | Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service with a community of more than 345 million users.",New York NY,Senior Data Engineer
Gloo,/company/Sansutek/jobs/Data-Engineer-d88e9246c76f0fd2?fccid=5a72960a48091ff5&vjs=3,"Job detailsJob TypeFull-timeContractQualificationsBachelor's (Preferred)SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Full Job Description**Data EngineerLong TermRemote/ long termSeattle, Austin or Bay AreaMust Have:1. Built and operated large scale data pipelines in a 24/7 operational environment2. 5+ years of Java or Scala programming experience as their primary language3. Experience with unit/integration testing and post-release validation.Note: The candidate has to work directly on our payrolls.Reach out to me for more details.Raj(Dot)Kumar(at)SansuTek(Dot)ComJob Types: Full-time, ContractSchedule:8 hour shiftEducation:Bachelor's (Preferred)Experience:SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Work Location:Multiple locationsWork Remotely:YesSpeak with the employer+91 4696945584",Austin TX,Data Engineer
Meredith Corporation,/rc/clk?jk=1ae03aed324d6e5a&fccid=c007936ceb766fe5&vjs=3,"Strength Through Diversity | Ground breaking science. Advancing medicine. Healing made personal. |  | The Mount Sinai Clinical Informatics Center (MSCIC) aims to translate and unify data from the Mount Sinai Health System (MSHS) into actionable information and insights that can be mobilized against COVID-19 and ensure that MSHS is ready for similar public health threats in the future. |  | MSCIC is hiring a centralized Engineering Core that will build an Informatics Crisis Response Platform. This platform will include two components: (1) a Critical Informatics Consultation Service that provides MSHS clinicians and researchers easy-to-digest answers to pressing clinical questions (2) a Rapid Clinical Intervention Toolkit that facilitates the practice of evidence-based medicine in the MSHS by feeding insights from data science into the daily workflow via the electronic medical record. | Who we are looking for: | MSCIC is looking to hire experienced data engineers who will help build the platform that will enable informaticists, researchers, and clinicians to collaborate in a common environment, bring data into clinical settings in novel ways, and provide practitioners with more information when making diagnoses and treatment plans. |  | Roles &amp; Responsibilities: | The Senior Data Engineer (Modeling) role will be responsible for working with informaticists and clinicians across the Mount Sinai Health System (MSHS) to gather requirements for important features to be built into the Critical Informatics Consult Repository. | The Senior Data Engineer will have the opportunity to: | curate valuable datasets in the Consult Repository, which includes ingestion, documentation | develop requirements with informaticists and clinicians to communicate with the engineering team, and rest of MSCIC | gain a wide understanding of the many systems throughout the MSHS | ingest data from a wide range of data platforms, formats, and sources | work with data coming in multiple formats (eg. structured, imaging, genetic) | impact patient care by ensuring data is immediately consumable and translatable by both clinicians and informaticists | Requirements: | Ideal candidates will have 5+ years technical experience – preferably in industry - including competencies in the following: | clinical data experience | data warehousing modeling practices (eg. dimensional/star schema, federated models, high normal form modeling) | working with highly sensitive and regulated Protected Health Information (PHI), and navigating related regulations/rules | deep understanding of ETL/ELT practices | experience in formal QA/QC environments | deep understanding of SQL for RDBMS | written production software in compiled languages (Java, Scala, Haskell, Go). | written production software in interpreted languages (Python, Ruby, Perl, or PHP) | code repository platforms such as Git, Subversion, Mercurial, etc. | cloud infrastructure administration in Azure, AWS, or Google Cloud | SysAdmin experience in Linux environments | Hadoop, Spark, or similar big data environments |  | Strength Through Diversity |  | The Mount Sinai Health System believes that diversity and inclusion is a driver for excellence. We share a common devotion to delivering exceptional patient care. Yet we’re as diverse as the city we call home- culturally, ethically, in outlook and lifestyle. When you join us, you become a part of Mount Sinai’s unrivaled record of achievement, education and advancement as we revolutionize healthcare delivery together. |  | We work hard to recruit and retain the best people, and to create a welcoming, nurturing work environment where you have the opportunity and support to develop professionally. We share the belief that all employees, regardless of job title or expertise, have an impact on quality patient care. |  | Explore more about this opportunity and how you can help us write a new chapter in our story! |  | Who We Are |  | Over 38,000 employees strong, the mission of the Mount Sinai Health System is to provide compassionate patient care with seamless coordination and to advance medicine through unrivaled education, research, and outreach in the many diverse communities we serve. |  | Formed in September 2013, The Mount Sinai Health System combines the excellence of the Icahn School of Medicine at Mount Sinai with seven premier hospitals, including Mount Sinai Beth Israel, Mount Sinai Brooklyn, The Mount Sinai Hospital, Mount Sinai Queens, Mount Sinai West (formerly Mount Sinai Roosevelt), Mount Sinai Morningside, and New York Eye and Ear Infirmary of Mount Sinai. |  | The Mount Sinai Health System is an equal opportunity employer. We promote recognition and respect for individual and cultural differences, and we work to make our employees feel valued and appreciated, whatever their race, gender, background, or sexual orientation. |  | EOE Minorities/Women/Disabled/Veterans",New York NY 10029,Data Engineer - Data Modeling - Mount Sinai Clinical Informatics Center
Mount Sinai,/rc/clk?jk=2ad5d76d95dc3c74&fccid=c7342f41f3559451&vjs=3,"Diamond Foundry Inc. is one of fastest growing, profitable technology startups. Creating the future of diamonds for jewelry and semiconductor applications, we use proprietary plasma reactor technology to produce high-quality diamond without carbon footprint and sell diamond jewelry online via our VRAI direct-to-consumer brand. | We are looking for a data engineer to conceptualize, test, and design improvements for manufacturing related data collection systems. We have built our data collection system from scratch, and use it to report key business metrics and analyze our manufacturing process in real-time. You'll be working with an experienced team of mechanical, electrical, metrology, and software engineers, both assisting senior engineers and taking responsibility for your own engineering projects. Your goal will be to achieve maximum productivity for our production line through improving data extraction techniques, standardizing data collection methods, and ensuring data security. | Responsibilities: | Leading the design, development, and deployment of new BI reports, frameworks, and tools, including visual insights, detail reports, and dashboards. | Building new BI reports to provide insight into production metrics for external customers while mitigating proprietary information exposure. | Optimizing all stages of the data lifecycle, from initial planning, to ingest, through final display and beyond | Designing and implementing data extraction, cleansing, transformation, loading, and replication capabilities | Organizing and maintaining documentation so others are able to understand and use it | Implement, troubleshoot and maintain security standards for data storage, transfer, and interpretation | Develop highly available, highly scalable, encrypted, data storage, and disaster recovery for our systems | Designing new capabilities for data analysis and sourcing solutions from software service providers when appropriate | Performing data correction and providing data correction tools to enable a self sustaining dataset | Requirements: | Strong communication and data presentation skills, experience analyzing data and communicating the results to senior business leaders | Ability to work cross-functionally, building and maintaining trust with internal stakeholders | Experience in querying and manipulating large data sets using data querying languages (e.g. SQL), scripting languages (e.g. Python), or statistical/mathematical software (e.g. R, SAS, Matlab) | Experience with GCP (Google BigQuery, CloudSQL, Looker) | Bachelor's degree in Computer Science or similar engineering discipline | 3+ years of software/SQL/CS engineering experience | Experience with maintaining security protocols across an organization | A qualified candidate will have a subset of these skills: | Software: SQL, Looker, Python, HTML, Java | Experience in building reports and real time BI analytics | Operational knowledge of firewalls, routers, switches, servers both locally and in the cloud | We are rapidly expanding and are hiring for positions in: |  San Francisco, CA - technology |  Los Angeles, CA - ecommerce |  Wenatchee, WA – foundry |  Shanghai, CN - ecommerce",San Francisco CA,Data Engineer
LEGO,/company/ibrain-Technologies-Inc/jobs/Mid-Level-Data-Engineer-69054ebff6872357?fccid=007aebf599ab5183&vjs=3,"Job detailsSalaryUp to $65,000 a yearJob TypeFull-timeContractNumber of hires for this role2 to 4QualificationsETL: 2 years (Preferred)Data Warehouse: 2 years (Preferred)US work authorization (Preferred)Full Job DescriptionHiring Mid-level Data Engineering role, so any experience in data warehousing, ETL, Azure data engineering would be ideal.Work Auth: US Citizen or GC holderLocation: RemoteType of Hire: Direct hireWe are looking for a Jr. Data Engineer to join our team in the ongoing development and support of enterprise solutions for data warehousing, enterprise analytics, and integrations across our enterprise, and support the ongoing buildout of our data management and analytics environment on Microsoft Azure platform.A Data Engineer will also be responsible for:Recreating existing application logic and functionality in the Azure and SQL Database environment.Working with external partners to bring data into the firm's environment.Designing and maintaining database schemas.Working with the Data Team to translate functional specifications into technical specifications.Additional Responsibilities:Work with external resources on large projects.Work with other internal technical personnel to troubleshoot issues and propose solutions.Support compliance with data stewardship standards and data security procedures.Apply proven communication and problem-solving skills to resolve support issues as they arise.QualificationsBachelor’s degree or equivalent experience.1-3+ years of experience in a Database Developer or Data Engineer role.Understanding of data management (e. g. permissions, security, and monitoring).Experience with scripting languages such as R and Python.Knowledge of software development best practices.Excellent analytical and organization skills.Effective working in a team as well as working independently.Strong written and verbal communication skills.Preferred applicants will also have:Expertise in database development projects and ETL processes.Experience in an agile SDLC environment.Experience planning and implementing QA and testing, and data warehousing.Experience with Microsoft Azure, Data Factory, and SQL DatabaseThanks,Siva636-489-1625Siva @ ibrain-tech.comJob Types: Full-time, ContractPay: Up to $65,000.00 per yearSchedule:8 hour shiftExperience:ETL: 2 years (Preferred)Data Warehouse: 2 years (Preferred)Work Location:Fully Remote",Remote,Mid Level Data Engineer
Spotify,/rc/clk?jk=9c2863a2f39e7b4d&fccid=fe404d18bb9eef1e&vjs=3,"Engineering | Data | The Personalization team makes deciding what to play next easier and more enjoyable for every listener. From Daily Mix to Discover Weekly, we’re behind some of Spotify’s most-loved features. We built them by understanding the world of music and podcasts better than anyone else. Join us and you’ll keep millions of users listening by making great recommendations to each and every one of them. | Location | New York, NY | Job type | Permanent | We are looking for a Senior Data Engineer to focus on technical designs, implementation, and support of the platform that helps power Spotify’s personalized features. You will play an active role in improving the health and scalability our data systems. You will join a team of technologists, product insight specialists, designers, and product managers in New York, with customers in New York, Boston, Stockholm, and London. | What You'll Do | Help drive optimization, testing and tooling to improve data quality. | Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform. | Use standard methodologies in continuous integration and delivery. | Collaborate with other software engineers, Machine Learning specialists, and partners | Tackle leadership opportunities that will arise every single day. | Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives. | Who You Are | You've worked with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra. | You know how to write distributed, high-volume services in Java or Scala. | You are knowledgeable about data modeling, data access, and data storage techniques. | You appreciate agile software processes, data-driven development, reliability, and responsible experimentation. | You understand the value of partnership within and between teams. | Perks of being in the band | Extensive learning opportunities, through our dedicated team, GreenHouse. | Flexible share incentives letting you choose how you share in our success. | Global parental leave, six months off - fully paid - for all new parents. | All The Feels, our employee assistance program and self-care hub. | Flexible public holidays, swap days off according to your values and beliefs. | Spotify On Tour, join your colleagues on trips to industry festivals and events. | Learn about life at Spotify | You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens. | Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service with a community of more than 345 million users.",New York NY,Senior Data Engineer - Personalization
Accenture,/rc/clk?jk=3ae075b3ae549f46&fccid=a4e4e2eaf26690c9&vjs=3,"Job detailsJob TypeContractFull Job DescriptionACCENTURE's Flexible Workforce solves clients’ toughest challenges by providing cross-industry expertise, unmatched innovation, World-class tech and talent. We help bring it all together to deliver tangible business outcomes for our clients with contractors and our extended workforce opportunities. Accenture is consistently recognized on FORTUNE’s 100 Best Companies to Work for and Diversity Inc’s Top 50 Companies for Diversity lists. And that's just the beginning. Now is the perfect time for you to consider opportunities through our Flexible Workforce. | What's In It For You: | Collaborate with a diverse network of people | Actively deliver innovative solutions for Accenture's clients | Apply your skills and experience to help drive business transformation | Work locally or remotely, significantly reducing or eliminating the demands to travel | Project Description: | We are seeking a Digital Data Engineer to work remote | This is a contract opportunity that does not offer sponsorship now or in the future. | Must be able to work on W2. | Responsibilities: | The Digital Data Engineer will be responsible for migration of applications on DataStage to Hadoop based platform. Individuals in this role are responsible for delivering the projects end-end using latest tools/technologies apt to the requirement playing multiple roles as the requirements demand. | Basic Qualifications: | IBM Datastage | Design, Develop and Implement ETL frame work for data movement through the enterprise. | Acquire data from primary or secondary data sources | Identify, analyze, and interpret trends or patterns in complex data sets | Transforming some legacy ETL logic using DataStage application into Oracle, SQL Server and HDFS Platform | Innovate new ways of managing, transforming and validating data | Establish and enforce guidelines to ensure consistency, quality and completeness of data assets | Apply quality assurance best practices to all work products | Analyze, design and code business-related solutions, as well as core architectural changes, using an Agile programming approach. | Extract Transform and Load (ETL) | Hadoop | IBM InfoSphere DataStage | SQL | Keywords: Datastage, ETL, SQL, Hadoop, Oracle, Agile | Equal Employment Opportunity Statement | Accenture is an Equal Opportunity Employer. We believe that no one should be discriminated against because of their differences, such as age, disability, ethnicity, gender, gender identity and expression, religion or sexual orientation. Our rich diversity makes us more innovative, more competitive and more creative, which helps us better serve our clients and our communities. |  |  | All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law. |  |  | Accenture is committed to providing veteran employment opportunities to our service men and women. |  |  | For details, view a copy of the Accenture Equal Opportunity and Affirmative Action Policy Statement |  |  | Requesting An Accommodation |  |  | Accenture is committed to providing equal employment opportunities for persons with disabilities or religious observances, including reasonable accommodation when needed. If you are hired by Accenture and require accommodation to perform the essential functions of your role, you will be asked to participate in our reasonable accommodation process. Accommodations made to facilitate the recruiting process are not a guarantee of future or continued accommodations once hired. |  |  | If you would like to be considered for employment opportunities with Accenture and have accommodation needs for a disability or religious observance, please call us toll free at 1 (877) 889-9009, send us an email or speak with your recruiter. |  |  | Other Employment Statements |  |  | Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture. | Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration. |  |  | Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process. |  |  | The Company will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. Additionally, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the Company's legal duty to furnish information.",Remote,Digital Data Engineer
USAA,/rc/clk?jk=146168467732ea9b&fccid=3a1edc2d763c4288&vjs=3,"Purpose of Job | We are currently seeking a talented Data Engineer III – Entry Level for our San Antonio, TX facility. |  | The candidate selected for this position will work in the IT P&amp;C Data and Analytics organization on Laura Newsom’s pricing team. They will be responsible for supporting property and auto insurance Pricing applications. |  | Data Engineers deliver quality reporting and data intelligence solutions to the organization and assist client teams with drawing insights to make informed, data driven decisions. Data Engineers (DEs) are engaged in all phases of the data management lifecycle; gather and analyze requirements, collect, process, store and secure, use, share and communicate, archive, reuse and repurpose data. Identify and manage existing and emerging risks that stem from business activities and ensure these risks are effectively identified and escalated to be measured, monitored, and controlled. | Job Requirements | About USAA | USAA knows what it means to serve. We facilitate the financial security of millions of U.S. military members and their families. This singular mission requires a dedication to innovative thinking at every level. | We embrace a robust veteran workforce and encourage veterans and veteran spouses to apply. | USAA Careers – World Class Benefits (31 seconds) | Identifies and manages existing and emerging risks that stem from business activities and the job role. | Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled. | Follows written risk and compliance policies and procedures for business activities. | Collaborate with senior engineers and assist in the implementation of technical solutions. | Design, write, test and deploy data pipeline code. | Participate in design and code review sessions. | Requirements | Bachelor's degree in related field of study, | OR | Certification from an approved technical field of study, | OR | 4 additional years of related experience beyond the minimum required. | 0 to 2 years of data management experience implementing data solutions or coursework in applicable discipline | *Qualifications may warrant placement in a different job level* | When you apply for this position, you will be required to answer some initial questions. This will take approximately 5 minutes. Once you begin the questions you will not be able to finish them at a later time and you will not be able to change your responses. | Preferred | Working knowledge of SQL and/or Python | Work experience of any of these technologies: Informatica, Snowflake, DBT, Control-M | 1 to 2 years of basic knowledge in data analysis, modeling, and data management | Compensation: | USAA has an effective process for assessing market data and establishing ranges to ensure we remain competitive. You are paid within the salary range based on your experience and market position. The salary range for this position is: $60,300 - $108,600 *(this does not include geographical differential, it may be applied based on your work location) | Employees may be eligible for pay incentives based on overall corporate and individual performance or at the discretion of the USAA Board of Directors. | Geographical Differential: Geographic pay differential is additional pay provided to eligible employees working in locations where market pay levels are above the national average. | Shift premium will be addressed on an individual-basis for applicable roles that are consistently scheduled for non-core hours. | Benefits: | At USAA our employees enjoy best-in-class benefits to support their physical, financial, and emotional wellness. These benefits include comprehensive medical, dental and vision plans, 401(k), pension, life insurance, parental benefits, adoption assistance, paid time off program with paid holidays plus 16 paid volunteer hours, and various wellness programs. Additionally, our career path planning and continuing education assists employees with their professional goals. | Please click on the link below for more details. | USAA Total Rewards | The above description reflects the details considered necessary to describe the principal functions of the job and should not be construed as a detailed description of all the work requirements that may be performed in the job. | At USAA our employees enjoy one of the best benefits packages in the business, including a flexible business casual or casual dress environment, comprehensive medical, dental and vision plans, along with wellness and wealth building programs. Additionally, our career path planning and continuing education will assist you with your professional goals. | USAA also offers a variety of on-site services and conveniences to help you manage your work and personal life, including seven cafeterias, two company stores and three fitness centers. | Relocation assistance is not available for this position. | For Internal Candidates: | Must complete 12 months in current position (from date of hire or date of placement) or must have manager’s approval prior to posting. | Last day for internal candidates to apply to the opening is 3/04/21 by 11:59 pm CST time.",San Antonio TX 78288,Data Engineer III – Entry Level
Honeywell,/rc/clk?jk=f1c8a14da32677fb&fccid=19ced65bee470f7d&vjs=3,"Mulligan Funding is a leading working capital lender providing businesses across the country with the capital they need to successfully compete, grow and thrive, now more than ever. With over $450M in funding to thousands of businesses throughout the country, Mulligan Funding is entering a new stage of growth. |  | We’re looking for a junior or entry-level Data Engineer to help us automate data processes and build datasets for analysis and reporting. In this role, you’ll work with Backend Engineers and Data Scientists to generate and validate data that is used for making business decisions and building predictive models. |  | Responsibilities: | Automate current data processes and add logging and monitoring.Partner with the Analytics team to deliver data solutions that meet internal requirements within agreed-upon timelines.Write scripts and queries to assist with ad hoc data analysis.Monitor and remediate data quality issues.Modify database schema. |  | Qualifications: | Strong organizational skills and attention to detail.Demonstrated problem-solving and decision-making ability.Good verbal and written communication skills.Ability to work in a fast-paced, rapidly-changing entrepreneurial environment. |  | Requirements: | Bachelor’s degree in computer science or related field; or equivalent experience.Bootcamp programs will also be considered.Experience programming in Python, Scala, or Java (Python preferred).Experience with SQL and relational databases. |  | Preferred: | Familiarity with the PyData stack, especially pandas.Experience with Excel.Experience with BI tools (PowerBI, Tableau).Experience with AWS or Azure products. |  | Benefits | Competitive compensation package.Great Medical, Vision and Dental Benefits.401K with matching contribution.Sick, Vacation, and Holidays.Gym membership contribution.Be part of a dynamic, growing team in a highly regarded organization.Start-up culture, within an established business with 10 years of experience.Great central location – Easy access from highway.An entrepreneurial, fast-paced, highly collaborative and growing environment, with the ability to contribute meaningfully to the success of the enterprise. |  | If you’re up for the challenge and want to learn more, send your resume and cover letter to careers@mulliganfunding.com. |  | Mulligan Funding is an Equal Opportunity Employer (EOE) and takes great pride in building a diverse work environment. Qualified applicants are considered for employment without regard to age, race, religion, gender, national origin, sexual orientation, disability or veteran status. | To apply, send an email to Careers@MulliganFunding.com. Don’t forget to attach your cover letter and resume!",San Diego CA 92123,Jr. Data Engineer
Q-Centrix,/company/PCS-GLOBAL-TECH/jobs/Data-Engineer-Integration-Developer-e9e534e7a9923d71?fccid=252beafbd99a6037&vjs=3,"Job detailsSalary$65,000 - $70,000 a yearJob TypeFull-timeInternshipContractQualificationsBachelor's (Preferred)SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)US work authorization (Preferred)Full Job Description** The job descriptions will vary depending on the individual project but here are the primary roles you could be working on after internship **Descriptions of some of them: **SQL Developer**ResponsibilitiesHelp write and optimize in-application SQL statements.Ensure performance, security, and availability of databases.Prepare documentation and specifications.Handle common database procedures such as upgrade, backup, recovery, migration, etc.Write TSQL scripts and objects such as stored procedures, user-defined functions, views, indexes per business requirements.Create ETL SSIS packages to migrate data from OLTP sources to OLAP destinations through available tasks and transformations in SSIS.Design and create user-interactive reports in SSRS, Power BI, and Tableau.Profile server resource usage, and optimize and tweak as necessary.Collaborate with other team members and stakeholders.**QualificationsBS or MS of a degree in Computer Science, Information Technology, Engineering or a related field is requiredProficiency with SQL and its variation among popular databasesSkilled at optimizing large complicated SQL statementsCapable of configuring popular database engines and orchestrating clusters as necessaryAbility to plan resource requirements from high-level specificationsGood communication skillsBusiness Intelligence DeveloperMaintain, support, and enhance the business intelligence data backend, including data warehouses and data lakes.Build interfaces between the business intelligence systems and other colleges' information systems to maintain a timely and accurate integration of data.Collaborate and work with data analysts in various departments to ensure that data meets their reporting and analysis need.Provide technical guidance for design and implementation of data governance systems and policy.Work extensively towards SQL Server development in writing core TSQL scripts to implement complex business logic.Develop SSIS packages to implement complex ETL strategies as a part of business requirements for the population of dimensional data structures.Work on creating SSAS cubes and various cube objects such as KPIs (Key Performance Indicators), calculated members, attribute hierarchies, and perspectives.Design eye-catching reports with data from OLTP database, dimensional data structure and OLAP cubes for business reporting purposes.Keep abreast of new business intelligence technologies**QualificationsBS or MS of a degree in Computer Science, Information Technology, Engineering or a related field is requiredAt least 1 year of experience using Microsoft SQL Server Database, SSIS, SSRS, SSAS is requiredExperience with other relational databases, BI reporting and data discovery tools are preferredData Analyst/Engineer**Responsibilities: Provides plan with data, reporting, and analyses that enable data driven decision making.Provides summary analyses in written and oral presentation settings.Builds database from scratch. And prepares complex presentations.Develops system test cases and documents results, researches system issues, and documents findings.**Requirements: Bachelor’s degree in Science, Technology, Engineering or MathematicsAbility to translate business requirements into non-technical, lay termsUnderstanding of addressing and metadata standardsHigh-level written and verbal communication skillsJunior Data Analyst/Engineer**Responsibilities: Managing master data, including creation, updates, and deletion.Managing users and user roles.Provide quality assurance of imported data, working with quality assurance analyst if necessary.Commissioning and decommissioning of data sets.Processing confidential data and information according to guidelines.Helping develop reports and analyses.Managing and designing the reporting environment, including data sources, security, and metadata.Supporting the data warehouse in identifying and revising reporting requirements.Supporting initiatives for data integrity and normalization.Assessing tests and implementing new or upgraded software and assisting with strategic decisions on new systems.Generating reports from single or multiple systems.Troubleshooting the reporting database environment and reports.Evaluating changes and updates to source production systems.Training end-users on new reports and dashboards.Providing technical expertise in data storage structures, data mining, and data cleansing.**Requirements: Bachelor’s degree in Science, Technology, Engineering or Mathematics1-3 years of work experience as a data analyst or in a related field is preferredAbility to work with stakeholders to assess potential risks.Ability to analyze existing tools and databases and provide a software solution recommendations.Ability to translate business requirements into non-technical, lay terms.High-level experience in methodologies and processes for managing large scale databases.Demonstrated experience in handling large data sets and relational databases.Understanding of addressing and metadata standards.High-level written and verbal communication skills.Due to the global situation, this internship will be remote until COVID-19.Job Types: Full-time, Contract, InternshipSalary: $65,000.00 - $70,000.00 per yearBenefits:Dental insuranceHealth insuranceLife insuranceVision insuranceSchedule:8 hour shiftMonday to FridayCOVID-19 considerations:Due to COVID-19, the internship will be remoteAbility to Commute/Relocate:Wayne, PA (Preferred)Education:Bachelor's (Preferred)Experience:SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Willingness To Travel:100% (Preferred)Work Location:Multiple locationsInternship Compensation:PayVisa Sponsorship Potentially Available:Yes: H-1B work authorizationWork Remotely:Temporarily due to COVID-19",Delaware PA,Data Engineer and Integration Developer
Healthfirst,/rc/clk?jk=ae17f4de81e6bfd5&fccid=61a3c30968b3442a&vjs=3,"Have you ever wondered what happens inside the cloud? |  | Based in New York, DigitalOcean is a dynamic, high-growth technology company that serves a robust and passionate community of developers, teams, and businesses around the world. We believe that today's entrepreneurs are changing the world through software. Our mission is to empower these entrepreneurs by bringing modern app development within reach for any developer, anywhere in the world. |  | We want people who are passionate about creating experiences that our customers will love. |  | As a Data Engineer, you will join a growing Data Engineering team within our Engineering Services organization that collaborates with decision-makers across the organization to catalyze business growth by providing insightful and actionable analysis, insights and data products. The Data Engineering team is hands-on with a wide variety of datasets, including user data, product behavior data, financial/payment data, upper-funnel marketing data, trust and safety data, and operational/infrastructure data, and is responsible for leveraging that data into usable analytical products by stakeholders across the company. |  | What You'll Be Doing: | Develop and implement metrics and dimensions for powering analytical use cases across the company, incorporating a wide variety of data sources across the company at varying levels of complexity and scale | Focus on data quality of the data environment and data products being delivered to the business, and effectively communicate to internal user base regarding production status | Interface closely with data infrastructure, engineering and technical operations teams to ensure correctness and soundness of metrics built in the data environment and availability of data product services | Pioneer initiatives around data quality, integrity, security and governance | Work closely with data stakeholders across the company, both technical and non-technical, to understand evolving needs as more complex data models are introduced for reporting and data science | What We'll Expect from You: | Bachelor's degree in Computer Science, Math, Statistics, Economics, or other quantitative field; or equivalent experience. | Experience in custom ETL design, implementation and maintenance | Track record of developing in complex data environments and intelligence platforms for business users | Demonstrable ability to relate high-level business requirements to technical ETL and data infrastructure needs, including underlying data models and scripts | History of proactively identifying forward-looking data engineering strategies, utilizing appropriate technologies, and implementing at scale | Extensive hands-on experience with schema design and dimensional data modeling | Experience interacting with key stakeholders in different fields, interpreting challenges and opportunities into actionable engineering strategies | Experience with analytics databases like Snowflake, Redshift, or BigQuery. | Advanced SQL and relational database knowledge (MySQL, PostgreSQL) in addition to warehousing and dimension modeling | Experience scripting in Go or Python or a similar scripting language. | Effective communication and interpersonal skills | Experience implementing dimensional modeling in a configuration tool like dbt or LookML a plus | Experience designing and building dashboards in BI tools like Looker, Tableau, or PowerBI a plus. | Experience with job schedulers (Airflow, Luigi, Azkaban, etc.) a plus | Experience with ephemeral/streaming metrics services (Prometheus, DataDog, etc) and with SLO/SLI driven analytics a big plus. | Why You'll Like Working for DigitalOcean: | We value development. You will work with some of the smartest and most interesting people in the industry. We are a high-performance organization that is always challenging ourselves to continuously grow. We maintain a growth mindset in everything we do and invest deeply in employee development through formalized mentorship, LinkedIn Learning tracks, and other internal programs. We also provide all employees with reimbursement for relevant conferences, training, and education. | We care about your physical, financial and mental well-being. We offer competitive health, dental, and vision benefits for employees and their dependents, a monthly gym reimbursement to support your physical health, and a commute or internet allowance to make your trips to your office or your desk easier. We offer generous parental leave with transition time built-in upon return to work. We offer competitive compensation and a 401k plan with up to a 4% employer match. | We support our remote employee experience. While we have great office spaces in NYC, Cambridge and Palo Alto, we're very distributed—we use a number of communication tools to connect across the company—and all remote employees have the opportunity to visit our offices and meet their teams face-to-face at team offsites. We also have an annual company offsite, Shark Week, to get quality in-person time with the entire company at least once a year. We also allow employees to outfit their workstations to meet their needs—whether remote or in office. | We value diversity and inclusivity. We are an equal opportunity employer and we do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. |  |  | Department: Engineering #LI-Remote | Want to learn more about our Engineering team? Click here! |  | Want an inside look into life at DO? Click here to hear from our employees!",New York State,Data Engineer
Oscar Health,/rc/clk?jk=338dced2115e92ed&fccid=402a1c36c3f1508f&vjs=3,"Hi, we're Oscar. We're a fully licensed health insurer. Our goal is to make healthcare simple, transparent, and human. In Engineering, we do that by building reliable and maintainable applications, infrastructure, and interfaces that make interacting with the health care system easier for everyone - members, providers, and (yes!) ourselves. As an Oscar Engineer, you'll be joining a tight knit, passionate team of builders. We take pride in our ability to move quickly (deploys &gt; 100/day), our small teams (engineers &lt;= 5), and our willingness to tackle big problems (frequency == everyday). |  | While our technology team of over 200 works in NYC, we are looking to build out a core team in our Los Angeles location. This is a unique opportunity to have the resources and stability of an established technology company, but have the freedom and flexibility of a 20 person startup. |  | You will report into our Engineering leader in our New York City office. |  | What we're actually doing: |  | The following is a list of projects our teams are currently working on: |  | Vitals alerting: Implement alerting on elevated vitals measurements from patients so clinicians can intervene and get patients care sooner | Virtual Primary Care (VPC) team management: Build tools for patient/doctor management to support the growth of our VPC offering | Hybrid visits: Extend our platform to support both Virtual and In-Person visits from our providers |  | You at Oscar |  | You care about what you do. | You care about what we do. | You are a self starter- you like being self led | You're willing and able to learn quickly, whether it be a new shiny technology or an arcane, ill-conceived data structure; our company may be new, but the health industry isn't! | You pride yourself on building high-performance, fault-tolerant, and scalable distributed software systems. | You have at least 4 years of professional work experience. |  | Anything else is a bonus. We believe diversity is a key ingredient in building something great. |  | Here is more information on who we are. |  | Oscar Tech Blog |  | At Oscar, being an Equal Opportunity Employer means more than upholding discrimination-free hiring practices. It means that we work to cultivate an environment where exceptional people can be their most authentic selves and find both belonging and support. We're on a mission to change healthcare - an experience made whole by your unique background and perspectives. |  | Oscar applicants are considered solely based on their qualifications, without regard to applicant's disability or need for accommodation. Any Oscar applicant who requires reasonable accommodations during the application process should contact the Oscar Benefits Team (accommodations@hioscar.com) to make the need for an accommodation known. |  | Pay Transparency Policy: Oscar's Pay Transparency Policy ensures that you won't be discharged or discriminated against based on whether you've inquired about, discussed, or disclosed your pay. Read the full policy here.",New York NY,Software Engineer: Data/Systems
IBM,/rc/clk?jk=56f5d6c8075d0151&fccid=66d394411dd2efad&vjs=3,"About Kraken | Kraken is changing the world. Join the revolution! | Our mission is to accelerate the adoption of cryptocurrency so that you and the rest of the world can achieve financial freedom and inclusion. Founded in 2011 and with over 4 million clients, Kraken is one of the world’s largest, most successful bitcoin exchanges and we’re growing faster than ever. Our range of successful products are playing an important role in the mainstream adoption of crypto assets. We attract people who constantly push themselves to think differently and chart exciting new paths in a rapidly growing industry. Kraken is a diverse group of dreamers and doers who see value in being radically transparent. | In less than a decade Kraken has risen to become one of the best and most respected crypto exchanges in the world. We are changing the way the world thinks about money and finance. The crypto industry is experiencing unprecedented growth and Kraken is leading the charge. We’ve grown from 70 Krakenites in January 2017 to over 1200 today and we have no intention of slowing down. | About the Role | The data engineering team is responsible for designing and implementing scalable solutions that allow the company to make data-driven decisions fast and accurately on several terabytes of data. The data engineering team has just launched the company’s new data warehouse, and you will be responsible for creating various pipelines to move vast amounts of data into the new warehouse. | Responsibilities | Build scalable and reliable data pipeline that collects, transforms, loads and curates data from internal systems | Augment data platform with data pipelines from select external systems | Ensure high data quality for pipelines you build and make them auditable | Drive data systems to be as near real-time as possible | Support design and deployment of distributed data store that will be central source of truth across the organization | Build data connections to company's internal IT systems | Develop, customize, configure self service tools that help our data consumers to extract and analyze data from our massive internal data store | Evaluate new technologies and build prototypes for continuous improvements in data engineering | Requirements | 5+ years of work experience in relevant field (Data Engineer, DW Engineer, Software Engineer, etc) | Experience with data warehouse technologies and relevant data modeling best practices (Spark, Presto, Druid, etc) | Experience building data pipelines/ETL and familiarity with design principles | Excellent SQL skills | Proficiency in a major programming language (e.g. Java, C++, etc) and/or a scripting language (Javascript, Python, etc) | Experience with business requirements gathering for data sourcing |  |  | We’re powered by people from around the world with their own unique backgrounds and experiences. We value all Krakenites and their talents, contributions, and perspectives.",Remote,Data Engineer
Apple,/rc/clk?jk=f729eb70ca7d490f&fccid=c1099851e9794854&vjs=3,"Summary | Posted: Feb 16, 2021 | Weekly Hours: 40 | Role Number:200223310 | Do you love problem solving and thinking beyond an obvious solution? Are you passionate about data accuracy and improving operational efficiency by managing complex data and information delivery? We are! | The Retail Business Intelligence team is looking for a resourceful, energetic Data Engineer / Information Analyst to lead and assist with the data needs of projects by crafting creative solutions to deliver data to end users. You’ll work with cross-functional teams to stitch data from various systems into unified, high-performance and low-latency reporting layers for end-to-end analyses of projects. Join us, and you’ll help provide the data needed for the organization to effectively manage the business and improve our customer experiences across the world. | Day-to-day responsibilities in the Retail Business Intelligence team include partnering with IT teams to build critical data pipelines, ad hoc data analyses, troubleshooting data and supporting project teams during all stages of projects including, requirement gatherings, gap reviews, development and testing. | If you are a strategic problem solver who can lead and work with cross-functional teams to handle changes, perform data analyses, and execute project requirements to support an ever-changing data landscape, apply today! | Key Qualifications | Structured thinking with ability to easily break down ambiguous problems and propose impactful solutions | Ability to lead, take action and provide goal-oriented direction in the face of ambiguity | Strong business mentality - ability to grasp business needs, translate into technical needs; ability to communicate complex technical concepts to business leaders | Ability to multi-task and own activities across diverse scope, business units and geographical boundaries | Proactive in bringing new, innovative ideas to the table and inspiring change across projects, programs and processes | Self-starter who can take a project from start to finish with minimal direction | Create and maintain comprehensive project documentation | Data savvy in understanding existing data sets and applications when designing new, flexible reporting layers | Excellent problem solving and debugging skills with attention to detail | Expert SQL skills and deep relational database design understanding | Ability to pick up new tech skills independently | Ability to react to re-sets and changes in a dynamic environment | Knowledge of Apple Retail and core products and operating systems preferred | Minimum of 6 years experience in data/technology field | Description | The Data Engineer / Information Analyst is responsible to encourage, mobilize and lead the data track of projects with a focus on common deliverables, goals and timelines. This role will partner with teams across Apple including IS&amp;T, Retail (Online &amp; Stores), Operations, Apple Media Products, and Finance. The following strengths are crucial to success in this role: | - Deal with Ambiguity: be comfortable and confident working in a dynamic environment, partnering with teams to move through the unknown | - Flexibility when handling directional changes and ability to concurrently balance multiple large-scale work streams | - Confidence in the ability to listen and be patient with others — working across dynamic organizations will highlight differences on how Apple departments operate | - Work closely with project managers, stakeholders and IT to deliver data requirements | - Participate in regular design and code reviews and perform debugging and coding to resolve issues | - Provide technical guidance and mentoring to fellow Analysts to help improve overall team growth | - Demonstrate solid understanding of data management, system integration and development methodologies including unit testing | - Continuously look for ways to improve and enhance data reliability and performance | Education &amp; Experience | Candidate must possess a Bachelor’s Degree in Information Technology, Mathematics, Computer Science, or equivalent with a minimum of 8 years relevant work experience. Master’s degree is preferred with 6+ years of experience directly related to role requirements.",Austin TX,Data Engineer
Apple,/rc/clk?jk=5cc256e90bb4b316&fccid=a321096b9f1b3c50&vjs=3,"Data is served to hundreds of our clients and their customers. There is a big demand for analytics and data-freshness, so if you are not afraid of complex business problems, and you think traditional tools are slow, and you won’t stop looking for new approaches and technologies, then we have a position for you! | As a Data Engineer, you should have the expertise in the design, creation, management, and business use of large data sets to drive practical insights. You know and love working with analytics tools and can use your technical skills and creative approaches to help clients solve their most critical business challenges. This individual will be an integral part of New York’s Data and Analytics Practice and, in addition to working with clients, collaborate closely with members of the Slalom team who are focused on data visualization, advanced analytics, data science, and data strategy. | What You’ll Do | Collaborate with engineers and business customers to understand data needs, capture requirements and deliver complete BI solutions | Conduct and support white-boarding sessions, workshops, design sessions, and project meetings as needed, playing a key role in client relations | Design and build data extraction, transformation, and loading processes by writing custom data pipelines using either cloud native services (AWS/GCP/Azure) or using open source tools (like Airflow and Python) | Design, implement, and support an Enterprise Data platform (Data Lake, Data Warehouse, etc.) that can provide ad-hoc access to large scale structured, semi-structured, and/or unstructured datasets | Experience with both traditional (i.e. SSIS, Informatica, Talend) and modern (i.e. Dell Boomi) data integration iPaaS technologies | Highly self-motivated to deliver both independently and with strong team collaboration | Ability to creatively take on new challenges and work outside comfort zone | Strong written and oral communications along with presentation and interpersonal skills | Deliver transformative solutions to clients that are aligned to industry best practices and provide thought leadership in data architecture and engineering space | What You’ll Bring | 5+ years of experience in using SQL and databases in a business environment | 5+ years of experience in custom ETL/ELT design, implementation, and maintenance | Experience with schema design and data modeling | Experience applying data architecture or engineering to solve real-world business problems | Experience with building integration and ingestion frameworks leveraging API based tools and platforms (i.e. Dell Boomi) | Manipulating/mining data from database tables (i.e. SQL Server, Redshift, Oracle) | SQL, ETL/ELT optimization, and analytics tools experience (i.e. R, HiveQL) | Prior implementation experience in building both batch and real-time/near real-time data ingestion frameworks using technologies like Kafka, AWS Kinesis etc. | Nice to Have but not Required | Previous management consulting experience | Implementation experience with various technologies under Hadoop eco-system (HDFS, Hive, HBase, Sqoop, Pig, Presto etc.) and Spark (PySpark preferred) | Experience with designing digital data platforms leveraging clickstream data from Adobe Analytics (Omniture/SiteCatalyst) or Google Analytics | Experience in languages such as Python and Spark | Experience working in Agile Scrum teams | Linux and Windows proficiency | About Us | Slalom is a modern consulting firm focused on strategy, technology, and business transformation. In 39 markets around the world, Slalom's teams have autonomy to move fast and do what's right. They are backed by regional innovation hubs, a global culture of collaboration, and partnerships with the world's top technology providers. Founded in 2001 and headquartered in Seattle, Slalom has organically grown to over 8,000 employees. Slalom has been named one of Fortune's 100 Best Companies to Work For five years running and is regularly recognized by employees as a best place to work. Learn more at slalom.com. | Slalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud it invest in benefits that include: meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, &amp; vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer additional benefits such as a yearly $350 reimbursement account for any well-being related expenses as well as discounted home, auto, and pet insurance. | Slalom is an equal opportunity employer that is committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans status, or any other characteristic protected by federal, state, or local laws. | #LI-AB1",New York NY,Data Engineer
Mulligan Funding,/rc/clk?jk=d92d8920e52f8d4a&fccid=c1099851e9794854&vjs=3,"Summary | Posted: Feb 20, 2021 | Weekly Hours: 40 | Role Number:200222711 | Imagine what you could do here. At Apple, new ideas have a way of becoming extraordinary products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. | The people here at Apple don’t just craft products - they build the kind of wonder that’s revolutionized entire industries. It’s the diversity of those people and their ideas that inspires the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it. | Are you passionate about building an extraordinary customer experience? The Support Operations and Technologies Data team is looking for a dynamic and motivated candidate for the role of a Data Engineer. This role will require partnering with multi-functional teams to analyze data and processes to provide opportunities and insights that improve the operation our global contact center teams. | Key Qualifications | 3+ years of data analytics or business analytics experience | Aptitude for synthesizing sophisticated data into key components that drive informed decisions | Excellent time-management skills | Exceptional project management skills | Flexible and able to multitask in a deeply dynamic business | Extraordinary analytic, problem solving, and written/oral communication skills | Strong Tableau experience | Experience with database platforms and reporting tools such as SQL, Teradata, Hadoop, Oracle, and/or MemSQL | Well-organized, detail oriented with excellent follow through | Excellent analytical mentality and attention to detail, especially in predicting and preventing potential future challenges | Ability to maintain a global perspective and see beyond current issues in order to understand downstream impacts of decisions | Self-motivated, driven individuals who are comfortable working in a global, fast-paced, often ambiguous environment with the ability to work with multi-functional teams | Extraordinary teammate with a positive demeanor, high emotional intelligence and the ability to make decisions | Description | Our Data Analysts become specialists in AppleCare’s systems and processes to help motivate change across the world. In collaboration with the business, you will develop data-driven approaches to distill big data into action. As the authority in AppleCare Support Data you will define requirements for the data points collected across AppleCare Support technologies and applications. You will partner with other organizations to ingest and validate data, and assist in educating consumers in utilizing the data efficiently. | You will think creatively and strategically, executing AppleCare Support strategies to make definitive and measurable improvements in support of the global Contact Center. You will provide leadership in a multi-functional environment to articulate data requirements to senior leadership, product owners, and business customers. | The ability to think out of the box and influence peers and management with data driven models is a must have. | Able to maximize data development using appropriate tools, standard methodologies, and techniques, to create quality data at the right time and place to support decisions is a measure of success for this role. | Excellent verbal and written communication &amp; presentation skills is desired. | Education &amp; Experience | Bachelors or Masters degree in Mathematics, Statistics, Computer Science, Operations Research, Data Analytics, or related quantitative social/physical science field.",Austin TX,Support Operations & Technologies Data Engineer
DigitalOcean,/rc/clk?jk=80a292441da3990f&fccid=2ddbc08fb5c49732&vjs=3,"About Orchard |  | Orchard is radically simplifying the way people buy and sell their homes. For the average American, the home purchase and sale process takes months, creates anxiety, and is filled with uncertainty and hassle. Orchard has reimagined the end-to-end experience of buying and selling, from innovative home search tools to find the perfect home to the ability to buy a new home before selling your current one. Orchard customers manage the entire experience through a personalized online dashboard, while also getting the support of best-in-class Orchard real estate agents. |  | Headquartered in New York City and with offices throughout Texas, Colorado, Georgia, North Carolina, and Virginia, Orchard has over 300 employees and growing. We have raised over $130 million in equity financing from top-tier investors including Revolution, Firstmark, Accomplice, Navitas and Juxtapose. Our investors have also backed the likes of Pinterest, AirBnb, Shopify and Sweetgreen. Orchard is proud to be recognized as part of Glassdoor's Best Places to Work. | About the Role |  | As a Data Engineer, you will be involved in creating, updating and maintaining the full business intelligence stack. Focusing on building scalable back-end data architecture but also producing high impact front-end dashboards. Working closely with the Product and Technology teams, you will extract and transform data from new products and build &amp; own the analytics layer of the company's data environment to support our business intelligence tooling. You will help lead the charge to surface critical data to end users and enable leadership to make fast, data-informed decisions. |  | This is a full-time role reporting to our Head of Engineering, and will be based out of our New York City office. |  | What You'll Do Here |  | Spearhead business intelligence infrastructure efforts, including owning the design and implementation of a modular, parallelized codebase of SQL scripts necessary for long-term business intelligence scalability | Dive into and deeply understand new data sources and their underlying data libraries to transform, integrate, and make them accessible for self-directed analysis by stakeholders across the business, including team leads from sales, marketing, product, and operations | Design, create, and continuously improve upon reports, dashboards, etc. to help company stakeholders measure performance and make informed decisions; closely collaborate with functional group leads throughout the reporting lifecycle, from initial scoping to deployment to maintenance |  | We'd Love to Hear From You if You Have |  | 4+ years of experience in a business intelligence, analytics, data science or engineering role. Experience working at a high-growth technology company a plus. | BA/BS degree in a quantitative discipline (Computer Science, Math, Statistics, Physics or Engineering) | Experience working in SQL and Python. Familiarity with Postgres, Redshift, Airflow or Looker is a plus. | Exceptional dimensional data modeling skills. Knowledgeable about data warehouse technical architecture, ETL frameworks, and OLAP principles. | Experience prioritizing, building, and deploying code using business intelligence reporting tools; Looker proficiency a plus. | Experience driving fast-paced projects from scratch to completion (e.g. building a new code base to tackle a complex problem) in a highly organized manner |  | Why Orchard |  | We're proud to be recognized by Glassdoor, Inc. Magazine, Fast Company and Forbes on their lists of best places to work. We also have a 4.9 Glassdoor rating! Orchard is building the first one-stop-shop in real estate and we're bringing together the most innovative professionals across real estate, business, marketing, technology and design. We also have some pretty great perks: |  | Equity participation | Flexible PTO | Up to 18 weeks of paid family leave | Employee discount on Orchard's services |  | We're currently working from home until it's safe for employees to return to the office. We anticipate returning later this year and are excited to welcome people back to our offices and see one another in person. Until then, your interviews will all happen virtually. If there is anything we can do to make your process easier, don't hesitate to let us know! |  | Orchard is proud to be an equal opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected status in accordance with applicable law.",New York NY,Data Engineer
kraken,/rc/clk?jk=6bdde6cb9ccfa957&fccid=8ae55af6d0072bfd&vjs=3,"Who are we? | Q-Centrix is a leading healthcare information solutions provider with offices in Chicago and San Diego, plus more than 900 clinical experts working remotely in 49 states. Our team of smart, ambitious, and fun-loving healthcare professionals are 100% focused on improving the quality of patient care at hospitals throughout the country. | What’s the job? | Q-Centrix is implementing a robust data infrastructure solution to provide further analysis capabilities for our partners in the areas of quality reporting, analysis and improvement. We’re searching for an ambitious and curious team member to report into the Data Architect, Engineering. The Data Engineer will embrace challenging work; while showcasing superior Nerf gun talents throughout the day. You’ll have a leg up if you don’t mind a few orange darts decorating your desktop, love memes and awful puns, and are open to wearing apparel prominently featuring the letter Q. | Applicants for employment with Q-Centrix must be legally authorized to work in the United States now or in the future without sponsorship. | As a Data Engineer, you will… | Work closely with the Data Architect to create and maintain optimal data architecture. | Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies. | Support the Product Development, Operations and Finance Teams in creating and analyzing reports both for our partners and internally. | Envision and create reports that are translated from internal ideas, client feedback and competitive analysis. | Partner with other disciplines within Q-Centrix to ensure project requirements are comprehensive and thoroughly planned and documented. | Work with internal and market-facing stakeholders to validate suggested features and enhancements to Q-Apps. | Be a vital contributor on a product development team accountable for building the industry’s only Healthcare Quality Information System. | You’re a great fit if you… | Experience in building data pipeline within Cloud architecture using workflow management tools(Brownie points if you are experienced in Airflow + DBT). | Experience with object-oriented/object function scripting languages: Python, Java, Scala etc. | Ability to perform root cause analysis on External and Internal processes and data to identify opportunities for improvement. | Have had exposure to data modeling for data warehousing - star, snowflake schema designs | Have worked within the framework of common Project Management structures and practices including waterfall, agile, project planning, scope control, customer relationship management | Keen on learning and growing technical skillset | Great if you have experience in healthcare or a related field | Brownie points if you’re experienced with PostgreSQL and have healthcare industry experience | Who are we? | At Q-Centrix, we hire people who love learning, value innovation and believe in our mission and values to improve outcomes in healthcare. We applaud qualified applicants who are accountable and committed to producing quality work. As an Equal Opportunity Employer, we support and value diversity, dignity and respect in our work environment, and are committed to creating an inclusive environment in which everyone can thrive. | We employ people based on the needs of the business and the job, and their individual professional qualifications. Here’s what does not impact our employment decisions: race, religious creed, religion, color, sex, sexual orientation, pregnancy, parental status, genetic information, gender, gender identity, gender expression, age, national origin, ancestry, citizenship, protected veteran or disability status, health, marital, civil union or domestic partnership status, or any status or characteristic protected by the laws or regulations in locations where we operate. If you are an individual with a qualified disability and you need an accommodation during the interview process, please reach out to your recruiter. | We celebrate and embrace these differences, and take pride in our commitment to being an equal opportunity team.",Chicago IL,Data Engineer
Ontrak Inc.,/company/Ontrak-Inc./jobs/Big-Data-Engineer-2513d4066042b9d5?fccid=ae3523a56c5cb332&vjs=3,"Job detailsSalary$99,000 - $109,000 a yearJob TypeFull-timeQualificationsApache Hive: 2 years (Required)Apache Pig: 2 years (Required)Python: 2 years (Required)US work authorization (Required)Bachelor's (Preferred)Full Job DescriptionTHE POSITIONABOUT THE ROLEAs a Big Data Engineer you will develop and support the effort to develop data-based solutions that benefit the OnTrak business. The BDE will create scripts for extracting, transforming, synchronizing and loading data throughout the HDFS using Hive, Pig in specific and with Spark. The BDE will be creating the new features to enable incredible growth in the coming year. The ideal candidate is a self-starter that can work in an agile development environment.The candidate will be developing primarily Spark, Hive, Pig, Kafka, YARN and Python/R based application scripts. BDE should be familiar with the principles of agile development within HDFS, Hadoop, Hive, Apache Spark (preferably Scala) and NoSQL environment. The ideal candidate will be familiar with micro service development and all web technologies. The candidate must work well in a distributed team environment. This entails an excellent ability to communicate and engage in team development efforts. Experience in the SQL Server, Informatica &amp; Google Cloud and data driven coding techniques is a big advantage.What you’ll get to do:Developing Apache Hive, Pig, Spark, Scala and Python based scripts to support extract transform and load (ETL) functionalities within the enterpriseGathering, cleaning, managing, and maintaining high quality dataArchitecture decision making on best practices in developmentDevelopment of DevOps practices including CI.CD practicesRequirement:3+ years in enterprise developmentFluency in Hive, Pig, Scala, Python and KafkaExpertise in one Database query languageExperience with Github or comparable source code management solutionBachelor's degree in technology-based major **In lieu of degree, equivalent education and/or experience may be consideredPreferred:Experience in solving ETL processesExperience with HDFS, Mongo or NoSQL database Integration experience with SaaS provider (ex Salesforce)Cloudera certificationsCloudera Administrator/Developer trainingHortonworks certificationsOur remote opportunities require:A quiet, private, distraction-free home office work environmentA reliable high-speed internet connection (cable, DSL, or fiber) with speeds of at least 10Mbps download and 5Mbps upload. Most positions will require that you be hard-wired to your internet access and Wi-Fi will be disabled. Traveling with your computer is not allowed unless travel is required for the position.Discipline to work from home while following a set scheduleBackground check clearanceABOUT USOntrak, Inc. is making a positive impact on people’s lives every day. We use predictive analytics to identify health plan members with unaddressed behavioral health conditions that worsen chronic disease, then engage, support, and guide these members to better health with a personalized, human-centered approach. This has led us to where we are today: growing fast and saving lives as we do.To support our explosive growth, we’re looking for compassionate, hard-working people-lovers to join our team. If innovating in the field of patient care is something you’re passionate about, we encourage you to join our mission to improve the health and save the lives of as many people as possible.Impact lives in so many waysYou'll be an integral part in supporting people coping with their unique life challenges. Every member of the Ontrak team contributes to accomplishing our goals and upholding our people-centric values.The new face of mental healthOur model is research-based, and we are invested in staying on the leading edge of treatment. You'll help us break down barriers and stigmas associated with mental health.Career optionsOur ongoing strong growth and evolution, we are looking for people who want to do their best at work. Join our team and take your career to the next level with Ontrak. We are committed to promoting from within.Excellent compensationIn addition to a competitive wage, we offer comprehensive benefits including medical, dental and vision insurance; a 401(k) plan; paid holiday, vacation and sick time; flexible spending accounts; Basic Life/AD&amp;D, Employee Assistance Program and Travel Assistance Program and more.Play 2021 Ontrak Recruitment CommunicationOntrak, Inc. is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status.Notice to candidates: Please visit our fraud alert to protect yourself from scams targeting job seekers: https://www.ontrak-inc.com/fraud-alert.html#LPIndeedJob Type: Full-timePay: $99,000.00 - $109,000.00 per yearCOVID-19 considerations:Virtual work environmentEducation:Bachelor's (Preferred)Experience:Apache Hive: 2 years (Required)Apache Pig: 2 years (Required)Python: 2 years (Required)Work Location:Fully RemoteVisa Sponsorship Potentially Available:No: Not providing sponsorship for this jobCompany's website:https://catasys.com/careers.htmlBenefit Conditions:Waiting period may applyCOVID-19 Precaution(s):Remote interview processVirtual meetings",Remote,Big Data Engineer
Slalom Consulting,/rc/clk?jk=5b0003520379caa2&fccid=33b2d0072564c18e&vjs=3,"The Data Engineer is responsible for data acquisition strategies and integration scripting and tools, data migrations, conversions, purging and back-ups; fulfills data acquisition strategy requirements. They work with product, financial control, analysts, users and other stakeholders to understand business requirements and supports data architecture to translate into data acquisition strategies. The Data Engineer will author artifacts defining standards and definitions for storing, processing and moving data, including associated processes and business rules. Additionally, the Data Engineer will map the details within these their artifacts to business processes, non-functional characteristics, QA criteria and technical enablement. The role is responsible to be constantly thinking through the needs of the business to support efficient and error free processes. | The Data Engineer will be responsible for finding trends in datasets and developing workflows and algorithms to help make raw data more useful to the enterprise. He or she will also be responsible for createing data acquisition strategy and develops data set processes. | Designs and implements standardized data management procedures around data staging, data ingestion, data preparation, data provisioning and data destruction (scripts, programs, automation, assisted by automation, etc). | Ensures quality of technical solutions as data moves across Healthfirst environments | Provide insight into the changing data environment, data processing, data storage and utilization requirements for the company and offer suggestions for solutions | Ensures managed analytic assets support Healthfirst’s strategic goals by creating and verifying data acquisition requirements and strategy | Develop, construct, test and maintain architectures | Align architecture with business requirements and use programming language and tools | Identify ways to improve data reliability, efficiency and quality | Conduct research for industry and business questions | Deploy sophisticated analytics programs, machine learning and statistical methods | Prepare data for predictive and prescriptive modeling and find hidden patterns using data | Use data to discover tasks that can be automated | Create data monitoring capabilities for each business process and work with data consumers on updates | Aligns data architecture to Healthfirst solution architecture; contributes to overall solution architecture | Help maintain the integrity and security of the company data | Minimum Qualifications: | Bachelor’s Degree in Computer Engineering or related field | 7+ years’ experience in a data engineering | 10+ years’ experience in data programing languages such as java or python | 4+ years’ experience working in a Big Data ecosystem processing data; includes file systems, data structures/databases, automation, security, messaging, movement, etc. | 3+ years’ experience working in a production cloud infrastructure | Preferred Qualifications: | Proven track record of success directing the efforts of data engineers and business analysts within a deadline-driven and fast-paced environment | Hands on experience in leading healthcare data transformation initiatives from on-premise to cloud deployment | Demonstrated experience working in an Agile environment as a Data Engineer | Hands on work with Amazon Web Services, including creating Redshift data structures, accessing them with Spectrum and storing data in S3 | Knowledge of SQL and multiple programming languages in order to optimize data processes and retrieval. | Proven results using an analytical perspective to identify engineering patterns within complex strategies and ideas, and break them down into engineered code components | Knowledge of provider-sponsored health insurance systems/processes and the Healthcare industry | Experience developing, prototyping, and testing engineered processes, products or services | Proven ability to work in distributed systems | Proficiency with relational, graph and noSQL databases required; expertise in SQL | Must be able to develop creative solutions to problems | Demonstrates critical thinking skills with ability to communicate across functional departments to achieve desired outcomes | Excellent interpersonal skills with proven ability to influence with impact across functions and disciplines | Ability to work independently and as part of a team | Ability to manage multiple projects/deadlines, identifying the necessary steps and moving forward through completion | Skilled in Microsoft Office including Project, PowerPoint, Word, Excel and Visio | WE ARE AN EQUAL OPPORTUNITY EMPLOYER. Applicants and employees are considered for positions and are evaluated without regard to race, color, religion, gender, gender identity, sexual orientation, national origin, age, genetic information, military or veteran status, marital status, mental or physical disability or any other protected Federal, State/Province or Local status unrelated to the performance of the work involved. | If you have a disability under the Americans with Disability Act or a similar law and want a reasonable accommodation to assist with your job search or application for employment, please contact us by sending an email to careers@Healthfirst.org or calling 212-519-1798 . In your email please include a description of the accommodation you are requesting and a description of the position for which you are applying. Only reasonable accommodation requests related to applying for a position within Healthfirst Management Services will be reviewed at the e-mail address and phone number supplied. Thank you for considering a career with Healthfirst Management Services. | All hiring and recruitment at Healthfirst is transacted with a valid “@healthfirst.org” email address only or from a recruitment firm representing our Company. Any recruitment firm representing Healthfirst will readily provide you with the name and contact information of the recruiting professional representing the opportunity you are inquiring about. If you receive a communication from a sender whose domain is not @healthfirst.org, or not one of our recruitment partners, please be aware that those communications are not coming from or authorized by Healthfirst. Healthfirst will never ask you for money during the recruitment or onboarding process.",New York NY,Data Engineer
Facebook,/rc/clk?jk=5f6830eeed0863f0&fccid=1639254ea84748b5&vjs=3,"Every month, billions of people leverage Facebook products to connect with friends and loved ones from across the world. On the Data Engineering Team, our mission is to support these products by delivering the best data foundation that drives impact through informed decision making. Within Business Integrity we ensure that people are able to make meaningful and trusted connections with businesses as part of experiencing Facebook products. As a highly collaborative organization, our data engineers work cross-functionally with software engineering, data science, and product management to optimize growth, strategy, and experience for our 3 billion plus users. In this role, you will see a direct correlation between your work, company growth, and user satisfaction. Beyond this, you will work with some of the brightest minds in the industry, and you'll have a unique opportunity to solve some of the most interesting data challenges around efficiency and integrity, at a scale few companies can match. Our data engineers consistently focus on the big picture and mobilize large cross-functional teams to help execute/create solutions that lead to many derived downstream enhancements for multiple stakeholders. Data engineers within Business Integrity own the large-scale engineering effort to create powerful foundational datasets, from inception to product delivery.As we continue to expand and create, we have a lot of exciting work ahead of us! | Own, conceptualize, architect, oversee, and execute large-scale data engineering projects, setting technical roadmaps and holding cross-functional teams accountable to agreed upon milestones | Configure complex data systems and successfully integrate them into powerful solutions and frameworks | Drive measurable impact across multiple products by translating business priorities into highly accurate, timely data sets | Write highly-optimized code that is maintainable by others, reusable, and adheres to consistent design patterns | Mentor team members through technical guidance, feedback and advocacy | Collaborate with engineers, product managers and product analysts to translate product goals into data requirements | Define and demonstrate a high standard for data quality and operational excellence | Design, build and launch new data models and ETL processes in production | Define and manage SLA for all data sets in allocated areas of ownership | 4+ years’ experience in the data warehouse space | 4+ years’ experience in custom ETL design, implementation and maintenance | 4+ years’ experience with object-oriented programming languages | 7+ years’ experience with schema design and dimensional data modeling | 7+ years’ experience in writing SQL statements | Understanding of features, design, and use-case scenarios across the modern data ecosystem | Experience analyzing data to identify deliverables, gaps and inconsistencies | Experience creating and operating multi-dimensional data sets used by multiple stakeholders and capable of processing very large amounts of data | Experience managing and communicating data warehouse plans to internal clients | BS/BA in Technical Field, Computer Science or Mathematics | Experience working with either a MapReduce or an MPP system | Knowledge and practical application of Python | Experience working autonomously in global teams | Experience influencing product decisions with data | Experience with programmatic advertising platforms | Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started. | Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",New York NY 10003,Data Engineer Analytics (Business Integrity)
Facebook,/rc/clk?jk=98627bb7afc03f4c&fccid=3c99d2c627a7f85f&vjs=3,"You are interested in large-scale data analysis, aggregation, data processing, and storage. We expect you to work closely with others and provide leadership, code discipline, and project design by example. You are extremely driven and want to assume ownership over an important part of our technology to build a solid product stack. You will use your expert Java skills to design and build high-load web applications and service-oriented systems for storing, processing, and searching a very large volume of unstructured text. In doing so, you will solve a wide variety of engineering challenges, ranging from data flow, to storage, to aggregation, to supporting APIs / presentation layer / data pipelines / web apps. You have experience developing in OSX and/or Linux. You are comfortable with change. You’ve had positive experience working for a startup before. |  | Required Skills | 4+ years of: | Python or Java or Scala using Spark Framework | SQL (NoSQL a plus) | Experience with schema design and dimensional data modeling | Experience designing, building, and maintaining data processing systems |  | Responsibilities | Collaborate with analytics and business teams to improve data models that feed business intelligence tools | Increase data accessibility and foster data-driven decision making across the organization | Defines company data assets (data models), Spark / Spark SQL to populate data models | Design data integrations and data quality frameworks | Design and evaluate open-source and vendor tools for data lineage",Brooklyn NY 11211,Senior Data Engineer
McGraw Hill,/rc/clk?jk=35e39d9067db30a1&fccid=58fef0e5bbfde084&vjs=3,"Build the Future | Do you enjoy testing the limits of possibility? At McGraw Hill, our Data Integration team drives progress and helps build the future of learning. If you have the passion and technical expertise to thrive in an innovative and agile environment, we want to learn more about you. | Your impact on the team | As a Data Engineer, you will help drive the design and development of highly scalable enterprise data processing and data warehouse use cases deployed on AWS for our core analytics and data science platform. You will create, own, manage, share support of, and drive best practices for a variety of existing and emerging data intensive applications. |  | The Data Integration team is part of our Digital Platform Group, which is responsible for building and supporting innovative digital platforms to power learning across K-12, Higher Education, International, and Professional segments. As part of this group, the Analytics and Reporting team is building best-in-class applications which leverage data and machine learning for advanced analytics and adaptive learning products. If you are interested in contributing to the future of digital and remote learning, join us on this mission! |  | What can you expect from the position? | Contribute to complex solution designs, hands-on software development goals, new tool and framework creation, and code reviews. | Identify gaps and proactively improve system service level agreements. | Provide technical knowledge sharing and coaching to engineers on the development team. | Work effectively with Technical Product Management and SCRUM masters to meaningfully contribute to our agile team. |  | What can you bring to the role? | At least 5 years of experience with ETL data processing concepts with full implementation cycle experience in enterprise data marts, including advanced SQL development skills. | At least 3 years of Architecture and Optimization experience for database systems technologies with a focus on data marts and data warehouses. | Having coding experience with a modern development language (Scala, Python, Java). | Experience with Apache Spark. | Strong understanding of data modeling concepts, including schema development, validation, and evolution (normalized and denormalized). | Experience with performance tuning and scaling production databases. | Experience with agile engineering practices. | As an education innovation company, we're proud to play our part by inspiring learners around the world. If you bring your curiosity, we'll help you grow in a collaborative environment where everyone shares a passion for success. |  | Are you ready for a new challenge? Apply for a career at McGraw Hill and together, we'll impact the world. | Other Locations | United States-New York-New York",Boston MA 02212,Data Engineer
Comcast,/rc/clk?jk=4331c4e1fd18f740&fccid=b03987f2456b7b44&vjs=3,"Company Description | At Literati, we believe in the power of great books. We’re on a mission to curate transformative literary experiences for every reader: anyone who dreams, anyone who wonders, of any age. There are some stories no child should live without and no adult should ever forget. |  | Job Description | As a Data Engineer focused on streamlining our data infrastructure, you will play a crucial role in building data pipelines, our data warehouse, and unlocking hidden answers in our data. You will be the first data engineer, so you’ll get to help establish HOW we do this. |  | Qualifications | WE’RE GOOD AT: | Bringing the fire. We are a startup, with all the fierce dedication and sparkling energy to accomplish great feats. Iron sharpens iron, and so we’ve refined our teams from only the strongest metals: open minds, bright ideas, and bold determination. | YOU’RE GOOD AT: | Building data pipelines from various internal and external sources using FiveTran and custom python | Architecting great data structures in various forms (3NF, Dimensional, semi-structured) | Reworking existing data structures into more logical and scalable forms | Building data objects, procedures, and data streams in Snowflake | Coaching and teaching others how to do great data engineering | Being part of the team, doing what’s needed for the company to succeed | YOU WILL NEED: | Expert knowledge in SQL, and in python and/or javascript | Extensive knowledge in Snowflake and FiveTran | Solid ability to design an enterprise data warehouse model, expert knowledge a strong plus | Passion towards making others better | YOUR DAY-TO-DAY WILL LOOK LIKE: | Working collaboratively with analytics team to build a self-service analytics infrastructure | Building new tables, schemas in the data warehouse to increase consistency and accuracy of the businesses reporting and analysis | Building new pipelines to bulk up the data lake for greater analytics | Innovating on ways to build automated pipelines and transformations to keep pace with the businesses growth | Additional Information | We seek people with drive and a touch of alchemy—and if that sounds like you, you should join us. Even if your experience isn’t a precise match for the role, passion and prowess will always win the day in our book. And if your career has taken you to some spectacular (or spectacularly strange) places? We love a good story. | At Literati, we value the power of reading for all: anyone who dreams, anyone who wonders, anyone of any age, from any background. We are dedicated to cultivating and preserving a culture of inclusion and connectedness that sees, reflects, welcomes, and celebrates the innovation and talent of a diverse array of people.",Austin TX 78702,Data Engineer
Allstate,/company/Marlabs/jobs/Senior-Data-Test-Engineer-fb120343e9a258ef?fccid=2edad2ddb1e570d3&vjs=3,"Job detailsSalary$60 - $65 an hourJob TypeContractNumber of hires for this role2 to 4QualificationsExperience:Hadoop/Big Data, 2 years (Required)AWS, 2 years (Required)ETL Testing, 7 years (Required)Full Job DescriptionRole: Senior Data Test Engineer, ETL/Data Tester – GC/USC/H4 EAD onlyLocation: Alpharetta, GA (Remote Until Covid) Duration: 12+ Months Key Skills : Hadoop, ETL, SQL, Oracle, AWS, HP ALM, Jira Description:7 to 10 years of relevant work experience in testing backend application primarily involving complex SQL queriesAt least 7 years’ experience in:Testing “Big Data” and Enterprise Data warehouseWriting and Executing complex queries in SQLExperience in analyzing ETL mapping (STTM) documentsWriting and Executing ETL scripts and Stored ProceduresDevelopment of SQL scripts based on ETL mapping and database documents to validate data inputs and compare data outputsUsing tools such as Informatica (PowerCenter)Integration testing - Services, ETL, Database (SQL/Oracle) – covering DML Validations, Batch MonitoringCloud databases (AWS /Redshift)/Relational databasesExperience testing mission critical Financial ApplicationsAdvanced level of expertise in Database testing (data cleansing, transforming, mapping, data validation) with excellent PL /SQL skillsAbility to create and execute test plans, strategies and test cases for applications that use ETL componentsExperience planning, writing, executing tests as well as preparing test strategiesExperience in using testing tools such as ALM and JIRAContract length: 12 monthsJob Type: ContractSalary: $60.00 - $65.00 per hourSchedule:8 hour shiftExperience:Hadoop/Big Data: 2 years (Required)AWS: 2 years (Required)ETL Testing: 7 years (Required)Work Remotely:YesCOVID-19 Precaution(s):Remote interview processVirtual meetings",Piscataway NJ 08854,Senior Data Test Engineer ETL/Data Tester - GC/USC/H4 EAD - 12+ Months
Spotify,/rc/clk?jk=e242109fd74d3a1d&fccid=3a71a4d2f7990a25&vjs=3,"Job detailsSalary$90,000 a yearFull Job DescriptionFounded by The Allstate Corporation in 2016, Arity is a data and analytics company focused on improving transportation. We collect and analyze enormous amounts of data, using predictive analytics to build solutions with a single goal in mind: to make transportation smarter, safer and more useful for everyone. | At the heart of that mission are the people that work here—the dreamers, doers and difference-makers that call this place home. As part of that team, your work will showcase both your intelligence and your creativity as you tackle real problems and put your talents towards transforming transportation. | That’s because at Arity, we believe work and life shouldn’t be at odds with one another. After all, we know that your unique qualities give you a unique perspective. We don’t just want you to see yourself here. We want you to be yourself here. | The Team | At Arity, analytics is at the heart of what we do. We are looking for a Data Analytics Engineer to join our amazing Insurance Telematics team. Our team builds analytics-centric products and solutions in the insurance space such as driving scores and mobility attributes. Our analytics community is made up of passionate and talented individuals and leaders with diverse backgrounds. We believe in a culture of recognition of good work, continuous learning, succeeding together, and having fun along the way. Come see why we’re one of Built In Chicago’s best places to work in 2021! | The Role | The Data Analytics Engineer will join a team of data scientists and data engineers to help the team access our most tremendous asset – data. This person will be transforming data from telematics, insurance, and other challenging data sources. This person will help design, create, and run data pipelines by fully utilizing their coding expertise, domain knowledge, and Arity’s technology stack. | Responsibilities | The day-to-day work of a Data Analytics Engineer will include: | Creating and running big data pipelines and improving infrastructure to support analytics | Exploring new data sources and making them usable for the team | Working closely with data scientists on the team to refine insurance product offering | Qualifications | Background in either Computer Science, Mathematics, Data Architecture, or related fields | Experience preparing datasets and an excellent understanding of what it takes to efficiently work with complex data | Excellent skills coding in Python and/or Scala | Familiarity with Spark. Familiarity with SQL is a plus. | Thrives working in a team environment | Enjoys communicating and collaborating with others | Familiarity working in a server environment | Detail-oriented, double checks data/work against expectations, and takes pride in good documentation | An innate curiosity and ability to learn new technologies | Compensation Data | The minimum salary for this position is $90,000. The salary offered will be commensurate with experience. | The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen. | That’s the day-to-day, now let’s talk about the rest of it. As we mentioned, Arity was founded by The Allstate Corporation. But you’ll be working for—and at—Arity. It’s the best of both worlds. You’ll get access to the full suite of Allstate benefits and work in a fast-paced startup culture. That’s more than just free breakfasts and brain breaks. It’s a culture that encourages you to be you. | Sound like a fit? Apply now! We can’t wait to meet you. | Arity.com Instagram Twitter LinkedIn | Allstate generally does not sponsor individuals for employment-based visas for this position. | Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component. |  | For jobs in San Francisco, please click ""here"" for information regarding the San Francisco Fair Chance Ordinance. | For jobs in Los Angeles, please click ""here"" for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance. | To view the “EEO is the Law” poster click “here”. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs | To view the FMLA poster, click “here”. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint. | It is the Company’s policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employee’s ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment",Chicago IL,Arity-Data Analytics Engineer - (Remote - Home Based Worker)
Major League Baseball,/company/TEKWINGS-LLC/jobs/Senior-Data-Engineer-a90ce7b856f54d5a?fccid=7116f31a9a01de2c&vjs=3,"Job detailsSalary$87,600 - $200,015 a yearJob TypeFull-timeContractNumber of hires for this role2 to 4Full Job DescriptionSr DATA ENGINEER -- MUST HAVE BIGQUERY100% REMOTEThe client is migrating from Snowflake to Big Query and are also building a new eventing Infrastructure along with full pipeline using Snowplow.Need a strong Data Engineer preferably with experience in migrations and building Pipelines.Build the high throughput eventing services that power the data platform for client's website and mobile appsWork in a small team of like-minded software engineers.Clients data platform is powered by Ruby, PostgreSQL, Google's Pub/Sub, Go, Python and BigQuery.Work with product and analytics teams to evolve data models and schemas based on business needs and facilitate data-driven decision makingScale eventing and data pipelining systems, with a penchant for delivering near real time insights capabilities to our users.skills and experience :strong database experience (PostgreSQL, Redshift, extremely strong on SQL, Snowflake, BigQuery, etc)strong understanding of data warehouse principlesExperience building pipelines, architecting and scaling large data systemsExperience with big data tools such as Beam, Spark, Airflow along with visualization tools such as Tableau or LookerSome experience with Kafka, Elasticsearch and/ or Go- desirable.Experience with Machine Learning frameworks and building out models alongside data scientists is a huge plusJob Types: Full-time, ContractSalary: $87,600.00 - $200,015.00 per yearSchedule:8 hour shiftContract Length:More than 1 yearFull Time Opportunity:YesWork Location:Fully RemoteCOVID-19 Precaution(s):Remote interview process",Remote,Sr. Data Engineer
Showtime,/company/Urbane-Systems-LLC/jobs/Data-Engineer-9f497c28f33e9e27?fccid=64870b6d9a7f7888&vjs=3,"Job detailsSalary$60 - $65 an hourJob TypeFull-timeContractFull Job DescriptionNeed experience with AWS DMS and AppSync3 to 6 years of experience working in Data Engineering or related roles.A degree in Computer Science or an equivalent major.Support existing and develop new data flows as needed by developing processes that verify, standardize, and scale data input, transformation and storage.Ability to write intermediate to advanced SQL/Python for data ingestion and processing.Experience with agile methodologies and short release cyclesExperience programming in various languages, especially Python, and SQL.Enjoys collaborating with other engineers on architecture and sharing designs with the teamHands on experience with the Hadoop ecosystem.Experience with cloud technologies (AWS)Experience with MongoDB a plusDistributed System Development for large-scale applicationsExperience working with PHI/Healthcare data.AI / ML experience.Job Types: Full-time, ContractPay: $60.00 - $65.00 per hourSchedule:Day shiftCOVID-19 considerations:RemoteContract Length:7 - 11 monthsContract Renewal:LikelyFull Time Opportunity:YesWork Location:One locationThis Job Is Ideal for Someone Who Is:Dependable -- more reliable than spontaneousPeople-oriented -- enjoys interacting with people and working on group projectsAutonomous/Independent -- enjoys working with little directionCompany's website:www.urbanesystems.comWork Remotely:Temporarily due to COVID-19",Reston VA 20191,Data Engineer
Spotify,/rc/clk?jk=6bc2d1d0e796f8dc&fccid=1b50fcfb150b1b48&vjs=3,"The New York Times is seeking inventive and motivated data engineers at all levels of experience to join the Data Engineering group. In this role, you will build critical data infrastructure that surfaces data and insights across the company. | About Us | Our Data Engineering teams are at the intersection of business analytics, data warehousing, and software engineering. As Maxime Beauchemin wrote in “The Rise of Data Engineering”, ETL and data modeling have evolved, and the changes are about distributed systems, stream processing, and computation at scale. They’re about working with data using the same practices that guide software engineering at large. A strong data foundation is essential for The New York Times and we’re responsible for it. We use our data infrastructure to power analytics and data products and to deliver relevant experiences to our customers in real-time. We enable our company to validate strategic decisions, make smarter choices, and react to the fast changing world. We are part of a New York based technology organization with a remote-friendly workplace that includes engineers around the world. We value transparency and openness, learning, community, and continuous improvement. Check out the Times Open blog, which is written by engineers and other technical team members, and follow @nytdevs on Twitter to see what we’re up to. | About the Job | We focus on the software engineering related to data replication, storage, centralized computation, and data API’s. We provide customers and partners with data tools, shared frameworks, and data services. These are the foundational core of our group which enables ourselves and others to work with data from a common underpinning. Our tools and services enable our group to scale and avoid blocking others. We reduce data redundancy by creating systems and datasets that serve as sources of record. We enable discovery and governance of our data. We support key business goals like growing our digital subscriber base, understanding how our customers use our products, and retaining our print subscribers. | As a data engineer, you will: | Run and support a production enterprise data platform | Design and develop data models | Work with languages like Java, Python, Go, Bash, and SQL | Build batch and streaming data pipelines with tools such as Spark, Airflow, and cloud-based data services like Google’s BigQuery, Dataproc, and Pub/Sub | Develop processes for automating, testing, and deploying your work | About You | To thrive in this role, you are excited about data and motivated to learn new technologies. You are comfortable collaborating with engineers from other teams, product owners, business teams, and data analysts and data scientists. You are own and shape your technical domain area and move the related business goals forward. You are eager to resolve upstream data issues at the source instead of applying workarounds. You analyze and test changes to our data architectures and processes, and determine what the possible downstream effects and potential impacts to data consumers will be. | This role may require limited on - call hours. An on - call schedule will be determined when you join, taking into account team size and other variables. On - call hours are unpaid, unless informed otherwise by your manager. |  | The New York Times is committed to a diverse and inclusive workforce, one that reflects the varied global community we serve. Our journalism and the products we build in the service of that journalism greatly benefit from a range of perspectives, which can only come from diversity of all types, across our ranks, at all levels of the organization. Achieving true diversity and inclusion is the right thing to do. It is also the smart thing for our business. So we strongly encourage women, veterans, people with disabilities, people of color and gender nonconforming candidates to apply. | The New York Times Company is an Equal Opportunity Employer and does not discriminate on the basis of an individual's sex, age, race, color, creed, national origin, alienage, religion, marital status, pregnancy, sexual orientation or affectional preference, gender identity and expression, disability, genetic trait or predisposition, carrier status, citizenship, veteran or military status and other personal characteristics protected by law. All applications will receive consideration for employment without regard to legally protected characteristics. The New York Times Company will consider qualified applicants, including those with criminal histories, in a manner consistent with the requirements of applicable state and local ""Fair Chance"" laws.",New York NY,Senior Data Engineer
The New York Times,/company/Nengu-Tech-Limited/jobs/Big-Data-Engineer-86113a3bccfc13b7?fccid=7dab2864cf892786&vjs=3,"Job detailsSalary$87,600 - $200,015 a yearJob TypeFull-timeContractNumber of hires for this role5 to 10QualificationsBachelor's (Preferred)SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Full Job DescriptionTitle: BIG Data engineerRemote6+ MonthsRequired Skills : Python (Coding), spark, hadoop, Java development, and KafkaHands-on experience in Big Data technologies like Apache Spark / Flink / Hadoop / Hive is a must.Experience with data pipeline building, backend microservice development, and REST API using Python, Java or comparable language.Experience with various offerings from AWS, including S3, EMR, Redshift, Athena; other cloud providers is a plus.Demonstrate a passion for developing well architected, elegant applications &amp; services.2+ years DevOps experience including configuration, optimization, backup, high reliability, monitoring and systems version control.Job Types: Full-time, ContractPay: $87,600.00 - $200,015.00 per yearBenefits:Health insuranceSchedule:8 hour shiftEducation:Bachelor's (Preferred)Experience:SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Work Location:Fully Remote",Remote,Big Data Engineer
Skyworks,/rc/clk?jk=d3ee14eff41cc6bb&fccid=ea25315ee9da22e5&vjs=3,"Job detailsJob TypeCommissionFull Job DescriptionJob Summary | Responsible for planning and designing new software and web applications. Analyzes, tests and assists with the integration of new applications. Oversees the documentation of all development activity. Trains non-technical personnel. Assists with tracking performance metrics. Provides guidance and support to other Engineers. Integrates knowledge of business and functional priorities. Acts as a key contributor in a complex and crucial environment. May lead teams or projects and shares expertise. | Job Description | Core Responsibilities | Collaborates with project stakeholders to identify product and technical requirements. Conducts analysis to determine integration needs. | Designs new software and web applications, supports applications under development and customizes current applications. Develops software update processes for existing applications. Assists in the roll-out of software releases. | Trains junior Software Development Engineers on internally developed software applications. | Oversees the researching, writing and editing of documentation and technical requirements, including software designs, evaluation plans, test results, technical manuals and formal recommendations and reports. | Keeps current with technological developments within the industry. Monitors and evaluates competitive applications and products. Reviews literature, patents and current practices relevant to the solution of assigned projects. | Assists with technical leadership throughout the design process and assists in guidance with regards to practices, procedures and techniques. Serves as a guide mentor for Software Development Engineers. | Assists in tracking and evaluating performance metrics. Ensures team delivers software on time, to specification and within budget. | Works with Quality Assurance team to determine if applications fit specification and technical requirements. Tests and evaluates systems, subsystems and components. | Acts as a technical contact and liaison for outside vendors and/or customers. | Presents and defends architectural, design and technical choices to internal and external audiences. | Consistent exercise of independent judgment and discretion in matters of significance. | Regular, consistent and punctual attendance. Must be able to work nights and weekends, variable schedule(s) and overtime as necessary. | Other duties and responsibilities as assigned. | Employees at all levels are expected to: | Understand our Operating Principles; make them the guidelines for how you do your job. | Own the customer experience - think and act in ways that put our customers first, give them seamless digital options at every touchpoint, and make them promoters of our products and services. | Know your stuff - be enthusiastic learners, users and advocates of our game-changing technology, products and services, especially our digital tools and experiences. | Win as a team - make big things happen by working together and being open to new ideas. | Be an active part of the Net Promoter System - a way of working that brings more employee and customer feedback into the company - by joining huddles, making call backs and helping us elevate opportunities to do better for our customers. | Drive results and growth. | Respect and promote inclusion &amp; diversity. | Do what's right for each other, our customers, investors and our communities. | Disclaimer: | This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications. | Comcast is an EOE/Veterans/Disabled/LGBT employer. | Education | Bachelor's Degree | Relevant Work Experience | 10 Years + | Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That’s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality – to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.",New York NY,Lead Software Engineer (Data)
Horizon Media Inc.,/rc/clk?jk=3165a42d33240fc9&fccid=6262eae176df0db9&vjs=3,"Major League Baseball Business Intelligence (MLBBi) team is looking for a Business Intelligence Data Engineer, who can assist our team in our evolving goal of providing enterprise class solutions for the game of baseball. The MLBBi team is responsible for gathering data from varied sources (internal/external) and consolidating them into our data warehouse, in order to provide analytics across the organization in order to deliver business critical analysis. Our platform is used across the enterprise in such situation as, but not limited to, player analytics, baseball development and operational reporting. |  | As the Business Intelligence Data Engineer, working in our NYC office, your primary task will be to design, build, and maintain various parts of the data warehousing and reporting infrastructure, including requirements gathering/documentation, architecture, ETL development and data modeling. We are a lean team and we all play multiple positions on the team (architecture, system administration, data evangelist, python developer, etc.). Your careful attention to detail and honed problem-solving talents will serve you well, as you develop not only visualizations and reports but have a key role in architecting our next stage data/analytics platform leveraged across the enterprise, which will be the backbone to some of baseballs operational decisions. |  | CORE REQUIREMENTS |  | B.S. or M.S. in computer Science, equivalent engineering degree, or relevant practical experience. | 3-5 years of progressively complex related experience. | High proficiency in SQL (ANSI) with the ability to write complex freehand SQL including sub-queries, nested queries, and other advanced SQL features. | Experience with a RDBMS (MySQL, PostgreSQL, SQL Server a plus). | Proven experience with development and scripting in Java, Object-Oriented Languages, Python or any of the major languages to build robust data pipelines and dynamic systems. | Proven ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources and build innovative solutions. | Proven knowledge of GCP including services in their compute, storage, databases, management tools, and analytics portfolio. (open to AWS/Azure equivalents) | A passion for data, and an understanding of how to work with large data sets consisting of as well as willingness to learn new technologies and methodologies under minimum guidance. | Excellent written and verbal communication |  | ADDITIONAL FUNCTIONS |  | Development and execution of data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs. | Demonstrate a deep knowledge of, and ability to operationalize, leading data technologies and best practices in order to identify and assess critical capabilities and recommend solutions. | Design and modify components of new and existing IT systems to promote integrated corporate business systems. |  | PREFERENCES |  | Baseball knowledge a plus | Experience with SAP DataService is a plus | Experience with MPP Data Warehouses (BigQuery is a plus) | Experience with implementing complex, enterprise-wide data transformation and processing solutions | Experience with data in various forms (data warehouses/relational SQL, NoSQL, JSON, unstructured data environments/PIG, HIVE, Impala) | Experience with GCP Data Pipelines (GCP Professional Data Engineer certification) a plus | End-to-end SDLC experience a plus |  |  | Why MLB? |  | Major League Baseball (MLB) is the most historic of the major professional sports leagues in the United States and Canada. Employees love working at MLB because of the culture of opportunity, collaboration, and professionalism. The professionals who are most successful at MLB take initiative, know how to identify and solve problems, put the team first, and work collaboratively. For those who are ready to join the ""Major Leagues"" of their careers, MLB takes the same approach as they do with their players: striving to empower their own ""workforce athletes"" to be at their best by engineering experiences that put employees in the best position to succeed. Major League Baseball is looking for candidates who are ready to step up to the plate and continue transforming America's pastime to best serve its fans for decades to come. |  | MLB's vision is to be the global sport of choice for youth to play, fans of all backgrounds to enjoy and a desired destination for employment. |  | With a belief that the journey to growth and greatness is ongoing, MLB gives employees the opportunity to continue learning and honing their skills with programs such as: tuition reimbursement; mentorship programs; lunch and learns; online course subscriptions; paid industry certifications; business resource groups; and more. |  | MLB provides its employees with exceptional medical, dental, and vision coverage. Premiums are 100% employer covered to help employees focus on being their best! |  | Are you ready to Step Up to the Plate? Apply below!",New York NY,Business Intelligence Data Engineer
Deloitte,/rc/clk?jk=266fee348898e412&fccid=e228a3c78d0f7f13&vjs=3,"Description | At First Republic, we care about our people. We offer extraordinary client service in private banking, private business banking, and private wealth management. Founded in 1985, we believe that personal connections are everything and our success is driven by the relationships we form with our colleagues and clients. You'll always feel empowered and valued at First Republic. |  | Incredible teams doing exceptional work, every day. | In Human Resources, we are committed to growing and protecting First Republic and its culture by attracting, retaining and supporting our most valuable resource: people. Providing our colleagues with extraordinary service is our commitment; their success is our reward. |  | Responsibilities | The People Analytics &amp; Insights team is an evolving team that dives deep into people data, enlightens with insights, and enables change through a comprehensive analytical approach. As a member of the team, you'll be detail-oriented and conscientious, analytical and curious. You're a strong problem-solver, who uses both quantitative and qualitative methods to get things done. You will play an important role in how we bring data to everything with your expertise and passion to help address the data and analytical needs of teams and clients across First Republic. Your knowledge ofPeople data,along with deep business context, will enable you to provide action-oriented analyses to ensure ourHRorganization and Business Leaders are well-informed and able to make thoughtful, data-drivenPeopledecisions. |  | What you'll do as a Data Engineer, People Analytics: |  | As a data engineer, you will be working closely with data architects and data analysts to implement data modeling solutions in order to streamline and support people analytics data and visualizations. | Develop conceptual data models, implement data strategies, and build data flows.Optimize and update logical and physical data models to support new and existing projects.Develop best practices for data coding to ensure high data quality and consistency within the system. Design and develop solutions to reduce data redundancy, streamline data movements, and improve enterprise information managementCommunicate complex information so that it is easy to understand and drive strategic use of data to help increase value of the People Analytics function.Partner closely with HR Technology and Operations to ensure that systems and data catalogs align to the analytical and data reporting needs; Build and maintain strong partnerships across HR and the business.Manage various analytics projects and collaborate within HR and cross-functionally across the businessUses knowledge of business objectives, strategies, and needs to identify opportunities where data can be leveraged to achieve the desired business benefits. |  | Qualifications | You could be a great fit if you have: | Hands-on experience in data modeling, data visualization and pipeline design and developmentHands-on experience with data platforms (Snowflake, AWS) and familiarity with data visualization (Tableau, Cognos BI) technologiesStrong in at least one of these programming languages: SQL, R, Python, GoIn-depth knowledge of data warehousing and relational database designExpert knowledge of metadata management and related tools.Demonstrated technical ability and analytical skills, with superb attention to detail and passion for data, data definition integrity, and process efficacyEffective communication skills, with the ability to interact with cross-functional stakeholders in spoken and written form.Ability to learn quickly and adjust to changing business needs.Experience with HR systems as well as other technical tools | Job Demands: | Must be able to review and analyze data reports and manuals; must be computer proficient.Must be able to communicate effectively via telephone and in person. | Own your work and your career - apply now | Are you willing to take initiative and make decisions? Are you willing to go the extra mile because you love what you do and how you can contribute as a team? Do you want the freedom to grow and the opportunity to take charge of your own career? If so, then come join us. |  | We want hard working team players. You'll have the independence to learn, lead and drive change. A culture of extraordinary service, empowerment and stability - that's the First Republic way. Come join us! |  | This job description is not intended to be all-inclusive. Employee may perform other related duties as assigned to meet the ongoing needs of the organization. The Company is an equal opportunity employer. In this regard, the Company makes reasonable accommodations for qualified applicants and employees with disabilities in order to enable them to perform all essential job functions, unless doing so creates an undue hardship. |  | First Republic is subject to federal laws that restrict the employment of individuals with certain types of criminal histories, including FDIA Section 19 and FINRA. To the extent not inconsistent with our obligations under those federal laws and regulations, First Republic will consider qualified candidates with criminal histories in a manner consistent with the Los Angeles and San Francisco ban-the-box laws.",San Francisco CA 94111,Data Engineer People Analytics
USA for UNHCR,/rc/clk?jk=96ddf44819934d66&fccid=f74a07f61f1a7daa&vjs=3,"In this position you will work with a talented engineering group to design, develop, install and test in scala and spark sql data pipelines which ingests and analyzes over a billion records every night. Leveraging technologies like Mesos, DCOS, Apache Nifi, Spark, ElasticSearch and ELK you will implement data integration projects on cloud platforms such as AWS. This role involves building the data pipelines which feed the cutting edge of population health and analytics within the healthcare industry and working to solve complex problems to ensure the best data for our clients. The ideal candidate will be team first, detail oriented and a self-starter to take ownership over each project they’re on. |  | What Success Looks Like |  | In 3 monthsTrain in the different areas of the data connector life cycleWork on initial data integration and egressWork on data quality and analytics around connectorWork on enhancement and issue triage |  | In 6 monthsStart to develop clinical and claims data connectorsWork on higher level enhancement requests and defectsDeliver Data Quality Reviews to clients |  | In 12 monthsDeveloping a range of data connectors with varying complexityWork on teams with Product, Engineering or Implementation to build out tools for better data integrationPick an SME (Subject Matter Expert) path for what excites you the mostWorking on standardized data connector developmentWorking with product to build out new data types for new requirements |  |  | What Will You Be Doing | Building standard and custom software to integrate large clinical and claims data sets into the Arcadia Analytics infrastructure | Use Nifi, Scala, Apache Spark, or other tools/languages to cleanse and transform incoming data into normalized formats | Design and implement software components | Performing code reviews, Unit &amp; Integration Testing | Deploy software components | Manage code repositories &amp; enforce software versioning | Establish and maintain efficient local development environments | Provide feedback and recommendations to improve software development processes | What You'll Need to Have | At least 2 – 5+ years of related work experience | Expert Level in SQL | SQL or NoSql database experience such as MySql, Postgres, Cassandra, MS SqlServer, or Oracle | Proficient in at least one of the following languages: Scala, Java, Python, R (Expert if no SQL Experience) | Experience working with complex data sets | Healthcare data experience | Experience with Business Intelligence software or advanced reporting queries/frameworks | Would Love for You to Have | Apache Nifi, Talend, IBM InfoSphere, TIBCO, Pentaho, or Informatica | ELK (ElasticSearch/Logstash/Kibana) | Distributed Hadoop-like technologies such as Spark, Storm and/or Kafka | Tableau, QlikView, Apache Zeppelin, IPython or Jupyter | Github | HL7, CCD, CCLF file formats/designs | What You Get | Learn a TON about healthcare and the bleeding edge of healthcare analytics and medical economics | Learn the Apache tech stack and distributed computing | Become an expert in clinical and claims healthcare data | Opportunity to be a part of a mission driven organization focused on helping provider organizations change the way they provide care to their patients | Chance to be surrounded by a team of extremely talented and dedicated individuals driven to succeed | Competitive compensation | Amazing benefits including unlimited FTO | About Arcadia | Arcadia.io helps innovative healthcare systems and health plans around the country transform healthcare to reduce cost while improving patient health. We do this by aggregating massive amounts of clinical and claims data, applying algorithms to identify opportunities to provide better patient care, and making those opportunities actionable by physicians at the point of care in near-real time. We are passionate about helping our customers drive meaningful outcomes. We are growing fast and have emerged as the market leader in the highly competitive population health management software and value-based care services markets, and we have been recognized by industry analysts KLAS, IDC, Forrester and Chilmark for our leadership. For a better sense of our brand and products, please explore our website, our online resources, and our interactive Data Gallery. | This position is responsible for following all Security policies and procedures in order to protect all PHI under Arcadia's custodianship as well as Arcadia Intellectual Properties. For any security-specific roles, the responsibilities would be further defined by the hiring manager.",Remote,Data Integration Engineer
First Republic Bank,/rc/clk?jk=47764f6d5c673205&fccid=278b465fce581bba&vjs=3,"At Kensho, we hire talented people and give them the freedom, support, and resources needed to build cutting edge technology and products for our parent company, S&amp;P Global. As a result, we produce technology that is scalable, robust, and solves the challenges of one of the world’s largest, most successful financial institutions. | We are seeking mid-level experienced Data Engineers to join our Data Engineering Team. The team is responsible for architecting, implementing, and maintaining Kensho’s data ingestion, processing, and storage solutions. They work with stakeholders across Kensho and S&amp;P to map the data landscape, triage data requirements, and design and implement a comprehensive data strategy. | Our ideal candidate has experience with the tasks and skills below: | Designing, coordinating, and implementing production data platforms using industry standard and/or open source software | Navigating between technical leadership and individual contributor roles | Negotiating between new requirements, legacy systems, technical debt, and best practices | Mentoring colleagues on data engineering best practices and systems design | What You'll Do: | Work with industry standard and/or open source software such as PostgreSQL, Kafka, Airflow, etc | Implement and maintain a data management and governance framework | Support machine learning and application teams by setting up custom data solutions | Design, maintain, and scale Kensho’s data pipeline and document processing platform | Build event and batch driven ingestion systems for stand-alone software products, machine learning R&amp;D, and API services | Develop and administer databases, knowledge bases, and distributed data stores | Create and use systems to clean, integrate, or fuse datasets to produce data products | Perform continuous and periodic studies on systems cost efficiency, performance, and overall health | Establish and monitor data integrity and value through visualization, profiling, and statistical tools | Implement and maintain a data management and governance framework | What You'll Need: | Experience with various data-store technologies, distributed messaging platforms, or data processing framework | Experience designing, architecting and building reliable data pipelines | Experience working with large structured and unstructured data sets | Effective coding, documentation, and communication habits | Proficient understanding of distributed computing principles | Experience integrating/fusing data from multiple data sources | Knowledge of various ETL techniques, frameworks, and best practices | Experience supporting and working with cross-functional teams in a dynamic environment | (Bonus) Experience with Site Reliability Engineering, DevOps (CICD), and Cloud Administration | At Kensho, we pride ourselves on providing top-of-market benefits, including: | Medical, Dental, and Vision insurance100% company paid premiumsUnlimited Paid Time Off20 weeks of 100% paid Parental Leave (paternity and maternity)401(k) plan with 6% employer matchingGenerous company matching on donations to non-profit charitiesUp to $20,000 tuition assistance toward degree programs, plus up to $4,000/year for ongoing professional education such as industry conferencesPlentiful snacks, drinks, and regularly catered lunchesDog-friendly office (CAM office)In-office gyms and showers (CAM, DC) or Equinox membership (LA, NYC)Stipend towards commuter or gym reimbursementBike sharing program membershipsCompassion leave and elder care leaveMentoring and additional learning opportunitiesOpportunity to expand professional network and participate in conferences and events |  | About Kensho | Kensho uses machine learning, artificial intelligence, natural language processing and data visualization techniques to solve some of the hardest analytical problems and create breakthrough financial intelligence solutions for our parent company, S&amp;P Global. |  | Kensho was founded in 2013 by Harvard &amp; MIT alums and was acquired by S&amp;P Global in 2018. Kensho continues to operate as a startup in order to maintain our distinct, independent brand and to promote our breakthrough, innovative culture. Our team of Kenshins enjoy a dynamic and collaborative work environment that runs autonomously from S&amp;P, while leveraging the unparalleled breadth and depth of data and resources available as part of S&amp;P Global. As Kenshins, we pride ourselves on maintaining an innovative culture that depends on diversity and inclusion. |  | We are an equal opportunity employer that welcomes future Kenshins with all experiences and perspectives. Kensho is headquartered in Cambridge, MA, with offices in New York City, Washington D.C. and Los Angeles. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin.",New York NY 10007,Software Engineer - Data
Microsoft,/rc/clk?jk=47018a9244d9d6d1&fccid=fc68da685e8aa986&vjs=3,"Job detailsSalary$60 - $70 an hourFull Job DescriptionPiper Companies is looking for a SQL Data Engineer for a leading financial services and mortgage provider in Conshohocken, PA. | Responsibilities for the SQL Data Engineer: | Write and develop SQL queries to move data from multiple source systems: flat files, SQL server, databases, xls files, etc. | Create reconciliation processes and error handling/logging processes in the ODS | Qualifications for the SQL Data Engineer: | 5+ years of experience in data engineering in an ODS environment | Expert experience in creating and modifying SSIS packages | Working knowledge of ODS concepts, data loading issues, and database structures | Experience with Redshift/Snowflake, Boomi, and 2016-2019 SQL Server upgrades – preferred | Financial services industry – preferred | Compensation and Benefits for the SQL Data Engineer: | $60-70/hr W2 | Comprehensive benefit package; Medical, Dental, Vision, 401k, and Paid Time Off | Keywords: | sql data engineer, etl developer, database developer, sql engineer, database engineer, sql, sql server, ssis, ssrs, ssas, cube, etl, database design, data warehouse, unit testing, informatica, data modeling, star schema, redshift, snowflake schema, t-sql, git, svn, devops, benefits, vacation, holiday, 401k",Conshohocken PA,Data Engineer (SQL)
Slack,/company/big-data-llc/jobs/Engineer-e52bbdfab973f7e2?fccid=ea3047cf48d14844&vjs=3,"Job detailsSalary$40 - $59 an hourJob TypeFull-timeContractNumber of hires for this role2 to 4QualificationsSpark: 2 years (Required)US work authorization (Required)Bachelor's (Preferred)Data Warehouse: 2 years (Preferred)java: 3 years (Preferred)Python: 3 years (Preferred)Full Job DescriptionAt Big Data LLC, we work on data engineering platforms and distributed computing, on various challenging and interesting problems.What we’re looking for:You’re a talented, creative, and motivated engineer who loves developing powerful, stable, and intuitive apps – and you’re excited to work with a team of individuals with that same passion. You’ve accumulated years of experience, and you’re excited about taking your mastery of Big Data and Scala/Java to a new level. You enjoy challenging projects involving big data sets and are cool under pressure. You’re no stranger to fast-paced environments and agile development methodologies – in fact, you embrace them. With your strong analytical skills, your unwavering commitment to quality, your excellent technical skills, and your collaborative work ethic, you’ll do great things here at BigData.What you’ll do:As a Senior Big Data Engineer, you’ll be responsible for designing and building high performance, scalable data solutions that meet streaming and batch platforms. You’ll design, develop, and test robust, scalable data platform components.Skills, accomplishments, interests you should have:BS in Computer Science, Engineering, or related technical discipline or equivalent combination of training and experience3+ years core Java experience: building business logic layers and back-end systems for high-volume pipelinesExperience with spark streaming and scalaCurrent experience in Spark, Hadoop, MapReduce and HDFS, Cassandra / HBase, ScalaUnderstanding of data flows, data architecture, ETL and processing of structured and unstructured dataCurrent experience using Java development, SQL Database systems, and Apache productsExperience with high-speed messaging frameworks and streaming (kafka, akka,reactive)Current experience developing and deploying applications to a public cloud (AWS, GCE, Azure)Experience with DevOps tools (GitHub, TravisCI, Jira) and methodologies (Lean, Agile, Scrum, Test Driven Development)Experience with data science and machine/deep learning a plusAbility to work quickly with an eye towards writing clean code that is efficient and reusableAbility to build prototypes for new features that will delight our users and are consistent with business goalsAbility to iterate quickly in an agile development processAbility to learn new technologies and evaluate multiple technologies to solve a problemGood written EnglishStrong work ethic and entrepreneurial spiritNice to haves:Experience mentoring or acting in a lead capacityJob Types: Full-time, ContractPay: $40.00 - $59.00 per hourSchedule:Monday to FridayEducation:Bachelor's (Preferred)Experience:Data Warehouse: 2 years (Preferred)java: 3 years (Preferred)Python: 3 years (Preferred)Spark: 2 years (Required)Work Location:Fully RemoteCompany's website:big-datai.comCOVID-19 Precaution(s):Remote interview processVirtual meetings",Remote,Big Data Engineer
Piper Companies,/rc/clk?jk=b63f195555ae2756&fccid=a00e0f48d2d490c3&vjs=3,"Full time | Remote | Job Languages: | English |  | Description |  | Requirements |  |  AWS: Glue, Athena, S3, Kinesis, SQS, Aurora DB |  Other: Python, PySparka, Kafka Streams, HTTP, Image Processing |  | Assignment | The Product Data Service team is importing ecommerce product data at scale from our merchants. We do quality assurance and normalization for this data before putting it into a structured data format. The data is then shared across multiple teams within our mobile app eco system. |  | Your mindset |  | Customer Obsession: is also our team core value but we want to take that one-step further and abstract it to our customers. Your opinions and ideas aim to improve teams work that will enable them to bring value to our Customers | Let the team shine: we value teamwork and collaboration within us. We practice a lot of pair / mob programming in order to spread the knowledge within the team and improve the quality of our deliveries. We are there for each-other. | Challenge the status quo: we expect you to challenge solutions if they do not provide impact to customers.",Remote,Data Engineer
C2S Technologies,/company/C2S-Technologies/jobs/Data-Engineer-f4d02bec00aa3560?fccid=eb4bc656c7659573&vjs=3,"Job detailsJob TypeFull-timeContractNumber of hires for this role2 to 4QualificationsExperience:Power BI, 3 years (Required)Azure Data factory and/or Azure Data lake, 3 years (Required)Full Job DescriptionJob Tilte: Data EngineerLocation: Redmond, WA (Remote)Only W2/1099Job Description:Bachelor’s degree in computer science or engineering, database systems, mathematics, or 5+ years of industry experience in a data engineering role.Advanced hands on experience with Azure Cloud Services (Data Factory, Data Explorer, HDInsight. Cosmos DB, SQL) or equivalentExperience with Data Lake infrastructures (Cosmos, Hadoop)Experience with data warehouse technical architectures, ETL/ELT, and reporting/analytic toolsExperience optimizing code for hardened, efficient deploymentsExperience designing and building data warehouse solutionsExperience building and maintaining data pipelinesExperience with production BI implementations in the CloudExperience with Machine Learning Model deploymentExperience building Power BI, Excel, and Reporting Services dashboards and reportsExceptional problem solving, technical and data analysis skillsGreat written and verbal communication and presentation skillsBe self-driven, and show ability to deliver on ambiguous projects with incomplete or dirty dataAbility to work in a team environment that promotes collaborationJob Types: Full-time, ContractSchedule:Monday to FridayExperience:Power BI: 3 years (Required)Azure Data factory and/or Azure Data lake: 3 years (Required)Work Remotely:YesCOVID-19 Precaution(s):Remote interview processSpeak with the employer+91 804-999-4152",Remote,Data Engineer
ISO,/rc/clk?jk=c71ff647c4cab3cb&fccid=d621f9a5603cfedd&vjs=3,"Job detailsJob TypeTemporaryInternshipFull Job DescriptionCompany Description | ISO, a Verisk business, has been a leading source of information about property/casualty insurance risk since 1971. For a broad spectrum of commercial and personal lines of insurance, ISO provides statistical, actuarial, underwriting, and claims information and analytics; compliance and fraud identification tools; policy language; information about specific locations; and technical services. ISO serves insurers, reinsurers, agents and brokers, insurance regulators, risk managers, and other participants in the property/casualty insurance marketplace. To learn more about ISO please visit us at: www.verisk.com/iso. We are proud to be a part of the Verisk family of companies! | At the heart of what we do is help clients manage risk. Verisk (Nasdaq: VRSK) provides data and insights to our customers in insurance, energy and the financial services markets so they can make faster and more informed decisions. | Our global team uses AI, machine learning, automation, and other emerging technologies to collect and analyze billions of records. We provide advanced decision-support to prevent credit, lending, and cyber risks. In addition, we monitor and advise companies on complex global matters such as climate change, catastrophes, and geopolitical issues. | But why we do our work is what sets us apart. It stems from a commitment to making the world better, safer and stronger. | It’s the reason Verisk is part of the UN Global Compact sustainability initiative. It’s why we made a commitment to balancing 100 percent of our carbon emissions. It’s the aim of our “returnship” program for experienced professionals rejoining the workforce after time away. And, it’s what drives our annual Innovation Day, where we identify our next first-to-market innovations to solve our customers’ problems. | At its core, Verisk uses data to minimize risk and maximize value. But far bigger, is why we do what we do. | At Verisk you can build an exciting career with meaningful work; create positive and lasting impact on business; and find the support, coaching, and training you need to advance your career. We have received the Great Place to Work® Certification for the fourth consecutive year. We’ve been recognized by Forbes as a World’s Best Employer and a Best Employer for Women, testaments to our culture of engagement and the value we place on an inclusive and diverse workforce. Verisk’s Statement on Racial Equity and Diversity supports our commitment to these values and affecting positive and lasting change in the communities where we live and work. |  | Job Description | Handle activities assigned and apply appropriate data engineering techniques to produce high quality deliverables. | Apply gained experience or academic knowledge in handling large datasets, including transformation, extraction and loading. Work on data lake projects and assist in producing data needed for Business Intelligence and Visual analytics. | Create a data catalog and document data process flow details. |  | Qualifications |  2022 grad, pursuing a bachelors or master’s degree.The ideal candidate will have some academic knowledge or experience in one or more of the following: SQL, Tableau or similar visualization tools, python, java, C#.Interest and ability to learn and use new tools, cloud-based platforms and programming languages. | The Intern Program is not eligible for work visa sponsorship. If you will require work visa sponsorship (e.g. H1-B visa) after completing your degree, you do not meet the basic requirements of the summer Intern role. Additional Information Verisk Analytics is an equal opportunity employer. All members of the Verisk Analytics family of companies are equal opportunity employers. We consider | Additional Information | Verisk Analytics is an equal opportunity employer. | All members of the Verisk Analytics family of companies are equal opportunity employers. We consider all qualified applicants for employment without regard to race, religion, color, national origin, citizenship, sex, gender identity and/or expression, sexual orientation, veteran's status, age or disability. | http://www.verisk.com/careers.html | Unsolicited resumes sent to Verisk, including unsolicited resumes sent to a Verisk business mailing address, fax machine or email address, or directly to Verisk employees, will be considered Verisk property. Verisk will NOT pay a fee for any placement resulting from the receipt of an unsolicited resume. | Consumer Privacy Notice",Jersey City NJ,Data Engineer intern- 2021 Summer Internship program- CR
Numerator,/rc/clk?jk=9b3f7888207a2e95&fccid=11f2ca9fb7278374&vjs=3,"Numerator is a data and tech company bringing speed and scale to market research. Headquartered in Chicago, IL, Numerator has more than 2,000 employees worldwide. The company blends proprietary data with advanced technology to create unique insights for the market research industry that has been slow to change. The majority of Fortune 100 companies are Numerator clients. | Job Description | Numerator is looking for a Data Engineer, Data Science to help us drive decision-making, find bigger opportunities and work with our established and rapidly evolving platforms that handle millions of requests and massive amounts of events, and other data. In this position, you will be responsible for taking on new initiatives to automate, enhance, maintain, and scale services in a rapidly-scaling environment. |  | The details |  |  | As a Data Engineer, Data Science at Numerator, you will help our team make better decisions by advocating for data-driven approaches across the organization. The role is cross-functional by nature and is responsible for developing data products and analytics, defining methodologies, conducting research and analysis on a variety of subject areas, and driving bottom-line growth through building operational efficiencies and leading special projects for our clients. |  |  | A major requirement for this role is to understand, author, and deploy production code, the ideal candidate should also be experienced with processing large quantities of data, building algorithms alongside software engineers, and delivering to production. |  | You will have a broad impact and exposure across Numerator as you help build out and expand our technology platforms across several software products. This is a fast-paced role with high growth, visibility, impact, and where many of the decisions for new projects will be driven by you and your team from inception through production. |  | What you get to do! |  | Identify new opportunities to build and/or improve new product features and data products. | Partner with Product, Data, and Engineering teams to identify, investigate and deliver solutions related to product and back-end data issues. | Deliver complex, end-to-end projects involving heavy data and statistical modeling (e.g. sampling, segmentation, classification, predictive modeling, etc.). | Lead the discovery and development of new and existing methodologies, tools, and algorithms. | Regularly communicate outcomes, new initiatives, and improvements, etc. to stakeholders. | Provide data science engineering consulting and support to both internal and external clients. |  | Skills &amp; Requirements | What we are looking for |  | 2+ years in building a data warehouse and data pipelines. OR, 3+ years in a data intensive engineering role. | Proficient software engineering experience in one major language ( preferably Python and/or R) along with SQL. You strive to write beautiful code. | Experience with ETL tooling (especially Airflow) and data processing, and knowing how to transform data to meet business goals. | You have a strong background in distributed data processing, software engineering and data modeling. | Outstanding written and verbal data storytelling, demonstrated consulting skill and ability to tailor communication style and depth to a variety of audiences. | Highly autonomous, versatile, intellectually curious, and resilient within a dynamic and fast-paced organization. | BS in Mathematics, Statistics, Computer Science, Engineering, Economics, Physics, or other behavioral and/or equivalent quantitative science. |  | Extra, nice to haves |  | Experience with developing, deploying and maintaining back-end production code, including (but not limited to) applied ML frameworks and applications (e.g. SciKit Learn, TensorFlow, etc.). | Exposure to AWS, Azure, or GCP. | Proven track record of delivering solutions to a production environment. | Experience with understanding, analyzing and modeling user data and behavioral trends. | Experience working with marketing insights, shopping data or in the retail industry. |  | What we offer |  | An inclusive and collaborative company culture- we work in an open environment while working together to get things done, and adapt to the changing needs as they come. | Market competitive total compensation package. | Volunteer time off and charitable donation matching. | Regular hackathons to build your own projects and work with people across the entire company | Strong support for career growth, including mentorship programs, leadership training, access to conferences and employee resource groups. | Great benefits package including health/vision/dental, exceptional maternity leave coverage, unlimited PTO, flexible schedule, 401K/RRSP matching, travel reimbursement and much more. |  | If this sounds like something you would like to be part of, we'd love for you to apply! Don't worry if you think that you don't meet all the qualifications here. The tools, technology and methodologies we use are constantly changing and we value talent and interest over specific experience. | We are an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by the law.",Remote,Data Engineer Data Science
Rhythm,/rc/clk?jk=95788b3073bd2870&fccid=56351c4e00252274&vjs=3,"Data Scientist |  | About Rhythm |  | Rhythm is a renewable energy and technology company empowering you to take control of your budget and your footprint. We combine energy market expertise with technology, design, and data science to create best-in-class products and services that are simple, delightful, and seamlessly integrated with the rhythm of your life. Our mission is to upend the energy status quo by setting a new standard of service excellence and customer partnership. We are a mission-driven, results-oriented group of engineers, businesspeople, designers and artists who love solving tough problems, all while making a positive impact on our communities. |  | About the Data Engineer |  | The ideal candidate loves being hands-on with data. You love building and maintaining the systems that see data orchestrated throughout its entire lifecycle, from source system to integrated warehouse, to data science models, and finally to a business intelligence environment where it is surfaced. You have extensive experience building batch and streaming data pipelines to ingest data, managing complex ETL dependencies using tools such as Airflow, and building out scalable ML pipelines. You like sitting at the intersection of data science and engineering, and are willing to jump on whatever task needs doing, whether it's troubleshooting a task queue or helping a business user understand the data they're seeing in a dashboard. At the end of the day, you're a team player, and are motivated by our mission to make saving money with clean, renewable energy simple for our customers. |  | Responsibilities |  | Work with our Engineering team to maintain the pipelines through which we ingest data into our Snowflake data warehouse, from both internal and 3rd-party platforms | Own the ETL pipeline through which that data is transformed, managing complex data and timing dependencies and a mixture of SQL and python-based transformations | Own our data science python application, which hosts our data and analytics services, including its containerized deployment, data access, monitoring, and overall uptime | Integrate a scalable ML pipeline within our ETL, providing our BI users and customer platform just-in-time access to our analytics | Be an expert in not only the orchestration but the nature of our data, understanding how it is integrated from disparate systems and built up into the business objects that our team and our customers depend on | Build out a streaming pipeline for customer events, enabling real-time intelligence within our platform and situational awareness for our marketing and operations teams | Be the data science team's go-to expert on our infrastructure, including database, container, and application management, responsible for monitoring, maintenance, and troubleshooting | Contribute to our BI environment, Looker, and support our business users in their use of it | On occasion, contribute to our data applications, writing data-driven python-based services | Be a champion for Rhythm's mission and values, both at work and in your community |  | Qualifications |  | 5+ years of technical experience working in modern, cloud-based environments (8-10 years preferred) | 3+ years of data engineering experience specifically | Bachelors degree in CS or a quantitative field (M.S. preferred) | Expertise developing, testing, deploying and maintaining production Python-based applications | Expertise with web and data access in Python, in particular via SQLAlchemy and Flask | Expertise in the AWS ecosystem, including ECS, S3, Aurora, SNS, and SQS | Expertise with ETL and data pipelines, using tools such as Airflow, Kinesis, DBT, and Celery | Expertise architecting and managing data warehouses (we use Snowflake and Timescale) | Expertise building, monitoring, and maintaining scalable ML pipelines, preferably on AWS (SageMaker a plus) | Expertise with Docker, Docker Compose, and containerized testing and deployment | Experience supporting data warehouse BI tools (we use Looker) | Experience with basic devops, including managing local dev environments and data access | Excellent written and verbal communication skills | Excellent organizational skills, and a willingness to work hard and jump on whatever the team needs |  | Geography |  | Rhythm is headquartered in Houston, Texas and is building out a New York City office | Remote candidates will be actively considered, but preference will be given to candidates in the NYC or Houston area | Travel required &lt;5% |  | Compensation and Benefits |  | Competitive base salary and bonus | Complete benefits package, including medical, 401k, HSA/FSA, and unlimited PTO.",New York NY,Data Engineer
First Republic Bank,/rc/clk?jk=24974baeae647cd2&fccid=e228a3c78d0f7f13&vjs=3,"Description | At First Republic, we care about our people. Founded in 1985, we offer extraordinary client service in private banking, private business banking and private wealth management. We believe that personal connections are everything and our success is driven by the relationships we form with our colleagues and clients. You'll always feel empowered and valued here. |  | Incredible teams doing exceptional work, every day | In Technology, we support First Republic's employees and clients through the acquisition, integration and management of the Bank's information technology systems and services. We drive innovation and explore emerging technologies so our people can be productive and focus on what matters most - providing extraordinary service. |  | Responsibilities | As a Data Engineer in Regulatory and Corporate technology you will be responsible for designing data model working with data architects. Involved in data pipeline development leveraging various methodology. Help modernize the current technology stack to be more cloud native with higher focus around data quality and security. |  | As a Data Engineer in Regulatory and Corporate technology you will be responsible for designing data model working with data architects. Involved in data pipeline development leveraging various methodology. Help modernize the current technology stack to be more cloud native with higher focus around data quality and security. |  | What you'll do as a Data Engineer: | Design, develop and maintain various data model for regulatory and corporate domainDevelop Pipelines, ensuring the best practice are implemented for data governance, data quality, data lineage and data cleansing.Apply data science skills to model data for quality verificationPerform detailed analysis to troubleshoot and resolve identified issues and maintain data integrityResponsible for driving and managing data source integration between various vendor systems. |  | Qualifications | You could be a great fit if you have: | Bachelors or Master degree in information technology, computer science or data scienceStrong skills in python and knowledge of various frameworks like pandas, pyspark.Experience in building cloud native data lakes, pipelines and stream processingExperience with cloud services preferably AWS and Snowflake.Background in data science, analytics, or data mining.Familiar with Data Virtualization concepts ideally with Denodo/Composite experience.Familiar with Data Virtualization concepts ideally with Denodo/Composite experience.Experience in DevSecOps and automation using CICD tools and processProven history of learning and implementing new technology in fast moving environment.Hands-On experience with data pipeline design and development.Experience with both SQL and NoSQL as well as their relevant data modeling patternsDemonstrated experience working in large-scale data environments which included real-time and batch processing requirements.Familiar with Data Virtualization concepts ideally with Denodo/Composite experience. | Job demands: | Must be able to review and analyze data reports and manuals; must be computer proficient.Must be able to communicate effectively via telephone and in person. | Own your work and your career - apply now | Are you willing to take initiative and make decisions? Are you willing to go the extra mile because you love what you do and how you can contribute as a team? Do you want the freedom to grow and the opportunity to take charge of your own career? If so, then come join us. |  | We want hard working team players. You'll have the independence to learn, lead and drive change. A culture of extraordinary service, empowerment and stability - that's the First Republic way. |  | This job description is not intended to be all-inclusive. Employee may perform other related duties as assigned to meet the ongoing needs of the organization. The Company is an equal opportunity employer. In this regard, the Company makes reasonable accommodations for qualified applicants and employees with disabilities in order to enable them to perform all essential job functions, unless doing so creates an undue hardship. |  | First Republic is subject to federal laws that restrict the employment of individuals with certain types of criminal histories, including FDIA Section 19 and FINRA. To the extent not inconsistent with our obligations under those federal laws and regulations, First Republic will consider qualified candidates with criminal histories in a manner consistent with the Los Angeles and San Francisco ban-the-box laws.",San Francisco CA 94111,Data Engineer
Neuberger Berman,/rc/clk?jk=0a94252ee2f9e0d6&fccid=474b9602e1305593&vjs=3,"Primary Skills: Data pipelines, ETL, Python, SQL | Duration: 12+ (Possible Extension/conversion) | Type: W2 |  | Job Description: | 4+ yrs of data engg exp - building ETL pipelines, having worked with large volume of data and with some understanding of databases and Python programming . |  | Required Skills: | Strong SQL skills and ability to create and tune queries | Experience working with large data systems | Experience with at least one major RDBMS (SQL Server, MySQL or Oracle) | Experience with data integration toolsets and writing and maintaining ETL jobs | Strong experience with SQL/TSQLand ability to create and tune queries | Scripting skills (Python/JS) | Agile and Scrum experience | To follow up with any questions, please contact Taabish Contact no 408-907-7669 |  | Akraya is an award-winning IT staffing firm and the staffing partner of choice for many leading companies across the US. We offer comprehensive benefits including Health Insurance (medical, dental, and vision), Cafeteria Plan (HSA, FSA, and dependent care), 401(k) (enrollment subject to eligibility), and Sick Pay (varies based on city and state laws). |  | If this position is not quite what you're looking for, visit akraya.com and submit a copy of your resume. We will get to work finding you a job that is a better fit at one of our many amazing clients. |  | Akraya is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender, race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law. Akraya is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",Los Angeles CA,Data Engineer: 21-00765
UnitedHealth Group,/rc/clk?jk=02d88dfc37e227c5&fccid=7e987b6ab8fc83d1&vjs=3,"Boston, MA | Full-Time | We need your help. We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base. | What you’ll do | As a data engineer, you will be responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, operation, and management of our client data hubs, including data intake, data quality assessment/evaluation and data curation and enrichment/preparation processes. Our client data hubs consist of various health plan data sources to support Decision Point services including our AI/ML platform, analytics platform, and OPUS application. If this interests you, read on. | The Position | Design and develop scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.) | Automate the processing of inbound customer data feeds | Design and develop tools to support data profiling and data quality methodologies | Work with our data science team to assist with data prep, enrichment and feature engineering for AI/ML | Engage with our software engineering team to ensure precise data points per application specification | Provide periodic support to the customer success team | Skills &amp; Experience | BS / MS in Computer Science, Engineering or applicable experience | 3+ years of experience with ETL/ELT and data pipeline principles | 3+ years of experience with Python, JavaScript, and/or PowerShell | 3+ years of SQL experience; Microsoft SQL Server or PostgreSQL preferred | Knowledge of data manipulation methodologies | Excellent verbal and written communication | Strong data profiling skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems | Keen understanding of EDW and other database design principles | Comfortable working with very large data sets and VLDB environments | Experience with CI/CD and version control tools: Git preferred | Understanding of data science and machine learning concepts preferred | Experience working within hybrid cloud environment; AWS experience is a plus | Familiarity with data visualization tools such as Tableau or QuickSight is a plus | Familiarity with healthcare data is a plus | Some familiarity with statistical software tools and libraries such as R and scikit-learn is a plus | A little more about Decision Point | We are a rapidly growing healthcare company in the healthcare market. This year, we’ll nearly double in size. Our products &amp; support services advise healthcare insurance and provider organizations on how to best target and engage their members so that members make better health decisions. The result? Our clients can identify their sickest members before they get sick and connect these members with the health and social support services they need to manage their condition. | Our innovative approach to member and provider engagement strategy and orchestration leverages cutting edge machine learning techniques to better inform clients on how to take action. Guided by our team of industry recognized healthcare and technology experts, with backgrounds from NASA, Microsoft, E&amp;Y, health plans, and other great organizations, we are putting healthcare data to good use.",Boston MA 02109,Data Engineer
LexisNexis Legal & Professional,/rc/clk?jk=5803294e4e0a3bc1&fccid=06206ab329e0eb22&vjs=3,"BASIC FUNCTIONS: This entry-level position performs basic data engineering assignments within a specific engineering functional area or product line. |  |  | QUALIFICATIONS: |  | Bachelor’s Degree (Engineering/Computer Science preferred but not required); or equivalent experience required. | 2+ years’ experience in data management area with a technical competencies demonstrated. |  | TECHNICAL SKILLS: |  | File management skills. | Ability to assist with process improvement and compliance. | Knowledge of data manipulation and scripting practices such as but not limited to: PowerShell, Regular Expressions, Perl, Python. | Understanding of root cause analysis. | Knowledge of applicable development languages including but not limited to: SQL, VBA, C#, ASP.NET, Delphi, JavaScript, Windows, HPCC. | Ability and desire to learn new processes and technologies. | Problem solving skills. | Good attention to detail. | Good oral and written communications skills. |  | ACCOUNTABILITIES: |  | Work on data transfers in various capacities including collection setup, data transfer setup, contributor/customer setup, etc. | Basic database management ensuring structure and dataflow adheres to department standards. | Perform daily data loads ensuring recurring updates are logged and tracked. | Utilize various data workflow management, analysis and reporting tools. | Write and review portions of detailed specifications for the development of system components of simple complexity. | Complete simple data engineering bug fixes. | Escalate issues and decisions to senior Data Engineering team members. | Assist with process improvement and compliance to successfully and consistently deliver high-quality data services on a timely basis. | Under the guidance of senior-level engineers, successfully implement processes, coding best practices, and code reviews. | Operate in various development environments (Agile, Waterfall, etc.) while collaborating with key stakeholders. | Resolve basic technical issues as necessary and provide support to production teams. | Keep abreast of new technology developments. | All other duties as assigned. | We are an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, or any other characteristic protected by law. If a qualified individual with a disability or disabled veteran needs a reasonable accommodation to use or access our online system, that individual should please contact accommodations@relx.com or if you are based in the US you may also contact us on 1.877.734.1938. | Please read our Candidate Privacy Policy",Orem UT,Data Engineer 1
Apple,/company/Intuites/jobs/Snowflake-Data-Engineer-00c24c29961d6bb2?fccid=cbc49844b6c953e3&vjs=3,"Job detailsSalary$45 - $55 an hourJob TypeFull-timeContractNumber of hires for this role5 to 10QualificationsBachelor's (Preferred)SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Full Job DescriptionSnowflake Data engineer HIGHLY PREFERRED ON W2Job Types: Full-time, ContractPay: $45.00 - $55.00 per hourSchedule:8 hour shiftEducation:Bachelor's (Preferred)Experience:SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Work Location:One locationWork Remotely:Temporarily due to COVID-19COVID-19 Precaution(s):Remote interview process",Alameda CA,Snowflake Data engineer
Akraya Inc.,/company/The-Virtual-Forge/jobs/Data-Engineer-c59a0c581e5456fb?fccid=dc40ad3a7629b7ed&vjs=3,"Job detailsJob TypeFull-timeNumber of hires for this role1QualificationsExperience:building and troubleshooting ETL pipelines, 3 years (Preferred)SQL, Microsoft SQL Server, 3 years (Preferred)HIPAA, 1 year (Preferred)Work authorization:United States (Required)Full Job DescriptionAbout us: We are an international company with offices in the US, UK, and Portugal. The Virtual Forge works with organizations to create digital and technology platforms to drive transformation, develop capabilities, and deliver digital and transactional experiences that build business around the world.Role Overview: We are seeking a Data Engineer to join our growing team. This person will be responsible for working across several Data Warehousing, Reporting, and Data Integration projects, assisting with the development of data best practices and governance, building reporting infrastructure, creating ad-hoc reports, and helping to optimize data flow and collection.The ideal candidate has knowledge of, experience with, and is excited to learn about all aspects of data from multiple complex sources and domains, and who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our developers, database architects, data analysts, and data scientists, and will ensure optimal data delivery architecture is consistently present across projects. They will also support non-technical colleagues in the collection and appropriate use of data. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by data, cross system integration, traditional and non-traditional forms of ETL.Duties and Responsibilities: Data Modeling – Evaluate structured and unstructured data, determine the most appropriate schema for new tables, fact tables, data marts, etc.Data Integration – Incorporate new business and system data into client Data Warehouses while maintaining enterprise best practices and adhering to data governance standards.ETL and Reporting – Apply business rules to data to migrate from source to target using ETL tools or scripting languages. Validate data to ensure quality. Collaborate with colleagues across the enterprise to scope requests. Extract data from various data sources, validate results, create relevant data visualizations, and share with requesters. Develop dashboards and automate refreshes as appropriate.Governance / Best Practices – Adhere and contribute to enterprise data governance standards. Assemble large, complex data sets that meet business requirements.KPI Development - Develop analytics that utilize data resources to provide actionable insights, operational efficiency and other key business performance metrics.Technical Support - Work with stakeholders including client and internal company teams to assist with data-related technical issues and support their data infrastructure needs.Essential Skills: Bachelor’s degree in a computer related field3+ years of experience building and troubleshooting ETL pipelines, experience with healthcare oriented or otherwise sensitive data preferred, familiarity with HIPAA a plusExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Strong communication and organizational skills.Strong analytic skills related to working with structured and unstructured datasets.RDBMS - strong skills in SQL, Microsoft SQL Server experience preferredProficient in an object oriented programming language in a data engineering capacityPreferred Skills: Exposure to Big Data and HadoopExposure to stream-processing systemsExposure to object-oriented/object programming: Python, Java, etcExposure to visual analytics tools: QlikSense, Tableau, Power BI, etc.Data Science / Machine Learning experienceFamiliarity with Agile methodology for development*All candidates must be legally authorised to work in the United States of America on a permanent basis. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available for any position.At The Virtual Forge, we have a dedicated recruitment team that works globally to fill all our recruitment needs. Therefore we don't need a response from recruitment companies. Thanks for understanding.*Job Type: Full-timeExperience:building and troubleshooting ETL pipelines: 3 years (Preferred)SQL, Microsoft SQL Server: 3 years (Preferred)HIPAA: 1 year (Preferred)Work authorization:United States (Required)Work Remotely:Yes",Philadelphia PA,Data Engineer
Intelligent Medical Objects,/rc/clk?jk=4097a73355e11348&fccid=b276df3fa16c132a&vjs=3,"Recorded Future is a guardian of the internet, making it a safer place by giving users the information they need to disrupt adversaries. Our harvesting pipeline reads over 700,000 web sources and structured data feeds, and our real-time multilingual natural language processing technology takes that content from raw text to alerts and visualizations in minutes. As a Data Engineer, you will be responsible for accurate, actionable distillation of this massive data set. The analytical summaries you automate will allow our users to take action based on verdicts for over 2 billion indicators. |  | What you'll do as a Data Engineer: |  | Build code to condense massive amounts of data into understandable and actionable summaries at scale. | Thoroughly understand the data we give to clients. Collaborate with your teammates to identify opportunities to improve the data. | Suggest and drive projects to expand the use of modern best practices in our codebase. Work across programming languages to take advantage of the right tools for each job. |  | What you should bring to the Data Engineer role: |  | Programming: You are comfortable writing production code in Python and JavaScript. You are familiar with best practices in software engineering and are excited about spreading their use. Bonus for experience with Java-backend JavaScript technologies such as TypeScript, NodeJS, Nashorn, GraalVM, or Rhino. | Curiosity: You enjoy puzzles and are invigorated by the challenge of understanding what a complex piece of code does. | Data: You have some experience working with databases and are comfortable manipulating heterogeneous data sets. | Excellent communication: Your clarity of thought is always apparent in your crisp and articulate emails, Slack chats, phone calls, and in-person conversations. |  | Why should you join Recorded Future? | From over 35 nationalities, our Futurists are the perfect recipe of humility, accountability, and collaborative attitudes. Our dedication to empowering clients with elite intelligence to disrupt adversaries has earned us a 4.7-star user rating from Gartner and 8 of the top 10 Fortune 100 companies as clients. |  | Want more info? | Blog &amp; Podcast: Learn everything you want to know (and maybe some things you'd rather not know) about the world of cyber threat intelligence | Instagram &amp; Twitter: What's happening at Recorded Future | The Record: The Record is a cybersecurity news publication that explores the untold stories in this rapidly changing field | Timeline: History of Recorded Future |  | We are committed to maintaining an environment that attracts and retains talent from a diverse range of experiences, backgrounds and lifestyles. By ensuring all feel included and respected for being unique and bringing their whole selves to work, Recorded Future is made a better place every day. |  | Recorded Future will not discharge, discipline or in any other manner discriminate against any employee or applicant for employment because such employee or applicant has inquired about, discussed, or disclosed the compensation of the employee or applicant or another employee or applicant. |  | Recorded Future is an equal opportunity and affirmative action employer and we encourage candidates from all backgrounds to apply. Recorded Future does not discriminate based on race, religion, color, national origin, gender including pregnancy, sexual orientation, gender identity, age, marital status, veteran status, disability or any other characteristic protected by law.",Boston MA,Data Engineer
JPMorgan Chase Bank N.A.,/rc/clk?jk=4164eb245777a0b1&fccid=92a41bb73b6d9960&vjs=3,"Overview | Overview: | The programmatic team at Radancy is seeking a Data Pipeline Engineer to support building new data products and services. | The Team | Radancy Programmatic works on data services across the programmatic platform within Radancy, and supports building customer facing data visualization products | The team has extensive experience in ETL development and works with large scale data in real time | About the Job | Providing technical support for all data pipeline environments | Set up, configure, maintain and enhance proper infrastructure to support a large scale data analytics environment | Evaluate the technical tradeoffs of every decision | Build and maintain ETL pipelines utilizing Python | Work with Cloud Computing Platforms (AWS), Kafka and other open-source technologies | Conduct data modeling, schema design, and SQL development | Ingest and aggregate data from both internal and external data sources | Collaborate with Product Owner and domain experts to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation | Assist with the development and review of technical and end user documentation including ETL workflows, research, and data analysis | Work with Product team to define data collection and engineering frameworks | Responsible for daily integrity checks, performing deployments and releases | Own meaningful parts of our service, have an impact, grow with the company | Desired Technical Qualifications | Excellent and proven knowledge of Python | Excellent and proven knowledge of SQL | Excellent and proven knowledge of streaming technologies, such as Kafka or Kinesis | Excellent and proven knowledge of queue based infrastructures such as AWS SQS, SNS, etc. | Good knowledge of the following technologies is a bonus: | Postgres on Amazon RDS | Amazon Redshift | Docker | Proficiency in Git, JIRA and Teamcity are a plus | 2+ years in a production environment a plus | Flexible Location: | Remote within the USA | radancy is an equal opportunity employer and welcomes all qualified applicants regardless of race, ethnicity, religion, gender, sexual orientation, disability status, protected veteran status, or any other characteristic protected by law. We actively work to create an inclusive environment where all of our employees can thrive.",Atlanta GA,Data Pipeline Engineer
Christian Brothers Automotive,/rc/clk?jk=98c16c5b7906c59c&fccid=3aa50f4ba6c3867e&vjs=3,"Here at 1stdibs, one of our core values is 'Say It With Data' and we are looking for an exceptional Data Engineer to join our New York,NY Engineering team and help the wider company gain access to and develop a better understanding of our data environment. As our data needs and infrastructure continue to grow, we are seeking our ideal teammate to help us build durable, scalable processes and tools to help support our ever-growing demand for quantitative metrics to advise our business decisions. From Audience Segmentation to Sales Growth Analysis to A/B Variant Testing, data drives all aspects of our business forward. |  | In this role, you will have the opportunity to learn and develop expertise around how a luxury e-commerce marketplace leverages data to drive its business decisions. You will partner with cross-functional teams including Product and Marketing to help define the roadmap, drive data strategy, and prioritize initiatives. You will improve and refine your business acumen by constantly balancing the needs of technical implementation, feature optimization and analytical measurement. |  | What you'll do |  | Work closely with various cross-functional stakeholders such as e-commerce, performance marketing, content, technology, product and advertising to capture the data needed to drive business decisions | Validate accuracy and completeness of data captured, and maintain technical documentation | Partner with Analytics team to ensure captured data meets the reporting/dashboarding requirements | Review data trends post-implementation to confirm accuracy and identify anomalies | Assist Analytics in automating reporting and data requests | Help design A/B test experiments and assist in reporting on experiment results |  | What you'll bring |  | B.S. in computer science, business analytics or a quantitative field | 1-3 years of relevant experience building and managing data pipelines | Experience working with Web Analytic software (Google Analytics, Adobe Omniture) | Exposure to BI software (Looker, Tableau, Chartio, SSRS/SSAS, etc.) | Ability to translate and document business requirements into site tracking and reporting guidelines | Proficiency in identifying/interpreting trends and anomalies in data, and troubleshooting accordingly | Strong organizational skills/attention to detail and capacity to manage multiple projects simultaneously | Exceptional verbal and written communication skills |  | Nice to haves |  | Data modeling experience in data warehouses or RDBS (Redshift, MySQL, Postgres) | Familiarity with debugging tools such as Chrome Debugger, Fiddler, HttPWatch, etc. | Exposure to web development tools (HTML/JavaScript/ DOM) | Experience with querying languages (SQL) and some flavor of database (relational or otherwise) | Knowledge of version control (GitHub) |  |  | 1stdibs is the world's largest online luxury marketplace for rare and desirable objects. It has become the go-to source for the world's leading interior designers and consumers to find antiques, furniture, jewelry, vintage fashion and fine art. |  | Backed by Benchmark Capital, Insight Venture Partners, Index Ventures, Spark Capital and Alibaba, 1stdibs is a unique blend of expert curators and seasoned Internet executives from companies including eBay, Gilt, Google, FreshDirect, Mlb.com, Shutterfly, and Twitter. |  | 1stdibs is an equal opportunity employer. We celebrate diversity and we are committed to creating an inclusive environment for all employees. |  | 1stDibs is actively hiring and, in response to COVID-19, conducting all interviews and onboarding virtually. Any new or current team members will be temporarily working remotely until our offices reopen, at which point all employees are expected and required to return physically to the New York office. |  | For additional information about location requirements, please discuss with the hiring team following submission of your application.",New York NY,Data Engineer
1stdibs.com,/rc/clk?jk=d285e3ea16579560&fccid=e0891492a6adc193&vjs=3,"Department Description |   |  | IPC is a global provider of mission-critical network services and trading communication technology to the financial markets community. With complete focus on this sector and over 40 years of expertise, IPC provides customers with integrated solutions that support traders and participants across the entire trade lifecycle, including sell-side and buy-side financial institutions, inter-dealer brokers, liquidity venues, clearing and settlement firms, independent software vendors, corporate finance departments, financial information exchange providers and market data providers. IPC’s offerings include a unified communications/application platform, award-winning trading positions, managed voice and data connectivity solutions, compliance technologies, infrastructure management and a suite of enhanced service offerings. |  | IPC’s global reach extends to more than 60 countries – including a network of 5,000 customer sites over 700 cities and an installed base of approximately 120,000 trading positions deployed worldwide. Headquartered in Jersey City, New Jersey, IPC has over 900 employees located throughout the Americas and the EMEA and Asia-Pacific regions. IPC’s mission is to continually innovate to support collaboration across the global financial community and address our clients’ needs in an ever-changing landscape. For more information, visit www.ipc.com. |  |  | Brief Description |   |  | Preferred applicant will be a staging and implementations specialist with a broad and varied background as it relates to various vendor hardware staging, configuring and installing. Applicant must be able to follow defined configuration parameters, implement scripts and test staged devices for basic and second level functionality. Understanding of Ethernet, MPLS, cabling solutions, patching schedules are a plus. Strong written communication skills and documentation skills are a must. |  |  | Detailed Description |  |  |  | Managing warehouse space and inventory control. | Staging and configuration of defined hardware specifications. | Communicate service timelines and status update to internal Project Teams in a timely manner. | Provides status of in progress orders. | Updates order management systems as appropriate | Manages multiple in-flight efforts for various projects, highlighting issues or contention for resources. | ALL EQUIPMENT BUILDS: Supporting the various IPC product offers within the confines of defined specifications and configurations. | CISCO, NOKIA, FUJI. |  |  | Hardware: | CISCO, NOKIA, FUJI, etc. | IOS, License and component build as per Engineering or M6 Documentations | Acquire correct IOS from SharePoint and apply as per Engineering or M6 Documentations | Interface with Engineers or Design team as to resolution or modification to design request | license purchase and PAK Key acquired from Cisco | Build base configuration text file for all devices | Apply Key and active, then apply configuration text | Validate and test components where warranted per device | (Install T1/E1 cards, adaptor cards, allocate correct DSP memory as per Voice port design, redundant power supplies) | Copy all configuration and statistics, then copy to team site (SharePoint) | Package devices and ship to destination |  |  | Job Requirements |  |  |  | A minimum of 5 years experience. | Practical working knowledge of and experience implementation and configuration of Layer 2 and Layer 3 devices. | Practical knowledge of Layer 3 devices used for implementation of Layer 3 Services. | Knowledge of technologies such as L2 VPN, and MPLS (A PLUS) | Understanding of basic networking concepts, systems, and carrier delivery. | Working knowledge of Microsoft Word, Excel, and PowerPoint | Working knowledge of Metasolv is a plus | High level awareness of network and customer information security | Ability to coordinate, organize, and prioritize multiple tasks | Excellent written and oral communication skills | Problem Solver/Troubleshooter: Systematically works through problems. | Technically Minded: Is comfortable learning and new systems and platforms (CISCO, NOKIA, etc). | Quality Driven: Builds trust with customers and teammates by consistently delivering error-free work. |  |  | Additional Details |   |  | IPC offers Medical, Dental, Vision, Life Insurance and Accidental Death and Dismemberment, Long and Short Term Disability, 401(k), Flexible Spending Accounts, Pre-tax Commuter Program, Employee Assistance Program, Aflac, Tuition Reimbursement, Vacation/Personal/Sick Days. |  |  | XJ6",New York NY,Data Tech Support Engineer
BGC Partners,/rc/clk?jk=13ed8053a97393d0&fccid=cc1aedce2805c74b&vjs=3,"BoostCTR is an enterprise solution for optimizing ad creative at scale across Search, Social, Display &amp; Mobile. |  | Boost increases advertiser profitability by using a combination of humans and software to drive increased ad relevance at scale. The Boost marketplace is comprised of over 1,000 expert copywriters and image optimizers who compete to provide a diverse array of perspectives. Boost’s proprietary software identifies opportunities for optimization and drives performance using a combination of workflow tools and algorithms. |  | The Company focuses on optimizing for enterprise advertisers and generates substantial improvements in conversions-per-impression for customers. Today, Boost is trusted to drive optimization and insights for over $300 million in advertising spend across networks and premium publisher sites such as Google, Microsoft+Yahoo Bing Ads and Facebook. |  | Position Summary: |  | BoostCTR is looking for a Data Engineer with the tool set to not only build data warehouses and pipelines to efficiently and reliably move data across systems, but also to research or build the next generation of data tools to enable us to take full advantage of this data. In this role, your work will broadly influence the company's data consumers and analysts. | This role requires excellent problem solving, communication, and decision making skills. If you have a blog then please pass along the URL so we can review your work. | Responsibilities | Build data expertise and own data quality for the awesome pipelines you build | Architect, build and launch new data models that provide intuitive analytics to your customers | Design, build and launch extremely efficient &amp; reliable data pipelines to move data (both large and small amounts) to Data Warehouse | Design and develop new systems and tools to enable folks to consume and understand data faster | Use your expert coding skills across a number of languages from SQL to C#, PHP, Python, and JavaScript | You have developed applications within the LAMP Stack environment | Work across multiple teams in high visibility roles and own the solution end-to-end |  | Requirements | 2+ years of C#, PHP, or Python development experience is necessary. | 2+ years of MySQL experience is required. | 2+ years of LAMP stack development experience is necessary | 5+ years of experience with dimensional data modeling &amp; schema design in Data Warehouses | 5+ years of experience building web applications and expert knowledge of web technologies (HTML/CSS/JS) | Experience building data warehouses (Redshift, Vertica) | Ability to write well-abstracted, reusable code components. | Excellent communication skills including the ability to identify and communicate data driven insights | BS or MS degree in Computer Science or a related technical field |  | Preferred candidates would also meet the following criteria: | Unmistakable passion for elegant and intuitive data model designs. | Experience working with either a Map Reduce or a MPP system on any size/scale. | Involvement with the Open Source Community. | Experience with one or more scripting languages. | Understanding of distrinbuted log processing techniques. | Experience with reporting and visualization tools. | Track record of working with product managers, account, and executive staff.",San Francisco CA 94102,Data Engineer
Nielsen,/rc/clk?jk=4f2e3756ce72e923&fccid=bfc79467906cbcaf&vjs=3,"T-REX is on a mission to empower responsible finance. We are revolutionizing the investment lifecycle of complex assets through increased data transparency, enhanced analytical capability, and streamlined collaboration. Our solutions combine SaaS technology, big data management, and asset class expertise to foster improved investment decision-making across alternative financial markets. T-REX’s managed data service and software platform bridge the gap between asset originators and investors, eliminating manual processes and automating workflow. T-REX solutions help our clients to mitigate risk, improve efficiency and drive greater performance across their investment process. | Join T-REX as we help unlock liquidity across the financial markets, responsibly and sustainably with the transformative power of fintech. We are looking for new team members who share our passion for innovation and our entrepreneurial, collaborative, and results-driven approach. |  | - | DATA ENGINEER | T-REX is seeking an experienced Sr. Data Engineer to join our Data Engineering team and work closely with the Data Services and Engineering teams to support us in the build out of our Data Management Platform and associated services. The Data Engineering team is in charge of building the infrastructure for the extraction of large datasets from a variety of sources in support of our Managed Data Services. The team is responsible for ensuring the timeliness delivery of high quality datasets in support of our clients workflows. |  | Our ideal candidate has: |  | Demonstrable experience working on mission critical ETL processes in the Financial Services industry | Ability to develop, create, test, and maintain data processing pipelines | Experience in recommending and implementing ways to improve data reliability, efficiency and quality | Experience in managing and supporting the acquisition of client and 3rd party data |  |  | What you’ll do: |  | As part of our Data team you will have an important role in the development and operations of our strategic Performance Data Service offering. You will work closely with our Data Services and Engineering teams to build out and continuously enhance the service and platform, while working with the Client Success team to align client requirements. |  | Build tooling and automation around data pipelines that improve the efficiency, quality and resiliency of our data engineering framework | Expand on and automate the data ingestion pipeline | Provide the highest quality data for our users by continuously defining, developing and adhering to a data validation process from ingestion to end user workflows | Ensure the efficiency and effectiveness of our data management capabilities and processes are world class and always improving | Stay informed and up-to-date with industry standards and trends; evaluate new tools and technologies that can improve the efficiency and quality of the data engineering work |  |  | What we’re looking for |  | Required: |  | 5-6 years of Data Engineering experience working on business critical, production level, data pipelines | Proficient in Python and relational/non-relational querying languages | Strong domain knowledge of data warehousing principles, tools and technologies | Experience working with APIs | Experience working in Financial markets | Exceptional written and verbal communication skills |  |  | Preferred: |  | Experience working with Structured Finance products and/or in the Structured Finance industry or related field | Experience working with any of the following technologies: ElasticSearch, Airflow, Pandas, AWS |  |  | What it’s like working at T-REX: |  | You’re going to be challenged: Building an inter-disciplinary SaaS platform that combines deep knowledge from both the financial and technology industries isn’t easy! |  | You’re going to make an impact: T-REX offers you the opportunity to have a deep impact on our growth, product and team. |  | You’re an asset we’re going to invest in: We believe in investing in our people so we have generous tuition reimbursement and wherever we can we promote from within. |  | You’re going to have fun: You’ll get to work with a team of diverse people; we’re an international team who loves everything from boardgames, Pi Day baking competitions and T-REX Trivia Nights. |  |  | T-REX - People Hiring People: |  | At T-REX, we are committed to equal employment opportunity regardless of race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression. We are proud of our diversity and strive to reflect the society we are a part of. T-REX is a team of people who love to put in good work and have fun together. |  |  | Join us on our mission to Empower Responsible Finance and have fun doing it!",New York NY 10005,Sr. Data Engineer
McKinsey & Company,/rc/clk?jk=ee67e43ebcf49885&fccid=ff9351c2a6df2c4a&vjs=3,"This position can be based remotely in the United States | Prognos’ is a NYC-based healthcare startup whose mission is to improve health by driving the best actions learned from the world's data. In order to achieve this goal we have curated the world’s largest clinical lab dataset, covering over 200M patients in the US, and are currently deploying cutting-edge technology for predicting disease at the earliest possible time. |  | The Mission of the Data Science team at Prognos is to develop, deploy and maintain analytic and machine learning pipelines within Prognos’ products, addressing business-relevant problems in close collaboration with our Engineering, Clinical and Product teams. |  | We are looking for an experienced engineer to join the team, and help us move this mission-critical task forward. This position will be focused on helping us learn about patient health from medical time series. Are you interested in applying modern data engineering, MLOps and devops practices to complex health data? Do you want to work on developing and deploying production feature engineering, data tracking and model deployment pipelines? Then come work with us! |  | Candidates must have at least three years of prior professional experience working with large datasets, especially data engineering pipeline development. A bachelor’s degree or higher in Computer Science, Computer Engineering, Electrical Engineering or a similar quantitative field is preferred but not strictly necessary, depending on industry experience. Experience with medical data (Claims, Rx, Clinical) is a big plus. |  | Required Skills and Experience | 3 years of professional software engineering experience, with data systems as a primary responsibility. | Deep modern database/data warehouse expertise, with emphasis on the Apache Spark ecosystem. | Professional experience dealing with large, complicated datasets. | Python programming expertise: best practices, packaging, modern libraries, etc. | Experience with Docker, Kubernetes and build/deployment. | Experience with common AWS products and tools (EC2, S3, etc). | Accustomed to working with git and shared codebases. | Preferred Skills and Experience | Experience developing and maintaining data pipelines powering production ML models. | Experience developing or using modern data pipelining and lineage tracking tools. | Experience with distributed computing systems. | Experience with healthcare data and/or insurance data is a plus. | About Prognos Health |  | Prognos is a leading clinically-focused healthcare analytics company with a platform that can query patient-centric data to answer key healthcare questions in minutes not months. The prognosFACTOR™ platform addresses payer, life sciences and provider needs, enabling clients to securely, efficiently and cost effectively analyze billions of lab and health records on more than 325 million de-identified patients. prognosFACTOR is HIPAA compliant and harmonizes and integrates lab data with other healthcare data assets from a trusted and diverse data ecosystem. For more information, visit prognoshealth.com. |  | Values &amp; Culture | We are collaborative. We put team trust and energy ahead of individual stardom. We are humble and willing to admit when wrong. | We go above and beyond. We exceed the needs of our partners and are not limited by our job descriptions. We are accountable for our actions, work, decisions, and results. | We are purposeful in all that we do. We focus on what matters and prioritize. We think in perspective and see the full picture. | We are curious. We learn from solving big problems. We are never satisfied and always strive for a better way. We aim to continually develop ourselves. | We are courageous and honest. We are not afraid to speak out. We challenge the process. We deal with conflict head on. | We are enthusiastic. We are optimistic for change and a better future. We believe in the greater good. We celebrate accomplishments and have fun. | Our Mission |  | To improve health by driving the best actions learned from the world’s data |  | Our Vision |  | To prevail over disease and empower people everywhere to live life to the fullest |  | Selected Perks | Flexible work arrangements (e.g. no set hours), fully remote work, and unlimited PTO | Health Insurance | Life Insurance | Long Term Disability | Dental | Vision | 401(k) | HSA | FSA | Dependent Care Flexible Spending | Commuter benefits | Free access to One Medical Group | Gym discounts | Flexible work hours and locations | Health Advocate | Employee Stock Option Plan | Powered by JazzHR | 7l9iunljdz",New York NY,Data Engineer
IPC Systems Inc.,/rc/clk?jk=3af203f634dfa6a7&fccid=39b0383f9383fb81&vjs=3,"SUMMARY | As a Data Engineer, you will take on big data challenges in order to deliver insightful analytics. You will build data pipelines and data models that will empower engineers and analysts to make data-driven decisions and deliver a deep understanding of the business. Your attention to detail will provide the stakeholders with the highest standard in data integrity. | JOB RESPONSIBILITIES | Use an analytical, data-driven approach to drive a deep understanding of our business. | Build data pipelines and data models that will empower engineers and analysts to make data-driven decisions | Build data models to deliver insightful analytics | Deliver the highest standard in data integrity | Strong analytical skills with ability to analyze and project sales, subscriber, and engagement data. Performs competitive analysis, reviews industry information for current trends and opportunities. Works closely with analytics teams to develop comprehensive analytical reports to enable data-driven decisions to increase engagement and conversions of target customer segments. | DESIRED QUALIFICATIONS, EDUCATION and/or EXPERIENCE | Experience in business intelligence, analytics, or an equivalent analyst position with experience in SQL and an additional object-oriented programming language (e.g., Python, Java). | High level of expertise in data modeling. | Effective problem solving and analytical skills. Ability to manage multiple projects and report simultaneously across different stakeholders. | Structured thinking with ability to easily break down ambiguous problems and propose impactful data modeling designs. | Attention to detail and effective verbal/written communication skills. | Bachelor’s degree in Engineering, Computer Science, Statistics, Economics, Mathematics, Finance, a related quantitative field, or equivalent practical experience. | 2-6 years of experience in consulting, business intelligence, analytics, or an equivalent analyst position with experience in SQL and Python. | COMPENSATION AND BENEFITS | Working in beautiful Cache Valley with access to an exceptional outdoor lifestyle, a university campus nearby, and the chance to test the fitness products we create. | Highly competitive compensation. | Full benefits package (Medical, HSA, FSA, Dental, Vision and Life insurance) | 401(k) with company match. | A PTO policy that ensures you are able to find a happy work-to-life balance. | Access to cutting-edge technology and hardware for work and fitness. | Collaborative work space and environment. | A free beverage center and snack bar to keep you hydrated and fueled throughout the day. | **Not all perks are applicable to all positions and/or locations** | Check us out! | Stack Overflow Company Page | iFit-About Us | List of states we are able to hire in: AK, AZ, AR, CA, CO, CT, FL, GA, ID, IL, IN, KS, MI, MA, MD, MN, MO, NC, NH, NJ, OH, OR, PA, SC, TN, TX, UT, VA, WA, WI. |  | DISCLAIMER | Your employment at ICON is ""at will"". You and the company each have the right to terminate the employment relationship at any time for any cause or for no cause at all. Nothing but an express written contract signed by you and a Vice President of this Company can modify this ""employment at will"" arrangement. | ""We do not discriminate in employment opportunities or practices on the basis of race, color, religion, sex, national origin, age, ancestry, mental or physical disability, sexual orientation, gender identity, medical condition, genetic information, marital status, Veteran status or any other characteristic protected by law.""",Logan UT,Data Engineer (Remote)
Spotify,/rc/clk?jk=3263046d05662f11&fccid=d08a054a512224de&vjs=3,"Reference Data QA Engineer | Major Responsibilities | Create &amp; execute test plans for enterprise-wide customer &amp; securities reference data platforms which include user interface, web services &amp; SDKs. | Independently work with Project Manager, Business Analysts and Developers across different regions. | Liaise with front office &amp; middle office trading systems other integrated back office systems for integration testing based on systems specific testing needs. | Develop, maintain &amp; execute regression automation for user interface, web services, SDKs &amp; ETL. | Write medium to complex SQL queries for data validation. | Build schedulers &amp; monitor results for automaton plans. |  | Required Skills | 3 or more years of professional work experience in software QA or engineering | Prior proven experience of developing automation framework using Java or python programming languages | Experience in functional black box &amp; ETL testing (candidate with testing of enterprise application is preferred) | Strong database background and should have experience working with relational databases (e.g. postgres, sybase) and non-relational databases (e.g. MongoDB) | Understanding of various trading instruments | Familiarity with Unix and its basic commands | Must have experience working with build &amp; deployment tools e.g. jenkins, maven, gitlab, perforce etc. | Must have experience working with schedulers tools e.g. airflow, autosys, bamboo, etc. | Must have experience working with test management tools (e.g. ALM) and requirement / defect work-flow management tools (e.g. JIRA) | Excellent written and verbal communication skills | Should be able to function independently with minimal or no supervision | Familiarity with Agile methodology &amp; various status reporting methods | Education | Graduate in computer related course or undergraduate with relevant computer related experience",New York NY,Ref Data QA Engineer
Amazon.com Services LLC,/rc/clk?jk=87e4f75813a1bbd8&fccid=27cef24446eb0bc4&vjs=3,"About Daily Harvest |  | Daily Harvest makes nourishing food built on fruits and vegetables accessible. We do this by delivering thoughtfully sourced, chef-crafted food to customers' doorsteps, all ready to enjoy in minutes. We're on a mission to take care of food, so food can take care of you. |  | Our team is collaborative, driven, and future-thinking. We're constantly learning, experimenting, and iterating, and celebrate failure just as much as success. We take risks, try new things, and we get things done. We love adaptogens and cruciferous vegetables but never say no to cake. Everything we do, we do in the service of our community. |  | Position Overview |  | Daily Harvest is looking for a Senior Data Engineer to join our Data team. The Data team uses data to create better customer experiences, smarter business decisions, and efficiencies company-wide. We do this by providing accurate, actionable analyses, building sophisticated data science products, and by maintaining a reliable data infrastructure. | We are looking for a Senior Data Engineer that is excited about building and maintaining reliable data infrastructure that is built to scale with a growing business. You will work with every part of the company and a wide variety of data sources. We've implemented an ELT data stack using tools like Fivetran, Segment, dbt, BigQuery, and Looker with orchestration in Airflow. |  | What you'll do: | Apply a consistent data modeling philosophy to our data warehouse that balances stakeholder requirements, interpretability, and performance | Help create a roadmap for the next iteration of our data warehouse and its downstream dependencies | Review engineering specifications to ensure measurement and analytics requirements are met | Help translate business requirements into feature requests for engineering | Help create a roadmap to develop infrastructure and tooling to support use of our data outside of Looker by data analysts and data scientists | Build, maintain, and optimize the backend of our business intelligence tool (we use Looker) | Develop tooling, testing, and processes to ensure our data infrastructure will be robust and observable | Write custom ETL scripts or automate tasks using Python | Who you are: | 5+ years of experience as a Data Engineer or in a similar role | Strong Python and SQL skills (experience using dbt and BigQuery is a plus) | Experience applying data modeling philosophies and aware of their tradeoffs | Experience using git for version control and collaboration | Experience working with CI / CD to test and deploy code | Experience using Airflow to orchestrate data pipelines | Experience using Docker | Experience working with cloud services (we use GCP) | You are open and willing to help others learn and look to learn from others, as well | You make thoughtful recommendations that consider impact, urgency, and long term implications and know when to say no | An ability to drive projects forward with strong communication and stakeholder management skills | Ability to thrive in a fast-moving, evolving, and high growth environment | Benefits: | Unlimited Daily Harvest in the office to keep you hustling, not hangry + cold brew (...always stocked) | Flexible time-off policy + flexible working hours (Unlimited PTO Plan) | Competitive medical, dental, and vision benefits, 401K + equity participation | Ancillary benefits: Commuter, Gym membership + Citi Bike discounts | Access to everything we make (including recipes in development) | Annual company retreat | Quarterly team outings, weekly onsite happy hours, + regular DH team gatherings to celebrate our co-workers | Book club + running club | Showers for post-morning or mid-workday workouts | A dynamic, ambitious, and fun work environment + dog friendly!! |  | At Daily Harvest, our mission is to take care of food, so that food can take care of you. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, color, religion, military or veteran status, sex, gender, gender identity or expression, sexual orientation, national origin, age, disability or genetic information. These are our guiding principles and apply across all aspects of employment.",New York NY,Senior Data Engineer
Icon Fitness,/rc/clk?jk=07916cbfcfe1b451&fccid=fe2d21eef233e94a&vjs=3," | 3+ years of experience as a Data Engineer or in a similar role | Experience with data modeling, data warehousing, and building ETL pipelines | Experience in SQL | Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.) | Knowledge of data management fundamentals and data storage principles | Knowledge of distributed systems as it pertains to data storage and computing |  | At Amazon Advertising, we are dedicated to drive measurable outcomes for brand advertisers, agencies, authors, and entrepreneurs. Our ad solutions—including sponsored, display, video, and custom ads—leverage Amazon’s innovations and insights to find, attract, and engage intended audiences throughout their daily journeys. With a range of flexible pricing and buying models, including self-service, managed service, and programmatic ad buying, these solutions help businesses build brand awareness, increase product sales, and more. |  | We are hiring a Data Engineer (DE) to help us deliver on a key global goal of setting up an ML-based, real time service that will help segment our customers and continuously feed recommendations to both global Ads and Consumer teams to improve customer success. |  | The person hired will help build the data-sets to underpin the ML models as well as help create the output pipeline. They will be working very closely with a data science team and a scale Data Engineering team. The ideal candidate will be passionate about working with big data sets and have the expertise to utilize these data-sets to answer business questions and drive growth. This is a highly visible role with global scope, senior leadership exposure, international stakeholder management and some international travel. The team is located in NYC and London, both locations are an option for this role. |  | The primary responsibilities of this role include: |  | Design, develop and maintain scalable, automated, derived / or non-derived, insightful data-marts and tables which will be the main input for the models, reports and dashboards | Use analytical and statistical rigor to solve complex problems and drive business decisions. | Write high integrity code to retrieve and analyze data from database tables (Redshift), learn and understand a broad range of Amazon’s data resources and know how, when, and which to use and which not to use | Explore new datasets, onboard them into our data cluster and scale for WW use | Set up ingest, analyse, output processes | Set up mechanisms to proactively measure quality of outputs | Create and maintain key program reports | 5+ years of experience as a Data Engineer, BI Engineer, Business/Financial Analyst or Systems Analyst in a company with large, complex data sources. | Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets | Experience working with AWS big data technologies (EMR, Redshift, S3) | Demonstrated strength in data modeling, ETL development, and data warehousing | Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy | Experience providing technical leadership and mentoring other engineers for best practices on data engineering | Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations |  | ",New York NY,Data Engineer Machine Learning Service
Reddit,/rc/clk?jk=03851e95d680bf73&fccid=6e3d8a314fe8af80&vjs=3,"The Engineers at Chainalysis are driven by working on new technical challenges every day. As relentless problem-solvers and big dreamers, we're building technology that enables widespread adoption of cryptocurrencies and lays the foundation for a new economic paradigm. |  | A Software Engineer on the Data Engineering team is really good at building reliable and scalable backend microservices to make data available to our products, ultimately enabling law enforcement agencies, the world's largest cryptocurrency businesses, and financial institutions to do things like take down dark web marketplaces, prosecute child predators, and solve the Twitter Hack (yeah, we do all that!). We measure success by how well we build components that are reusable and replaceable, reducing time to production, building tools that simplify data analysis and fostering collaborative relationships across the company. |  | In one year you'll know you were successful if… |  | You contributed to increasing the scalability of our backend systems | Solved complex problems with peers and stakeholders across the organization | You have helped the team to modernize our stack to a streaming architecture |  | A background like this helps: |  | You're an experienced scala developer (or other JVM language) | You have an interest in functional programming | You are comfortable contributing to a code base with high code coverage standards | You are comfortable building backend ETL pipelines and services in a Cloud environment (AWS, GCP) | You have some knowledge of one or more blockchains (Bitcoin, Ethereum, etc) | You're comfortable (or interested in) working with distributed No SQL databases | You've worked in an environment where you support the systems you put into production | You're interested in distributed systems |  | Bonus points: |  | Experience in functional programming | Experience working with Kafka | Experience working with No SQL Key-value databases such as Cassandra or Dynamo | Experience working deploying services on to Kubernetes |  | At Chainalysis, we help government agencies, cryptocurrency businesses, and financial institutions track and investigate illicit activity on the blockchain, allowing them to engage confidently with cryptocurrency. We take care of our people with great benefits, professional development opportunities, and fun. |  | You belong here. |  | At Chainalysis, we believe that diversity of experience and thought makes us stronger. With both customers and employees around the world, we are committed to ensuring our team reflects the unique communities around us. Some of the ways we're ensuring we keep learning are an internal Diversity Committee, Days of Reflection throughout the year including International Women's Day, Juneteenth, Harvey Milk Day, and International Migrant's Day, and a commitment to continue revisiting and reevaluating our diversity culture. |  | We encourage applicants across any race, ethnicity, gender/gender expression, age, religion, ability, experience and more. Additionally, if you need any accommodations to make our interview process more accessible to you due to a disability, don't hesitate to let us know. You can learn more here. We can't wait to meet you. |  |  | Applying from the EU? Please review our Candidate GDPR Notice.",New York NY,Software Engineer Data Pipelines
CrossBorder Solutions,/rc/clk?jk=b02458d9b9546e5b&fccid=49d86eba2fe6034f&vjs=3,"The New Tech for Restaurant and Hospitality | Fourth is the first global cloud-based intelligent back office platform in the restaurant and hospitality industries. We’re no ordinary software company! We pride ourselves on the culture we have created and work hard to ensure this is never compromised. This is a place where our people want to work; where they enjoy and develop in their roles; where our people pull together to work as a team and ultimately be a company we are all proud to be a part of. |  | By developing innovative products, building relationships with our customers and enabling great careers; we are focused on building lifetime loyalty with our customers and each other. The vast majority of us have worked within the industry; we believe we need to be passionate about and have experienced the industry in order to help our customers grow their businesses successfully. |  | When recruiting, we look for those who want to join our journey; those who want to be a part of our personality and culture; those who are professional; have energy, enthusiasm, and ambition, are entrepreneurial and those who can show they can live by our company values each day. Working at Fourth provides an opportunity to build a successful career where our people develop and grow in a dynamic, fast paced environment. | Role overview: |  | The Data engineers will be an integral part of the newly forming data science team at Fourth. Their role fits into our broader data science team together with data scientists. The main role of data engineer will be to (1) maintain and develop the modelling environment and (2) deploy and monitor the ML models. |  | Your daily job will include: | Ensuring that the cloud-based modelling environment is up and runningEnsuring all data is available in the right format and reliable quality as well as all modelling tools (mainly Python and related packages) are in place.Deployment of the ML models developed by colleagues in the data science team - this includes dockerisation of ML pipelines, code review, optimisation of the code for speed and memory requirements, deployment to the IT platform (microservice architecture)Writing proprietary packages / frameworks to be used for internal purposes to make the standard tasks easier (such as data load, model testing, exploratory data analysis, etc.).A/B testing and monitoring of the models in production |  | Experience and competencies required: | Hands-on experience (2+ years) with building data-powered solutions.Hands-on experience with the development, deployment and monitoring of data and machine learning solutions in production.Very strong coding skills (clean and commented code, version control, documentation), experience with the database administration (design, etl, query optimisation) and development skills (dev/uat/prod environments, automated testing and deployment).Hands-on experience with data lakes (HDFS, Snowflake and similar), feature stores (SQL-like databases) – design of new features, adding new data sources from API and other services, database design, query optimisation, distributed computing, performance optimisation.Hands-on experience with DataOps and MLOps – namely development, testing, deployment and monitoring of data and ML solutions; using tools like MLFlow, KubeFlow, AirFlow or similar; Git and Docker.Hands-on experience with feature engineering and data pipelines using Python, SQL and similar tools.Experience with larger data sets and production environments.Interest in data science, machine learning, and forecasting is a big plus. |  | Considered as an advantage: | Experience with DockerExperience with Microservices architecture | Only short-listed candidates will be contacted. |  All personal data provided by you in your application will be processed by Fourth Bulgaria EOOD, UIC 203576042, solely for our recruitment purposes. If you do not provide your personal data, Fourth Bulgaria EOOD will not be able to review your application. Fourth Bulgaria will transfer your personal data only within its corporate group and to companies which provide recruitment support to Fourth Bulgaria. Fourth Bulgaria will ensure you can exercise your rights to access, correction and erasure of your personal data. More information on Fourth Bulgaria’s privacy policy for job candidates is available here and by applying you confirm that you have read it. | Our Story | In July 2019 Fourth joined forces with HotSchedules to become the global leader in end-to-end restaurant and hospitality management technology solutions. Together, the merged company now represents the world’s largest and only provider of end-to-end restaurant and hospitality management solutions for customers across the globe and of all sizes—from a single location or franchisee restaurant to a global restaurant or hotel chain. The combined company’s complete software-as-service (SaaS) solution suite including: scheduling, time &amp; attendance, applicant tracking, training, inventory management / procurement, HR / benefits and payroll services now serves customers in 120,000 locations worldwide and is supported by a dedicated, unified team across offices in the US, UK, Bulgaria, China, Australia and UAE."" | We are an Equal Opportunity Employer | All qualified applicants will receive consideration without discrimination because of sex, gender identity, gender expression, sexual orientation, marital status, race, color, age, national origin, military status, religion, or disability or any other legally protected status.",Sofia NM,Data Engineer
Roofstock,/rc/clk?jk=019e68beff650949&fccid=a7d44e65387f5f78&vjs=3,"Join SafeGraph’s small but growing Engineering team. You'll spend your time using functional programming to build and improve upon our product - the most accurate dataset on places in the US - and solving interesting data problems in the geospatial and temporal world. You should be excited by the prospect of shaping the vision of and building something new, getting in at the ground floor and helping the SafeGraph team continue to build and support a world-class data product. |  | About you: |  | Have attention to detail and bias for action, are a prolific communicator, and thrive in uncertainty. | Are passionate about big data, looking to work in a fast-paced environment, focusing on hard problems where the solutions are often not predefined. | Authorized to work within North America and are comfortable working remotely. |  | Requirements: |  | Minimum 3+ years of backend engineering work experience. | Proficiency in writing production-quality code, preferably in Scala, Java, or Python. | Familiarity with all things building data products - schema design, modeling, optimization, scalability. | Deep understanding of Apache Spark and distributed data systems - that allows you to solve production-scale problems. | Excellent communication skills. |  | Great-to-haves: |  | Experience and passion for functional programming in solving data problems (Scala). | Experience with AWS. | Experience working with huge data sets. | Experience with building ML models from the ground up. | Experience with open source development. |  | About SafeGraph: |  | Our goal is to be the dominant place to get any data on a physical Place. We sell our product - our datasets - to data scientists and machine learning engineers at companies of all sizes. | At SafeGraph, we’ve taken a measured approach to build a long term company. We were profitable in 2019, have hired experienced leadership from the start and care deeply about democratizing access to data to everyone. | We currently have ~50 employees and are growing quickly. We raised a $20 million Series A, and the CEO was previously the founder and CEO of LiveRamp (NYSE:RAMP). | While SafeGraph was started in San Francisco, we've been distributed across North America since 2019, and currently have small offices (in non-pandemic times) in SF, NYC &amp; Denver. We get the entire company together in the same place as often as possible, and cannot wait to do this again soon! | We offer our employees a robust set of benefits, including health, dental &amp; vision insurance coverage, a 401k, work-from-home stipend, mental health benefits, and much more.",Remote,Engineer - Data
Prognos,/rc/clk?jk=908501875ff084f2&fccid=fe404d18bb9eef1e&vjs=3,"Engineering | Data | We are looking for backend engineers to join our team of talented engineers that share a common interest in distributed backend systems, big data, their scalability and continued development. By joining the Data and Insights organization, you’ll be a key contributor in making the systems that power our large scale data processing, insights, and machine learning efforts more reliable. Above all, your work will alter the way the world experiences music! | Location | New York, NY | Job type | Permanent | What you'll do: | Coordinate technical projects across teams within Spotify | Be a technical leader in the team you work with and within Spotify in general | Be a valued member of an autonomous, multi-functional agile team | Build, automate, maintain, scale, and monitor user-facing systems using best practices, with reliability and scalability in mind | Work with the other specialists to debug and fix issues | Implement high-quality release engineering practices to facilitate rapid development, safe changes, and engineer efficiency | Maintain system architecture documentation and runbooks | You’ll collaborate with other engineers, product managers, and designers to identify and tackle complicated problems, creating an awesome engineering experience within Spotify | You’ll initiate, influence and drive technical projects across teams within Spotify | Use industry standard, cloud native tech, which means easily transferable skills and a focus on your professional development | Take an active part in the operational responsibilities for our own infrastructure | Work on what you want during regular hack days and bi-annual hack weeks | Work from our awesome office in New York City where we tackle problems in Data, Monetization, Music Recommendations, Features for Artists, and Social Networking | Who you are: | Have experience crafting and building distributed, well designed services in Java | You know how to work with large scale data systems | Are able to work across tech stacks, implementing features end-to-end | You will be able to apply your knowledge of the Java runtime environment to help us improve our backend service and data landscape. | You are experienced with deploying and operating services on Linux. | You have a deep understanding of system design, data structures, and algorithms. | You care about quality and you know what it means to ship high quality code. | Google Cloud Platform experience is a bonus | Comfortable working both independently and collaboratively (pairing and mobbing) | As a phenomenal influencer with great communication skills, you love sharing your knowledge with others and helping them grow | Perks of being in the band | Extensive learning opportunities, through our dedicated team, GreenHouse. | Flexible share incentives letting you choose how you share in our success. | Global parental leave, six months off - fully paid - for all new parents. | All The Feels, our employee assistance program and self-care hub. | Flexible public holidays, swap days off according to your values and beliefs. | Spotify On Tour, join your colleagues on trips to industry festivals and events. | Learn about life at Spotify | You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens. | Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service with a community of more than 345 million users.",New York NY,Backend Engineer - Data Foundations Platform
HotSchedules,/rc/clk?jk=7b2c45167d59626b&fccid=220fbd4f54e31ecb&vjs=3,"At Zuora, data is a key strategic pillar for operating our business. All our teams rely on accurate, timely data to measure and improve outcomes, including building better products, improving the customer experience, optimizing the sales process, and ultimately increasing the overall value we deliver to our customers. |  |  | As an engineer on the Zuora Corporate Data Team, you'll have the opportunity to design and build robust end-to-end solutions to support data-driven decision making across the entire company, including: |  |  | Pipelines and tools for delivering a reliable flow of metrics from all our products to help understand customer usage | Orchestration for extracting and transforming data from all our business applications and systems | A centralized data warehouse that assembles key data and insights about customers, users and products into one central location | Integrations with BI tools and other applications to provide business and technical users with secure, flexible trustworthy access to the data they need, through interfaces that meet their specific needs |  |  | Specific technical skills that you'll have a chance to learn and refine include: |  |  | Building and monitoring data pipelines using open source tools and frameworks | Designing, implementing and operating robust API services | Provisioning and maintaining complete architectures on public cloud infrastructure (AWS) | Working with a variety of data stores and technologies | Data modeling and solving complex analytic transformations using SQL |  |  | As an engineer, advancing your career depends on much more than your ability to solve isolated technical problems. Joining the Corporate Data Team will also give you the opportunity to develop the whole range of skills you need to scale your effectiveness as an organizational leader: |  |  | Working directly with end users in a variety of different roles to understand business requirements and identify appropriate technical solutions | Working cross-functionally to support an effective corporate data governance program | Writing design and implementation documentation | Running a transparent, predictable software delivery lifecycle, including planning, testing and release management | Operating high-availability, mission-critical services |  |  | Successful candidates for this position should be able to demonstrate a solid foundation as software engineers, experience building and operating complex data pipelines, excellent communication skills, and a strong desire to learn. |  |  | Specific desired experience and knowledge includes: |  |  | python for data integration and application development | complex SQL for data transformation, validation, and analytics | data pipeline orchestration (we currently use Airflow) | provisioning and monitoring public cloud infrastructure (AWS) | interactive data exploration and visualization",California,Data Engineer
PepsiCo,/rc/clk?jk=8c89ef16714377bc&fccid=6576e7250aa78c3c&vjs=3,"QUALIFICATIONS | Degree from a reputable college/university focusing on a technology-related field | 4 years of IT experience with appetite for data management | Exposure to all facets of software development life cycle like analysis, design, development, data conversion, data security, system integration and implementation | Coding ability in the field of large data handling through SQL and NoSQL databases, data manipulation and ETL tools using Snaplogic | Successful command of implementations of effective Cloud-based data migration and data integration strategies across ERP systems using industry standard IPAAS Tools | Understanding of data warehousing, data lake concepts and Tableau dashboards, visualizations, etc. | Best practices adaptation of data architecture, data modeling and policies in the data management space is a plus | Solid problem-solving capabilities including the ability to disaggregate issues, identify root causes and recommend solutions | Strong interpersonal, written/verbal communications skills | WHO YOU'LL WORK WITH | You’ll work in our Waltham office as part of our Technology &amp; Digital (T&amp;D) function. This group provides for all the firm’s internal software development needs. The goal is to provide McKinsey’s business thought leaders the tools and knowledge they need as they work with clients around the globe. | Our development teams are small, flexible and employ agile methodologies to quickly provide our user community with the solutions they need. We combine the latest open source technologies together with traditional enterprise software products. | WHAT YOU'LL DO | You will provide data ingestion and integration solutions for our product development including integration of products with firm’s MDM (datahub) system, data warehouse and other digital systems. | In this role, your responsibility will include identification, organization and ingestion of data spread across multiple sources (operational, analytical and reporting). You will work in collaboration with stakeholders and team members to analyze user needs and develop data integration solutions. | You will be a member of a squad, having technical capability to develop and implement our data identification, ingestion and integration solution. You will implement new and ongoing data integration needs to complete end to end setup of the product. You will also work with data scientists to understand | their needs and provide them with necessary information. You will adhere to data architecture strategy, best practices, standards and roadmaps. You will do management of data including definition, usage and quality via architecture repositories like data dictionary, data models, metadata and data quality logs. You will also design, build and support data pipelines for ingestion, transformation, conversion and validation. | You will conduct data assessment, perform data quality checks, transform and load raw data using SQL and ETL tools. Your work will contribute to the overall implementation of product solutions.",Waltham MA 02451,Data Engineer
XO,/rc/clk?jk=009192d0833fdf41&fccid=e7c7bace894a9291&vjs=3,"The front page of the internet,"" Reddit brings over 430 million people together each month through their common interests, inviting them to share, vote, comment, and create across thousands of communities. |  | The User Understanding team is a key part of the machine learning strategy at Reddit and serves as the central location for accessing and leveraging inferences and aggregations about users. We power ML with real-time, reliable, and ethics-driven user features. |  | How You'll Have Impact |  | As the 6th largest site on the internet, Reddit generates billions of events and terabytes of data in a day. You will own projects from ideation to production instead of being stuck making small incremental gains on enterprise systems. We are looking for Infrastructure and Data Engineers to join us in solving problems in order to enable products that millions of users will love, and ultimately bring community and belonging to Reddit's users. We are a team of builders that value impact, personal growth, openness and kindness. |  | What You'll Do |  | As an engineer on the User Understanding team, you will design and build high-scale pipelines that process billions of events a day. You will partner with machine learning engineers and data scientists to build foundational user features for training and serving. Your work will power feeds ranking, content understanding, recommendations and much more. |  | Responsibilities: |  | Design, build and maintain Reddit's User Feature Store | Build feature pipelines that process and transform 40B+ daily events | Collaborate with data engineers, infrastructure engineers, and machine learning engineers to find technical solutions to complex challenges | Participate in the full software development cycle: design, develop, QA, deploy, experiment, and analyze | Partner in the development of a state-of-the-art ML platform that powers the next generation of deep learning, natural language processing, recommendation systems, representation learning and computer vision |  | Qualifications: |  | 4+ years of experience with large-scale data pipelines and tools such as Kafka/Flink | 4+ years of experience writing clean, maintainable code in Python or Scala | 4+ years of experience with distributed large scale data storage systems like Cassandra and Redis | Experience with backend development | Strong design and architecture skills | Able to take ownership of entire projects and communicate about them | Passionate about sharing knowledge and growing others around you |  | Bonus: |  | Experience with Kubernetes | Experience with batch data processing | Experience with Machine Learning",New York NY,Software Engineer Machine Learning - Data
Grubhub Holdings Inc.,/rc/clk?jk=24a008a2fd4f3830&fccid=ad7ad02fc5bc10d7&vjs=3,"About Dorilton Capital: |  | Patience |  | We prefer to create value over the long term by reinventing cash flow while avoiding excessive leverage. |  | Partnership |  | We work actively with existing management teams recognizing that long-term business success is the result of a team effort. Dorilton views its role as providing additional capital for acquisitions and growth projects and support and expertise to take its companies to the next level. |  | Continuity |  | We partner with companies that are led by strong management teams and have a successful history and culture. We firmly believe in our companies continuing with the elements that made them successful. |  | About the Data Engineer: |  | The Data Engineer will work within BI/DW and will be responsible for Informatica Cloud Development. The person should have experience in developing End to End Data Pipelines using Informatica Cloud (Including Parameters) and have worked on using multiple types of connectors including Sql Server, Snowflake, Dynamics 365 and Azure Blob |  | Responsibilities: |  | Understand the requirements and understand the larger the initiatives that each project would be part of and build solutions with end state in mind | Create end to end solutions for data engineering, such as: | Analyzing source data | Identifying optimal connector | Develop frameworks or reuse/leverage on existing frameworks | Parameterize and Automate | Work directly with end users and understand requirements |  | Requirements: |  | 8+ years' experience in Informatica PowerCenter including Informatica Intelligent Cloud Services. | 8+ years' experience working with Relational Databases and Data Warehouses with ability to write, analyze and optimize complex Sql queries | 3+ years in Snowflake and have experience as Sql Developer and DB Administration | 2+ years' experience with Azure | Experience in IICS Application Integration components like Processes, Service Connectors, and Process Object | Must have experience in IICS pipelines using parameterization, REST API and task flows | Should have experience in automation and scheduling of Jobs (Eg: RunAJob) | Strong understanding of SQL, PL/SQL programming | Ability to write Complex queries (joining multiple tables), optimize, tune and analytical functions | Good understanding of Azure Blob, Kubernetes | Well versed with all Informatica Client Components (PowerCenter Designer, Workflow Manager, Workflow Monitor, Repository Manager) | Experience with Data Model creation (Ability to adopt to any major tools) | Should have created Data Models in 3NF and Dimensional Modelling | Experience with Data Vault method of Data Warehousing | Experience in working with Python for Data Science and knowledge of Python libraries | Certification with Snowflake and Informatica is preferred | Good Statistical and Analytical knowledge | Excellent communication skills and a good team player | Communicate effectively with both business, technical stakeholders as well as leadership. | Experience gathering and refining requirements. | Demonstrated ability to produce high-quality results with attention to details | Excellent analytical, organization, prioritization and oral/written communication skills | Should have experience working with Agile methodology and experience in working with JIRA | Positive and attitude and can-do attitude | Should be able to work in a fast-paced environment with ability to build systems",New York NY 10013,Data Engineer
Braintrust,/rc/clk?jk=13f5ce3672a0ddba&fccid=6b1be984647e492b&vjs=3,"How you will help | Working with our product and data science teams, you will act as the healthcare data SME while engaging with data suppliers to understand the technical and operational challenges of their data models, data collection processes and data remittance. You will help with the data ingestion and onboarding of key supplier data and with comprehensive ingestion and staging of this data for team. Our team will also rely on you for your expertise with healthcare data and your strong SQL skills. | What you will doAnalyze all vendor data assets and correct complex data anomaliesProvide guidance and support during the vendor onboarding process which includes data ingestion, normalization, and QC activitiesProvide expertise on all healthcare data types: medical claims transactions (e.g. 837 and 835), pharmacy claims (NCPDP D.0), EMR/EHR, lab transactions, and other emerging data assetsSource and oversee the lifecycle of reference data sources specified by the Data Architecture teamSupport Data Architect in the mapping and normalization process, providing expertise and authority on accurate meanings in source data |  | About You | You are...A data geek with enviable SQL skills and a passionate sense of ownershipA self-starter who enjoys working in a small, rapidly changing, fast paced environmentConfident enough to course correct a process or team when requiredMethodical, executing through several approaches to determine the best fitEnergized by learning even if outside the scope of day-to-day responsibilitiesComfortable working on several different tasks throughout your workday | Desired Skills and ExperienceBS degree in math, statistics, or similar4+ years’ experience in the healthcare data industry, preferably in a consulting environmentProficient in programming against large data assets with a working knowledge of SQL, preferably also knowledgeable in SAS and/or RSubject matter expertise in a wide variety of healthcare data assetsKnowledge of the healthcare industry and analytics utilized by pharmaceutical marketing teamsProven analytical, evaluative, and problem-solving abilitiesExtensive experience working in a team-oriented, collaborative environment | Our company challengesEmpowering clients with highly rewarding data discovery and licensing toolsIngesting and managing billions of healthcare records from a wide variety of partnersStandardizing on common data models across data typesOrchestrating an industry-leading HIPAA privacy layerInnovating our proprietary de-identification and data science algorithmsBuilding a culture that supports rapid iteration and new possibilities | The infrastructure and culture we are building will provide an environment that cultivates innovation. We want to move fast knowing we can fix anything we break along the way. If a new need arises, we want to turn around a solution quickly. We want to solve our challenges in ways that create even more possibilities. We’re creating a platform that lets us discover what else we might do. | We have big plans | We’ve built a platform that will scale to support an ever-growing array of data providers and innovative products and services. You must be able to think big while still delivering on near-term requirements. | HealthVerity, based in Center City Philadelphia, is a venture-backed technology company that is transforming the way data-led organizations make critical decisions. Our technology platform serves as the foundation for the rapid creation, exchange and management of healthcare and consumer data in a fully-interoperable, privacy-protecting manner. Advantaged by highly sophisticated identity resolution and matching capabilities, HealthVerity is on a mission to increase transparency, forge interoperability and activate deeper insights. | HealthVerity is an equal opportunity employer.",Philadelphia PA,Data Acquisition Engineer
Radiant Systems,/rc/clk?jk=3cc877c343ae6fe8&fccid=c1099851e9794854&vjs=3,"Summary | Posted: Feb 24, 2021 | Role Number:200226156 | Here at Apple, we build products that revolutionize entire industries. It's the diversity of our people and their ideas that encourage the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple and help us leave the world better than we found it. | We are always on the lookout to improve our efficiency squeeze that last bit of performance, creatively scale and increase the reliability of our payment services that power Apple Online Store, AppStore, Apple Music, and more. Apple Pay brought mobile payments to millions of customers, and it's just the beginning. Are you passionate about building massively scalable services that surprise and delight? Do you have a natural curiosity for learning new technologies? If you answered yes, the Apple Pay Performance Engineering team wants to hear from you. | Key Qualifications | Strong programming skills. | Understanding of data structures, software design principles and algorithms. | Experience with Python scientific libraries like numpy, scipy. | Expertise in design and development of data intensive applications. | Experience in handling time series data for stream processing, storage and modeling. Experience in developing front-end tools and visualization is a plus. | Solid understanding of Performance Engineering fundamentals. | 3+ years experience in using data processing engines like Apache Spark. | 3+ years experience in building robust data pipelines. | Description | We are seeking a self-driven, data-savvy candidate who can analyze time series data from multiple data sources and build smart solutions that can help improve efficiency and productivity of Apple Pay Performance Engineering team. | You will | • Transform data from multiple sources to identify emergent trends, actionable insights. | • Use data modeling techniques to identify performance bottlenecks and opportunities for | optimizations. | • Build forecasting models to plan datacenter capacity. | • Build models that informs and helps in planning product launches. | • Design and develop data pipelines that feed from multiple sources. | • Visualize data to make it easily ingestible to leadership team. | • Evangelize data driven processes and best practices across Apple services. | • Mentor junior engineers in the team. | Education &amp; Experience | BS, MS, or PhD in Computer Science or equivalent industry experience",Austin TX,Data Engineer Apple Pay
Apple,/rc/clk?jk=64c0ad28082903b3&fccid=12e41f7040f168e8&vjs=3,"Job detailsJob TypeFull-timeTemporaryFull Job DescriptionAbout The Opportunity: |  | Grubhub’s Reconnect Returnship Program is a 16- week paid returnship for experienced professionals returning to the workforce after taking time off for caregiving. The program is open to the individuals who have at least five years of professional experience and have been out of the paid workforce for at least two years to focus on caring for a child or other dependent. If you meet these criteria, we welcome you to apply. | If you don’t meet the above criteria for the returnship program, please don’t apply to this opportunity. Instead, check out all of our other full-time careers opportunities here. Whether you’re a current college student, recent graduate or a currently employed professional looking to join our team, there’s a role for you and we want to hear from you. | At Grubhub, we are excited to launch our Reconnect Returnship Program because we appreciate the skills you can offer, the perspective you provide, and the contributions you will make. This program offers you a chance to revamp your skills, update your resume with new experience, and make connections with other individuals transitioning back to the workforce. It also offers support through Path Forward, a nonprofit organization on a mission to empower people who’ve been focused on caregiving transition back to the paid workforce. | This program is 100% virtual during the 16-week returnship program. Upon successful completion of the returnship, there is a possibility of an offer for full-time employment.A full time offer will likely be based out of New York and a full-time opportunity will offer relocation package. | We’re all about connecting hungry diners with our network of over 300,000 restaurants nationwide. Innovative technology, user-friendly platforms and streamlined delivery capabilities set us apart and make us an industry leader in the world of online food ordering. When you join our team, you become part of a community that works together to innovate, solve problems, grow, work hard and have a ton of fun in the process! | Why Work For Us | Grubhub is a place where authentically fun culture meets innovation and teamwork. We believe in empowering people and opening doors for new opportunities. If you’re looking for a place that values strong relationships, embraces diverse ideas–all while having fun together–Grubhub is the place for you! | About the team | Data Engineers at Grubhub are responsible for building efficient data pipelines that transform raw data into a format usable by downstream applications that serve both analytical and operational use cases. As a Data Engineer on the Commerce Platform will be working closely with business stakeholders, product managers and engineering teams to meet data requirements of various initiatives in Grubhub. This includes improving the Commerce Platform data usability within Grubhub Data Warehouse, empowering business users, data analysts, data scientists in decision making. As a Data Engineer you will have the opportunity to build data pipelines related to Orders, Transactions and Payments data using cutting edge Grubhub Big Data Platform tools. | The Impact You Will Make: | Working with high volumes of data to efficiently process and expose for analysis. | Collaborating with other engineering teams on strategies for data. | Work with cutting edge data processing technologies. | Understand our stakeholder requirements and write complex and efficient code to transform raw data into an easy to approach data marts. | Doing deep dives on business verticals where you become one of the foremost experts on that vertical in the company. | Analyze data to measure impacts of data schemas and use it to iterate on improvements. | Translate from technical to business, and vice versa. | What You Bring to the Table: | Tech-savvy and passion: You have an aptitude and eagerness to learn new and complex technologies at Grubhub. | Strong cross-functional communication: You can explain complex technical information to both technical and non-technical stakeholders in a clear and concise way. | Detail-oriented problem-solving skills: You enjoy the process of scratching beneath the surface to find the best solution to challenges. | Curiosity and self-awareness: You’re able to articulate when you may need help and are curious enough to ask questions to gain a deeper understanding. | Team work: You’re able to work both independently and collaboratively with the team, bringing your best to every interaction and inspiring others to do the same. | Enthusiasm for Grubhub’s mission: You’re excited to help connect hungry diners with great, local restaurants. | Basic Qualifications | Strong computer science fundamentals from a college degree in Computer Science, a related field of study, or equivalent experience. | 3+ years experience with SQL, data modeling and patterns. | 3+ years experience with general purpose programming language, preferably python. | Background in data engineering writing ETL jobs within a Business Intelligence context. | Open to relocate to New York at the conclusion of the program if offered Full-Time employment post-returnship. | Preferred Qualifications | Experience big data processing with Spark and other big data tools a plus. | Exposure to Amazon AWS or another cloud provider. | Excellent communication skills, including the ability to crystallize and broadly socialize insights. | And Of Course, Perks!: | Learning and Career Growth. Your personal and professional development is a priority at Grubhub. From day one, we empower you to lead and be an active participant in your career growth. We provide continuous learning opportunities, training, and coaching and mentorship programs. | MealPerks. Who’s ready for some lunch? We provide our employees with a weekly Grubhub credit to enjoy and support local restaurants. We also offer company-wide meals several times a year to bring our Grubhub family together. | Fun. Every Grubhub office has an employee-led Culture Crew that connects people through fun, meaningful events and initiatives. Some of our popular past events include: Wing-eating contests, Grubtoberfest, 5k Runs, Bring Your Child to Work Day, regular happy hours, and more! | COVID-19 Response. All of our employees are currently working from home and will be for the foreseeable future. We look forward to seeing everyone in-office when it’s safe to return. | Grubhub is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics. If you are applying for a job in the U.S. and need a reasonable accommodation for any part of the employment process, please send an e-mail to reconnect@grubhub.com and let us know the nature of your request and contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this email address. | CA Privacy Notice: If you are a resident of the State of California and would like a copy of our CA privacy notice, please email privacy@grubhub.com.",New York NY 10018,Data Engineer- Returnship Commerce Platform
Zuora,/rc/clk?jk=0b9f44376ff927d1&fccid=0228cab72298ad78&vjs=3,"The Enterprise Data and Analytics team is responsible for managing and developing our strategic data assets, empowering analysts and business partners to further develop consistent organizational knowledge and insights. This means we are responsible for bridging core data engineering workloads and robust repeatable analytical solutions. As a Data Engineer you will partner engineering managers, product managers and analysts across our company to ensure data is conducted and delivered in a secure and predictable manner throughout the org. We put data engineering and software development best practice at the core of our product and strive to deliver an intuitive product foundation that analysts and business stakeholders can rely on. |  | RESPONSIBILITIES: |  | Develop and maintain robust data pipelinesMake the best use of modern cloud infrastructure and patterns, primarily AWSParticipate in data modelling across domains such as sales, marketplace, flight ops, financeConduct exploratory data analysis with technical and business teamsDeliver thorough quality analysis, monitoring and relevant alerting via Datadog, PagerDutyPublish timely wiki material describing data assets, catalog, data lineage including ERD and DFDSupport for data governance initiatives including profiling, de-duplication, classification |  | QUALIFICATIONS &amp; EXPERIENCE: |  | 5+ years of experience with ETL/ELT and integration flows in SSIS, Airflow, Fivetran or similar4+ years of in-depth practical experience with SQL, performance tuning, database admin3+ years of experience in data warehouse or data lake delivery and evolutionDemonstrated knowledge of cloud infrastructure, data lineage and data quality pipelinesAbility to rapidly investigate and assimilate new systems and datasetsClear and concise verbal and written communicationBachelors or advanced degree in analytical fieldExperience with data governance and MDMPulumi for IaC |  | POSITION: |  | Location: Remote or Fort Lauderdale, FL | Department: XO Global | Reports to: Director, Enterprise Data and Analytics | FLSA Classification: Exempt",Fort Lauderdale FL 33309,Data Engineer
Discovery,/rc/clk?jk=f3653cc59f150b99&fccid=54318639950c6d34&vjs=3,"Journera is building the first real-time data platform for the travel industry, and we're backed by some of the biggest brands in the world. We're changing the way the industry works by unlocking the power of its data and are seeking a data engineer to join the Technology team. |  | We are building an Experience Management Platform that analyzes data from airlines, hotels, transportation and beyond. That requires us to handle a lot of real-time data in a secure, timely, and scalable way, which is where you come in. |  | Data engineering is core to what we do and we are looking for folks to join our team who share our passion for data. |  | What you bring | Bachelor's in science or engineering, or relevant work experience | Real passion for wrangling data | Experience building secure and scalable data solutions | Software design and development skills | Experience with relational and NoSQL datastores | Automated test and continuous integration experience | Experience with languages such as Python, SQL, R, Scala, and Go | What you bring bonus edition | Advanced academic degree | Stats skills | Your personal story about using data to save the world | Cloud experience - AWS preferable | Experience working in a startup or other entrepreneurial environment | Your ideas on how to make travel easier through by using data | Experience with Redshift, Airflow or Glue | Remote work experience | What you'll do | Work closely with data scientists and software engineers to build and run a world-class data streaming environment | Solve data engineering problems at a scale and speed most don't need to worry about | Design and build new analytical and data solutions |  | We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Illinois,Data Engineer
Etsy,/rc/clk?jk=aa4078e249ab5648&fccid=c2269800a97efd1e&vjs=3,"Cybersecurity Engineer (ISSO/ISSM) | Data Privacy | Risk Assessment | eMASS &amp; ATO | About Us: | VetCentric is focused on delivering outstanding services to the federal government. We have extensive experience in the fields of cybersecurity, supply chain &amp; logistics management, strategy, business analytics, and IT services such as system design, continuous improvement, virtualization, and data center management. VetCentric is an SBA-certified HUBZone company and VA CVE certified Service-Disabled Veteran-Owned Small Business (SDVOSB). We operate in 15 states with offices in Washington DC and Northern Virginia. | Perks Working with Us: | Competitive compensation | Comprehensive health, vision, dental benefits | 15 days leave and 10 days of paid Federal Holidays | 401(k) with a matching plan | Annual training budget | Fantastic company culture | Location: Remote, US (Any location). Candidates from HUBZones (https://maps.certify.sba.gov/hubzone/map#center=37.901871,-100.550019&amp;zoom=5) preferred | Hours: Monday - Friday, 8:00 AM to 5:00 PM | Employment Eligibility: Eligible to work for any employer in the United States |  | Position Summary: | This position is responsible for creating and implementing security plans, enforcing information systems security policies, standards, and methodologies, creating security plans, conducting Risk Assessment Reports and System Requirements Traceability Matrices (SRTMs) | Required Skills &amp; Experience: | Education: Bachelor's or higher in computer science/ technical discipline | Years of related experience: 5+ years of experience is required as an ISSO/ ISSM including experience implementing, and enforcing information systems security policies, standards, and methodologies; creating security plans, policies &amp; procedures (SSP's), Risk Assessment Reports or SRTM's | Technical Skills: FISMA, FedRAMP, RMF (Risk Management Framework), NIST 800.53 controls, and understanding SA&amp;A processes, eMass, ATO (supporting system processes &amp; control implementation) | Clearance: Must have or be able to attain and maintain Public Trust or higher | Desired Skills &amp; Experience: | Certifications: Cybersecurity certifications, such as CISSP, CISSO, CISM, CompTIA Security+, CISA, CEH, GCIH, GCIA, GCFA, GCFE, CDMP, CDP-DG or similar | Job Description: | Support adoption and implementation of NIST-based standards across the agency. | Support all steps of NIST 800.53 | Participate in the selection of the organization's common security controls and in determining their suitability for use in the information system | Review the security controls regarding their adequacy in protecting the information and information system | Prepare and review documentation to include Systems Security Plans (SSPs), Risk Assessment Reports, Certification and Accreditation (C&amp;A) packages, and System Requirements Traceability Matrices (SRTMs); support security authorization activities | Implement and enforcing information systems security policies, standards, and methodologies | Evaluate security solutions to ensure they meet security requirements for processing classified information; perform vulnerability/risk assessment analysis to support certification and accreditation. | Manage changes to the system and assess the security impact of those changes. | “E-Verify Employer, EOE Females/Minorities/Protected Veterans/Individuals with Disabilities; VetCentric partners will offer equal employment opportunities to all persons without regard to race, color, religion, sexual orientation, gender, gender identity, age, national origin, physical or mental disability, veteran status, or other characteristic protected by applicable law.”",Remote,Remote Cybersecurity Engineer (ISSO/ ISSM) | Data Privacy | Risk Assessment | eMASS & ATO
Colgate-Palmolive,/rc/clk?jk=dd36335545038d44&fccid=6e3d8a314fe8af80&vjs=3,"The Engineers at Chainalysis are driven by working on new technical challenges every day. As relentless problem-solvers and big dreamers, we're building technology that enables widespread adoption of cryptocurrencies and lays the foundation for a new economic paradigm. |  | A Software Engineer on the Data Engineering team is really good at building reliable and scalable backend pipelines to make data available to our products, ultimately enabling law enforcement agencies, the world's largest cryptocurrency businesses, and financial institutions to do things like take down dark web marketplaces, prosecute child predators, and solve the Twitter Hack (yeah, we do all that!). We measure success by how well we build components that are reusable and replaceable, reducing time to production, building tools that simplify data analysis and fostering collaborative relationships across the company. |  | In one year you'll know you were successful if… |  | Learned the ins and outs of our clustering techniques | Developed new clustering techniques that are used in production | Conducted complex investigations that resulted in success for our customers | Become one of the experts in analyzing BTC/UTXO data | Contributed to the effectiveness and speed of the Blockchain Intelligence team |  | A background like this helps: |  | You're an experienced Java developer (or other JVM language) | You are comfortable building backend ETL pipelines and services in a Cloud environment (AWS, GCP) | You're comfortable working with SQL databases | You've worked in an environment where you support the systems you put into production | Deep understanding of the Bitcoin protocol both from a protocol and data perspective | You've worked in an environment where you support the systems you put into production | You're interested in distributed systems |  | Bonus points: |  | Experience working with Kafka | Experience working deploying services on to Kubernetes | Experience with statistical analysis on real world datasets |  | At Chainalysis, we help government agencies, cryptocurrency businesses, and financial institutions track and investigate illicit activity on the blockchain, allowing them to engage confidently with cryptocurrency. We take care of our people with great benefits, professional development opportunities, and fun. |  | You belong here. |  | At Chainalysis, we believe that diversity of experience and thought makes us stronger. With both customers and employees around the world, we are committed to ensuring our team reflects the unique communities around us. Some of the ways we're ensuring we keep learning are an internal Diversity Committee, Days of Reflection throughout the year including International Women's Day, Juneteenth, Harvey Milk Day, and International Migrant's Day, and a commitment to continue revisiting and reevaluating our diversity culture. |  | We encourage applicants across any race, ethnicity, gender/gender expression, age, religion, ability, experience and more. Additionally, if you need any accommodations to make our interview process more accessible to you due to a disability, don't hesitate to let us know. You can learn more here. We can't wait to meet you. |  |  | Applying from the EU? Please review our Candidate GDPR Notice.",New York NY,Software Engineer Data Intelligence
Adobe,/company/Abtsus-LLC/jobs/Microstrategy-Data-Engineer-99a5e1bb05a6d0fe?fccid=1d3074ca80c15a92&vjs=3,"Job detailsSalary$50 - $60 an hourJob TypeFull-timeContractNumber of hires for this role1Full Job Description7+ years of data engineering experienceMicroStrategyProficient in working with databases like Hadoop, Oracle, Teradata, Netezza, Sql Server, DB2.Job Types: Full-time, ContractPay: $50.00 - $60.00 per hourSchedule:Monday to FridayWork Location:One locationVisa Sponsorship Potentially Available:No: Not providing sponsorship for this job",Concord CA,MicroStrategy Data Engineer
Piper Companies,/company/Ursi-Technologies-Inc/jobs/Pyspark-Data-Engineer-03a0e1008a684d09?fccid=4080d24e411d5a46&vjs=3,"Job detailsSalary$81,402 - $177,705 a yearJob TypePart-timeContractNumber of hires for this role2 to 4QualificationsBachelor's (Preferred)SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Full Job DescriptionHello,Hope you are doing great,We have an urgent requirement for Database Developer based in Westlake, TX for 1+ years contract so please let me know if you are interested.Looking for candidates who can work on W2, ( Green Card, US Citizens, H4-EAD, H1B Transfers).Job Description: What Will You Be Doing:You will work closely with your team including analysts, TPMs, and data scientists to define tasks, provide estimates, and work together to deliver a world class solution. The ideal candidate will have the balance of technical skills and business acumen to help the client better understand their core needs while understanding technical limitations. Your solutions will enable machine learning and data-driven decisions to improve safety, customer experience, and efficiency.Experience partnering &amp; communicating with executive management team to understand business needs and pain pointsAbility to communicate engineering concepts to business stakeholdersPassion for building large scale machine learning pipelinesAdept at developing and iterating solutions rapidlyRequirements: Bachelor’s degree in Business or Technology required5+ years of previous experience in Data Engineering,3+ years of experience in PySparkAdvanced engineering skills with PythonPrevious experience with Cloud platform, AWS preferredData engineering implementation experience in Python, Spark and PySpark, or SparkSQLPrevious experience with very large dataDemonstrated ability to identify business and technical impacts of user requirements and incorporate them into the project scheduleStrong communication and interpersonal skillsAbility to work both independently and as part of a teamWork across team to solve technical roadblocks for our customers.It is plus…Experience building data and computational systems that support machine learningKnowledge of AWS servicesKnowledge of modern software delivery practices, including source control, testing, continuous deliveryPrevious Agile experienceData streaming experience in SparkJob Details: Type: 6-12 month W2 contractStart Date: 02/22/2021Contract End Date: 08/27/2021 with a possibility of extensionTime Zone: If you a remote candidate, you need to be able to work during PST timezone.Location: Seattle, WARemote Work: They are currently 100% remote.Job Types: Part-time, ContractPay: $81,402.00 - $177,705.00 per yearSchedule:8 hour shiftEducation:Bachelor's (Preferred)Experience:SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)",Atlanta GA,PySpark Data Engineer
VetCentric,/rc/clk?jk=3802b82b9efa8a1c&fccid=f17180ffbe436a3c&vjs=3,"Discovery hires the very best and brightest talent who are enthusiastic and passionate to fulfill the company’s mission of empowering people to explore their world and satisfy their curiosity. | In exchange for their talent and drive, employees are provided with an engaging, diverse workplace and the resources they need to learn, thrive and grow in their careers. | Job Summary | Discovery Digital Media covers a wide array of vertical platforms, including TV Everywhere, OTT, and DTC. Our Digital Analytics team plays an integral role in delivering insights to inform all Digital product and strategy decisions that Discovery takes. We’re looking for a seasoned Sr. Data Engineer to join the team. | Responsibilities | Be responsible for supporting the development of digital analytics capabilities, processes and best practices across the business. | Leverage different systems and various testing tools to ensure data quality and data integrity of our growing array of data sources on various Digital platforms. | Work with Product and developers to fix data issues. | Be responsible in generating data-driven insights to help turn business problems and challenges into opportunities. | Take a leading role in growing scale and deepening understanding of our data assets– including internal, external, structured, and unstructured data– while testing hypotheses using appropriate analytical techniques. | Requirements | Work with the larger Digital and Commercial Insights Lab team to: | Support the design and construction of relevant data sets | Ability to create test cases, capture meaningful logs, review test results, report/document technical issue, and work with developers to help troubleshoot. | Support development of data quality framework to proactively identify data inconsistencies and errors; work to ensure that issues are quickly remedied. | Video player knowledge required. | Understanding of video and product quality metrics such as video start time, EBVS, etc. | Knowledge with Adobe Analytics | Familiar with app distribution platforms: TestFlight and HockeyApp | Assist and contribute to development of predictive models | Apply machine learning techniques to solve complex questions or fuel new business opportunities. | Understand the data- and analytics-related activities in various business lines, focused but by no means limited to digital platforms | Promote and assist driving understanding and adoption of digital data assets | Requirements (cont.) | Bachelor’s, Master’s in a related field of studies (Computer Science, Statistics, Applied Math preferred)Proficient with programming/data tools such as SAS, SQL, R, Python, Hadoop, HiveExperience working with data science and/or software engineering teamsExperience with statistical and predictive modelling techniques such as machine learning, decision trees, probability networks, association rules, anomaly detection, recommender systems, natural language processing, clustering and regression.Experience is Data processing and manipulation on Cloud systems such as AWS and Google CloudKnowledge and understanding of the media/digital media industry a plusKnowledge of web analytics platforms such as Google Analytics and Adobe Analytics is a plusExcellent quantitative and analytical skills with the ability to draw conclusions based on dataVery detail orientedStrategic thinker, flexible problem solver, great listener and team orientationComfortable working in a deadline-driven environmentExcellent communication skillsSkilled at project tracking and executionMust have the legal right to work in the United States | Discovery Communications, Inc. is an equal opportunity employer. Discovery is committed to being an employer of choice, not just a good place to work, but a great and inclusive place to work. To that end, we strive to recruit and maintain a workforce that meaningfully represents the diverse and culturally rich communities that we serve. Qualified applicants will receive consideration for employment without regard to their race, color, religion, national origin, sex, sexual orientation, gender identity, protected veteran status or disabled status or, genetic information. | We will consider for employment all qualified applicants, including those with criminal histories, in a manner consistent with the requirements of applicable state and local laws, including but not limited to all local Fair Chance Ordinances. | EEO is the Law | Pay Transparency Policy Statement | California Job Applicant Privacy Policy | If you are an individual with a disability and need an accommodation during the application process, please send an email request to HR@discovery.com.",New York NY 10022,Sr Data Engineer
VidMob,/rc/clk?jk=d8249654e7a9b065&fccid=bf09bb071838aa6a&vjs=3,"Overview |  | VidMob is an award-winning Intelligent Creative Platform that provides an end-to-end technology solution for all a brand's creative needs. It is an integrated platform combining first-of-a-kind creative analytics with best-in-class creative production to understand and improve marketing effectiveness. |  | We seek candidates that are curious, collaborative and committed to excellence. We take every hire seriously and only choose seriously talented team members. We care deeply about our employees and are dedicated to making VidMob an exceptional company to work for. VidMob is proud to offer comprehensive health plans paid for by the company, enhanced Maternity/Paternity Programs and unlimited vacation plans. We also provide employees with access to 401K plans, healthy food and snacks, and pre-taxed transit. |  | VidMob is an Equal Opportunity Employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. |  | Who We're Seeing |  | We are looking for a Data Engineer with the passion and technical skills to make data come to life. The ideal candidate must blend expertise in data with hands-on experience in engineering. You should strive to design and build large-scale, high-performance data systems through your insights into data and how to manage it at scale. If faced with roadblocks, you continue to reach higher to make greatness happen. You believe in not only serving customers but also empowering them by providing knowledge and tools. |  |  | What You'll Do |  | Play a pivotal role in growing our data initiatives, supporting both data insights and software engineering teams. Rapidly process data on an immense scale. Create reports that drive business outcomes and optimizations. Balance granularity and business value with resource cost and efficiency. Develop the maturity of VidMob's platform by reducing latency and error rates. Maintain speed, availability, and reliability as our client base grows. |  | Responsibilities |  | Work with data insights teams to define and extract data sets for use in analysis and machine learning | Work with software engineering and data science teams to design, build, and manage our application DB, machine learning components, and our data infrastructure | Maintain, extend, and automate reporting infrastructure | Manage the design and architecture of our data warehouse | Create software tools to automate and manage ETL processes and dependencies |  | Minimum Qualifications |  | B.S. in computer science or equivalent experience | Advanced working knowledge of SQL and relational databases, MySQL preferred | Understanding of statistics and data modeling methodologies | Experience collaborating with Data Scientists and Data Analysts | Ability to create fast solutions to problems introduced in a changing environment with iteration towards optimal solutions | Experience with data-related AWS services such as RDS, Redshift, and Kinesis | Experience with programming or scripting language, preferably Python, Java, or Scala |  |  | Details |  | Location: Pittsfield, MA |  | Compensation: Competitive salary and equity (based on experience). |  | Benefits: |  | Health Care Plan (Medical, Dental &amp; Vision) | Unlimited Paid Time Off (Vacation, Sick &amp; Public Holidays) | Family Leave (Maternity, Paternity) | Training &amp; Development | Stock Option Plan | 401k Plan",Pittsfield MA,Data Engineer
Bank of the West,/rc/clk?jk=1ff2c9b1385c4e67&fccid=6f31f1ca1e1177d9&vjs=3,"ABOUT US: | GroupM Data &amp; Analytics Services is a Marketing Science, Technology, and Consulting group specializing in fact-based business strategies and assessment. We provide expertise in data management, advanced analytics, technology deployment, and marketing effectiveness with one clear remit: help clients achieve significant and lasting improvements in marketing effectiveness and profitability. | YOUR IMPACT: | Support development and deployment of GroupM products and services across multiple cloud environments | Collaborate with a cross-functional team of client leads, application developers, operations engineers, and architects to translate complex product requirements into technical specs and design requirements | Optimize performance and cost efficiency of cloud based processes across multiple cloud environments (AWS, Azure, GCP) | Maintain a high degree of knowledge in cloud data architecture and ETL best practices especially across Google Cloud Platform products and services | Act as a consultant and subject matter expert for internal stakeholders in GroupM Data &amp; Analytics, GroupM Engineering and agency data science and tech leads | Develop and deploy automated scripts in BigQuery and other Google cloud services to be used by other teams to increase productivity | Facilitate architectural discussions and initiatives to ensure cloud-based products are optimally deployed with maximum availability of design features | Formulate and execute robust UAT protocols to identify and address latent errors in product functions | Design, build and deploy ETL and data management processes with reliable error/exception handling and rollback framework | Provide production support for data load jobs and develop customized query to generate automatic periodic reports | Build applications writing SQL/Python scripts to manipulate data and/or writing specific instructions for an off-shore programmers to write the scripts | YOUR QUALIFICATIONS: | Bachelor’s degree in Computer Science, Engineering, Mathematics or other technical field is highly preferred | Good written and verbal communication skills | Strong expertise with Data Architecture fundamentals, database design and programming, ETL and custom query development | Experience with the following GCP Services: Cloud Storage, DataProc, Dataflow, CloudSQL, BigQuery | Experience with building data pipes landing large files into Azure/GCP/AWS for processing, developing/ cleansing data for AI/ML purposes | Experience using the Linux Command Line | Experience with source code management systems | Use of Data Profiling Tools, ETL and Data Management Tools | Data warehousing/data modeling experience, with strong understanding of semantic and physical data models | Knowledge of Agile methodology | Able to deliver a broad range of data engagements in areas such as Data Architecture, Data Integration, Data Analytics, Data Governance, Data Quality and BI Reporting | Understanding of data analysis techniques and how they can be applied in the marketing context beneficial | Experience with the full development life cycle of an application stack - from architecture through test and deployment preferred | Prior knowledge of advertising ecosystem, understanding of marketing metrics, and analytical products offered as a service is preferred | ABOUT GROUPM: | At the heart of the world’s leading media agency network is a future-facing product company, building the tools to make media work for everyone. In partnership with the globe’s leading clients, agency teams, media companies and technology platforms, we’re using our privileged position to help our customers ascend to vantage points unique in our industry. | Our teams bring together agile product management, cutting edge data science and enterprise scale engineering to build products that will shape the next decade of data driven marketing. We believe consumer privacy, client confidentiality, brand growth and user experience are essential to performance and the sustainability of the advertising ecosystem and have assembled a global team with diverse skills and experience to help shape that future. | GroupM and all its affiliates embrace and celebrate diversity, inclusivity, and equal opportunity. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. We are a worldwide media agency network that represents global clients. The more inclusive we are, the more great work we can create together.",New York NY 10007,Sr. Data Engineer
Net Health,/rc/clk?jk=35e1b511713b8fda&fccid=fab75f149007c659&vjs=3,"Job detailsSalary$113,000 - $145,000 a yearFull Job DescriptionWe are an insurtech company where smart people can see the impact of their work as we tackle meaningful problems together. We think it’s fun to disrupt an industry that has been slow to change. But we aren’t shaking things up for the sake of change, we’re here to solve big problems using technology and an innovative approach to improve how small business owners access insurance. Like our small business clients, we are a diverse team of builders, dreamers, and entrepreneurs, so at the heart of every decision we make is the idea that if it doesn’t serve our clients, it doesn’t serve us. We hire passionate people who like to work hard, yet we also know that life exists outside the office. Small businesses are the backbone of the economy; talented team members are the backbone of Pie. We are pie-oneering a whole new approach to insurance. | As a Data Engineer at Pie Insurance, you’ll be a member of the team responsible for transforming the commercial insurance market by delivering best-in-class data architecture solutions and driving more accurate data-driven decision making. | Our team is looking for an experienced data engineer. We expect you’ll have spent at least 3 years in the data warehouse and/or data analytics space. Of course, you’ll also need certain skills and abilities to do the work. | How You'll Do It | As a data engineer with Pie, you will work with our data architect to Pie-oneer our data environment. This individual will be a key member that will work directly with our data architect to define the future state of our data architecture. This role will work in data architecture, data analytics, ETL development, and data reporting. | Success in this position will be establishing how data comes into and flows through the Pie insurance platform. This data will be used to help our organization quote customers based on data on best policy and prices for their workers compensation insurance. | The Right Stuff | 3-5 years working in data as an engineer. Building data solutions for a company who uses data as a primary part of their business. | Experience in data warehouse and/or data analytics. Qualified candidates may also come from a strong database skill-set involved in analytics architecture. | Strong experience in writing complex SQL queries. | Strong experience in ETL/ELT platforms is strongly preferred. | Exposure to one major SQL RDBMS or analytics database. (Snowflake, Redshift, MySQL, Postgres, Oracle, SQL Server, etc.) | Big Data and Business Intelligence exposure would help in the success of this role. | Our goal is to make all aspects of working with us as easy as Pie! That includes our offer process. When we have identified talent that is a good fit for Pie, we work hard to present an equitable and fair offer. We look at your knowledge, skills and experience that you bring, along with your compensation expectations and align that with our company equity processes to determine our offer ranges. | We value and want to support our team members, and are proud to offer a comprehensive compensation package which includes the following: | Compensation Range for position: $113,000 - $145,000 | Other Benefits: Each year Pie reviews Company performance and may grant discretionary bonuses to eligible team members. |  | Pie Insurance is an equal opportunity employer. We do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, marital status, age, disability, national or ethnic origin, military service status, citizenship, or other protected characteristic. |  | Our Achievements | Pie Insurance Named a Top Colorado Company 2020 | Pie Insurance Raises $45M Series B | Trustpilot | Glassdoor",Denver CO,Data Engineer
Etsy,/rc/clk?jk=36fa3d4d5ed86a96&fccid=9ecb91618c39a24f&vjs=3,"Company Description | Etsy is the global marketplace for unique and creative goods. We build, power, and evolve the tools and technologies that connect entrepreneurs with buyers around the world. As an Etsy employee, you’ll tackle unique problems alongside talented coworkers committed to Keeping Commerce Human. We're large enough that you'll focus on meaningful, complex challenges, but small enough that you can make a rewarding impact. |  | Job Description | Etsy's Data Engineering department builds systems and infrastructure and writes data pipelines for collecting, storing, and analyzing huge sets of data in batch and streaming contexts. As a member of the Data Pipeliners team, you'll help us identify, extract, and write shared data pipelines that power hundreds to thousands of downstream data processing jobs; you'll be responsible for designing new methods and tooling for ensuring the validity and quality of our data; and you will help other teams write robust data pipelines in Scala Spark, BigQuery SQL, and Kafka Streams. We value curiosity, enthusiasm, responsibility and generosity of spirit. | We're looking for a flexible engineer who cares about the data that flows through pipelines and the internal customers who will use that data. We care more about being excited to pivot to the appropriate tool for the job than having deep expertise in single systems. We work in Scala, Python, Java, and SQL, and we work with technologies like Spark, Kafka, Scalding, Airflow, Looker, Dataproc, and BigQuery. Experience with any of these is helpful but not required. | The technical staff at Etsy believes that code is craft, and that the work we do is part of a larger creative culture that includes the hundreds of thousands of inspired artists and designers who make Etsy such a unique marketplace. We believe that small, empowered, self-motivated teams can do big things. We believe in measuring everything, taking advantage of our continuous deployment system to ship code early and often, and keeping up a blameless culture based on trust and a commitment to learning. | This is a full time role. Our team is headquartered in Brooklyn, but we support remote work and accept remote applicants. | For candidates who will work remotely from Colorado, visit this link for information related to Colorado's Equal Pay for Equal Work Act. | Full-time employees are also eligible for Etsy's bonus program, equity program, and amazing benefits package. See our Careers Page for more details. |  | Qualifications | ABOUT THE ROLE | Our team is responsible for the daily delivery of hundreds of business-critical datasets. | We help define, maintain, and monitor the ETL pipelines and data models that power our core business analytics datasets and the upstream feature data that feeds all of Etsy's machine learning systems. | We build and maintain internal tools for monitoring and validating high-quality data. | Our team is leading the company's migration from Scalding to Spark by developing data onboarding materials, defining best practices, and adding tooling to ensure that pipelines are well-written. |  ABOUT YOU | You understand that being an effective software engineer is about communicating with people as much as it is about writing code. | You are willing to work with and improve code you did not originally write. | You are generous with your time and experience, and can mentor and learn from other engineers. | You can tackle unconstrained problems and know when to seek help. | You are flexible with languages and tools and are willing to learn whatever is necessary to get the job done. | You have familiarity with a few of the following: writing and scheduling ETL pipelines, writing SQL queries for exploration and analysis, tuning mapreduce jobs and cluster resources, building and monitoring cloud services, stream processing systems like Kafka Streams, Spark, or Dataflow. | Additional Information | What's Next | If you're interested in joining the team at Etsy, please send a cover letter and resume telling us why you'd be right for the position. As you’ve hopefully seen already, Etsy is a place that values individuality and variety. We don’t want you to be like everyone else — we want you to be like you! So write to us and tell us what you’re all about. | Our Promise | At Etsy, we believe that a diverse, equitable and inclusive workplace makes us a more relevant, more competitive, and more resilient company. We encourage people from all backgrounds, ages, abilities, and experiences to apply. Etsy is an equal opportunity employer. We do not discriminate on the basis of race, color, ancestry, religion, national origin, sexual orientation, age, citizenship, marital or family status, disability, gender, gender identity or expression, pregnancy or caregiver status, veteran status, or any other legally protected status. We will ensure that individuals with disabilities are provided reasonable accommodations to participate in the job application and interview process, to perform essential job functions, and to receive other benefits and privileges of employment. While Etsy supports visa sponsorship, sponsorship opportunities may be limited to certain roles and skillsets.",Brooklyn NY 11201,Senior Software Engineer Data Pipelines
Mount Sinai,/rc/clk?jk=733112d043ec20f3&fccid=5f95a7dbb2e62ed5&vjs=3,"We have an outstanding opportunity in our Technology Operations team. As a Data Operations Engineer, your main mission will be to set and maintain all aspects of Pansophic Learning's data solutions, including importing, reviewing, and validating data quality. You'll ensure data meets or exceeds quality standards and routinely work with stakeholders, handling their support requests, troubleshooting, and providing durable solutions, leveraging our evolving data platform. The drive to collaborate, gather feedback, solve problems, and tackle challenges through test and learn is highly valuable in this position. |  | **This position is remote and can be performed from any state except within California and Colorado** |  | PRIMARY RESPONSIBILITIES |  | Manage the end-to-end data processing (importing/validating/exporting) | Monitor and update all data processes and outputs to ensure quality | Analyze, and validate data from ingestion to production; assure data accuracy | Establish standards with cloud and data technologies | Develop, deploy and support processes to support data projects | Design, develop and execute unit testing plans | Develop technical and business process documentation for data processing projects | Design, develop and/or manage monitoring solutions for data processing projects | Maintain and continually improve data processing projects; establish and maintain automation to validate data | Conduct data pipelines capacity planning, optimization, troubleshooting and support | Consult with the development and operations teams to determine tools and technologies; Assist in establishing standards for the design, development, implementation and support of data processing projects | Provide data support to internal and external stakeholders; Communicate with customers to discuss any issues with received data and help them identify and fix data issues | Solve day-to-day production and customer challenges |  | REQUIREMENTS |  | Engineering Degree in Computer Science | Must be detail oriented | Familiarity with SQL Queries and Database concepts required | Familiarity with cloud-based databases | Good understanding of data science concepts | Demonstrated ability to absorb information, manage your own time and commitments, apply conceptual skills in practical applications, and achieve desired results in a highly technical, operations environment | Strong interpersonal, oral and written communication skills | Ability to work independently and in a team setting |  | EQUAL EMPLOYMENT OPPORTUNITY |  | It is our policy to abide by all federal, state and local laws prohibiting employment discrimination based solely on a person's race, color, religious creed, sex, national origin, ancestry, citizenship status, pregnancy, childbirth, physical disability, mental and/or intellectual disability, age, military status, veteran status (including protected veterans), marital status, registered domestic partner or civil union status, familial status, gender (including sex stereotyping and gender identity or expression), medical condition, genetic information, sexual orientation, or any other protected status except where a reasonable, bonafide occupational qualification exists. |  | #INDCP #ZR",McLean VA,Data Operations Engineer
Mount Sinai,/company/Vanguard-Solutions-&-Analytics/jobs/Data-Engineer-61841d41da9ecb68?fccid=36229fb1b2e21718&vjs=3,"Job detailsSalary$113,321 - $178,393 a yearJob TypeFull-timeQualificationsBachelor's (Preferred)Full Job DescriptionData EngineerExample of Duties:Provide expertise on all data concepts for the broader advanced analytics group, and inspire the adoption of advanced analytics, data engineering and data science across the organization.This will include Installing continuous pipelines of large pools of filtered information so that data analyst/scientists can pull relevant data sets for their analysesQualifications:Minimum seven (7) years of experience in the data engineering field, at least three of which must have been in a data analytics environment preferably in DoD or the intelligence community.Minimum Bachelor’s Degree in a relevant field. A Master's degree in a relevant field may be substituted for 3 years of general experience.Active DOD Secret or Higher ClearancePreferred / Desired Skills:Familiarity with the manipulation of unstructured data in a data analytics environment, and the use of open-source tools, cloud computing, machine learning and data visualization.Familiar with specialized languages relevant to the technologies employed such as Apache, Hadoop, etc.Vanguard Solutions &amp; Analytics supports some of the most complex Healthcare, Defense, and Intelligence projects across the country. We transform the ways clients invest in, integrate, and innovate solutions. We bring the expertise needed to understand and advance critical missions with a commitment to innovation and integrity.Vanguard Solutions &amp; Analytics is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.Job Type: Full-timePay: $113,321.00 - $178,393.00 per yearBenefits:401(k)Dental insuranceDisability insuranceEmployee assistance programHealth insuranceLife insurancePaid time offParental leaveProfessional development assistanceTuition reimbursementVision insuranceSchedule:Day shiftMonday to FridayEducation:Bachelor's (Preferred)Work Location:Multiple locationsThis Job Is Ideal for Someone Who Is:Detail-oriented -- would rather focus on the details of work than the bigger pictureAutonomous/Independent -- enjoys working with little directionInnovative -- prefers working in unconventional ways or on tasks that require creativityThis Job Is:A job for which military experienced candidates are encouraged to applyA job for which people with disabilities are encouraged to applyBenefit Conditions:Only full-time employees eligibleWork Remotely:Temporarily due to COVID-19COVID-19 Precaution(s):Remote interview processSocial distancing guidelines in placeVirtual meetingsSanitizing, disinfecting, or cleaning procedures in place",Washington DC,Data Engineer
Bloomberg,/rc/clk?jk=0a827d29bd3202c9&fccid=e8f998ddd15bac9c&vjs=3,"Minimum Clearance Required to Start: | Not Applicable/None | Job Description: | Welcoming different perspectives, like yours, is one of our strongest attributes | We’re a community of individuals with different points of view who enjoy opportunities to share our skills and ideas. We genuinely value people who will articulate their perspectives while respecting those of others. We value diversity, so here, you will find no limits to your professional growth. We will welcome you into the stimulating process of exchanging ideas for personal and professional growth. | Let’s talk about what you’ll be doing: | Parsons Corporation actively seeking summer interns to work on our data engineering team. We offer hands on access to cutting edge technology and a great work culture. Your responsibilities will vary and may include working on items such as expanding and optimizing our data and data pipeline architecture, analyzing data and integrating it into dashboards. Location: Remote. | Here's What We’re Looking for: | Required Qualifications | Due to the nature of the work US Citizenship is required. | Student pursuing a bachelor’s degree in Computer Science or Math | Rising Junior or Senior preferably | Minimum GPA of 3.0 | Desired qualifications | Knowledge of data files or databases | Self-driven and motivated | Should have strong written and interpersonal communication skills | Experience with programming languages, SQL language is a plus | Knowledge of data integration | Parsons is a leading technology firm driving the future of defense, intelligence, and critical infrastructure. By combining unique technologies with deep domain expertise across cybersecurity, missile defense, space, connected infrastructure, and smart cities, we're providing tomorrow's solutions today. | Ready to join our team as an intern this summer? | The anticipated annualized full time target compensation (median) for this position is: $45,000.00. | Benefits for this position include: 401k.",Centreville VA 20120,Data Engineer Intern (Summer 2021)
BlueFox Technologies,/rc/clk?jk=87c37cd41822e66c&fccid=c007936ceb766fe5&vjs=3,"Strength Through Diversity | Ground breaking science. Advancing medicine. Healing made personal. |  | Role &amp; Responsibilities: | Launched on March 29, 2019, and under the leadership of director Erwin Bottinger, PhD, the Hasso Plattner Institute for Digital Health at Mount Sinai (“HPI.MS”) will combine innovative, complementary research resources and talents in health care, health sciences, data sciences, biomedical and digital engineering to empower citizens and their healthcare providers for better health outcomes with real-time predictive and preventive digital health solutions. | With world-class expertise and complementary resources in health care, data sciences and biomedical and digital engineering, HPI.MS will conduct patient-engaged and data-driven research. The longer-term goals of HPI.MS include: | Establishing an organizational framework in which researchers at HPI and Mount Sinai collaborate and co-innovate seamlessly across health care and digital engineering | Extending funding opportunities for researchers in the United States and abroad | Researching and testing prototype digital health solutions for consumers, patients, providers, and health systems in the United States, Europe, and beyond. | We are looking for a Senior Data Engineer to be responsible integrating healthcare data from various sources into an integrated platform. Your primary focus will be to develop and manage ETL pipelines for the HPI.MS flagship data science platform. Your will also have responsibilities related to development of all server-side logic, definition, and maintenance of the central database, and ensuring high performance and responsiveness to request. Specific duties are as follows. | Developing and managing ETL pipelines to integrate health data for the HPI.MS data science platform | Managing sensible data as PHI data | Integration of multimodal elements with server-side logic | Building reusable code and libraries for future use | Optimization of applications for maximum speed and scalability | Implementation of security and data protection policies | Design and implementation of data storage solutions | Managing infrastructure for test and production | Requirements: | M.S. or equivalent in Computer Science, Electrical Engineering or related field | Minimum 2 years working with object-oriented/object function scripting languages (Python preferred). | Experience with creating and querying relational and NoSQL databases (MySQL, PostgreSQL, and MongoDB preferred) | Experience building and optimizing ‘big data’ data pipelines, architectures and data sets | Experience with data pipeline and workflow management tools (such as Azkaban, Luigi, Airflow) | Experience in preserving work using source control versioning tools (Git preferred) | Familiarity with using and managing cloud architectures (Microsoft Azure preferred) | Strong analytic skills related to working with structured and unstructured datasets. | Strong knowledge of data privacy and security and best practices | Must be able to work closely with others | Must be willing to pivot with the needs of the organization and learn/ grow skillsets and new technologies | Must be able to understand user requirements from specifications provided by business/non-technical personnel | Additional Optional Requirements: | Experience working with healthcare data, standards and technologies | Experience with electronic health records (OHDSI/OMOP is a plus) | Experience working with anonymization / de-identification technologies | SAP HANA database knowledge | Experience with image data warehouse (DICOM file knowledge is a plus) | Strength Through Diversity |  | The Mount Sinai Health System believes that diversity and inclusion is a driver for excellence. We share a common devotion to delivering exceptional patient care. Yet we’re as diverse as the city we call home- culturally, ethically, in outlook and lifestyle. When you join us, you become a part of Mount Sinai’s unrivaled record of achievement, education and advancement as we revolutionize healthcare delivery together. |  | We work hard to recruit and retain the best people, and to create a welcoming, nurturing work environment where you have the opportunity and support to develop professionally. We share the belief that all employees, regardless of job title or expertise, have an impact on quality patient care. |  | Explore more about this opportunity and how you can help us write a new chapter in our story! |  | Who We Are |  | Over 38,000 employees strong, the mission of the Mount Sinai Health System is to provide compassionate patient care with seamless coordination and to advance medicine through unrivaled education, research, and outreach in the many diverse communities we serve. |  | Formed in September 2013, The Mount Sinai Health System combines the excellence of the Icahn School of Medicine at Mount Sinai with seven premier hospitals, including Mount Sinai Beth Israel, Mount Sinai Brooklyn, The Mount Sinai Hospital, Mount Sinai Queens, Mount Sinai West (formerly Mount Sinai Roosevelt), Mount Sinai Morningside, and New York Eye and Ear Infirmary of Mount Sinai. |  | The Mount Sinai Health System is an equal opportunity employer. We promote recognition and respect for individual and cultural differences, and we work to make our employees feel valued and appreciated, whatever their race, gender, background, or sexual orientation. |  | EOE Minorities/Women/Disabled/Veterans",New York NY 10029,Senior Data Engineer - Hasso Plattner Institue for Digital Health
Parsons,/rc/clk?jk=d32917156139e059&fccid=e2d351f67eb360af&vjs=3,"Learn and work on meaningful initiatives with some of the best and brightest in the market research industry. The NPD Group provides the world’s most successful brands with leading market research, combining consumer and retail point-of-sale data with analytic solutions to interpret today’s market trends while anticipating tomorrow’s. In addition, we offer a career filled with innovation and growth to the forward-thinking problem solvers who join our team. | Position Overview | This candidate works with team on daily activities involving the design, management, maintenance, and utilization of databases. Provide leadership and drive strategy, standards, and processes for applicable database environments. | Location: Virtual | Experience and Education: | 7+ years of DBA experience | BS Engineering/Computer Science or equivalent experience required | Key Responsibilities: | Ensure maintenance of all databases and related technologies. | Mitigate customer-impacting issues and support product and infrastructure releases according to development plans and schedules. | Assign personnel to various projects and direct their activities. | Review and evaluate work and prepare performance reports. | Project long-range requirements for database administration in conjunction with other managers in technology as well as business function managers. | Develop and maintain ETL systems, both in T-SQL and with ETL Tools, such as SSIS. | Advise on most efficient database, table, index, data type and other database object design practices. | Ability to develop in, and administer successfully, all SQL Server services, including: SSIS, SSAS and SSRS. | Provide the strategic direction of all hosted database architectures and assess the impact of all hosted product requirements. | Provide technical leadership and mentoring to Database Administrators on complex database design and maintenance topics. | Review and implement enhancements to systems and procedures. | Resolve performance issues and root cause resolution. | Ensure clear and early communication, in particular ensuring that the line manager and/or relevant parties are kept informed of progress, issues, and difficulties. | Lead research into new or alternative technologies. | Review and sign off recommendations for upgrades or changes to the back-office infrastructure environment. | Engage with Technical Architects and other senior members of Development teams to provide technical designs and guidance to the Database Operations team, taking responsibility for the implications and impact of their design decisions. | Responsible for maintenance and “bug fix” activities for existing database infrastructure and equipment. | Undertake research into new or alternative technologies and make recommendations for upgrades or changes to the database infrastructure environment. | Responsible for installation of new database infrastructure and equipment and monitoring its stability. | Undertake thorough impact analysis of all assigned database infrastructure activities and understand the impact of work on database infrastructure systems and the business. | Respond to issues and support calls in-line with relevant procedures, including security. | Perform tests of upgrades and new database infrastructure and equipment. | Monitor database operations and support maintenance activities efficiently in order to ensure minimal issues and incidents. | Install new or upgrade database infrastructure equipment. | Carry out management responsibilities in accordance with the organization’s policies, procedures, and applicable laws. Responsibilities include interviewing, hiring, and training employees; planning, assigning, and directing work; appraising performance; rewarding and disciplining employees; and addressing complaints and resolving problems. | Ensure all staff is provided with training and resources needed to perform their jobs to the most outstanding degree possible. Ensure all staff is provided with frequent feedback and coaching in order to meet and exceed individual and team performance goals consistently. | Manage and encourage new ideas from staff to foster improvements through innovations. | Empower the staff to be accountable and responsible for their own actions and decisions. | All other duties as assigned. | Technical Skills: | Strong written skills. | Mentoring. | Strong interpersonal skills. | Strong understanding of database infrastructure, maintenance, administration, and design. | Ability to complete root cause analysis on database-related issues. | Ability to complete impact analysis on database-related issues. | Strong problem-solving skills in multiple operating environments. | Strong organizational skills. | Proven ability to troubleshoot connectivity and performance issues that involve database infrastructure. | Ability to complete database infrastructure installations. | Ability to research new or alternative database technologies. | Familiarity with Cloud (Azure, AWS and technologies associated with data warehousing and big data) | Experience working with cloud database services from Azure or AWS | Ability to complete database infrastructure installations. | Strong skills in setting, communicating, implementing, and achieving business objectives and goals through the direct management of others. | Strong organization/project planning, time management, and change management skills across multiple functional groups and departments, and strong delegation skills involving prioritizing and reprioritizing projects and managing projects of various size and complexity. | Strong problem-solving experience involving leading teams in identifying, researching, and coordinating the resources necessary to effectively troubleshoot/diagnose complex project issues; prior success extracting/translating findings into alternatives/solutions; identifying risks/impacts and schedule adjustments to facilitate management decision-making. | Strong communication (verbal and written) and customer service skills. Strong interpersonal, communication, and presentation skills applicable to a wide audience including senior and executive management, customers, etc., including diction/terminology and presenting information in a concise and effective manner to clients, management, and various departments using assorted communication mediums. | Competencies | Exhibits intellectual curiosity about and openness to new concepts and solutions for clients, the industry and market. (Intellectual Curiosity) | Shares insights from inside and outside NPD to stimulate greater innovation. (Innovation Management) | Does the right thing — acts ethically and respectfully and upholds NPD’s values. (Ethics and Values) | Operates effectively in a matrix situation — navigates different interests and leads through influence and engagement of colleagues, not simply by authority of their own position. (Political Savvy) | different parts of NPD to collaborate well. (Political Savvy)5. Switches altitude readily — knows when to adopt a big picture perspective and when to dig into the details. (Strategic Agility) | Fosters speed and agility — strives for prompt, timely action while avoiding excessive pursuit of perfection. (Strategic Agility) | The NPD Group, Inc. is an Affirmative Action/Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status or any other characteristic protected by law.",New York NY,Data Engineer Lead
LOCKHEED MARTIN CORPORATION,/rc/clk?jk=e21a6045cbec9eb3&fccid=74869e36bf8410c1&vjs=3,"Our mission is to give renters everywhere greater financial freedom to plan and enjoy their lives. |  | Our first product eliminates cash security deposits and puts more money back in renters' pockets. With over $45 billion tied up in security deposits for 110 million renters in the United States alone, it's time for security deposits to officially become a thing of the past. Tying up money at one of life's biggest and most expensive moments just isn't fair. |  | So we threw out the antiquated ""way of doing things"" and built a technology-driven insurance product to help bring renting into the 21st century. With Rhino, millions of renters across the country now have the opportunity to save with our award-winning deposit insurance. We've already saved hundreds of millions of dollars for renters and are trusted in over 1 million homes nationwide, and most importantly, we're just getting started... |  | As a Data Engineer at Rhino, you will build data infrastructure to enable data driven decision making for the whole company. |  | In this role you will: | Build scalable data pipelines and API integrations to support continuing increase in data volume and complexity. | Design and build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources. | Create data tools for data science and BI teams to assist them in building and optimizing our product into an innovative industry leader. | Evaluate and integrate open source and vendor tools for various parts of data infrastructure. | Participate in code reviews, standups, and planning sessions, while listening to feedback and commenting on others' approaches. | Work with stakeholders including the Executive, Product, Data Science and BI teams to assist with data-related technical issues and support their data infrastructure needs. | We're ideally seeking: | Interest in learning cutting edge technologies such as serverless computing and distributed systems. | Knowledge of object-oriented programming. | Knowledge of SQL. | Strong problem solving skills. | Someone who is excited to quickly ship features in a collaborative, rigorous, and fast-paced environment. | Experience with building ETL pipelines, data warehousing and data modeling. | Experience with Python. | Experience with Google BigQuery and Postgres. | Experience with data pipeline and workflow management tools such as Airflow or Luigi. | Experience with Google Cloud Platform or another cloud platform. | Benefits: | Competitive compensation and 401k | Unlimited PTO to give our employees a little extra R&amp;R when they need it | Stock option plan to give our employees a direct stake in Rhino's success | Comprehensive health coverage (medical, dental, vision) | Remote Work Program to allow for flexibility between home and the office | Generous Parental Leave to create a family-friendly culture | Wellness Perks (Gym, Classpass, &amp; Citibike Memberships) | Commuter Benefits through a Flexible Spending Account | Fintech Equality Coalition Founding member |  | Rhino is committed to the principle of equal employment opportunity for all employees, and to providing employees with a work environment free of discrimination and harassment. All employment decisions at Rhino are without regard to race, color, age, religion or belief, sexual orientation, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. Rhino has a zero-tolerance policy against discrimination or harassment based on any of these characteristics. This includes recruitment, hiring, promotions, transfers, discipline, terminations, wage and salary administration, benefits, and training.",New York State,Data Engineer
Sony Interactive Entertainment PlayStation,/rc/clk?jk=64ec8b375c9bb9ed&fccid=1058973f84077674&vjs=3,"Haven Life is an insuretech innovator at MassMutual that offers a new way to get life insurance online that's actually simple. |  | We combine the culture of a startup with the stability and backing of a Fortune 100 company to create an environment that's truly unique. |  | Our diverse team is comprised of smart, collaborative people who think big, execute quickly and don't take themselves too seriously. We're located in New York's Flatiron District and in case you're wondering, yes, we provide free snacks. Cold brew too. |  | If you're creative, professional and kind, we love to hear from you. |  | You will be joining a small, experienced technology team as one of the main developers responsible for our advanced online financial services application. Experience with web development is a plus, although more important to us is real programming experience and thoughtfulness about how and why you do what you do. |  | Your job will require you to work closely with your fellow technology team members, coordinate with QA resources, and to communicate with business team members in the completion of your work. Working with other developers you might work on items such as: developing components for algorithmic or manual underwriting module, creating workflow tools for routing insurance applications, defining and applying rule sets for third party data feeds, or building customer-facing components for taking new business applications. The job will be varied and challenging. |  |  | REQUIREMENTS |  | These are essential to success in this role: |  | 4+ years of of experience as a developer in a data-centric application environment | Involvement in all aspects of data integration projects - from business requirements and interface discovery and definition through design, modeling, and development, to testing and deployment | A solid grounding in data-driven applications and concepts (data/object modeling, database systems, structured and unstructured data, enterprise data frameworks) | Extensive experience with diverse systems integration paradigms including nightly batch, intra-day incremental data loading, and near-real-time interfaces | Strong experience with production support, troubleshooting, and batch/interface optimization | Define and implement data quality and reconciliation processes across complex systems | High proficiency with Talend (ETL tool); extensive experience of SQL and query evaluation &amp; optimization | Solid experience of DB development with PostgreSQL, Oracle, or other RDBMS (stored proc, trigger etc.) | Good knowledge with traditional object-oriented languages (especially in Javascript or GraphQL; Ruby and Python are plus) | Hands on experience on GraphDB (Neo4j) and Cypher or GraphQL query language | Comfortable expressing and defending points of view while respecting those of others | Excellent communication skills with a variety of audiences and varying degrees of technical knowledge | BA/BS degree in CS or equivalent major | Authorized to work in the US with or without sponsorship | Self-starter and self-motivated team player |  | These skills are not required but are a plus: |  | Testing strategy definition and framework creation | Experiance with Informatica, AbInitio | Experience and familiarity with financial services or other mathematically oriented applications. | Working in a startup or agile development environments | Working experience of AWS Cloud and EKS Kubernetes framework | Experience in multi-platform websites (desktop, mobile) | Exposure to modern MVC (Model-view-controller) frameworks | Experience with, node, meteor, and/or angular frameworks a plus |  |  | BENEFITS: |  | We have a stellar team of co-workers, a really cool office, and lots of fun activities. Oh yeah, and we pay competitive base salaries and we reward performance. Our salary structure is commensurate with experience. In addition, you will be eligible to participate in our comprehensive benefits program including medical insurance and 401(K). |  | The privacy of your personal information is important to us, click here to review our privacy notice.",New York NY,Data Engineer - Data & Integration
The NPD Group,/rc/clk?jk=66ad07c3f5ea21b5&fccid=c007936ceb766fe5&vjs=3,"Strength Through Diversity | Ground breaking science. Advancing medicine. Healing made personal. |  | Dept- Supply Chain | REQ #2678854 |  | Job Title: Data Engineer | The Data Engineer I is responsible for extract, transformation and loading of data from various source systems into a data store where it can be used for reporting and analytics. This individual works with both classic ETL tools like SQL Developer, Toad and SSIS as well as modern data transformation languages like R and Python (with RStudio or Jupyter Notebooks) but ultimately modeling the data into a relational data warehouse for use in visualizations and analytics. This individual will also have access to our enterprise data management tools, currently Pentaho, to help with scheduling, automation and alerting. The focus is on choosing the right solution for the job, working with business analysts or business subject matter experts who can explain what they want to do with the data and can help with decisions about the data; but this individual must be able to profile new data sources quickly, document issues with the data and work with the stakeholder to resolve data quality issues either at the source, with the 3rd party or handled within the ETL process. Then this individual will build, maintain and monitor the datastore, looking for ways to improve data quality and efficiency, finding new sources and always being mindful of the overarching goal of meeting the business’ needs. |  | Roles &amp; Responsibilities: | Facilitates data collection from a variety of different sources, getting it loaded into the data store, and working to improve data quality and standardization along the way. | Work closely with other technical team members including other data engineers, devOps, DBAs, analysts and data scientists. | Responsible for creating the data model that will facilitate the creation of reports and analytics (often through cubes and flattened data marts). | Translates business requirements into data models and ETLs. Create centralized documents and diagrams of all solutions. | Documents the data catalog into a local catalog or contributes to the enterprise data catalog. | Works with the DBA team to design a backup and disaster recovery plan for the data stores and ETLs | Approaches all relationships with a world-class customer service approach. Maintains a customer-focused approach with users to provide solutions. | Responsible for the integrity and security of data in all forms of storage throughout the Data Architecture. | Works with other IT professionals through Mount Sinai effectively. Comply with HIPAA and other data use policies and procedures. | Assists in the development of standards and procedures affecting data management, design and maintenance. Documents all standards and procedures. | Provides presentations and training to other team members in the above. | Possesses an extremely flexible attitude. Willing to work with multiple types of technologies and languages with an open mind and without technology bias. Continuous interest in updating skill sets and knowledge of trends in the Big Data Technology space. | Other duties as assigned. |  | Requirements: | Bachelor’s degree in Computer Science, data science or a related discipline; Advanced degree preferred. | 5+ years relevant professional development experience. | Strong SQL Database Knowledge of either Oracle or SQL Server. | Proficiency with at least one of these programming languages: R, Python or Julia. Must be flexible and fast to pick up new languages. | Experience with calling web services and web scraping. | Strong Healthcare Supply Chain experience is required. | Knowledge of Tableau, ODI, Pentaho is beneficial | Knowledge of cloud platforms like Azure or AWS and cloud-first databases like Synapse, RedShift, Snowflake, BigQuery is beneficial. | Experience with serverless computing, creating VMs, cloud security, and other cloud services is also beneficial. | Experience working with JIRA is a plus |  | Strength Through Diversity |  | The Mount Sinai Health System believes that diversity and inclusion is a driver for excellence. We share a common devotion to delivering exceptional patient care. Yet we’re as diverse as the city we call home- culturally, ethically, in outlook and lifestyle. When you join us, you become a part of Mount Sinai’s unrivaled record of achievement, education and advancement as we revolutionize healthcare delivery together. | We work hard to recruit and retain the best people, and to create a welcoming, nurturing work environment where you have the opportunity and support to develop professionally. We share the belief that all employees, regardless of job title or expertise, have an impact on quality patient care. | Explore more about this opportunity and how you can help us write a new chapter in our story! | Who We Are | Over 38,000 employees strong, the mission of the Mount Sinai Health System is to provide compassionate patient care with seamless coordination and to advance medicine through unrivaled education, research, and outreach in the many diverse communities we serve. | Formed in September 2013, The Mount Sinai Health System combines the excellence of the Icahn School of Medicine at Mount Sinai with seven premier hospitals, including Mount Sinai Beth Israel, Mount Sinai Brooklyn, The Mount Sinai Hospital, Mount Sinai Queens, Mount Sinai West (formerly Mount Sinai Roosevelt), Mount Sinai St. Luke’s, and New York Eye and Ear Infirmary of Mount Sinai. | The Mount Sinai Health System is an equal opportunity employer. We comply with applicable Federal civil rights laws and does not discriminate, exclude, or treat people differently on the basis of race, color, national origin, age, religion, disability, sex, sexual orientation, gender identity, or gender expression. | EOE Minorities/Women/Disabled/Veterans",New York NY 10029,Data Engineer - Supply Chain
The New York Times,/rc/clk?jk=ba7f94580f096460&fccid=1b50fcfb150b1b48&vjs=3,"The New York Times is seeking a creative, motivated, and experienced front-end or full stack software engineer to join the Experimentation Platform Team. | About the Team | The Experimentation Platform Team is a member of the Data Engineering group at The New York Times. The Experimentation Team’s responsibilities are deeply technical and include building and maintaining web platforms, developing and sustaining reporting APIs, and providing support to teams which leverage our platform infrastructure within their own applications (such as the Front page of the New York Times!). The Experimentation Platform Team’s work helps enable the Times to make smarter choices for our readers and react and adapt to our readers preferences. | Our systems are primarily built with Node.js and React.js and are deployed using Google Cloud Services. We also use Go, Drone, Fastly, Typescript, and AWS. |  About the Job | As a member of the Experimentation Platform team, your day-to-day job would include the following activities: | Build and evolve our experimentation capabilities through the creation and refinement of our experimentation platform user interface | Work closely with partner teams to research, strategize, and propose solutions for creating user friendly interfaces for test configurations, test monitoring, and experimentation analyses | Contribute to designing, implementing, and maintaining team tooling | Supporting platform web applications and APIs through their full product life-cycle | Participate in Agile scrum ceremonies, working closely with our Product and PMO partners | Production support by participating in on-call rotation for the tools we build | Contribute to the Times mission of reaching 10+ million paid subscribers by 2025 | About You | You should have a passion for the Times’ mission and want make an impact on the organization through the use of Data as well as demonstrate the following: | Experience building modern web applications using React | Experience building tools for large, diverse systems with lots of different clients | Experience working with users to improve the look and feel of a website | Experience building visual reporting and analytical tools is a plus | Familiarity with GCP of other cloud provider products and services | Comfortable in a Linux environment | Experience and knowledge of different databases and storage technologies, i.e. relations DBs, columnar storage, etc. | Familiarity with data processing pipelines for creating analytical data sets is a plus | This role may require limited on-call hours. An on-call schedule will be determined when you join, taking into account team size and other variables. On-call hours are unpaid, unless informed otherwise by your manager. | The New York Times is committed to a diverse and inclusive workforce, one that reflects the varied global community we serve. Our journalism and the products we build in the service of that journalism greatly benefit from a range of perspectives, which can only come from diversity of all types, across our ranks, at all levels of the organization. Achieving true diversity and inclusion is the right thing to do. It is also the smart thing for our business. So we strongly encourage women, veterans, people with disabilities, people of color and gender nonconforming candidates to apply. | The New York Times Company is an Equal Opportunity Employer and does not discriminate on the basis of an individual's sex, age, race, color, creed, national origin, alienage, religion, marital status, pregnancy, sexual orientation or affectional preference, gender identity and expression, disability, genetic trait or predisposition, carrier status, citizenship, veteran or military status and other personal characteristics protected by law. All applications will receive consideration for employment without regard to legally protected characteristics. The New York Times Company will consider qualified applicants, including those with criminal histories, in a manner consistent with the requirements of applicable state and local ""Fair Chance"" laws.",New York NY,Senior Software Engineer Data Engineering
Spotify,/rc/clk?jk=db5090b765c26d5d&fccid=fe404d18bb9eef1e&vjs=3,"Engineering | Data | We are looking for a Senior Staff Engineer to join our Data and Insights engineering organization in the Platform mission in our New York office. The Platform mission's team consists of six organisational units (known as ‘tribes’) that reflect the needs and groupings of our internal customers – from the shared approaches to supporting and aligning the craft and practice of designers, engineers, researchers, and data scientists – to the frameworks, capabilities and tools they need to do their work as effectively, quickly, and safely as possible. We are an amplifier for productivity, quality, and innovation across Spotify. | Location | New York, NY | Job type | Permanent | The Data and Insights tribe is 150 talented engineers, product managers, designers, data scientists and user researchers with the mission of enabling machine learning, data science, and data engineering at Spotify. |  | You will grow the team by promoting sound engineering practices, drive work on technical strategy and roadmaps, regularly assess the technical state of teams, advocate for best practices broadly, and give focused support to specific teams as necessary. You will help ensure that our solutions are scalable, sustainable, and architecturally sound. You are expected to mentor other engineers to support them in their career growth. | What you'll do | Be a strong engineering voice alongside other leaders through advising and driving Spotify’s technical vision and strategy. | Represent engineering in settings where deep knowledge of our tech stack is needed (Technical Steering Groups, cross-company work streams, Special Interest Groups). | Support engineering leaders in defining and driving software craftsmanship and sustainable development processes. | Take responsibility for the planning, execution and release of complex technical projects, define architectural vision and provide leadership for teams. | Be a technical leader within the team you work with to initiate, influence and drive technical projects across teams within Spotify. | Review code to ensure it is high quality, efficient, well-tested and documented. | Find ways to spread learning and knowledge sharing across the organization, leveling up our engineering practices and mentoring other engineers. | Who you are | You have 10+ years of experience in engineering, with a strong background in data infrastructure. | You have experience working in a cloud environment, preferably GCP. | You have experience in Java and/or Scala. You have experience working on large scale, distributed infrastructure systems. | You work across organizational boundaries and build effective relationships to tackle highly complex technical problems. | You have experience working in an agile environment characterized by rapid change. | Perks of being in the band | Extensive learning opportunities, through our dedicated team, GreenHouse. | Flexible share incentives letting you choose how you share in our success. | Global parental leave, six months off - fully paid - for all new parents. | All The Feels, our employee assistance program and self-care hub. | Flexible public holidays, swap days off according to your values and beliefs. | Spotify On Tour, join your colleagues on trips to industry festivals and events. | Learn about life at Spotify | You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens. | Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service with a community of more than 345 million users.",New York NY,Senior Staff Engineer - Data Infrastructure
Applied Information Sciences,/company/Checkmate-Partners/jobs/Backend-1a3ed6aa7f94e709?fccid=7d4fd8e89420ac8a&vjs=3,"Job detailsSalary$140,000 - $180,000 a yearJob TypeFull-timeNumber of hires for this role1QualificationsNode.js: 4 years (Required)US work authorization (Required)Bachelor's (Preferred)AWS: 3 years (Preferred)Postgres: 3 years (Preferred)React: 3 years (Preferred)React Native: 1 year (Preferred)Full Job DescriptionAs our Founding Engineers, you'll have the opportunity to: Architect and build our core backend services from ground up, including our data processing pipelines, data warehouse and APIs.Design and scale up our tech infrastructure to keep pace with exponential increases of data.Define, train and deploy new machine learning models.Create scalable services that enable our enterprise customers to analyze and understand complex data and relationships.Our current tech stack: Backend: NodeJS, Postgres, deployed on AWSFrontend: React Native, ReactOur tech stack will evolve and change over time as we build more things - we're looking for folks with experience in equivalent technologies.We're looking for the best talent out there, so we're offering competitive base salaries with generous early-hire equity.*About you:You have a strong sense of ownership and you're comfortable driving the development of an entire part of the product.You have the ability to manage your own time and roadmap effectively.You are comfortable moving across the stack when necessary.You have strong communication skills and are collaborative, especially when teammates are faced with new challenges.You have designed, built and maintained production services.You have deployed and operated ML-based solutions into a production environment serving enterprise customers in the past.You have experience with Spark, Amazon Redshift/Snowflake, AWS EMR (or equivalent tech)You are comfortable researching new technologies and spinning things up from scratch.Why you might be excited about us:Work on a big problem at a critical juncture in time. We're seeing tectonic shifts when it comes to data ownership, accessibility of new forms of data through APIs, and the immediate need for first party user consented data with new privacy regulations and the removal of all 3rd party cookies by 2021.Shape our product and engineering culture from ground up. As our first backend engineer hire, you will fully own a part of the product. You'll also be critical in shaping our company culture, engineering practices and the people we hire.You're joining a team with unique expertise in this space. Prior to starting the company the founders spent the last few years creating the first product to tie online ads to in-store visits, scaling it from 0 to $7MM in annual revenue in two years.We have strong early traction. We launched our app in Q4 and we're seeing best-in-class user retention &amp; engagement metrics that would place us in the top 200 apps worldwide according to Andrew Chen's benchmarks. We're also backed by some of the best in the industry, giving us the ability to really step on the gas in 2021.We're committed to investing in you and your career. We'll work with you to craft a role and path that helps you achieve your long term goals. We want to create an environment where you feel like you can do the best work of your life.*Job Type: Full-timePay: $140,000.00 - $180,000.00 per yearSchedule:8 hour shiftEducation:Bachelor's (Preferred)Experience:Node.js: 4 years (Required)AWS: 3 years (Preferred)Postgres: 3 years (Preferred)React: 3 years (Preferred)React Native: 1 year (Preferred)Work Location:Fully RemoteVisa Sponsorship Potentially Available:No: Not providing sponsorship for this jobBenefit Conditions:Only full-time employees eligibleWork Remotely:YesCOVID-19 Precaution(s):Remote interview process",Remote,Backend / Data Engineer
VMware,/rc/clk?jk=8ccb451832e47d67&fccid=3187d6d05b329ebf&vjs=3,"The Data Intelligence Group (DIG) is a key part of BAM’s continued growth. Year over year, the knowledge needed to leverage data plays an increasingly important role in the firm’s core business. The analysis, services, software, and operational expertise that DIG provides are part of BAM’s competitive advantage. | Role Overview | We are looking for a creative and meticulous developer to join our Webscraping team. The data we provide drives investment decisions across the firm and we work hard to make sure it’s timely and accurate. The optimal candidate will be strongly self-motivated with the ability to work and solve problems independently. In your role, you will: | Collaborate with analysts to understand and anticipate requirements | Design, implement, and maintain webscrapes for a wide variety of alternative datasets | Author tests to validate data availability and integrity | Maintain alerting systems to ensure smooth day-to-day operations | Investigate and defuse time-sensitive data incidents | Minimum Qualifications | Bachelors/Masters degree in Computer Science or a related field | 1-3 years web development experience (Python/SQL/HTML/CSS/HTTP) | Linux experience (Windows experience a plus) | Excellent verbal and written communication skills | Preferred Qualifications | Aptitude for designing infrastructure, data products, and tools for Data Scientists | Familiarity with scraping and common scraping tools (Selenium, scrapy, Fiddler, Postman, xpath) | Experience containerizing workloads with Docker (Kubernetes a plus) | Experience with build automation (Jenkins, TeamCity) | Experience with AWS",Chicago IL 60602,Data Engineer – Webscraping
Warby Parker,/rc/clk?jk=08b76b3518c3d8a0&fccid=85021dfa4578f124&vjs=3,"About Us |  | vidIQ’s mission is to advance the creator's journey with actionable data-driven insights. We pursue this through our values of being creator obsessed, lean and fast, and being scientific. We have already helped millions of creators, and we are looking for stunning co-workers to join us in helping millions more. | So Why Join Us? |  | Our work is exciting as we are transforming the creator analytics space. This has provided many of us the opportunity to work on new and exciting projects. Equally, we’ve set up our people for success by giving them professional development opportunities like courses or conferences that will help them acquire desirable skills/experience. |  | Our company has met the future of work head on, with a fully remote company, capable of giving you flexibility to balance work and life. When it’s time to go on a break, we have an unlimited vacation policy so you can recharge. Lastly, we celebrate our wins and try to enjoy work by going on fun retreats to exciting destinations, such as Spain, Portugal and amazing places to come. |  | We are committed to diversity and inclusion . We work hard to enable creators of all kinds to succeed and, to that end, we prioritize diverse talent and an inclusive environment that encourages collaboration and creativity. We’re committed to building a company and a community where people thrive by being themselves and are inspired to do their best work every day. |  | What you will be doing | Building efficient, critical data pipelines, including ETL, partitioning data, data compaction, and AWS optimization | Collaborate closely with data scientists, product analysts to create the data sets that power vidIQ’s algorithms | Be an advocate for data quality, acquisition of new data sources, and data infrastructure tooling | Work closely with cross-functional teammates, including product managers, designers, product analysts, and data scientists, to deliver the highest impact to our users |  | Who you are | You have experience using Python for internal data pipelines (moving data inside AWS account), including numpy and pandas, additionally you have experience DynamoDB, Lambda, Redshift, and S3 | Hands-on experience with data workflow orchestration | Proven track record of working with cross-functional teams in an agile-like environment | Ability to communicate data concepts, requirements, and risks clearly to cross-functional team members, especially product analysts, data scientists, and product managers | Preferably, you have experience developing ML-based products &amp; services and/or working with Spark",Remote,Data Engineer (Remote)
Balyasny,/rc/clk?jk=85b0793ed971b9a1&fccid=aeb15e43a6800b9d&vjs=3,"This position is for a Data Engineer within the Lockheed Martin Chief Data and Analytics Office (CDAO). This is a great opportunity for someone who is looking to work in a fast-paced Information Technology organization where the latest technologies will be developed and applied. |  | We are looking for someone with strong hands on experience in all layers of data integration and analytics! The ideal candidate has experience and a solid understanding of delivering full-stack data solutions across the entire data processing pipeline and strong skills in data visualization. |  | The work location can be virtual or at any major Lockheed Martin facility. |  | Duties and responsibilities include, but are not limited to: |  | Primary Point-of-Contact for day-to-day Tableau user support on various Tableau administrative tasks, requests, and issues.Develop, maintain, and manage business-critical dashboards utilizing TableauCoordinate and work with Tableau admin teamSupport the process of data discovery by exploring and cultivating data sets for dashboard solutionsSupport tasks such as data validation, performance tuning and facilitate end user testing on designed reports, dashboards and statistical modelsAct as the liaison in supporting ongoing production operations, maintaining and modifying supplied solutionsCollaborate with data governance to proactively seek, locate data, and make data available to users.Develop and automate workflows that clean, transform, and aggregate unorganized data into databases or data sourcesWork through all stages of a data solution lifecycle, e.g., analyze/profile data, create conceptual, logical and physical data model designs, architect and design ETL, testing, reporting and analyticsResponsible for collecting, ingesting, processing, storing, and virtualizing large datasets from a wide variety of data sources and stakeholdersMaintain data systems performance by identifying and resolving production and application development problems; calculating optimum values for parameters; evaluating, integrating, and installing new releasesParticipate in on-call rotations to monitor and resolve production issues during off-hoursProvides ongoing support, monitoring, and maintenance of deployed solutions | Basic Qualifications: | Experience in Tableau user support, troubleshooting, performance optimization and security controls | Knowledge of Tableau server architecture and building integration of Tableau with various data sources like SQL Server, Hadoop, HANA, Oracle, etc.Experience in designing data structures, developing data pipelines and interactive dashboardsStrong knowledge of data virtualization, data engineering, SQL, data modelling and data managementStrong problem solving, teamwork and interpersonal skills; ability to communicate and thrive in a cross-functional matrix environmentDegree in Computer Science, Systems Engineering, or related field, with 5 years of professional experience; or 3 years of professional experience with a related Masters degreeUS Citizenship | Desired Skills: | Experience with interfacing and communicating effectively with business representatives and consulting users on building their own dashboardsExperience in modern enterprise data architectures, design patterns, and data toolsets and the ability to apply them | Data modeling | - Data Visualization tools - Tableau | Extraction, Transformation and Load (ETL) tools (i.e. Data Services, Informatica)Data warehousing solutions (HANA modeling, SDI/SLT HANA replication) | Database systems (SQL and NO SQL) – HANA, Oracle, SQL ServerDistributed data systems (e.g., Hadoop, HBase, Cassandra, Spark) | Languages: Java Script, SQL, Python, XML, Java, Shell, Python | BASIC QUALIFICATIONS: | job.Qualifications |  | Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status. | Join us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They’re dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about. |  | As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories. | EXPERIENCE LEVEL: | Experienced Professional",Littleton CO 80127,Data Engineer Sr
Pinterest,/rc/clk?jk=74169a61d0ad83af&fccid=43014b1412e0a7b6&vjs=3,"About Pinterest: |  | Millions of people across the world come to Pinterest to find new ideas every day. It’s where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you’ll be challenged to take on work that upholds this mission and pushes Pinterest forward. You’ll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. |  | As a technical lead on the core data team you’ll provide intelligence to the rest of the company that will enable making better product decisions. You’ll make use of the latest advances in large scale data processing to uncover insights in data. You’ll work on building critical data warehouse tables with a world-class team of engineers towards the mission of enabling data-driven products and insights at Pinterest. |  | What you’ll do: |  | Work with cross functional stakeholders to design and architect data warehouse and analytics solutions | Build robust data pipelines that collect, process, and compute business metrics from activity data using Spark, Hive, SparkSQL | Create critical datasets for machine learning, growth funnels, business forecasting, and many other strategic initiatives | Work with business analysts and data engineers to build new analysis tools and metrics for measuring product engagement |  | What we’re looking for: |  | Minimum 5 years of industry experience | Degree in computer science or a relevant quantitative field | Proficient in Python/Scala/Java | Extensive experience with one or more of the following frameworks: Spark, SparkSQL, Hadoop. |  | #LI-KL1",San Francisco CA 94103,Software Engineer Data Warehouse
Spotify,/rc/clk?jk=ceff924928c3b578&fccid=01fd1f2f239c15f4&vjs=3,"Job Title: Data Engineer |  | Department: Engineering |  | Who We Are: |  | As our co-founders transitioned from military to civilian life, they encountered barriers and inefficiencies navigating the most basic health and social services. Together, they set out to improve that experience for others. Recognizing that social care is essential for better health outcomes, they embarked on a mission to connect people to the care they need, when they need it, in communities across the country. Unite Us has developed an intuitive and seamless technology solution that goes beyond the traditional referral method. We work side-by-side with local organizations, and track data and outcomes to build coordinated care networks that support all aspects of community health. Unite Us is boldly changing the nation's care delivery system. If you want to do well and do good, join Unite Us– we can build healthier communities together. |  | Description: |  | Unite Us has experienced rapid growth over the past couple of years. As we have grown, our clients' skills and requirements around data have grown more sophisticated. Data has long been a foundation of our system. Now, we need to start taking the data that exists within our platform and transform it into an actionable source of information for our clients in order to assist them in better understanding the impact that Social Determinants of Health (SDoH) are having on those they help. To do this, we need to build an experienced team of data technologists who want to have an impact on healthcare across the entire country. |  |  | The impact of SDoH data is revolutionizing the healthcare industry. As we continue our rapid pace of growth, we want to build a solid data platform that can serve the needs of our clients and shape the way the industry thinks about and relates to social determinants of health. We are looking for a strong technical individual contributor who has experience building data warehouses, lakes and pipelines to help us continue growing our impact on the healthcare industry and the lives of people around the country. |  | What You'll Do: |  | Execute a data architecture and infrastructure to meet business objectives | Work closely with Solutions Architects and Product Managers to make sure that the technical infrastructure can support client requirements | Develop ETL and data pipeline solutions to load data warehouse | Test internal data pipelines for reliability and performance |  | What's Required: |  | Experience as a Data Engineer in which you set up data pipelines | Experience using and building solutions to support various reporting and data user tools (Chartio, Tableau, Looker, etc) | Experience with Spark using Scala and Python. | Experience working with data warehouses, data lakes and ETL pipelines (Snowflake, Infomatica, Redshift, Postgres, SQL, etc) | Experience setting up an maintaining databases within AWS | Experience working on applications serving large enterprise clients | Experience working on healthcare and/or social determinants of health data products | A focus on building performant systems | Ability to think forward and build a scalable solution that satisfies various needs of enterprise clients with dedicated data teams |  | Environmental Job Requirements and Working Conditions: |  | This position is remote (US Based) |  | Unite Us is committed to building a diverse team and fostering an inclusive culture, and is proud to be an equal opportunity employer. We embrace and encourage our employees' differences in race, religion, color, national origin, gender, family status, sexual orientation, gender identity, gender expression, age, veteran status, disability, pregnancy, medical conditions, and other characteristics. If you require assistance in applying for open positions due to a disability please email us at peopleops@uniteus.com to request an accommodation.",New York State,Data Engineer
Apple,/company/sadhguru-technology-solutions/jobs/Data-Engineer-8456f660d70cdbeb?fccid=26f7e504b903466b&vjs=3,"Job detailsSalary$108,880 - $178,831 a yearJob TypeFull-timeNumber of hires for this role1QualificationsExperience:Scala, 5 years (Preferred)Spark, 5 years (Preferred)Azure DataBricks, 5 years (Preferred)Full Job DescriptionData EngineerLocation: Plano, Texas (Remote until COVID)Duration: Long-term5+ Years ExperienceJob Description:Develops Python/PySpark HQL queries.Develops new data models as necessary with Lead Backend Developer and Architect.Performs data visualization and analysis. Produces data samples for UI/UX Designer. Communicates insights to Architect, Lead Backend Developer, Lead Frontend Developer, and client.Bigdata Engineer with Spark, Scala experienceNeed to be an Azure expert with, Databricks experienceExperience with developing, testing, and deploying Databricks jobsTechnologies to be used: Python, Spark, Scala, Hive, Databricks, Azure, Hadoop, Jupyter notebooksJob Type: Full-timeSalary: $108,880.00 - $178,831.00 per yearSchedule:8 hour shiftMonday to FridayExperience:Scala: 5 years (Preferred)Spark: 5 years (Preferred)Azure DataBricks: 5 years (Preferred)Work Remotely:YesCOVID-19 Precaution(s):Remote interview process",Plano TX,Data Engineer
DXC,/company/zilliz/jobs/Data-Engineer-5e2e2074db71701c?fccid=899644d307189ecd&vjs=3,"Job detailsSalary$100,000 - $120,000 a yearJob TypeFull-timeCommissionNumber of hires for this role2 to 4QualificationsLocation:San Francisco, CA (Required)Language:English (Required)Work authorization:United States (Required)Full Job DescriptionData Engineer, User SuccessShanghai / San FranciscoWHO ARE WE? Zilliz data engineers are experts in data science echo system, will promote Zilliz Solutions including AI-data search engine Milvus, cloud platform and AI solutions built around to drive the user and committer growth globally.You have a strong knowledge of the data science and deep learning space, and experience with open source data science products and technologies as TensorFlow, pytorch and Elasticsearch etc.You will establish user relationships ranging from DBA to CIO and will provide technical expertise on engagements and identify new users and target collaborate opportunities within Milvus open source community.Responsibilities: · Promote Zilliz products and solutions to the user community and prospects.· Understand user data and analytics challenges and help translate to business requirements in positioning Zilliz products and services.· Understand and communicate user-specific values of Zilliz products and how they integrate with the users’ ecosystem· Manages relationships with users’ stakeholders as the key advisor by partnering with the customer’s architecture experts.· Create blogs, guides, examples, tutorials, demos, videos, etc, and then promote them via social media, speaking at conferences and meetups, and more to nurture our user community.· Advocate for developers internally and externally to provide input to the product team on roadmap requirements to maximize our user experience and product development.· Explorer new methods to engage more users and promote Zilliz products to drive user growth.Qualifications: · Computer science/software engineering or related majors, bachelor’s degree or above;· With strong hands-on ability, the following experience is preferred:Proficiency in Python, one-year development experience.Familiar with common databases, such as MySQL, Postgres, MongoDB, etc., have actual development experienceUnderstand the data science ecosystem and related deep learning technologies, such as Hadoop, Elasticsearch, Cassandra, Rapids, TensorFlow, pytorch, Sklearn, BigDL, Caffee, Spark, etc.Can use Linux operating system for development and maintenance· If you have a GitHub address and a technical blog address, you can provide it· Have good writing skills, excellent communication, collaboration skills, time management skills and the ability to solve complex problems; · Team awareness, self-driven, proactive work, strong sense of responsibility, curiosity for new technologies.· Experience scaling outreach globally.· Strong presentation skills with the ability to converse with both technical and executive audiences.· Experience with enterprise data solutions in a pre-sales capacity is a plusJob Types: Full-time, CommissionSalary: $100,000.00 - $120,000.00 per yearLocation:San Francisco, CA (Required)Language:English (Required)Work authorization:United States (Required)Work Remotely:Temporarily due to COVID-19",San Francisco CA,Data Engineer
Tesla,/rc/clk?jk=b9f588b0a0fa31eb&fccid=35d653c09c2712b6&vjs=3,"59153BR | Technology &amp; Engineering | Operations &amp; Technology | Responsibilities | As a member of the Data Engineering Team, the Senior, Data Engineer, AdSmart will be directly responsible for the design, management, and development of parts of a software platform and products for NBCUniversal’s AdSmart. NBCUniversal’s AdSmart products will enable NBCUniversal to better understand its brand’s audiences such as NBC News, Bravo, The Tonight Show, Saturday Night Live, and USA Network as well as audiences that cross brands. The goal is to ensure we know who is watching what, where, and when. In turn, enabling NBCUniversal’s sales teams to properly align our audiences with the market advertisements that can benefit them the most. |  | You’re a big thinker who can analyze and evangelize a long-range opportunity, architect a groundbreaking solution, and roll-up your sleeves to get code out the door when needed. You are data-driven and analytical. You understand the concept of a value proposition and evaluation criteria, and you know how to align them with low-level milestones to get the work done. You can apply domain knowledge from one technical subject, in order to quickly ramp and deliver on a new one. You know how to learn from failure until you succeed, and you are able to articulate and quantify the reasons for your decisions. |  | You will be part of the AdSmart Data Engineering team, participating in the data architecture that will drive both current and future data management initiatives within NBCUniversal’s AdSmart group. | Serve as a senior data engineer for AdSmart products. | Participate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and leadership | Develop and code the software components that are core to Audience Studio, under the leadership of the VP/Chief Architecture | Support product with the overall roadmap and ensure updates to senior leadership are 100% technically correct. | Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership |  | Qualifications: | Bachelor’s degree in Computer Science or related field | 3+ years of software development experience, as a developer | Fluency in Scala and/or Java programming languages | Strong OO &amp; FP design patterns, data structure, and algorithm design skills | Extensive experience developing Apache Spark applications | 2+ years of experience with both relational database design (SQL), non-relational (NoSQL) databases, big data, real-time technologies | Familiar with various cloud data sources and architectures such as AWS/S3, HDFS, Kafka | Experience with software containerization, such as Docker | Experience developing and / or consuming web interfaces (REST API) and associated skills (HTTP, web services) | Self-directed, ability to multi-task, sharp analytical abilities, excellent communication skills, capable of working effectively in a dynamic environment |  | Desired Characteristics | Experience as a development manager (with direct authority over development staff) | Experience with Cluster Management and Container Orchestration technologies such as Mesos, Kubernetes, Hadoop/Yarn | Experience with Apache Kafka or similar streaming technologies | Experience with digital advertising technologies. | Able and eager to learn new technologies | Able to easily transition between high-level strategy and day-to-day implementation | Excellent teamwork and collaboration skills | Results-oriented, high energy, self-motivated | Sub-Business | Global Media Operations | Career Level | Experienced | City | New York | State/Province | New York | Country | United States | About Us | At NBCUniversal, we believe in the talent of our people. It’s our passion and commitment to excellence that drives NBCU’s vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It’s what makes us uniquely NBCU. Here you can create the extraordinary. Join us. | Notices | NBCUniversal’s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.",New York NY,Sr. Data Engineer AdSmart
Tesla,/rc/clk?jk=f0c40e4f1f845af6&fccid=c1099851e9794854&vjs=3,"Summary | Posted: Feb 26, 2021 | Weekly Hours: 40 | Role Number:200217551 | Apple’s Finance Business Process Reengineering team is seeking a Data Engineer to join our organization. In this role you will help architect, maintain and continually improve data analytics and automation capabilities. Sitting inside a business unit, you must adapt to varied and shifting needs. You will be working with business users, Apple IS&amp;T developers, and data analysts on your team to deliver complete, accurate, well-secured data that enables reporting and analytics across all Finance functions. Comfort with and ability to learn and perform a wide variety of development and engineering tasks combined with knowledge of Finance business processes sets you apart in this role. | To succeed in this role, several of the following should apply: | Key Qualifications | 5+ years hands-on experience in data architecture, relational database/data warehouse development | Very strong SQL skills; Teradata experience a big plus | Excellent with Python, shell scripting | Experience with various database technologies including Teradata, Snowflake, Oracle, MySQL, PostgreSQL, MongoDb, and other NoSQL databases | Exposure to container technologies like Docker | Exposure to cloud/orchestration architectures such as AWS, Kubernetes | Experience with scripted SQL interaction | Experience with ETL tools and job orchestration. Airflow experience a plus. | Experience with Git | Understanding of tools like Splunk, Nagios, and other monitoring technology | Understanding of data cataloging/lineage tracking concepts and implementation | Experience developing performant, secured data sources for Tableau | Experience preparing data for and using traditional BI tools like Business Objects, Essbase, Crystal, etc., a plus | Able to quickly learn new technologies | Experience with applying data encryption and data security standards | Experience leading complex projects from inception through production support | Excellent oral and written English communication skills | Description | The Finance Business Process Reengineering organization supports every Apple Finance function worldwide. | Our Finance Data team enables the Finance organization by providing quality data accessibility, analytics, reporting and automation services. We are looking to expand capabilities in the areas of data privacy, high-performance computing, advanced analytics and general business intelligence. | Most of the data we used to support our customers resides on Apple’s Teradata date warehouse, so much of your role will involve identifying and combining data in new ways to help answer business questions. Doing this in an efficient, scalable manner will be key to your success. When the necessary data is not available on an enterprise system or is generated offline by business users or third parties, you must develop methods to reliably source, validate, and integrate the data onto an enterprise database platform. | Much of the end analysis and consumption of our team’s work is accomplished in Tableau. You should have a deep understanding of the various approaches of providing data for analysis in Tableau, and which to use under which circumstances. You should also be proficient in Tableau to be able to review users’ content for optimization as well as to build and maintain Tableau workbooks used for internal monitoring, validations, etc. | In addition, you will help architect, develop and maintain our analytics platform infrastructure. Certain environments will emphasize security and data privacy. The machine learning platform will require scalability, integrity, and API interactivity for analysts to train and predict on their models. You must learn and understand a variety of available IS&amp;T solutions, when and how to use them, and what to build ourselves. | You will need a broad background with exposure to many different technologies and functions. This paired with a proven record of excellent problem solving and a sharp, open mind will be more meaningful than deep expertise in any one area. | Education &amp; Experience | Undergraduate degree in Computer Science, MIS, Engineering, Mathematics or other quantitative field required.",Santa Clara Valley CA 95014,Finance Data Engineer
Regions Bank,/rc/clk?jk=c6e27864b5bcd096&fccid=86e9be6ce380173e&vjs=3,"As a Datacenter Engineer you will be responsible for participating the day-to-day operations of the Tesla datacenter engineering team. The team performs all the on-premise datacenter work that supports all production and engineering work that makes Tesla a world leader in self-driving EV, energy storage, and solar power technology. Continuous deployment, monitoring, maintenance, improvement, and rapid turn-around on service requests from all over the organization is imperative to drive a successful production environment in the datacenter. | You’ll be a core member in a closely integrated, cross-functional, and versatile team that performs most racking, stacking, wiring, and implementation designs, implements, and maintains all Tesla datacenter resources. With the ever-growing need for more and more data, compute, storage, and networking locally, and in remote locations – datacenter operations need to follow suit, be scalable through more automated processes for deployment, monitoring, and alerting. You will be responsible for ensuring greatly improved processes in precision deployments of production systems by leveraging the combined resources the team provides. | Responsibilities: | Daily rack, stack, and maintenance of computer, storage, and network equipment | Plan, spec, and pull wires to connect infrastructure equipment as needed while maintaining datacenter standards | Help maintain inventory of components in the datacenter, and to keep the datacenter clean / organized | Leverage and improve upon existing data center deployments to ensure continuous operation | Work with engineering teams to understand useful metrics to collect and implement such monitoring and alerting with existing monitoring solutions at the datacenter level. | Organize and document implemented solutions for long term information retention with our internal ticketing and documentation system. | Work closely with involved parties automated workflows that can be easily implemented by remote hands with little or no understanding of internal systems. | As part of the team, respond to, and document submitted support tickets relating to the functionality of various systems present in the datacenter. | Help develop automated tools to collect information that can be directly used to assist users creating root cause analysis for issues reported. | Requirements: | BS in Computer Science, Electrical Engineering or related field or a Bachelor’s degree with 3 years of additional equivalent experience | 5+ years experience with: | Computer deployment and operations (CPU / GPU) - Rack and Stack | 3+ years experience with: | Linux operating system flavors (CentOS/RHEL, Ubuntu) | Storage systems (On-prem and/or in-cloud) | Excellent time management and communication skills are absolute musts | Ability to step up and take ownership to bring complex tasks to completion | Ability to travel around sites in the bay area | Nice to have: | Experience with multi-site on-prem and in cloud hybrid software and hardware deployments | Previous experience at the large-scale data center and remote systems management",Fremont CA,Data Center Engineer
NBCUniversal,/rc/clk?jk=e9a067b3f00d9535&fccid=fe404d18bb9eef1e&vjs=3,"Engineering | Data | The Platform department builds the technology ecosystem that enables Spotify to learn and deliver quickly, while safely and easily scaling to billion customers, and enabling our rapid employee growth around the globe. Our team consists of six organizational units that reflect the needs and groupings of our internal customers. We support the craft and practice of designers, engineers, researchers, and data scientists by developing the frameworks, capabilities and tools they need to do their work effectively, quickly, and safely. We are an amplifier for efficiency, quality, and innovation across all of Spotify! | Location | New York, NY | Job type | Permanent | We are looking for a versatile Software Engineer, with a strong focus on data, to help grow our infrastructure offering. You would join a team that focuses on supporting how data engineers and data scientists produce and monitor their data ensuring that it adheres to the highest standards of quality. | What you'll do | Craft and build the infrastructure in support of our data quality objectives, including but not restricted to large-scale batch and real-time data pipelines, backend services, libraries, UIs and other tools that enable testing, monitoring, validation, sampling, outlier detection and other techniques that help assess and monitor the quality of data | Use standard methodologies in continuous integration and delivery | Take an active part in the operational responsibilities for our own infrastructure | Work in agile teams to continuously experiment, iterate and deliver on new product objectives. | Who you are | You are experienced with JVM-based data processing frameworks such as Beam, Spark or Flink | Have experience crafting and building maintainable backend services in Java | Are able to work across tech stacks, implementing features end-to-end | Knowledgeable about data modeling, data access, and data storage techniques. | Know and care about sound engineering practices like continuous delivery, defensive programming and automated testing | You care about agile software processes, data-driven development, reliability, and responsible experimentation. | You are comfortable working both independently and collaboratively (pairing and mobbing) | Perks of being in the band | Extensive learning opportunities, through our dedicated team, GreenHouse. | Flexible share incentives letting you choose how you share in our success. | Global parental leave, six months off - fully paid - for all new parents. | All The Feels, our employee assistance program and self-care hub. | Flexible public holidays, swap days off according to your values and beliefs. | Spotify On Tour, join your colleagues on trips to industry festivals and events. | Learn about life at Spotify | You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens. | Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service with a community of more than 345 million users.",New York NY,Data Engineer - Platform Mission
Unite Us,/rc/clk?jk=8577824913c807d2&fccid=35d653c09c2712b6&vjs=3,"59160BR | Technology &amp; Engineering | Operations &amp; Technology | Responsibilities | As a member of the AdSmart Product Engineering Team, the Senior Engineer, Data Science, AdSmart will be directly responsible for data science engineering as part of building out the necessary platform and products for NBCUniversal’s AdSmart product suite. NBCUniversal’s AdSmart products will enable NBCUniversal to better understand its brand’s audiences such as NBC News, Bravo, The Tonight Show, Saturday Night Live, and USA Network as well as audiences that cross brands. The goal is to ensure we know who is watching what, where, and when. In turn, enabling NBCUniversal’s sales teams to properly align our audiences with the market advertisements that can benefit them the most. |  | You’re a big thinker who can analyze and evangelize a long-range opportunity, architect a groundbreaking solution, and roll-up your sleeves to get code out the door when needed. You are data-driven and analytical. You understand the concept of a value proposition and evaluation criteria, and you know how to align them with low-level milestones to get the work done. You can apply domain knowledge from one technical subject, in order to quickly ramp and deliver on a new one. You know how to learn from failure until you succeed, and you are able to articulate and quantify the reasons for your decisions. |  | You will be part of the AdSmart Engineering team, participating in the data science that will drive both current and future data science initiatives within NBCUniversal’s AdSmart group: | Serve as a senior engineer of data science for AdSmart products. | Participate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and leadership team | Develop and code the data science services that is core to AdSmart, under the leadership and direction of the VP/Chief Architecture | Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership |  | Qualifications: | Bachelor's degree in Computer Science or related field | 3+ years of software development experience | Strong knowledge of data structures, algorithms | Ability to apply data science theory to effectively to advertising &amp; audience data | Fluency in at least 1 of the following programming languages (R, Python, or Scala) | Understanding of basic statistical concepts | Experience in forecasting, numerical optimization, natural language processing, or machine learning techniques. | 3+ years of experience with both relational database design (SQL), non-relational (NoSQL), big data, real-time technologies] | Able to easily transition between high-level strategy and day-to-day implementation |  | Desired Characteristics: | Self-directed, ability to multi-task, sharp analytical abilities, excellent communication skills, capable of working effectively in a dynamic environment | Excellent written and verbal communication skills | Excellent teamwork and collaboration skills | Results-oriented, high energy, self-motivated | Sub-Business | Global Media Operations | Career Level | Experienced | City | New York | State/Province | New York | Country | United States | About Us | At NBCUniversal, we believe in the talent of our people. It’s our passion and commitment to excellence that drives NBCU’s vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It’s what makes us uniquely NBCU. Here you can create the extraordinary. Join us. | Notices | NBCUniversal’s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.",New York NY,Senior Engineer Data Science AdSmart
Fidelity Life Association,/rc/clk?jk=c6ec3a1d35884dfc&fccid=feaabf8e6a699d3c&vjs=3,"Job detailsJob TypeFull-timeContractFull Job DescriptionJob Description: |  | DXC Technology (NYSE: DXC) is the world’s leading independent, end-to-end IT services company, helping clients harness the power of innovation to thrive on change. Created by the merger of CSC and the Enterprise Services business of Hewlett Packard Enterprise, DXC Technology serves nearly 6,000 private and public sector clients across 70 countries. The company’s technology independence, global talent, and extensive partner alliance combine to deliver powerful next-generation IT services and solutions. DXC Technology is recognized among the best corporate citizens globally. For more information, visit http://www.dxc.technology/. | In Poland, we are located in Warsaw, Wrocław, Łódź. | Currently, we are looking for | Data Engineer | Locations: Warsaw/remote | Contract type: Full time | Travel: not required | The successful candidate will be part of a growing Analytics Team located in Poland, focusing on various Analytics and Cloud technologies with a strong focus on Microsoft Azure. You will be working with a team of professionals providing projects and services to international clients from across the globe. He/she will participate in end-to-end project implementations delivered in a dynamic and young environment that encourages innovative solutions. We offer an opportunity to develop both in the technology and business areas. This role is an excellent opportunity for people with 1 to 3 years of experience who would like to broaden their skills in Azure and other Data engineering technologies as well as learn all there is about Data Warehousing. We will invest time and resources to deliver customized training to enable your professional growth. | This is a Mid Seniority role - we are looking for candidates who already have some experience in the field of data processing and who want to use a modern technological stack. | We offer: | Full-time employment contract with a compensation package, adjusted to your experience and qualifications. | Package of social benefits, including medical care, Multisport, relax corner, sport communities, technical and soft skill training, life insurance and many others. | Training and certifications provided by our DXC Partners: Microsoft, AWS, Google | Opportunity to be a part of a fast-growing, innovative, and high-performing within the multicultural organization. | Challenging projects, clear career path, and professional training enabling personal development and growth. | Friendly atmosphere, independence, and flexible working hours (e.g. possibility of working from home). | We require: | Knowledge of SQL | Knowledge of Python | Good written and spoken English | Solid problem solving and analytical skills | Strong focus on teamwork | Eagerness to learn | Adaptability to changes | Knowledge of Operating System Unix/Linux | We appreciate: | Knowledge of any of the following solutions: | Azure, AWS, or Google cloud, | GitHub, | JIRA | Azure ADF, Synapse, SQL Server, Databricks, DevOps, Analysis Services | Spark/Scala/Java/R | Why would you join us? | We offer a job in a heavily invested area, cooperating with inspiring people from across the world. A place where you can learn, work with experts, share your knowledge, have occasions to enjoy many activities suitable for our Employees and their lifestyles. | Additionally, you will receive an attractive employee benefits package: | Work in an international company, full-time job, and permanent job contract | Opportunity for personal and professional development and advancement within the company by taking part in interesting and challenging projects | A modern and friendly work environment with an open-door policy | Professional technical and soft skill training (internal DXC University and certification program), opportunity to learn and evolve within a team of experienced colleagues | Private medical care, the social benefits system, life insurance, me-office culture is in our DNA",United States,Data Engineer
NBCUniversal,/rc/clk?jk=5c3e5bc7136810da&fccid=b0a331d5b0460bd4&vjs=3,"As a key member of the Big Data Analytics team, you will be responsible for creating and operating the Big Data environment that enables eFinancial and Fidelity Life Association to optimize business processes and grow rapidly. You will move and integrate data across multiple disparate systems, aggregate and organize large sets of data in order to enable lead operations, analytics, and predictive modeling. If you would love being an owner-operator of a fast-growing complex online business with modern technology stacks, building analytical and infrastructure solutions, and driving success via analytics, this role is for you! | Who we are: | Fidelity Life is a leading provider of financial security for middle market consumers. With a history of innovation, the company is redefining the life insurance industry with patented products and processes. Fidelity Life pioneered the use of predictive analytics to streamline the new business process and revolutionize the speed with which policies can be issued. Established in 1896, Fidelity Life enjoys a long track-record of success and continues to build its reputation of sound fiscal management and customer-focused innovation. | In concert with Fidelity Life, eFinancial is an online and call-center-based insurance agency with a proven direct-to-consumer life insurance model. Using a proprietary and patented sales technology platform, eFinancial operates call centers in Chicago, IL; Seattle, WA; and Tempe, AZ. eFinancial’s licensed agents and representatives reach thousands of consumers each day to help meet their unique life insurance needs – often with just a single phone call. To complement this channel, the company recently expanded to offer an entirely digital purchase experience. | Together, Fidelity Life and eFinancial are revolutionizing the life insurance industry to make life insurance more accessible and affordable for everyday Americans. With an integrated marketing, product manufacturing, and controlled distribution system, the enterprise is uniquely positioned for growth. | What the job looks like: | As a key member of the Big Data Analytics team, you will be responsible for creating and operating the Big Data environment that enables eFinancial and Fidelity Life Association to optimize business processes and grow rapidly. You will move and integrate data across multiple disparate systems, aggregate and organize large sets of data in order to enable lead operations, analytics, and predictive modeling. If you would love being an owner-operator of a fast-growing complex online business with modern technology stacks, building analytical and infrastructure solutions, and driving success via analytics, this role is for you! | What you will contribute: | Own the creation and maintenance of company data structures and databases in AWS to support predictive model development, analytics, and business intelligence. | Implement, test, document, and deploy integrations and maintain production readiness for cloud-based technology stack including real-time streaming, batch feeds, and predictions models from multiple cloud-based and on-premise sources. | Take on specific ETL projects to assimilate data from multiple new structured and unstructured data sources in batch or real-time, as appropriate. | Assimilate and implement business rules to process raw data. Set up visualization tools to meet the needs of both cloud and on-premise user community. | Serve as data steward for Analytics team and third-party technology partners. | What you should bring to the table: | Bachelor’s degree in Computer Science, Mathematics, Management Information Systems, Physics, or related field is required | 1-3 years of professional data engineer experience working with large data sets and predictive services | 1-3 years of professional experience working with SQL and Python with working knowledge PySpark. | Must have hands-on experience with AWS services: Lambda, Athena, Redshift Spectrum, Glue, S3, EKS, EC2, EMR, Kinesis, and RDS. | Knowledge and experience in Big Data and in building Data Pipelines/ETL processes. | Ability to think strategically to anticipate and plan for future business needs. | Passion for taking ownership of new opportunities and projects. | Team Player mentality - you thrive in a collaborative environment. | Excellent multi-tasking skills, you are able to shift gears quickly and comfortably. Strong verbal and written communications skills that enable you to explain data infrastructure complexity with clarity and precision to executive management. | Nice to have skills and experience: | Experience with DevOps and CI/CD preferred. | Reporting Systems experience with PowerBI &amp; Tableau | Front End Engineering—JavaScript (Angular, React, Vue, Node, etc.), HTML, CSS | Efinancial LLC, Fidelity Life Association, its parents subsidiaries and affiliates (collectively “The Company”) are equal employment opportunity employers. We adhere to a policy of making employment decisions without regard to race, color, religion, sex, sexual orientation, national origin, citizenship, gender identity, marital status, age, disability or any status protected by law. We assure you that your opportunity for employment with this Company depends solely on your qualifications.",Chicago IL 60631,Data Engineer I (Remote)
Christmas Tree Shops andThat!,/company/SCHRILL-Technologies-Inc./jobs/Senior-Data-Engineer-9cdb98546007bf71?fccid=1bda51b2f895e763&vjs=3,"Job detailsJob TypeFull-timePart-timeContractNumber of hires for this role2 to 4QualificationsPython: 5 years (Required)AWS: 1 year (Required)Big data lake: 1 year (Required)Spark: 1 year (Required)Gitlab: 1 year (Required)SQL: 1 year (Required)Oracle: 1 year (Required)US work authorization (Required)Bachelor's (Preferred)Full Job DescriptionJob Description:Basic Qualifications Strong in Python scripting, minimum 4+ yrs,Must have hands on experience implementing AWS Big data lake using EMR and SparkStrong exp with Snowflake Database Architecture , SQL , Database performance/Optimization .Experience with Airflow tool and DAG's creation, Jobs Orchestration.Good to have Media AdSales experience .Experience leveraging open source big data processing frameworks, such as Apache Spark.Experience developing and deploying data pipelines within a cloud native infrastructure preferably AWSExperience in using CI/CD pipeline (Gitlab)Experience in Code Quality implementation (Used Pep8/Pylint) tools or any other code quality tool.Experience of Python Plugins /operators like FTP Sensor, Oracle Operator etc.Implement Industry Standards /Best Practices.Excellent analytical and problem-solving skillsExcellent verbal and written communication skillsTop must have skills: 1. Snowflake 2. Linear Ad sales experience 3. Spark, Airflow w/ linear ad sales 4. Gitlab this is remote anywhere - any time zone but must be cognizant of meetings - project will be dealing with linear data- data comes in and they have to process and support stakeholders- data transfer and data analyticsJob Types: Full-time, Part-time, ContractSchedule:8 hour shiftEducation:Bachelor's (Preferred)Experience:Python: 5 years (Required)Snowflake: 2 years (Required)AWS: 1 year (Required)Big data lake: 1 year (Required)Spark: 1 year (Required)Gitlab: 1 year (Required)SQL: 1 year (Required)Oracle: 1 year (Required)Contract Length:More than 1 yearWork Location:Fully Remote",Remote,Senior Data Engineer
Deep Well Services,/rc/clk?jk=428bb401efe23a5b&fccid=0c4cb88efa29d448&vjs=3,"Job detailsSalary$140,000 a yearFull Job DescriptionTITLE: Data Engineer – Python, SQL, AWS Data Products | LOCATION: Portland, San Francisco, Los Angeles (Remote Potential) | PAY: Target pay for this role is $100,000 – 180,000 + Equity but may vary based on experience. |  | OVERVIEW: | ProFocus is searching for a Data Engineer – Python and SQL for a direct hire full time role | This is a Mid to Senior level Data Engineer role with great pay and is responsible for building out new systems to provide top-notch analytics. Please email us your resume so we can discuss. |  | REQUIREMENTS | 2+ years of experience as a Data Engineer working with Python and SQL | Minimum of 1 + year experience working with AWS Data Products | Have strong verbal and written communication skills | Have proven experience deploying data infrastructure | Authorization to work in the US is a precondition of employment. We do not sponsor work visas. |  | PREFERRED | 1+ year of experience developing in Scala/Java | Bachelor’s degree in Computer Science, Data Science, or Information Systems or demonstrate equivalent experience | Experience working in a fast growing/cross functional organization | Provide a strong portfolio with work samples |  | ABOUT OUR CLIENT | Our client is a large and fast growing company with an excellent reputation and culture |  | WHY ProFocus | Candidates come first. ProFocus recently earned the Best in Staffing Award for Talent Satisfaction due to our World-Class service to our amazing candidates. | Quality process. We invest the time to learn about your skills, experience, and career goals in detail so we can find you a position that is a great fit. | Access to hiring managers. We have close relationships with some of the most respected companies in Portland. Due to those relationships, we can provide direct access to managers and positions that may not be available anywhere else. | Excellent benefits. We offer medical, dental, vision, 401k, education reimbursement, sick leave, and employer-paid short-term disability and life insurance. |  | Want to learn more? Call us at (503) 236-2000 or email info@ProFocusTechnology.com. | Please email us your resume to apply: Resume@ProFocusTechnology.com | Visit our Job Seekers page to learn more and review other opportunities. |  | ProFocus is an equal opportunity employer. We value diversity in our workplace and encourage all qualified applicants regardless of race, color, age, sex, religion, national origin, physical or mental disability, pregnancy, marital status, veteran or military status, genetic information, sexual orientation, or any other characteristic protected by federal, state, or local laws.",San Francisco CA 94103,Data Engineer
Epi Source,/rc/clk?jk=36b93c043ac61723&fccid=b77a6a445d12b205&vjs=3,"Job detailsJob TypeFull-timeInternshipFull Job DescriptionEpisource, with over 3500 employees worldwide, is a leading provider of risk adjustment services and solutions for health plans. Our services help Medicare Advantage, Commercial/Exchange, and Medicaid managed care plans improve and support accurate reimbursement in a post-Affordable Care Act market. Our services include retrospective chart reviews, medical record retrieval, HEDIS/ACO quality reporting, and data analytics, allowing payers and providers to dramatically reduce costs and improve quality of care. | EPISOURCE |  | Are you looking for an opportunity to maximize your potential? At Episource, our mission is to provide transparency, efficiency, and elegance to a complex and constantly changing healthcare industry. In short, our aim is to Simplify Healthcare. As a fast-moving, entrepreneurial healthcare technology and services company, we are committed to hiring and developing exceptional talent. Our teams are creating cutting edge solutions to transform the healthcare system so we can impact lives for the better, and our teammates are the fuel that ignite us every day. In order to achieve our goal, we seek talented, passionate professionals looking to build a career with an organization focused equally on agility and innovation. |  | WHAT WE’RE ALL ABOUT |  | At Episource, our journey started in 2006 with medical billing and coding services for physicians, then pivoted to HCC coding for payers. Today, we are an end-to-end enterprise for Risk Adjustment solutions. We’ve grown by learning and listening to our customers. Our entrepreneurial mindset drives innovation, collaboration and integrity. Our goal is to always provide exceptional service and positive experiences. And now, after 14 years, we are a customer-focused Platform Company – with our people delivering insights and interventions using superior technology and workflows. |  | Internship Summary: |  | Episource is looking for a Data Science &amp; Engineering intern with interests in machine learning, data analytics and data pipelining. Interns will work with high-volume healthcare data alongside our Engineering Team to build out production quality machine learning and data engineering solutions; primary duties will be split between these two roles. |  | Desired Qualifications &amp; Experience: | Must be currently enrolled in a full-time, degree-seeking program and in the process of obtaining a Bachelors or Masters degree in Computer Science or a related field | Passion for learning and problem solving | Fluency in Python; exposure to Py/Spark &amp; SQL as well as cloud-hosted infrastructures for data warehousing and computing (i.e. AWS, GCP, Azure) | Experience with common data science libraries, such as scikit-learn, tensorflow/keras, pandas, XGBoost/LightGBM | Familiarity with Machine Learning concepts &amp; strategies, especially for tabular &amp; time-series data | Ability to answer questions from large &amp; complex datasets | Links to GitHub, Kaggle or StackOverflow pages demonstrating the above | Curiosity to explore and iterate to try and reach an optimal solution | Excitement to work with and learn from other engineers |  | What You’ll Do and Learn: | Use state-of-the-art technologies such as Kubernetes, Docker, Py/Spark, Python, and Tensorflow to deploy production Data Engineering and Machine Learning workflows on AWS | Use neural networks to address population health challenges towards the goal of improving health outcomes for those who need it most | Perform data analytics and feature engineering over complex, multi-source time-series data | See projects through, from raw data, to prototypes, to production code | Help build model performance monitoring tools and infrastructure | Pitch in on ETL jobs | May be assigned other duties as priorities dictate | Requires an individual to maintain the ability to work in an environment with PHI / PII data | Must maintain compliance with all company policies and procedures |  | Additional Responsibilities: | Participate in code review, Scrum, and software development best practices | Pair program with our engineers to code and debug software applications | Attend monthly tech talks with the option and opportunity to present your own work |  | Things you’ll Love | An awesome engineering team that is rapidly growing | Culture focused on growing, learning, and working together to solve complex challenges and have fun along the way | Our office is a modern work space where we openly interact and collaborate | Lunch is catered once a week, and our kitchen comes fully stocked with snacks for everyone to enjoy |  |  | Episource, LLC provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws. Episource, LLC complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including, but not limited to, hiring, placement, promotion, termination, layoff, recall, transfer, leave of absence, compensation, and training. |  | Episource, LLC expressly prohibits any form of unlawful employee harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of Episource, LLC employees to perform their expected job duties is absolutely not tolerated.",Remote,Data Science and Engineer Intern
Facebook,/rc/clk?jk=f602a4901caf18eb&fccid=aa78349c929d97f5&vjs=3,"CDPHP and its family of companies are mission-driven organizations that support the health and well-being of our customers and the communities we are proud to serve. | CDPHP was founded in Albany in 1984 as a physician-guided not-for-profit, and currently offers health plans in 29 counties in New York state. The company values integrity, diversity, and innovation, and its corporate culture supports those values wholeheartedly. At CDPHP, the employees have a voice and are encouraged to make an impact at both the company and community levels through engagement and volunteer opportunities. CDPHP invests in employees who share these values and invites you to be a part of that experience. | CDPHP and its family of companies include subsidiaries Acuitas Health LLC, Strategic Solutions Management Consultants (SSMC), Practice Support Solutions (PSS), and ConnectRX Services, LLC | Acuitas Health LLC is looking for experienced, data-oriented professionals. You will step into a multi-faceted position which will include the design, development, implementation, and maintenance of efficient systems used in delivery of health care within a clinical, operational, or support-service domain; also supporting the measurement of the impact of improvement initiatives on clinical, financial, and patient experience outcomes. | QUALIFICATIONS: | Bachelor's degree or equivalent experience | Minimum one (1) year of relevant experience required. | Experience with SQL, database structures, and data modeling required. | Experience writing analytical Python and/or R scripts required. | Experience developing and managing analytic data models required. | Experience working with diverse clinical health care systems including, but not limited to, Electronic Healthcare Record (EHR) systems such as Allscripts, EPIC, or Athena, and health information exchange preferred. | Experience working with medical claim records such as Medicaid/Medicare claims or claims from a commercial insurance provider preferred. | Experience developing Qlik/SHINY-R/Bokeh/Plotly dashboards preferred. | Demonstrated ability to explain complex technical concepts and issues to all levels of internal and external business partners, including those with limited or no technical background. | Demonstrated ability work in a collaborative, team-oriented environment. | Demonstrated ability to pro-actively identify problems, as well as recommend and/or implement effective solutions. | Must be detail-oriented with strong organizational skills. | Demonstrated ability to work with and maintain confidential information. | Excellent verbal and written communication skills. | Flexibility to adapt to a changing and fast-paced environment. | As an Equal Opportunity / Affirmative Action Employer, CDPHP will not discriminate in its employment practices due to an applicant’s race, color, creed, religion, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity or expression, transgender status, age, national origin, marital status, citizenship, disability, criminal record, genetic information, predisposition or carrier status, status with respect to receiving public assistance, domestic violence victim status, protected veterans status, or any other characteristics protected under applicable law. To that end, all qualified applicants will receive consideration for employment without regard to any such protected status.",Troy NY 12180,Data Science Engineer
Luxoft,/company/CSI-Interfusion/jobs/Data-Engineer-d0579f888e9f3339?fccid=f7fcfb775a279c04&vjs=3,"Job detailsJob TypeFull-timeContractNumber of hires for this role5 to 10QualificationsUS work authorization (Required)Bachelor's (Preferred)SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Full Job DescriptionAs a BI data engineer specializing in scalable ETL, you are an expert in designing, developing, and maintaining secure, scalable data warehousing, data pipelines, and back-end services for driving impactful decisions unlocking data insights, and providing timely data solutions for the studio.Responsibilities· Work with Project Management, data scientists, and business stakeholders to understand requirements and translate to technical requirements.· Design, architect, and support high-quality data frameworks that will provide actionable information to various teams across the studio.· Experiment with and recommend new technologies that improve the team’s ability to innovate.· Collaborate and communicate with engineers, data scientists, and analysts to optimize data flows, tools, operational costs, and reporting infrastructures.· Use best engineering practices to ensure security and privacy compliance across all development projects.· Build data marts for different use cases across the business, using Microsoft and other big data technologies including Azure Synapse.· Facilitate ingestion of raw telemetry, and perform cooking, joining, and aggregation to facilitate consumption downstream.Qualifications· Bachelor’s degree in computer science or engineering, database systems, mathematics, or 5+ years of industry experience in a data engineering role.· Advanced hands-on experience with Azure Cloud Services (Data Factory, Data Explorer, HDInsight. Cosmos DB, SQL) or equivalent· Experience with Data Lake infrastructures (Cosmos, Hadoop)· Experience with data warehouse technical architectures, ETL/ELT, and reporting/analytic tools· Experience optimizing code for hardened, efficient deployments· Experience designing and building data warehouse solutions· Experience building and maintaining data pipelines· Experience with production BI implementations in the Cloud· Experience with Machine Learning Model deployment· Experience building Power BI, Excel, and Reporting Services dashboards and reports· Exceptional problem-solving, technical, and data analysis skills· Great written and verbal communication and presentation skills· Be self-driven, and show ability to deliver on ambiguous projects with incomplete or dirty data· Ability to work in a team environment that promotes collaborationJob Types: Full-time, ContractBenefits:401(k)401(k) matchingHealth insurancePaid time offVision insuranceSchedule:Monday to FridayEducation:Bachelor's (Preferred)Experience:SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Work Remotely:Temporarily due to COVID-19",Bellevue WA 98005,Data Engineer
Southern Star Central Gas Pipeline,/rc/clk?jk=ff33d5ac613598de&fccid=4a5690b6309f8602&vjs=3,"The Sr. Data Engineer will play a key role in modernizing the data &amp; analytics capabilities of Christmas Tree Shops, working to replace a disparate collection of data mart and data warehouses with a modern data platform. During the transition away from Bed, Bath and Beyond, this position will be expected to take a lead role in the effort to ingest all CTS data into a modern, cloud-based, virtual data warehouse technology that can be used as the source for existing, migrated, user reports. Post-transition, this position will be relied upon to help execute a long-term data strategy to ensure the CTS business has curated data on which to perform analysis to drive business value. This is a remote position. | SKILLS &amp; RESPONSIBILITIES | Data Ingestion – design and build reusable pipelines that can ingest data from source systems, into cloud-base data storage technologies. Ensure pipelines are equipped with logging and error handling mechanism, such that support the team’s cans easily triage and resolve issues. | Data Curation – ensure data ingestion from the source is curated into a useable form for various business use cases. This includes data enrichment, data cleansing and data transformation. When data assets live in multiple systems, work to harmonize disparate values cross systems to present a single version of the truth. | BI &amp; Reporting – work to build canned reports to turn data into information, focusing on tactical needs such as daily flash reports, top/bottom performers and exception reporting. Advocate and educate the business on the benefits of consuming information visually, and work to build dashboards to allow for key KPIs to tracked by leadership. | Business Alignment - be an active participant in the business and a student of the business. Pursue a deep understanding of business functions. While providing the best possible technical solutions and services, act as an advocate for your business partners and the business at large. | New and Leading Edge Technologies - Maintain a curiosity, an awareness and knowledge of new technologies, their possible advantages and applicable relevance to our business. | Production Support - Maintain 24x7 availability to support production services. | Policies and Procedures - Support and adhere to departmental policies and procedures, including, but not limited to Project Management, Change Management, and Issue Resolution. | Security &amp; Compliance - support the efforts of the Security and Compliance associate(s) in the IT department to ensure that all necessary steps are taken to achieve the security and compliance goals of the department and the company. | Vendor Relationship Management - Maintain good working relationships with our third-party vendors, while also holding them accountable for quality service and results. | Customer Relationship Management - Maintain good working relationships with business associates at all levels of the organization. Be honest, open, respectful, and helpful. Be empathetic toward business associates, be a good listener and provide pragmatic solutions to their challenges. | Performs other related duties as assigned. | OTHER CHARACTERISTICS | Results oriented with a high degree of resilience and perseverance. Ability to navigate in the grey and lead others through times of ambiguity. | Weighs the impact on the customer in planning and decision making and acts with urgency to resolve issues impacting service or sales. | A player/coach with the ability and is willing to roll up his or her sleeves when required. | Skilled in change management and can work across all levels of the organization. | Exhibits optimism and enthusiasm. People feel positively challenged when working with this person. Is responsive and enjoys helping other achieve their goals. | Strong Ethics and Discretion: Must morbe a steward of ethics and discretion when it comes to handling sensitive information. | QUALIFICATIONS AND REQUIREMENTS | Bachelor’s degree required in Information Technology, Computer Science, Mathematics or related field. | Minimum of 7 years’ experience as a technologist, with at least 5 of those years in data intensive positions such as database developer, data engineer, ETL developer, data warehouse developer, BI developer, etc. | Strong experience with ETL/ELT development, and strong knowledge of data architecture principles, including data modelling and high-level architecture of data warehouse and data platform systems. | Exposure to working with cloud-based technologies such as iPaaS solutions, data storage/compute solutions, etc. | PHYSICAL REQUIREMENTS | Able to travel to stores and other locations as needed. | Prolonged periods of sitting at a desk and working on a computer. | Must be able to lift up to 15 pounds at times. | ABOUT CHRISTMAS TREE SHOPS | Christmas Tree Shops is an off-price brick and mortar home goods retailer with a specialty in seasonal products. The Christmas Tree Shops experience revolves around a trend-right, always-changing mix of merchandise that makes each customer visit a shopping adventure of anticipation and delight. The 50-year-old company has roots in the Northeast with the first store located on Cape Cod, Massachusetts, and today operates 80 stores in 20 states under the banners of Christmas Tree Shops, Christmas Tree Shops andThat! or andThat! |  | An Equal Opportunity Employer | It is the policy of Bed Bath &amp; Beyond Inc. to recruit, hire, train, promote, transfer and compensate our associates and provide all other conditions of employment including Company sponsored events without regard to race, color, creed, religion, national origin, age, sex, gender identity, genetic information, marital status, lawful alien status, sexual orientation, physical or mental disability, citizenship status, veteran status, employment status or any other basis prohibited by applicable law. | Location: MA, Middleboro, 64 Leona Dr",Middleboro MA 02346,Sr. Data Engineer
StrongArm Technologies,/rc/clk?jk=cf6562831eaad7a8&fccid=0ec0c1548a2fca68&vjs=3,"POSITION DESCRIPTION |  | As a Data Engineer, you will be responsible for developing next generation data pipelines for IoT wearable devices. We are seeking an engineer with experience in the domain, proficient at SDLC and engineering cycles. You have deep experience with the big data software stack(s). You are up to the challenge of evolving StrongArm's big data systems, scaling these to hundreds of thousands of industrial athletes, and leveraging advanced techniques like ML edge computing, geolocation analysis, and computer vision. |  | You must be able to uphold quality engineering principles and enjoy holding responsibilities solving advanced problems. |  | We are highly collaborative. Our open office environment is people-focused - we joke, have fun, and enjoy each other's company when we can, but aren't afraid to roll up our sleeves and get things done. We are looking for driven and open-minded engineers with exceptional analytical abilities who can wear a number of hats from data engineering, dev-ops, software design, software development, system administration and system architecture. |  |  | PRIMARY RESPONSIBILITIES |  | Individual contributor using Python with a system trajectory towards Scala | Provide technical leadership in Data Engineering design principles | Design and implement highly available / scalable data pipelines using Apache Spark for ETL jobs that process data from 100s of thousands of sensors. | Build big data interfaces compatible across IoT systems using SWIG | Partner with the Data Science team to implement advanced statistical models and machine learning that run on edge devices | Partner with the Embedded Engineering team to build interfaces between IoT devices and data pipelines for ingesting data | Own data modeling implementation for huge scale | Optimize data pipelines for performance and scalability | Establish automated mechanisms to improve data integrity across all big data sets. | Leverage strategic and analytical skills to understand and solve customer and business centric questions | Monitor and troubleshoot performance issues for production pipelines | Learn about new technologies and add to StrongArm's Big Data tech stack | Data Warehouse management in Databricks using Delta Lake |  | TECHNICAL QUALIFICATIONS |  | 2+ years experience in data engineering | Experience building systems using Apache Spark that have processed terabytes of data in production | Experience productionizing data science models and algorithms to run at scale | Experience with distributed data streaming frameworks like Spark Structured Streaming, Apache Flink, Kinesis, etc | Advanced Experience with RDBMS and SQL | Experience with automated testing for distributed systems in Spark (unit testing, end to end testing, QA, CI/CD) | Experience designing end to end pipeline architectures | Experience managing data warehouses in a production environment (Delta Lake, Snowflake, Redshift, Bigquery, Presto) | Scala and Python proficiency | Experience leveraging cloud systems to build data pipelines (BigQuery, Redshift, AWS Kinesis, AWS S3, GCS) | Linux proficiency, this is the means by which things are engineered well. |  |  | BONUS SKILLS |  | Experience extending Apache Spark (DataSource API, Catalyst Optimizer) | Experience with productionizing machine learning at scale and A/B testing new models: scikit-learn, tensorflow, pytorch | Experience building systems to train machine learning models at terabyte scale. | Experience working with Hadoop | Experience using workflow management systems like Airflow or equivalent | Experience working with datasets that measure physical phenomena. | noSQL solutions: Cassandra, HDFS and/or Elasticsearch | Experience as an open source contributor | Experience with BI tools (Looker, Redash) | Experience building systems with data governance | Strong Mathematics background (Linear Algebra, Statistics, Physics, Complex Variables, Calculus) |  | StrongArm Technologies is an equal opportunity employer",New York State,Engineer Data
CDPHP,/rc/clk?jk=4393f54e434b4083&fccid=25b5166547bbf543&vjs=3,"Xandr’s ad platform has one of the world’s largest collections of digital, film and tv properties. We developed technology that powers the real-time sale and purchase of Digital Advertising. Our platform is engineered to provide one of the fastest, most reliable, and massively scaled advertising systems in the industry. | About the team | The Deal Metrics Team is responsible for developing and maintaining the mission critical reporting and analytics backbone of the Xandr Monetize and Xandr Invest Platforms. As a Software Engineer II, you will be responsible for creating and maintaining the high-throughput data pipelines that power the reporting and analytics data used by customers to make well-informed decisions on their spend on our platform(s). These pipelines are powered by data streams from our Real-Time Auction Platform at incredible scale and transformed into actionable reports. The reports are used to troubleshoot issues, optimize, analyze performance, and help customers maximize monetization and spend. |  | About the role | This role offers the opportunity to own and build the streaming data pipelines that give our customers insights into the performance of their advertising business. | As a Software Engineer, you will: | Work with high-throughput data streams (Gigabytes/Sec) from by our real-time auction platform | Develop data pipelines that refine and extract data into actionable reports used by customers | Implement strategies that ensure the quality, performance, and accuracy of reports | Ensure high availability of data pipelines and reports as platform requirements scale |  | 2+ years of experience as a Software Developer | Experience building software in Java or other JVM-based languages | 2+ years demonstrated work experience developing data pipelines and/or working with streaming technologies such as RabbitMQ or Kafka | 2+ years of experience working with analytical data sets +/- 10 TB, OLAP paradigm, and OLAP databases such as Vertica or Snowflake | Strong experience with and knowledge of object-oriented coding, primarily Java or other low-level and/or back-end programming languages | Experience working within Kubernetes Container Environment | Strong SQL skills with the ability write SQL Aggregation Queries at a high-level | Strong experience writing well-tested code, deploying code safely, and working in a team with coding standards, including unit testing, functional testing of applications, working with build systems such as Jenkins, Code Review, Design Review, etc. | Nice to Have Skills | Experience with Samza | Familiarity with Protobuf | Experience within Ad-Tech space",New York NY,Software Engineer II – Streaming Data
Southern Star Central Gas Pipeline,/company/Southern-Star-Central-Gas-Pipeline/jobs/Data-Engineer-f3c2970e41c80180?fccid=f9335f5aaf400499&vjs=3,"Job detailsSalary$77,000 - $92,000 a yearJob TypeFull-timeFull Job DescriptionOverviewSouthern Star Central Gas Pipeline is a leading transporter of natural gas to America’s heartland, providing quality service since 1904. At Southern Star, our people are our greatest asset with growth and personal development at the forefront of our focus. We’re committed to building a culture for everyone to do meaningful work and be recognized for their efforts.Southern Star is a family-friendly, forward-thinking, customer-oriented company striving for innovation and collaboration in all parts of our business. You can find your second family here in an atmosphere that values trust, learning, teamwork, and having fun while getting the job done. We’re an established company known as an employer of choice in our industry and recognized for safety, credibility and integrity. We need one more talented individual with goals to encourage excellence and create positive outcomes – you may be the perfect match.In return for this level of talent and experience, Southern Star offers excellent medical benefits, competitive salary, 401k match, paid time off and reimbursements for tuition and fitness.The position will partner with departments throughout the organization to lead process improvement. Employees will use a variety of tools to design, analyze, and develop targeted solutions addressing specific business needs; and be able to analyze complex business problems and issues using data from internal and external sources to provide insight to decision-makers. The position will require the ability to use operational, process, financial and customer service data to optimize performance and customer service. Must have the ability to develop and maintain a support structure for Data Analytics (DA) tools; and deliver business insights through enhanced visualization.ResponsibilitiesPrimary responsibilities include (but not limited to): Ability to communicate, network, and manage stakeholders effectivelyParticipates in the BI project planning processAbility to self-manage timelines and deliverablesUnderstands how data is turned into information and knowledge; and how that knowledge supports and enables key business processesWorks closely with IT teams to turn data into critical information and knowledge for sound business decisionsStays abreast of Data Analytics products/tools and of business strategies and directionMaintains the support structure for self-service BI toolsMaintains and supports the PI systemWorks with internal and external customers and IT partners to gather and validate requirementsGains knowledge of emerging data science technologiesPrepare structured and unstructured data that can be used for predictive and prescriptive modelingApply the scientific process to data evaluation, performing statistical inference, and data miningDevelop data set processes for data modeling, mining, and productionImplement a platform for efficient data processing and machine learningMonitor model performancePartner with business units to gain business process knowledgeOther requirements: Valid driver’s license and insurabilityReliable attendance, scheduling flexibility when job demands requireAbility to work positively and effectively prioritize in a high-stress, demanding environmentQualificationsMinimumBachelor’s Degree in Business, Computer Science, Computer Information Systems, Data Science/ Data Analytics; if no bachelor’s degree 3+ years equivalent relevant experienceFundamental knowledge of business processes and proceduresBasic knowledge in the use of modern IT tools and methodologies for the purpose of assisting in the design and development of applicationsBasic experience with relational databasesFundamental knowledge of structured and unstructured data and statistical toolsInterest in learning data science methodologies or Advance AnalyticsAbility to work in a fast-paced changing environmentAbility to communicate at all levelsAnalytical skills with creative, problem solving skillsDemonstrated ability to work independently with limited or no supervisionPreferredBachelor’s DegreeMajor in computer science or business-related fieldPractical business experienceExperience with OSIsoft PI SystemHow to Apply: Please go to the following link to apply: https://careerssscgp.icims.com/Job Type: Full-timePay: $77,000.00 - $92,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceDisability insuranceEmployee assistance programFlexible scheduleFlexible spending accountHealth insuranceHealth savings accountLife insurancePaid time offParental leaveProfessional development assistanceRelocation assistanceTuition reimbursementVision insuranceSchedule:Monday to FridaySupplemental Pay:Bonus payCOVID-19 considerations:Protective masks, six feet distance, santization at work stations, virutal meetings through Microsoft Teams, etc.Experience:relevant work experience if not Bachelor's Degree: 3 years (Preferred)This Job Is:A job for which military experienced candidates are encouraged to applyA good fit for applicants with gaps in their resume, or who have been out of the workforce for the past 6 months or moreA job for which all ages, including older job seekers, are encouraged to applyA job for which people with disabilities are encouraged to applyCompany's website:https://careers-sscgp.icims.com/Work Remotely:YesCOVID-19 Precaution(s):Remote interview processPlastic shield at work stationsSocial distancing guidelines in placeVirtual meetingsSanitizing, disinfecting, or cleaning procedures in place",Owensboro KY 42301,Data Engineer
AT&T,/rc/clk?jk=de2d9a0facb2f990&fccid=aeb15e43a6800b9d&vjs=3,"The Lockheed Martin Artificial Intelligence Center (LAIC) team is seeking a high energy team member with a strong working knowledge in Data Engineering and Infrastructure Design to support the Data Ops portfolio for the business. In this role you will be part of the AI Factory team consisting of colleagues possessing varying skill sets including; front-end and back-end developers, data scientists, machine learning / AI specialists, product managers, and other data engineers. The selected candidate will focus on Data Ops but will be included as part of a broader range of projects across the LAIC portfolio. The team is focused on exploiting small, innovative and agile teams to rapidly iterate and mature solutions from prototypes to deployment for utilization across the Lockheed Martin enterprise. |  |  | The selected candidate will focus on… | Creating automation to ensure that data quality rules are enforcedCreation of data models within and management of a schema registryWork with product owners and customers to understand the relevant test casesDevelop and utilize tools to continuously monitor data qualityDevelop and implement automated regression test plans | Basic Qualifications: | Bachelor’s Degree in Engineering, Computer Science, or other related discipline2+ Years of experience working with SQL and NoSQL technologiesExperience with DevOps tools: Docker, Git [GitLab, GitHub], Continuous Integration [CI], Continuous Deployment [CD]Data source analysis and profiling experienceKnowledgeable in multiple programming languages (Python, Bash, etc.) | MUST BE U.S. CITIZEN | Desired Skills: | Ability to focus on needed deliverables, working within a DevSecOps teamAbility to translate customer needs into Data Quality solutionsExperience working with Big Data Technologies such as Spark, Kafka, &amp; PrestoHas a strong teaming mentality, can work collaboratively to develop optimum solutionsKnowledge of various data processing architectural design patternsPast experience working in a multi-functional team in an applied machine learning context involving streaming and batch processing. | BASIC QUALIFICATIONS: | job.Qualifications |  | Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status. | Join us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They’re dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about. |  | As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories. | EXPERIENCE LEVEL: | Experienced Professional",Orlando FL 32825,Data Engineer
Verizon,/rc/clk?jk=96e2452a08511eda&fccid=1639254ea84748b5&vjs=3,"Facebook's Edge &amp; Network Service (ENS) team is actively looking for passionate people to help us tackle the unique set of challenges and opportunities we face as we scale our global network infrastructure. Our team is execution focused, responsible for the deployment and support of all production networks (Edge, Backbone, Optical, and Datacenter) as well as our content delivery network (CDN) across the globe. We have entered into a new era of unprecedented opportunities to re-think traditional networking and iterate quickly in an area that was previously very closed to innovation. This truly global team offers a unique career opportunity to work with all of the latest network technologies and talented engineers solving some of the most complex problems in the industry. This is a full-time, 12 week internship position. |  | Innovate to Scale: Applicants should have the ability to identify repetitive and time-consuming tasks that can be automated to save time and resources. They should have a systematic approach to their work, always thinking of how tooling and automation or reengineering of business processes can be leveraged to streamline and standardize network deployment and support activities while scaling with the needs of business. |  | Moving Fast: Applicants need to be a master of prioritization and quickly ramp to the speed at which we do work at Facebook. We expect this person to be able to decipher signal vs. noise, quickly gain context, and begin prioritizing impactful work. This person needs to be able to operate autonomously with minimal supervision and thrive in our fast-paced environment. | Build and optimize new process and automation to improve execution efficiency | Support and lead network deployment projects expanding Facebook's production edge and backbone network infrastructure | Learn and understand how the network is built in order to design and troubleshoot, including building Engineering Design Packages and Methods of Procedures for new builds and migrations | Maintain databases and related documentation for circuits and network infrastructure | Work cross-functionally to plan and execute on network projects and manage operational incidents | Manage and update projects through our internal tools and repair ticketing systems | Must obtain work authorization in the country of employment at the time of hire, and maintain ongoing work authorization during employment | Currently has, or is in the process of obtaining, a BS or MS in Computer Science, Engineering, Telecommunications, or a related technical discipline | Knowledge of configuring and troubleshooting IP routing/switching technologies and protocols in a dual-stack environment | UNIX/Linux and TCP/IP network fundamentals | Ability to communicate cross-functionally and solve problems under pressure | Experience coding in at least one language, preferably for network or systems automation | Intent to return to degree-program after the completion of the internship/co-op | Knowledge of physical sites (Data Center, Colo, POPs) including power, space, and fiber. | Knowledge of Optical networking and DWDM technologies | Experience with IPv6 | Familiarity with programming/scripting languages (perl, python, bash, etc.) | Intent to return to degree-program after the completion of the internship/co-op | Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started. | #LI-REMOTE",Remote,Network Deployment Engineer Intern Infrastructure Data Centers
LOCKHEED MARTIN CORPORATION,/rc/clk?jk=d0d73495a3114be8&fccid=4b841d912d46e4fd&vjs=3,"Job detailsJob TypeContractFull Job DescriptionProject Description: | Luxoft has contract with a major Telecommunications provider looking to build a team of strong technical consultants to drive cost savings through data exploration. This agile team will work closely with clients to identify data sources, bring them together and normalize the data structure to be ingested for analytics. The work will be centered around telco network equipment to create a repeatable framework of data exploration. This high visibility project will drive business value realization using big data techniques and agile methodologies. |  | This project will implement a 3 step process including the following: | o First to identify the contacts, source of information and availability of data. | o Second, using the requirements, develop a pipeline for the data into a normalized structure called the BRD (Business Ready Dataset). | o Third will be to automate the process of data acquisition and BRD creation into a production environment with checks and balances (monitoring, etc.). |  | These steps will be implemented by various roles/team members and repeated for multiple equipment types using an Agile team method. |  | Responsibilities: | o Woking in a cloud environment (Azure) | o Development of a self-service platform to be used by our customers driving cost savings initiatives. | o Contribute to an Agile team of developers focused on data ingestion across multiple sources. | o Operate in a CI/CD environment. | o Implement data extraction from various/distributed sources utilizing scripting and storage in a SQL/HSQL environment. | o Strong analytical, planning, and organizational skills with an ability to manage competing demands | o Strong experience in data exploration with large data sets |  | Skills Required: | ",Plano TX,Cloud Data Engineer
M3 Global Research,/rc/clk?jk=c15da7d675af55a3&fccid=2fbb65a7684e7449&vjs=3,"**This candidate must have authorization to work in the U.S full time. This position does NOT offer sponsorship now or in the near future at this time.” |  | M3 Global Research, part of M3 Inc., provides the most comprehensive and highest quality market research recruitment and support services available to the industry with relationships reaching respondents in more than 70 countries worldwide. | M3 Global Research maintains ISO 26362 and 27001 certifications with the highest quality data collection and project management capabilities that cover the spectrum of quantitative and qualitative techniques utilized today. M3 services incorporate all of the most advanced statistical and attitudinal methodologies allowing clients to provide world-class offerings and support services to their end-client customers throughout multiple industry sectors. Due to our continued growth and expansion, M3 is seeking an experienced DBA/Data Engineer to join our Global Research division. | The DBA/ Data Engineer should have experience in cloud hosted environment managing backup recovery, replication, high availability, designing and managing schema, monitoring diagnosing and optimizing database performance. Using your experience, you will develop and maintain technical standards, procedures, and documentation. You will also maintain highly available system supporting PostgreSQL database in multi-region and container environments. This person will work with business owners, engineers and other groups to design strong reliable solutions. |  | Duties Include: | Work alongside our cloud infrastructure team to plan, administer, maintain and secure PostgreSQL databases in an AWS environment. | Define standard practices and procedures for operational support, writing and updating documentation | Ensure systems are running at the highest possible performance level as well as analyzing and tuning queries. | Analyze database changes requested by the Agile development teams for impacts to database designs, naming standards, design standards, data conversions, and documentation. | Desire to learn about the data and it’s use to make recommendations for improvements in how it is stored and processed | Build and maintain highly available systems to ensure maximum up time. This includes Postgres databases in containers | Work in unity with our Business Intelligence (BI) team to design, build and maintain Data Warehouse for BI and analytics | Manage system security and protect PII data. | Requirements | A Bachelor’s Degree and 5+ years of PostgreSQL DBA | Hands-on experience and in-depth knowledge of AWS database offerings (e.g. RDS, Aurora, and Dynamo) | Technical expertise regarding data models, database design and development | Demonstrated experience in data modeling tools. | Experience with Data Warehousing and utilizing ETL tools such as AWS Glue | Experience working with PostgreSQL database in containers | Knowledge of Oracle a plus | Benefits | A career opportunity with M3 USA offers competitive wages, and benefits such as: | Health and Dental | Life, Accident and Disability Insurance | Prescription Plan | Flexible Spending Account | 401k Plan and Match | Paid Holidays and Vacation | Sick Days and Personal Day |  | M3 reserves the right to change this job description to meet the business needs of the organization |  | ",Fort Washington PA 19034,DBA/Data Engineer
CDW,/rc/clk?jk=52e3d5ee6381eac5&fccid=dfc44f3b8c44a6db&vjs=3,"Who We Are: | We are part of the Cisco’s Teamwork Technology Group (CTG), the global leader in Collaboration Technologies, which provides outstanding coordinated solutions that redefine the partnership experience. | Our Customer Care Business Unit is an industry leader in the market in developing and delivering intelligent customer and employee experiences at cloud speed and scale. Who You’ll Work With: | Product Managers that will help you know our customers and partners so you can deliver solutions that meet their needs and wants. | Engineering Managers who will enable you to focus on doing what you love: delivering extraordinary software; they will also be your mentor and guide on this exciting stage of your career journey. | Technical Architects shall help you craft, develop, and operate with the standard methodologies, patterns the industry has to offer. | Data Scientists who will rely on you and your data wrangling and serving expertise to find artificially intelligent ways to enrich the value we deliver to customers. As a Data Engineer, you will: | Design, develop and maintain the underlying data infrastructure for the entire Customer Journey Solutions platform. | Build a scalable, reliable, and high-throughput data pipeline capable of ingesting hundreds of gigabytes of data per hour in multiple formats from multiple sources. | Build a leading-edge data processing engine for batch and stream-based computations and aggregations for reporting and sophisticated analytical use cases. | Design and develop APIs for efficient and scalable data retrieval and querying. | Own what you build – providing end-to-end automation, quality assurance, deployment and monitoring of your data services. | Envision, design, and build solutions to enable advanced analytical and machine learning (ML) applications. Skills Required: | 6+ years of Strong Programming expertise in Java | Experience in Elastic Search, Kafka, MongoDB, Redis | Expertise with high-throughput messaging technologies and patterns especially Apache Kafka or other large-scale data processing frameworks and runtimes. | Experienced in Cloud development and deployment | Experience working with databases of all types like relational, analytical, columnar | Experience building, testing, and consuming REST APIs; GraphQL Nice to Have: | Experience with data analysis toolkits such as PyData pandas and analytical storage frameworks like Druid/Pivot. | Experience developing, testing, and deploying to public cloud infrastructures such as AWS, GCP, or Azure. | Experience with machine learning algorithms and techniques; real-world ML application delivery | Experience with Git source control and building/maintaining Continuous Integration and Delivery pipelines; Jenkins | Experience working in an Agile/Scrum-inspired delivery methodology. | Cisco is an Affirmative Action and Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis. | Cisco will consider for employment, on a case by case basis, qualified applicants with arrest and conviction records. |  | We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",Boxborough MA,Data Platform Engineer
Allstate,/rc/clk?jk=76083ca25a63c9e0&fccid=1603a30593aeda56&vjs=3,"Company Description | ELLKAY is a nationwide leader in healthcare connectivity, providing innovative, customizable solutions and unparalleled services for over a decade. We empower diagnostic laboratories, PM/EMR vendors, ACO and HIE companies, hospitals, and other healthcare organizations with cutting-edge technologies and solutions that improve their bottom lines. | Our 'Client-first' focus has made ELLKAY one of the most respected healthcare IT companies in the nation. We value our clients and believe that strong relationships are the foundation for a strong company, and we're dedicated to providing connectivity to the healthcare industry. | We deal with medical data and we take our work very seriously, but not ourselves. If you’re a smart, hard-working, dedicated individual who thrives in a laidback, friendly work environment, ELLKAY may be the place for you. We’re committed to attracting good people who are passionate about the work they do. | ELLKAY was founded over a decade ago on the values of innovation, efficiency, and service created in a collaborative work culture. As we have grown, we are proud to still possess the same energy and passion for what we do. We strive to provide exceptional customer experiences to our clients, which begins with first employing amazing people. ELLKAY is proud to maintain a high-quality, innovative, and diverse workforce. |  | Job Description | Extract data from Client’s machines and restoring databases.Identify data elements from various database as requested by client.Working with Several Different Database Management Systems.Be very proficient with SQL Server.Strong SQL skills (SQL Server, Oracle, MySQL, Postgres etc.).Experience in database design and structure, with an emphasis on scalability.A strong desire to develop new and innovative ways to improve our data storage and processing.Prepare and perform data analysis and transformations to align data to business rules.Work with our clients Subject Matter Experts to obtain a greater understating of the business needs and goals.Contribute to knowledge management activities and promote best practices for project execution.Programming experience required in any object oriented programming languages like C#, Java etc.Excellent SQL skills, with experience in building and interpreting complex queries. |  | Qualifications | A minimum 3 years of professional experience as SQL Developer or Data EngineerA minimum 5 years of professional experience in information technology.A minimum of bachelor's or higher in Computer Science, Information Systems, or equivalent degree or strong industry experience. | Nice-to-Haves:Healthcare Domain knowledge preferred.Experience in HL7,CCDA preferred. | Additional Information | This is a full-time, onsite position at our HQ located in Elmwood Park, NJ. Remote work may be available. | For more information on our company, visit www.ELLKAY.com. | Interested applicants should submit a letter of interest with salary requirements and resume. | ELLKAY LLC is a Smoke-Free Workplace. | AA/EOE.",Elmwood Park NJ 07407,SQL Developer/Data Engineer
ELLKAY LLC,/rc/clk?jk=d83fe19228e72c7b&fccid=423dc0697be201e8&vjs=3,"As the leading independent modern media company, Vox Media ignites conversations and influences culture. Across digital, podcasts, TV, streaming, live events, and print, we tell stories that affect our audience's daily lives and entertain as much as they inform. |  | Our portfolio features influential and respected editorial properties including Vox, New York Magazine, The Verge, The Cut, Eater, Vulture, The Strategist, Polygon, SB Nation, Intelligencer, Curbed, Grub Street and Recode. Off-platform, the Vox Media Podcast Network offers one of the largest collections of popular podcasts, and Vox Media Studios produces and distributes the award-winning nonfiction shows. Powered by innovative technology that scales quality, the Chorus publishing platform and Concert advertising marketplace answer the always-changing needs of modern audiences, creators and marketers. |  | Vox Media has been named one of Fast Company's ""Most Innovative Companies in Media,"" an Inc. ""Company of the Year,"" Digiday's ""Best Company for Parents,"" and one of the Best Places to Work for LGBTQ Equality by the Human Rights Campaign. |  | We're seeking an experienced and motivated Machine Learning Data Engineer to join our Revenue Product engineering team. |  | About Vox Media's Data Team: |  | Our Data Team is a small but knowledgeable group of data enthusiasts. The team is currently composed of a data engineer, data scientist, product manager, project manager and an engineering manager. |  | As part of our data team, you will be an integral part of Vox's data ecosystem. Our team works closely with data science, product and editorial teams to ingest and transform data, develop models to improve user engagement, and develop products and strategies for handling data requests. We work full stack, playing with infrastructure, data stores, APIs and every once in a while, client-side code. |  | This role is part of the Revenue Product group and will report to the Data Engineering Manager. We are excited for you to join us! |  | What you'll do: |  | Build and maintain data pipelines that are trustworthy and reliable. | Contribute to our data lake infrastructure. | Train, evaluate, deploy, serve and monitor Machine Learning models and platforms. | Support and improve our toolset to serve the data needs of our internal customers. | Build tools that empower others to access and understand answers to their own data questions | Work alongside product, revenue and editorial colleagues to deliver the best experience for our business. | Participate in Agile ceremonies (scrum/daily standup, sprint planning, retrospectives) | Utilize available documentation and contribute to writing new documentation where it is needed. | Showcase your favorite background during our regular ""SELECT * FROM life WHERE subject != 'work'"" Zooms. | Actively track and communicate task status using project management systems (Jira). | Collaborate often and efficiently with teammates via Slack, Zoom, Jira and email. | Be a part of a thoughtful and energetic engineering team at Vox. | Have fun tackling challenges and learning within a diverse team of Engineers, Data Analysts, Product Managers and Project Managers. |  | What you'll bring: |  | Proficiency with: | SQL (particularly with Postgresql and BigQuery) | Python | GCP and AWS | Machine Learning algorithms and operations | Willingness to work with remote-first teams; comfort communicating remotely through Slack, docs, video calls and diagrams. | Experience with ML techniques (text classification, clustering, recommender systems). | Proficiency in operating machine learning solutions at scale, covering the end-to-end ML workflow. | Ability to work independently as well as collaboratively with larger multi-disciplinary teams. | Appreciation for people. Curiosity-fueled empathy to improve those around you through culture, engineering and relationships. | Comfort balancing the demands and intersections between business needs, privacy and security, user experience, editorial requirements and technical excellence. | An interest in designing and implementing scalable data products. | Experience in data instrumentation and monitoring | Champion for engineering and operational standards, including infrastructure-as-code, change control, testing, and automation | Ideas, opinions and the ability to share them through respectful code and design reviews, proposals, talks, diagrams and team-wide discussions. | Overwhelming desire to learn from, and teach, those around you. | Comfort giggling. Even at yourself sometimes. |  | Bonus points for: |  | Experience with any of the following tools that we use: | Go | Kubernetes | Airflow | Experimentation and optimization tools | Ruby, Rails, Node.js | NoSQL | Awareness of data privacy practices (GDPR, CCPA) | Remote work experience in a team |  | For more on the things we're building and problems we're solving, and what it's like to work with the Product, Design &amp; Technology teams, see our Product Team blog. In an effort to provide an inclusive and respectful working environment, all team members adhere to our code of conduct. |  | About working at Vox Media: |  | This is a permanent, full-time position with excellent benefits—including flexible hours and generous parental leave. Vox Media strives to provide comprehensive healthcare options for our employees and to ensure that our healthcare and other benefits are LGBTQ-inclusive. You'll be joining a group of focused, hard-working, creative people who are passionate about doing work that's challenging and fun—and who strive to maintain a healthy work/life balance. |  | Vox Media is committed to building an inclusive environment for people of all backgrounds and everyone is encouraged to apply. Vox Media is an Equal Opportunity Employer and does not discriminate on the basis of race, color, gender, sexual orientation, gender identity or expression, religion, disability, national origin, protected veteran status, age, or any other status protected by applicable national, federal, state, or local law.",New York State,Machine Learning Engineer Data
Altria,/rc/clk?jk=1603f3b5669a6c25&fccid=d9cf78b5cbf8e15d&vjs=3,"DOD CLEARANCE IS REQUIRED! |  | Relocation is not a requirement. Position will require travel to various Federal DOD client sites. |  | Let’s change the world together |  | CDW is building teams that are growing service offerings to our customers. A successful Associate Consulting Engineer (ACE) will immerse in a close-knit group of techies, to collaborate and master with. Hone your learning by partnering with skilled engineers that value your ideas and perspectives. The Program is a meaningful paid, industry-leading training program for people who have real passion for technology, an aptitude for problem solving, and a drive for customer dedication. Further expand your knowledge, gain hands-on experience with leading-edge technology, while accelerating your career dreams and leveraging your Security Clearance by working on Federal initiatives on various Bases and Client Sites. Upon completion of the program, you will advance into an Engineer Consultant on CDW’s services team where you will accelerate your career. |  | What you’ll be doing: |  | Participate in a best-in-class training program while gaining valuable on-the-job skills. As an Associate Consulting Engineer (ACE), you will be actively involved in technical solving opportunities and assigned to meaningful projects that align to your skill set. Learn from top technology professionals as well as pursue certifications that are incorporated into the program. | ACEs focusing in this Data Center practice will work with our lead engineers designing, planning, and implementing Data Center Solutions including Network Virtualization, Storgae, Data Center Security (ADD) and Compute platforms. |  |  | CDW is a leader in providing Information Security risk assessment services and is the predominant Cisco solutions provider. By joining our team, you will be partnered with industry leading Engineers, Consultants and Technical Architects as colleagues to help enable your success |  | Authorized to work for CDW in the United States; immigration sponsorship (H-1B, TN, etc.) is not currently available for this position. | Valid U.S. driver’s license | DoD Secret Clearance and above | BA/BS including concentration in; Telecommunications, Computer Science, Engineering, or related discipline or one of the following: | Associate degree in Information Security, Computer Science, or related technology focused concentration plus 1 year of relevant practical experience or; | 3+ years relevant practical experience in IT networking, information systems management or applications development or; | 1 year relevant practical experience plus one or more applicable technology related certifications (certifications to be in active standing) | Ability to travel up to 75% (can vary by location) | Ability to work select weekends and/or after hours when business needs arise |  | Preferred Qualifications | At least 1 semester or equivalent experience of programming experience with Python, Perl, Ruby, or PowerShell | Experience with Linux | One or more of the following professional certifications: CompTIA Security+, CompTIA Network+, CCNA, CISSP, OSCP, SANS GIAC | Experience in cyber competitions. | Public speaking experience | Military Information Security background is a plus | National technology related challenges, awards or achievements",New York NY 10001,Associate Consulting Engineer – Federal (Data Center)
SmartAsset,/rc/clk?jk=81331506c6445fb3&fccid=7d0efc941428af5e&vjs=3,"Job Summary: | The Data Engineer utilizes a wide range of technologies to design, develop, and deploy innovative programming and technical solutions to data analytics and data processing. The Data Engineer is expected to demonstrate increased proficiency in newly acquired industry-related skills. This person can work independently and produce work according to clear-cut and complete specifications. | Supervisory Responsibilities: | None. | Duties/Responsibilities: | Proficiency in designing and developing innovative data analytics software and methods. | Contributes across whole project lifecycle, utilizing peers for guidance where necessary. | Operate independently and seeks assistance or guidance when required. | Ability to recognize trends and patterns in the data that can be exploited into a repeatable analysis process | Performs triage of product support requests, problem determination and assists with escalation when appropriate | Demonstrates a complete understanding of a core-product or service offering’s features, construction and operating characteristics | Incorporates effective test procedures, logging and monitoring in software with minimal oversight | Participates in regular review of individual output to ensure it conforms to department and company standards | Contributes to efforts in maintaining and improving product quality | identification and submission of product improvement when appropriate | Creates quality product and support documentation | Identifies risks to projects, communicates and formulates mitigation plans | Actively contributes to cross-functional team efforts | Conducts self-assessments by comparing required skills with existing knowledge to develop, present and execute plans for improvement | Consistently delivers to deadlines at the required quality standards |  | Required Skills/Abilities: | Embodies and demonstrates maturity, professionalism, and ethics | Articulate in oral and written communication | Working-level knowledge of algorithms | Demonstrates sound coding techniques | Able to break-down complex requirements into workflows and identify key performance indicators. | Proficient in the use of databases: query and data definition | Proficiency in one or more core languages: Golang, Python, SQL, Bash, Perl | Proficient in the use of industry standard tooling (i.e. the Atlassian Stack, etc.) | Competent with Linux | Solid oral and written communications skills | Consistently adheres to commitments with respect to delivery and timeframe | Working knowledge of networking protocols |  | Additional Desired Skills/Abilities | Familiarity with design patterns and industry best practices | Experience with one or alternative database technologies like: ElasticSearch, Apache Cassandra, Mongo DB, Spark | Experience with Cloud technologies like: AWS, Google Cloud, Azure | Ability to effectively create and utilize REST APIs | Proactively creates automated analytics solutions to push team’s capabilities and increased situational awareness | Knowledge of MVC frameworks | Ability to execute complex queries and design relational databases in PostgreSQL using referential integrity, views, stored procedures and proper indices | Ability to create visualizations from resultant analytic results | Experience creating and distributing Jupyter Notebooks for repeatable data analysis |  | Education and Experience: | High school diploma or equivalent required | Typically has two to four years combined industry / education experience | Some specialized training or education beyond high school is preferred |  | Physical Requirements: | Prolonged periods of sitting at a desk and working on a computer. | Must be able to travel up to 5% of the time. |  | Location: | Virtual",Remote,Data Engineer
Cisco Systems,/rc/clk?jk=6a7ba904798d77a4&fccid=a168335bbdcce5e0&vjs=3,"About EAB |  | At EAB, our mission is to make education smarter and our communities stronger. We harness the collective power of more than 1,900 schools, colleges, and universities to uncover and apply proven practices and transformative insights. And since complex problems require multifaceted solutions, we work with each school differently to apply these insights through a customized blend of research, technology, and services. From kindergarten to college and beyond, EAB partners with education leaders, practitioners, and staff to accelerate progress and drive results across three key areas: enrollment management, student success, and institutional operations and strategy. |  | At EAB, we serve not only our partner institutions but each other—that's why we are always working to make sure our employees love their jobs and are invested in their community. See how we've been recognized for this dedication to our employees by checking out our recent awards. |  | For more information, visit our Careers page. |  | The Role in Brief: |  | Associate Data Engineer (Full Time Starting Summer 2021) |  | Opportunity based in Richmond, VA. |  | Primary Responsibilities: |  | Responsible for data modeling and schema design that will range across multiple business domains within higher education | Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas | Work with clients to research and conduct business information flow studies | Codify high-performing SQL for efficient data transformation | Coordinate work with external teams to ensure a smooth development process | Support operations by identifying, researching and resolving performance and production issues |  | Basic Qualifications: |  | Experience working with relational or multi-dimensional databases | Experience developing logical data models within a data warehouse | Experience developing ETL processes | Demonstrated mastery in one or more SQL variants: PostgreSQL, MySQL, Oracle, SQL Server, or DB2 | Demonstrated mastery in database concepts and large-scale database implementations and design patterns | Proven ability to work with users to define requirements and business issues | Excellent analytic and troubleshooting skills | Strong written and oral communication skills |  | Ideal Qualifications: |  | Bachelor’s or Master’s degree in Computer Science or Computer Engineering | Experience working in an AGILE environment | Experience developing commercial software products | Experience with AWS data warehouse infrastructure (redshift, EMR/spark) | GIT expertise |  | Benefits: |  | Consistent with our belief that our employees are our most valuable resource, EAB offers a competitive and inclusive benefits package. |  | Medical, dental, and vision insurance; dependents and domestic partners eligible | 401(k) retirement plan with company match | 20+ days of PTO annually, in addition to paid firm holidays | Daytime leave policy for community service or fitness activities (up to 10 hours a month each) | Paid parental leave for birthing or non-birthing parents | Phase Back to Work program for employees returning from parental leave | Infertility treatment coverage and adoption or surrogacy assistance | Wellness programs including gym discounts and incentives to promote healthy living | Dynamic growth opportunities with merit-based promotion philosophy | Benefits kick in day one, see the full details here. |  | At EAB, we believe that to fulfill our mission to “make education smarter and our communities stronger” we need team members who bring a diversity of perspectives to the table and a workplace where each team member is valued, respected and heard. |  | To that end, EAB is an Equal Opportunity Employer, and we make employment decisions on the basis of qualifications, merit and business need. We don’t discriminate on the basis of race, religion, color, sex, gender identity or expression, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law.",Richmond VA 23219,Associate Data Engineer (Full Time Starting Summer 2021 - Richmond VA)
Discover Financial Services,/company/Promontory-Interfinancial-Network/jobs/Data-Engineer-859dca2ffc59dcf1?fccid=22e4270ce95a751c&vjs=3,"Job detailsSalary$102,947 - $165,318 a yearJob TypeFull-timeNumber of hires for this role1QualificationsBachelor's (Preferred)SQL: 2 years (Preferred)AWS: 3 years (Preferred)Python: 3 years (Preferred)Full Job DescriptionOpen Position:  Data EngineerHeadquartered in Arlington, Virginia, IntraFi Network (formerly Promontory Interfinancial Network) – the nation’s largest deposit allocation service provider and the inventor of reciprocal deposits – provides dynamic, all-weather balance sheet and liquidity management solutions to help financial institutions grow franchise value. The company, chosen by thousands of banks since its founding nearly two decades ago, has assembled the largest bank network of its kind. Its solutions help institutions to acquire high-value, local relationships; purchase funding; and reduce collateralization costs.At IntraFi Network, we prosper by working hard in an open and creative environment. Our efforts to create and maintain a culture that values our people have led us to be designated among the top fintech companies to work for by American Banker. Promontory Network has also been recognized as one of the best places to work by the Washington Post’s Top Workplaces and the Washington Business Journal’s Best Places to Work. Our company is also recognized among Fortune’s lists of Best Workplaces—Small &amp; Medium Businesses and BestWorkplaces in Financial Services and Insurance.What is the role? You will be a key member of our IT Data Engineering team responsible for providing the company with the accurate and timely data gathered from our business processes and external sources. Your primary responsibility will be to assist in establishing the security, architecture, engineering, operations, and governance of the company’s data pipelines, data stores, and associated cloud-based infrastructure.Your responsibilities will include: Designing cloud-infrastructure architecture options for the company’s data pipelines and stores Researching and implementing best practices derived from industry experiences and standards Facilitating and leading cross-functional teams to define, expand, document, and implement the company's data lake resources Further developing subject matter expertise in data and cloud architecture and technologies to support initiatives across the company Managing identity and access-management solutions for the cloud infrastructure Refining data-governance and compliance standards to ensure the security and auditability of the data platform Learning, understanding, and assisting with the operational management of the existing on-premises data applications and infrastructureYou should possess the following experience, skills, and qualifications: 3+ years of progressive systems-infrastructure and cloud experience 5+ years of infrastructure-as-code or software-development experience; Python and Java experience are preferred 3+ years of ETL and data-pipeline experience using tools such as Talend, Airflow, SSIS, and AWS Glue 3+ years of database experience, including database administration, database queries, and data modelling A bachelor’s degree in computer science or equivalent experience; master’s degree is preferred Experience designing, deploying, and supporting cloud technologies using AWS with expertise in AWS storage, security, data lake, database, and serverless compute tools An infectious sense of exploration and experimentation, allowing you to learn new technologies quickly The ability to create standards and procedures for the production use of sufficiently mature technologies Exceptional presentation, written, and verbal communication skills directed to both technical and nontechnical audiences Outstanding interpersonal skills for listening to the input and needs of clients and building consensus among the team Excellent organizational, planning, and project-management skills with the ability to demonstrate mature, timely, professional problem-solving abilities Must demonstrate initiative and be customer-focused, self-directed, and results/goal-orientedIntraFi Network LLC is an Equal Opportunity Employer and does not discriminate on the basis of race, color, national origin, sex, religion, age, veteran status, disability, or sexual orientation in employment of the provision of services. Job Type: Full-timePay: $102,947.00 - $165,318.00 per yearBenefits:401(k)401(k) matchingDental insuranceDisability insuranceEmployee assistance programEmployee discountFlexible scheduleFlexible spending accountHealth insuranceHealth savings accountLife insurancePaid time offParental leaveProfessional development assistanceReferral programRetirement planVision insuranceSchedule:Monday to FridaySupplemental Pay:Bonus payEducation:Bachelor's (Preferred)Experience:SQL: 2 years (Preferred)AWS: 3 years (Preferred)Python: 3 years (Preferred)Data Engineering: 3 years (Preferred)Work Location:One locationVisa Sponsorship Potentially Available:No: Not providing sponsorship for this jobCompany's website:https://www.intrafi.com/Company's Facebook page:https://www.facebook.com/IntraFiNetworkBenefit Conditions:Waiting period may applyWork Remotely:Temporarily due to COVID-19",Arlington VA 22209,Data Engineer
Apptad Inc,/rc/clk?jk=5401a06679947371&fccid=2d499a4c1fa5dde0&vjs=3,"Front powers the heart of business: the meaningful connections between teams and customers that lead to lasting relationships. Bringing email and apps together in a collaborative customer communication platform, Front drives business impact by scaling the natural conversations that create customers for life. More than 6,000 businesses are using Front today to cultivate personalized customer relationships at scale and transform their work into impact. | With $138M in funding, Front is backed by Sequoia Capital, and other leading venture capital firms, as well as independent investors including top executives of Atlassian, Okta, Qualtrics, and Zoom. Front was recently named to Wealthfront’s Career-Launching Companies in 2021 and Forbes Cloud 100 2020, the definitive list of the top software companies in the world. We have also been recognized on LinkedIn’s 2020 Top Startups, The New York Times’ The Next Wave of ‘Unicorn’ Start-Ups, Fortune’s Best Small Workplaces, and Glassdoor’s Best Places to Work. | As a part of the Data Team at Front, your core responsibility will be to help maintain and scale our infrastructure for analytics as our data volume and needs continue to grow at a rapid pace. This is a high impact role, where you will be driving initiatives affecting teams and decisions across the company and setting standards for all our data stakeholders. You’ll be a great fit if you thrive when given ownership, as you would be the key decision maker in the realm of architecture and implementation. | This position is required to report to our San Francisco HQ! | What will you be doing? | Architect data pipelines that provide fast, optimized, and robust end-to-end solutions for internal users of the analytics infrastructure. | Automate manual processes and create a platform in favor of self-service data consumption. | Own the quality of our analytics data and ensure the Data Team’s SLAs are met on a timely basis. | Design data schemas and fine-tune queries around large, complex data sets. | Interface with data scientists, analysts, product managers and all data stakeholders to understand their needs and promote best data practices. | Keep our data available and secure across multiple data centers and regions. | Implement a robust monitoring &amp; logging framework that guarantees the traceability and auditability. | Vet tools and technologies for the most viable solution for each problem at hand. Manage tools and data vendors involved. | What skills and experience do you need? | BS/BA in Computer Science, Mathematics, or relevant technical field. | At least 4 years of experience as a Data Engineer, or in a role with ETL expertise. | Experience in managing data warehouse plans and communicate them to internal stakeholders. | Strong overall programming skills, able to write modular, maintainable code. | Advanced level of SQL is required. | Working knowledge and experience with at least one of the big data technologies: HDFS, EMR, Redshift, Spark, Flink, or Presto. | Experience with workflow management tools: Airflow, Luigi, etc. | Experience with R or Python is a plus. | You are proactive, have a positive attitude with a“can-do”, service-oriented mentality. | Ability to juggle multiple projects/tasks with multiple stakeholders. | Front provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age or disability.",San Francisco CA,Data Engineer
Vox Media,/rc/clk?jk=a55a966b0c5c6ccd&fccid=94eb61eaf1bf49bc&vjs=3,"Data/Analytics Engineer |  | About You: | We are looking for someone with data, systems, and analytics skills to join our small and growing team. This role will have you working cross functionally with internal stakeholders to help structure, automate, centralize, and optimize our business processes. This is a great opportunity for the right person to enhance their skill set across Data Engineering and Data Science - curiosity, creativity, and tenacious problem solving are a must! |  | Required Skills and Experience: | Bachelor's Degree in a hard science, mathematics, or engineering | 2+ years of development experience with exposure to Data Science | Solid Python and SQL skills | Good knowledge of pandas with hands on data wrangling experience | Comfortable in the Linux environment | Solid math skills - Probability/statistics/calculus | Preferred Skills and Experience: | PhD/MSc | ETL/data pipelining experience | Experience with AWS services/EC2 and cloud storage | Data Science experience",New York NY,Data/Analytics Engineer
St. Luke's University Health Network,/rc/clk?jk=cdacd48d07414606&fccid=91cfe3314360dc22&vjs=3,"Hi there! |  | Zapier is looking for an experienced Data Warehouse Engineer to join our Data Operations team. We are a diverse, globally-dispersed team of passionate folks who allow Zapier to make data-based decisions about our customers, our product, and our direction. If our Data team's mission of using data to enrich Zapier's understanding of the needs and behavior of users, customers and partners inspires you, then read on: |  | We know applying for and taking on a new job at any company requires a leap of faith. We want you to feel comfortable and excited to apply at Zapier. To help share a bit more about life at Zapier, here are a few resources in addition to the job description that can give you an inside look at what life is like at Zapier. We hope you'll take the leap of faith and apply. |  | Our Commitment to Applicants |  | Culture and Values at Zapier |  | Zapier Guide to Remote Work |  | Zapier Code of Conduct |  | Diversity and Inclusivity at Zapier |  | Zapier is proud to be an equal opportunity workplace dedicated to pursuing, hiring, and maintaining a diverse workforce. Diversity, Inclusion, Belonging, and Equity. |  | About You | You are a skilled written communicator. Zapier is a 100% remote team and writing is our primary means of communication. You can communicate about technical topics without unnecessary jargon. You translate unfamiliar data modeling concepts into approachable ones to teammates with less experience working with data. | You have experience in SQL. You've used written complex SQL queries that join across data from multiple systems, matching them up even when there was not a straightforward way to join the tables. You've designed tables with an eye towards ease of use and high performance. You've documented schemas and created data dictionaries. | You appreciate our team's values and mission of eagerness to collaborate with diverse teammates from any function of the organization or with any level of data knowledge, iterating over your deliverables, and being curious. We look to uncover new strategic avenues, drive faster user-centric decisions and make Zapier's products smarter. | You enjoy learning about the business or product goals so you can design data models that will empower analysts to build their own reports and dashboards and spend more time exploring the data. You have put together tools for data analysts and data scientists to build their own transforms. | You have experience with star schema: You understand star schemas and have a reference copy of The Data Warehouse Toolkit on your desk. You have a nuanced appreciation for when and where the traditional approaches work and when they don't. You have experience in ETL development. Bonus if you have modeled financial data. | You understand that the perfect is the enemy of the good and default to action by shipping MVP code and iterating as needed to get towards better solutions. | Things You Might Do | Develop ETL to ingest and transform data from upstream databases and APIs into a data warehouse. The tools used include AWS Redshift, NiFi, Kafka, Matillion ETL, and custom Python. | Data modeling, including designing and expanding slowly changing dimension and fact tables. | As a part of Zapier's all-hands philosophy, help customers via support to ensure they have the best experience possible. | About Data at Zapier |  | Zapier relies on dozens of systems that emit data about Zapier and our potential and current users and partners. This data is useful for us to make a better product, better decisions, and understand our weaknesses and opportunities. The data team at Zapier pulls this data from DBs, APIs, and event streams, collocates it, and then processes it through all the disparate systems to bring them together in a reliable, timely, performant, and easy-to-understand way to employees and systems that need it. |  | Within the data team, we're made up of several subteams: Data Ops focusing on data infrastructure and storage, compute, ingest, and dimensional modeling; Data Products focusing on building statistical and ML tools and models; and Data Governance. |  | Curious about our stack? We use AWS Redshift (and the related AWS products AWS Glue, Redshift Spectrum, AWS S3), Looker, Airflow, Matillion ETL, Kafka, Python, and NiFi. |  | About Zapier |  | Since 2011, Zapier has been helping people across the world automate the boring and tedious parts of their job. We do that by helping everyone connect the web applications they already use and love. |  | We believe that there are jobs a computer is best at doing and that there are jobs a human is best at doing. We want to empower businesses to create processes and systems that let computers do what they are best at doing and let humans do what they are best at doing. |  | We believe that with the right tools, you can have big impact with less hassle. |  | We believe in small teams. Small teams are fast and nimble. Small teams mean less bureaucracy and less management and more getting things done. |  | We believe in a safe, welcoming, and inclusive environment. All teammates at Zapier agree to a code of conduct. |  | Zapier Guide to Remote Work | Culture and Values at Zapier | The Whole Package |  | Location: Earth |  | Our team of 4000+ is distributed across the world because it lets us work with the best people. We have team members that live in the United Kingdom, Thailand, India, Nigeria, Taiwan, Guatemala, New Zealand, Australia, and more! You just need the skills and drive to succeed in this role and the ability to work from anywhere. You'll work in your timezone vs defaulting to the US, with occasional meetings that will rotate to encompass everyone. Can't make it? We'll record it for you! |  | Competitive salary (we pay based on the norms of your country) | Great healthcare + dental + vision coverage* | Retirement plan with 4% company match* | Profit-sharing | 2-3 annual company retreats to awesome places (we'll resume these after travel is safe for all of us!) | 14 weeks paid leave for new parents of biological or adopted children | Pick your own equipment. We'll set you up with whatever Apple laptop + monitor combo you want plus any software you need. | Unlimited vacation policy. Plus we require you to take at least 2 weeks off each year. We see most employees take 4-5 weeks off per year. This isn't a vague policy where unlimited vacation means no vacation. | Work with awesome companies around the world. We partner with great software companies all over the world and you'll constantly get to interact with people from these great companies |  | While we take care of our international folks as best we can, currently, healthcare and retirement plans are only available to Canada, UK, and US-based employees. |  | How to Apply |  | We have a non-standard application process. To jump-start the process we ask a few questions we normally would ask at the start of an interview. This helps speed up the process and lets us get to know you a bit better right out of the gate. Please make sure to answer each question fully with enough detail that we can picture the answer in our heads. Writing is critical to your success at Zapier. |  | After you apply, you are going to hear back from us, even if we don't seem like a good fit. In fact, throughout the process, we strive to make sure you never go more than seven days without hearing from us. |  | Zapier is an equal opportunity employer. We're excited to work with talented and empathetic people no matter their race, color, gender, sexual orientation, gender identity/expression, religion, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or related condition (including breastfeeding), or any other basis protected by law. Our code of conduct provides a beacon for the kind of company we strive to be, and we celebrate our differences because those differences are what allow us to make a product that serves a global user base.",Oregon,Data Warehouse Engineer - Open to all locations
Naval Nuclear Laboratory,/rc/clk?jk=98ad6b857bedaeac&fccid=b5ff6a9d6acf7028&vjs=3,"Description |  | The Naval Nuclear Laboratory is seeking a Data Engineer to join a growing team dedicated to collecting and transforming data from the Naval Nuclear fleet for analytics throughout the Laboratory. | As a Data Engineer, you will be part of a team responsible for designing, developing, optimizing, and maintaining the Extract, Transform, and Load (ETL) process for incoming shipboard data used in naval nuclear propulsion systems in a Hadoop environment. | You will evaluate new and emerging technologies to deliver scalable solutions to transforming shipboard data in an enterprise-wide data warehouse from which operational metrics and support analytics can be easily and reliably generated. |  | Requirements |  | BS degree in engineering or Bachelor's degree in a science related field from an accredited college or university and a minimum of four years of relevant experience; or | MS degree in engineering or Master's degree in a science related field from an accredited college or university and a minimum of two years of relevant experience |  | Preferred Skills |  | Knowledge of distributed technologies, such as Hadoop, Hive, Spark. | Experience in Linux and scripting, such as SQL, Python, Scala. | Expertise in optimization and performance tuning of ETL workloads and understanding of database internals.",Niskayuna NY 12309,Data Engineer
EAB,/rc/clk?jk=177829ff001d6a18&fccid=3a71a4d2f7990a25&vjs=3,"Job detailsSalary$85,000 a yearFull Job DescriptionThe world isn’t standing still, and neither is Allstate. We’re moving quickly, looking across our businesses and brands and taking bold steps to better serve customers’ evolving needs. That’s why now is an exciting time to join our team. You’ll have opportunities to take risks, challenge the status quo and shape the future for the greater good. | You’ll do all this in an environment of excellence and the highest ethical standards – a place where values such as integrity, inclusive diversity and accountability are paramount. We empower every employee to lead, drive change and give back where they work and live. Our people are our greatest strength, and we work as one team in service of our customers and communities. | Everything we do at Allstate is driven by a shared purpose: to protect people from life’s uncertainties so they can realize their hopes and dreams. For more than 89 years we’ve thrived by staying a step ahead of whatever’s coming next – to give customers peace of mind no matter what changes they face. We acted with conviction to advocate for seat belts, air bags and graduated driving laws. We help give survivors of domestic violence a voice through financial empowerment. We’ve been an industry leader in pricing sophistication, telematics, digital photo claims and, more recently, device and identity protection. We are the Good Hands. We don’t follow the trends. We set them. | Job Description | Data, Discovery, and Decision Science (D3) is the research and analytics organization at Allstate. We are solving some of today’s most complicated analytics problems, making the lives of our Allstate colleagues easier and more productive, and driving our mission to deliver perfect insurance solutions to our customers. We are avid about learning new ways to get the most value from our massive data resources. We are partnering to incorporate analytics into every aspect of the enterprise. Developing ourselves and others is key to our success. | As a Data Analytic Engineer in D3, you'll be responsible for driving the use and development of data infrastructure projects &amp; proof of concept business solutions for users in analytics and Data Science. You will work directly with data scientists and analytic engineers on needs and tactical solutions. You will execute new data engineering work tracks for Data Science and analytics from inception and prototyping to fully developed solutions. You will also have the opportunity to begin to manage projects. | Key Responsibilities | Work with Data Scientists and business partners on cross functional teams; developing subject matter expertise in the business as well as advanced analytics. | Provide support on requirement development for analytic data sources, breaking down business problems into solvable components and assist with documenting requirements with minimal supervision. | Execute rapid development of new data and analytic work tracks with fast iteration over quick sprints | Help develop and deliver the data infrastructure required to support needs of predictive modeling and analytics with minimal supervision. | Builds test scripts, executes testing, works with data scientists and business to ensure end user acceptance | Leverage “agile” data analysis with technology fluency in parallel processing/programming, software/programming languages and technologies (i.e. Oracle, MongoDB, SQL, Python, Spark, Kafka, Scala and Hadoop), paired with a high degree of analytic agility to be able to meet fluid and dynamic business needs in this space. | Participate in the development of enterprise data assets, information platforms or data spaces designed for exploring and understanding the data. | Participate in the development of new concepts, proof of concept designs, and prototypes for business or research data solutions so that business users or predictive modelers may visually understand and explore a new feature or functionality before implementation to expose design assumptions and drive ideation. | Mentor other team members in a business technical environment and promote an environment that supports innovation and process improvement. |  | Location | This role can be home-based anywhere in the United States. You many also have the option to work at an office or do a hybrid of office and remote work if you are near Charlotte, NC, Chicago, IL, Bothell, WA, or Redwood City, CA. This role will be fully work-from-home regardless of location until at least July. | Job Qualifications | 2 or more years of related experience | Bachelor’s degree or equivalent experience | Database development knowledge and experience (i.e. SQL) | Programming skills (i.e. Python, R, Java) | Computer Proficiency in Oracle, UNIX/Linux | Intermediate Analytic, Data Sourcing, and Data Management skills | Ability to extract data from various data sources. | Solid experience in time and task management | Ability to learn new technologies | Strong attention to detail | Good written and verbal communication skills, including the ability to effectively collaborate with multi-disciplinary groups and all organizational levels | Minimum base salary $85K. Target bonus eligible. Offer will be commensurate with experience and could be significantly higher than the minimum. | The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen. | Good Work. Good Life. Good Hands®. | As a Fortune 100 company and industry leader, we provide a competitive salary – but that’s just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, you’ll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy. | Learn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video. | Allstate generally does not sponsor individuals for employment-based visas for this position. | Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component. | For jobs in San Francisco, please click “here” for information regarding the San Francisco Fair Chance Ordinance. | For jobs in Los Angeles, please click “here” for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance. | To view the “EEO is the Law” poster click “here”. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs | To view the FMLA poster, click “here”. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint. | It is the Company’s policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employee’s ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.",Northbrook IL,Data Analytic Engineer
IntraFi Network,/rc/clk?jk=66eca6320f5533c2&fccid=88f5eba43de70e76&vjs=3,"About Conduent: |  | Through our dedicated associates, Conduent delivers mission-critical services and solutions on behalf of Fortune 100 companies and over 500 governments – creating exceptional outcomes for our clients and the millions of people who count on them. | You have an opportunity to personally thrive, make a difference and be part of a culture where individuality is noticed and valued every day. | Job Description: |  | Job Overview | We are looking for a Data Engineer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is the development and mainteneance of data pipelines including acquisition, transformation, loading and processing of data. |  | Responsibilities: | Engineer and maintain a modern Cloud data pipeline to collect, organize, and process data from disparate sources. | Performs data management tasks, such as conduct data profiling, assess data quality, and write SQL queries to extract and integrate data | Develop / maintain efficient data collection systems and sound strategies for getting quality data from different sources | Consume and analyze data from the data pool to support inference, prediction and recommendation of actionable insights to support business growth. | Design, develop and maintain ETL processes using tools and scripting. Troubleshoot and debug ETL processes. Performance tuning and opitimization of the ETL processes. | Provide support to new or existing applications while recommending best practices and leading projects to implement new functionality. | Collaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations. | Learn and develop new ETL techniques as required to keep up with the contemporary technologies. | Reviews the solution requirements and architecture to ensure selection of appropriate technology, efficient use of resources and integration of multiple systems and technology. | Support presentations to Customers and Partners | Advising on new technology trends and possible adoption to maintain competitive advantage |  | Experience Needed: | 5+ years of related experience is required. | A BS or Masters degree in Computer Science or related technical discipline is required | ETL experience with data integration to support data marts, extracts and reporting | Experience connecting to varied data sources | Excellent SQL coding experience with performance optimization for data queries. | Understands different data models like normalized, de-normalied, stars, and snowflake models. Worked with transactional, temporal, time series, and structured and unstructured data. | Worked in big data environments, cloud data stores, different RDBMS and OLAP solutions. | Experience in cloud-based ETL development processes. | Experience in deployment and maintenance of ETL Jobs. | Is familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery. | Has strong technical background and remains evergreen with technology and industry developments. |  | Additional Requirements | Demonstrated ability to have successfully completed multiple, complex technical projects | Prior experience with application delivery using an Onshore/Offshore model | Experience with business processes across multiple Master data domains in a services based company | Demonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality. | Demonstrates high standards of professional behavior in dealings with clients, colleagues and staff. | Is able to make sound and far reaching decisions alone on major issues and to take full responsibility for them on a technical basis. | Strong written communication skills. Is effective and persuasive in both written and oral communication. | Experience with gathering end user requirements and writing technical documentation | Time management and multitasking skills to effectively meet deadlines under time-to-market pressure | Requires some travel (on average 10%-20%) | Closing: |  | Conduent is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, creed, religion, ancestry, national origin, age, gender identity, gender expression, sex/gender, marital status, sexual orientation, physical or mental disability, medical condition, use of a guide dog or service animal, military/veteran status, citizenship status, basis of genetic information, or any other group protected by law. | People with disabilities who need a reasonable accommodation to apply for or compete for employment with Conduent may request such accommodation(s) by clicking on the following link, completing the accommodation request form, and submitting the request by using the ""Submit"" button at the bottom of the form. For those using Google Chrome or Mozilla Firefox please download the form first: click here to access or download the form. You may also click here to access Conduent’s ADAAA Accommodation Policy.",Morrisville NC 27560,Data Engineer
Conduent,/rc/clk?jk=a5a22a23aafda915&fccid=c92584a296fd1924&vjs=3,"St. Luke's is proud of the skills, experience and compassion of its employees. The employees of St. Luke's are our most valuable asset! Individually and together, our employees are dedicated to satisfying the mission of our organization which is an unwavering commitment to excellence as we care for the sick and injured; educate physicians, nurses and other health care providers; and improve access to care in the communities we serve, regardless of a patient's ability to pay for health care. | The Sr. Data Engineer performs complex research and data engineering assignments within an engineering functional area or product line, and provides direct input to project plans, schedules, and methodology in the development of cross-functional products. This position performs date engineering design - typically across multiple systems; mentors more junior members of the team; and talks to users/customers and translates their requests into solutions |  |  | The candidate will be self-learning, motivated and team-oriented individual. Willingness to learn new technologies quickly. Pairs up with Informatics team to develop cutting edge Analytic applications leveraging Big Data and Streaming modern cloud technologies. | JOB DUTIES AND RESPONSIBILITIES: |  | Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions | Interface with other team members internal or external to understand and finalize complex data requirements, improve data engineering processes, design, architecture and deliver robust data platform that drives automated and or self-service Data Onboarding, Data Engineering, Data Warehousing, and BI solutions | Write routines and build data pipelines to ingest, clean, and prepare data for analysis. | Develop views and aggregated datasets that can be easily loaded into analytical tools. | Build business processes for on-boarding new data sources. | Support the Data Management Lifecycle engineering processes from inception and design through deployment, operation, and optimization. | Manage day to day activities of the Data Engineering/Reporting team via Agile/Scrum methodologies | Passionate about staying on top of tech trends especially in cloud, experimenting with and learning new technologies, participating in internal &amp; external technology communities, mentoring other members of the team | Ensure solutions have proper documentation and regularly updated. | Troubleshoot and debugs code developed by themselves as well as other developers. | Train and mentor entry-level and or less-senior data/report engineers/analysts ensuring they are knowledgeable in critical aspects of their roles, SDLC methodologies best practices and optimization techniques. | Help implement proper data protection and storage, secure and performant solutions that adhere to defined standards ensuring high data reliability, efficiency, quality. | Operate in various development environments (Agile, Waterfall, etc. as deemed fit for purpose) while collaborating with key stakeholders. Is able to run Scrum/Kanban meets | Keep abreast of new technology developments. | Is readily available and easily able to handle high pressure situation. Convert chaos into opportunity. | All other duties as assigned. | PHYSICAL/SENSORY DEMANDS: | Sitting for up to 7 hours per day, 4 hours at a time; standing for up to 7 hours per day, 4 hours at a time; walking for up to 2 hours a day, 1 hour at a time. Seeing as it relates to general, near, color, and peripheral vision. Hearing as it relates to normal and telephone conversations. | EDUCATION: | BA/BS degree in an analytical area such as Computer Science, Physics, Mathematics, Statistics, Engineering, or similar field | TRAINING AND EXPERIENCE: | Minimum 7 yrs. of Programming experience on various stack, primarily focused on data engineering. | Recent 3+ years of experience in hands-on Data engineering, BI role with extensive cloud-based data processing application design and development. | Experience in developing and delivering Big Data related services on AWS or Azure cloud. | Extensive experience using Python, Java, Unix/Linux scripting, SQL, ETL/ELT, Data Warehousing solutions and database modeling in a business environment | Strong understanding of Data Ingestion pipelines, Data Lakes and Data cataloging techniques. | Experience in designing, developing, and implementing advanced data collection methods, sizing for data storage, index strategies and processes, transforming/normalizing data to common standards, data enrichment and/or anonymization of data upon ingest. | Experience transforming large datasets into consumable assets for self-service analytics and reporting. | Experience building self-service reporting solutions using business intelligence or data visualization software (e.g. Tableau, PowerBI, AWS Quicksight). | Experience with high volume data streaming methods and platforms such as Kafka/Kinesis and Spark. | Experience using enterprise data orchestration tool. | Strong data analysis skills, problem-solving abilities with an analytic and qualitative eye for reasoning under pressure. | Flexible to changing priorities and comfortable in a fast-passed dynamic environment. | Self-starter with the ability to independently prioritize and complete multiple tasks with little to no supervision. | Experience with the Software Development Life Cycle (SDLC) and Agile/Scrum methods. | Experience with APM tools like JIRA/Confluence and Source control tools like Github. | Experience leading small data engineering team from technical perspective. | Strong communication skills. | PREFERRED: | Master’s degree in analytical area. | Experience in Healthcare vertical. | Cloud certification is preferable: AWS/Azure Developer/Architect/Big Data. | Experience with MS Office suite. | Please complete your application using your full legal name and current home address. Be sure to include employment history for the past seven (7) years, including your present employer. Additionally, you are encouraged to upload a current resume, including all work history, education, and/or certifications and licenses, if applicable. It is highly recommended that you create a profile at the conclusion of submitting your first application. Thank you for your interest in St. Luke's!! | St. Luke's University Health Network is an Equal Opportunity Employer.",Allentown PA 18109,Sr. Data Engineer
Grubhub Holdings Inc.,/rc/clk?jk=a68ca3dc3d827ffa&fccid=12e41f7040f168e8&vjs=3,"About The Opportunity: |  | Hey! We’re Grubhub | We’re all about connecting hungry diners with our network of over 300,000 restaurants nationwide. Innovative technology, user-friendly platforms and streamlined delivery capabilities set us apart and make us an industry leader in the world of online food ordering. When you join our team, you become part of a community that works together to innovate, solve problems, grow, work hard and have a ton of fun in the process! | Why Work For Us | Grubhub is a place where authentically fun culture meets innovation and teamwork. We believe in empowering people and opening doors for new opportunities. If you’re looking for a place that values strong relationships, embraces diverse ideas–all while having fun together–Grubhub is the place for you! | We are looking for data engineers to help us expand and improve our big data and machine learning pipelines powering Grubhub’s restaurant supply growth. You will work with smart, humble, and committed colleagues to create innovative, robust, automated solutions for Grubhub’s three-way marketplace. This is an unbeatable opportunity for data engineers who hope to work on and deliver world-class data products in a friendly and fun environment. | The Impact You Will Make: | Work with high volumes of data and distributed systems using technologies such as Spark, Hive, AWS EMR, AWS S3, Azkaban, Presto and etc. | Work with data scientists and analysts to productionize data pipelines and machine learning models, so that they can scale and accommodate various business requirements. | Doing deep dives on business verticals where you become one of the foremost experts on that vertical in the company | Translate from technical to business, and vice versa. You need to be able to speak with the least technically-minded client (internal or external) and make technology make sense to them. Then turn around and do it the other way. | Actively contribute to the adoption of strong data engineering architecture, development practices, and new technologies. | Maintain up-to-date technical documentation while continuously delivering technical solutions and business impacts. | What You Bring to the Table: | A bachelor's degree, preferably in a computer-related discipline. | Excellent knowledge of SQL, data modeling, and patterns. | 5+ years experience with Python or another general-purpose programming language | Background in ETL and data processing, familiar with how to transform data to meet business goals | Willingness to understand the larger business context | Enthusiasm for a fast-paced, tech, and product-oriented environment, and the desire to work with a great team! | Got These? Even Better | 5+ years of experience developing large data processing pipelines with Apache Spark. | Excellent communication skills, including the ability to crystallize and broadly socialize insights | Rigorous attention to detail and accuracy | Exposure to Amazon AWS or another cloud provider | Growth mindset. | Be passionate about continuous learning and knowledge sharing | Familiarity with Agile software development methodologies | And Of Course, Perks!: | Flexible PTO. Grubhub employees enjoy a generous amount of time to recharge. | Health and Wellness. Excellent medical benefits, employee network groups and paid parental leave are just a few of our programs to support your overall well-being. | Competitive Pay. You’ll receive a competitive base salary with eligibility for generous incentives, bonuses, commission or RSUs (role-specific). | Learning and Career Growth. Your personal and professional development is a priority at Grubhub. We empower you to be a leader and grow your career through training, coaching and mentorship opportunities. | MealPerks. Get meals on us! Our employees get a weekly Grubhub credit to enjoy and support local restaurants. | Fun. Every Grubhub office has an employee-led Culture Crew that connects people through fun, meaningful events and initiatives like Wellness Wednesdays, Slack competitions and virtual happy hours! | Social Impact. At Grubhub we believe in giving back through programs like the Grubhub Community Relief Fund and donating $1 million to the Equal Justice Initiative in 2020. Employees are also given paid time off each year to support the causes that are important to them. | COVID-19 Response. All of our employees are currently working from home and will be for the foreseeable future. We look forward to seeing everyone in-office when it’s safe to return. | Grubhub is an equal opportunity employer. We welcome diversity and encourage a workplace that is just as diverse as the customers we serve. We evaluate qualified applicants without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics. If you’re applying for a job in the U.S. and need a reasonable accommodation for any part of the employment process, please send an email to TalentAcquisition@grubhub.com and let us know the nature of your request and contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this email address. | CA Privacy Notice: If you are a resident of the State of California and would like a copy of our CA privacy notice, please email privacy@grubhub.com.",New York NY 10018,Data Engineer II
RevSpring Inc,/company/RevSpring-Inc/jobs/Data-Engineer-f1ca07405e77a056?fccid=076456eb3ed7f084&vjs=3,"Job detailsJob TypeFull-timeFull Job DescriptionSenior Data Engineer The Senior Data Engineer will play a critical role in supporting the company's Analytics organization. It is preferred that the candidate lives near a RevSpring corporate office (Newark OH, Oaks PA, Nashville TN, Livonia MI, Ann Arbor MI, Arden Hills MN, or Phoenix AZ) and can occasionally commute to the office post pandemic. The engineer will work closely with a team of data scientists, data visualization developers, and engineers. He or she will be primarily responsible for building and maintaining big data pipeline systems that are used for analytics: reporting, data visualization, predictive models, etc. We are looking for a candidate that has a start-up mentality and is comfortable rolling up their sleeves and learning new technology based on the business’ changing demands.Requirements · Bachelor’s degree in Computer Science, Mathematics, Engineering or similar· 8+ years of relevant work experience· Experience building and optimizing big data pipelines· Data management experience in both traditional data warehousing and big data· Distributed query systems experience with tools such as Presto/Trino, AWS Redshift, Google BigQuery, Snowflake· Experience working on AWS, GCP or Azure (AWS preferred)· Strong programming and scripting skills (Python preferred)· Containerization and orchestration (Docker preferred)· Ability to translate task/business requirements into written technical requirements· Ability to troubleshoot and document findings and recommendations· Ability to communicate risks, problems, and updates to leadership· Ability to keep up with a rapidly evolving technology spaceRevSpring, Inc. (www.RevSpringinc.com) is headquartered in Livonia, Michigan. RevSpring is the leading provider of consumer receivables communication and payment technology. Its core service offerings include data hygiene and analytics, mail document creation and delivery (via US Mail), multi-channel communications, electronic billing and archival services and online payment tools, all while ensuring compliance with regulatory guidelines.RevSpring offers a competitive benefit package including medical, dental, prescription drug, vision, life, 401(k), and paid time off. This Job Description may not describe all of the job responsibilities and standards assigned to this position. The duties may change from time to time. RevSpring does not discriminate against any group in hiring or employment practices. Nothing in this job description constitutes a contract for employment.Job Type: Full-timePay: $0.00 per hourSchedule:Monday to FridayExperience:relevant work: 8 years (Preferred)Work Location:Fully RemoteCompany's website:revspringinc.comWork Remotely:Temporarily due to COVID-19",Remote,Data Engineer
Centricity,/rc/clk?jk=168575ad156ccbbd&fccid=1333c40c36156074&vjs=3,"About Centricity |  | Centricity is an AI &amp; Machine Learning company bringing actionable, customer centric insights to today's modern retailer &amp; CPG companies. We harness billions of customer data points for insights that lead to strategic purchase decisions. In a disrupted retail landscape, our automated data and trend spotting keeps you ahead of the curve. Modern retailing demands modern tools, and Centricity is built for that exact purpose. |  | Currently, we are expecting to be largely human-powered, with our first NLP product delivered by the end of the year. We need to train that product and serve results to our Clients in an efficient, visually appealing manner. Our annotation tool needs to be as highly optimized as possible to maximize throughput and minimize cost per URL while preserving accuracy and preventing systemic bias. Our Client UI needs to be intuitive to first-time users, and highly functional for expert users. |  | About the Role |  | We are looking for a Software Engineer to join our Data Ingestion team. This is a great opportunity for a passionate technologist who loves problem solving and values learning and growth. |  | Responsibilities: |  | Build robust, scalable and interoperable applications and services that can ingest and transfer very large amounts of data | Provide support for your product and applications by being the point of contact for client IT teams | Implement new features while applying software development and security best practices | Assist with the client onboarding process by facilitating ingestion of client data | Maintain and develop new features for existing api's and UI's | Troubleshoot and fix bugs | Writing and maintaining developer documentation | Writing unit tests |  | Skills and Knowledge |  | *NOTE: This list is meant as a nice to have, not a list of requirements, we strongly encourage applicants of all backgrounds and skill sets to apply* |  | Python, Node.js, Go, or other back end programming language | Strong experience with either AWS or GCP | Spark | Airflow | Hadoop/Hive | HTML/JS/CSS | SQL | Relation or NOSQL database | Git | Linux administration",New York NY,Software Engineer Data Ingestion
Siemens,/rc/clk?jk=401995d4695238f5&fccid=3b89b9ec324f96c8&vjs=3,"At Siemens, we are always challenging ourselves to build a better future. We need the most innovative and diverse Digital Minds to develop tomorrow’s reality. Find out more about the Digital world of Siemens here: www.siemens.com/careers/digitalminds | Company: SISW - Siemens EDA | Job Title: Data Engineer – 232825 | Job Location: US - OR- Wilsonville | Job Category: Engineering | Position Overview | Siemens Digital Industries Software is looking for a talented Data Scientist/Engineer to be part of the Data and Analytics team. The ideal candidate will have proven experience and passion for driving data science projects to generate impact and value to the company. | Responsibilities | As a Data Scientist/Engineer on the Data Analytics &amp; Automation team (DAA), your primary responsibilities will be: | Detailed responsibilities: | Build production grade models on large-scale datasets to optimize marketing performance by utilizing advanced statistical modeling, machine learning, or data mining techniques and marketing science research | Work with data engineering team to create data pipelines | Work with operations team to create data integration pipelines between marketing systems on our data lake strategy | Assist and collaborate with other data analysts and scientists to generate value from data | Build backend data services for internal applications teams to consume | Build tools and frameworks to facilitate efficient and reliable data processing | Enable data science models including model building, scripting, data preparation and model management. | Create engineered feature sets for training ML models | Build classification, regression, and clustering models using Python ML libraries and Automated ML software tools | Identify and define new AI and ML approaches to driving better business performance | Required Knowledge/Skills, Education, and Experience | Bachelor of Science in computer science or related area, or equivalent experience | Experience working with customer data platforms | Java, or Scala for data processing | Working experience with analytical databases, such as Snowflake or Redshift, as well as database and query optimization | Highly Proficient with SQL | Experience with relational SQL and NoSQL databases | Familiar with BI visualization software like Tableau or Qlik | Hands-on experience with versioning, continuous integration, and build and deployment tools and platforms such as Github, GitLab, and Circle CI | Knowledge of data pipelining and workflow management tools such as Airflow, AWS Data Pipelines, and Luigi | Familiar with dimensional data modeling and data normalization | Familiar with data-lake architecture and data serialization formats such as JSON, Avro, and Parquet | Practical experience building, validating, and deploying ML based predictive or clustering models | Bonus Points: | Experience with AWS cloud services: Kinesis, Redshift, SNS, EC2, EMR Spark, EMR Hive, RDS, Athena, Spectrum, DynamoDB, and AWS Glue | Spark Streaming, Kafka, Kinesis, Spark Streaming, and Terraform | Hands-on experience with data stream processing | LI- #MGRP | LI- KM1 |  | Organization: Digital Industries | Company: Siemens Industry Software Inc. | Experience Level: Experienced Professional | Job Type: Full-time |  | Equal Employment Opportunity Statement | Siemens is an Equal Opportunity and Affirmative Action Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to their race, color, creed, religion, national origin, citizenship status, ancestry, sex, age, physical or mental disability unrelated to ability, marital status, family responsibilities, pregnancy, genetic information, sexual orientation, gender expression, gender identity, transgender, sex stereotyping, order of protection status, protected veteran or military status, or an unfavorable discharge from military service, and other categories protected by federal, state or local law. |  | EEO is the Law | Applicants and employees are protected under Federal law from discrimination. To learn more, Click here. |  | Pay Transparency Non-Discrimination Provision | Siemens follows Executive Order 11246, including the Pay Transparency Nondiscrimination Provision. To learn more, Click here. |  | California Privacy Notice | California residents have the right to receive additional notices about their personal information. To learn more, click here.",New York NY,Data Engineer- SISW- EDA- 232825
EXL Services,/rc/clk?jk=17bf9d5d83056ee9&fccid=e3e300fc88f0e813&vjs=3,"Data Engineer- Healthcare Analytics | Hartford, CT, USA  New York, NY, USA Req #149 |  Friday, February 26, 2021 | Overview |  | EXL (NASDAQ: EXLS) is a leading operations management and analytics company that designs and enables agile, customer-centric operating models to help clients improve their revenue growth and profitability. Our delivery model provides market-leading business outcomes using EXL’s proprietary Business EXLerator Framework™, cutting-edge analytics, digital transformation and domain expertise. At EXL, we look deeper to help companies improve global operations, enhance data-driven insights, increase customer satisfaction, and manage risk and compliance. EXL serves the insurance, healthcare, banking and financial services, utilities, travel, transportation and logistics industries. Headquartered in New York, New York, EXL has more than 32,000 professionals in locations throughout the United States, Europe, Asia (primarily India and Philippines), South America, Australia and South Africa. For more information, visit www.exlservice.com. | EXL Health is seeking a Data Engineer - Healthcare Analytics, this role will iniitally be remote but once Stay at Home orders are lifted, this role will be located in New York. | Data Engineer will be responsible for expanding and optimizing healthcare payer data, data pipeline architecture and data flow to enable analysis across several dimensions. | EXL (NASDAQ:EXLS) is a leading operations management and analytics company that helps businesses enhance growth and profitability in the face of relentless competition and continuous disruption. EXL supports companies to improve global operations, enhance data-driven insights, increase customer satisfaction, and manage risk and compliance. Headquartered in New York, New York, EXL has more than 24,000 professionals in locations throughout the United States, Europe, Asia (primarily India and Philippines), Latin America, Australia and South Africa. | Please visit www.exlservice.com for more information about EXL Analytics. |  | Role and Responsibilities: | Experience with Snowflake cloud data platform including hands-on experience with Snowflake utilities like SnowSQL , SnowPipe , and experience in administering Snowflake. | Accountable for clinical data review and analysis for complex, global projects collated from various data sources formatted in various industry standards (HL7, FHIR, C-CDA, JSON etc.) | Understand healthcare data, including clinical data in both proprietary and industry standard formats (FHIR, C-CDA etc.) and develop mappings / transformation solution between various formats. | Understanding of enterprise data management concepts (Data Governance, Data Engineering, Data Science, Data Lake, Data Warehouse, Data Sharing, Data Applications) | Expert level skills in SQL, data integration, data modeling and data architecture. | Experience in building data models, including conceptual, logical, and physical for Enterprise Relational, and Dimensional Databases. | Strong Database experience in Hadoop/Hive , DBMS , SQL server | Experience with programming scripting and data science languages such as Python, UNIX, SQL, Pyspark. | Build large-scale batch and real-time data pipelines using Snowflake cloud data technologies. | Working knowledge of Big Data concepts and Hadoop environment. | Some cloud experience as a developer/engineer like Google cloud platform | Qualifications: | Experience with Snowflake cloud data platform including hands-on experience with Snowflake utilities like SnowSQL , SnowPipe , and experience in administering Snowflake. | Accountable for clinical data review and analysis for complex, global projects collated from various data sources formatted in various industry standards (HL7, FHIR, C-CDA, JSON etc.) | Understand healthcare data, including clinical data in both proprietary and industry standard formats (FHIR, C-CDA etc.) and develop mappings / transformation solution between various formats. | Understanding of enterprise data management concepts (Data Governance, Data Engineering, Data Science, Data Lake, Data Warehouse, Data Sharing, Data Applications) | Expert level skills in SQL, data integration, data modeling and data architecture. | Experience in building data models, including conceptual, logical, and physical for Enterprise Relational, and Dimensional Databases. | Strong Database experience in Hadoop/Hive , DBMS , SQL server | Experience with programming scripting and data science languages such as Python, UNIX, SQL, Pyspark. | Build large-scale batch and real-time data pipelines using Snowflake cloud data technologies. | Working knowledge of Big Data concepts and Hadoop environment. | Some cloud experience as a developer/engineer like Google cloud platform | What we offer: | EXL Analytics offers an exciting, fast paced and innovative environment, which brings together a group of sharp and entrepreneurial professionals who are eager to influence business decisions. From your very first day, you get an opportunity to work closely with highly experienced, world class analytics consultants. | You can expect to learn many aspects of businesses that our clients engage in. You will also learn effective teamwork and time-management skills - key aspects for personal and professional growth | Analytics requires different skill sets at different levels within the organization. At EXL Analytics, we invest heavily in training you in all aspects of analytics as well as in leading analytical tools and techniques. | We provide guidance/ coaching to every employee through our mentoring program wherein every junior level employee is assigned a senior level professional as advisors. | Sky is the limit for our team members. The unique experiences gathered at EXL Analytics sets the stage for further growth and development in our company and beyond. | ""EOE/Minorities/Females/Vets/Disabilities"" |  | EEO/Minorities/Females/Vets/Disabilities | Other details | Pay Type Salary",New York NY,Data Engineer- Healthcare Analytics
Gannett,/company/FAR-Consulting/jobs/Data-Engineer-Bi-Developer-02e6873248a8def6?fccid=5d51de09c5b02ce6&vjs=3,"Job detailsSalaryFrom $40 an hourJob TypeFull-timeContractQualificationsUS work authorization (Required)Bachelor's (Preferred)Tableau: 3 years (Preferred)Data analytics: 3 years (Preferred)Business intelligence: 3 years (Preferred)Full Job DescriptionFAR Consulting is seeking a Data Engineer/BI Developer professional to solve complex problems, test innovative approaches and research new solutions to storing and presenting information. We are looking for you to apply your computer science expertise to projects that seek to create new standards for the transformation of information.As a Data Engineer/BI Developer, you will be part of the technology team responsible for data modeling, application development, technical product assistance and tuning to meet performance and functional requirements. The ideal candidate is self-motivated with a proven success managing stakeholders at all levels. The Data Engineer/BI Developer will play a key role in building state-of-the-art business intelligence solutions that deliver business insights and support data-driven decision making.Essential FunctionsDevelop data strategy and data analysisDevelop analytics using power BI, Looker or TableauEmbed analytics into web applications hosted on the cloud (AWS, GCP, Azure)Participate in the design, development and implementation of end-to-end Business Intelligence SolutionsBasic QualificationsBachelor's Degree in Information Systems, Computer Science or Computer Engineering or other related fieldsAt least 2-5 years of hands-on data experienceExperience developing embedded analytics in software applicationsMotivated self-starter enjoying the fast-paced world of analytics developmentSolid interpersonal and communication skills (written and verbal) to technical and non-technical audience of wide variety of levels including client-facing senior managementAnalytical scripting language skills, such as Python, R, SASData Visualization tools: Tableau, Power BI, LookerFamiliarity with the scripting data and machine learning ecosystems - (Jupyter Notebooks, scikit-learn, SciPy, NumPy, pandas, Matplotlib, TensorFlow, etc.)Experience with wide array of analytical approaches (correlation analysis, predictive and explanatory modeling, data mining, unsupervised clustering, analysis of unstructured data)Experience with SQL via exposure to RDBMS’s such as Amazon Redshift, Google Cloud BigQuery, SQL Server, Oracle, Teradata, postgres, etc.Familiarity with cloud technologies Data warehouse (BigQuery, RedShift) / cloud storage (GS, S3) / app engine (GCP App Engine, AWS Elastic Beanstalk)Experience with ""Big Data"" environmentsAbility to gather and document business requirements and translate them to technical requirementsHas investigative mindset and related skills that can be used with any number of tools to explore available dataAbility to self-direct and manage priorities; ability to handle issues that are not well-defined and/or conflict with available information; ability to successfully navigate ambiguity to resolutionPreferred QualificationsMaster's degree in Management Information Systems, Analytics or Data EngineeringProfessional certifications in data technologies (BI tools, cloud technologies)Note: This is a 1099 Contract Position (REMOTE)Job Types: Full-time, ContractPay: From $40.00 per hourSchedule:Monday to FridayEducation:Bachelor's (Preferred)Experience:Tableau: 3 years (Preferred)Data analytics: 3 years (Preferred)Business intelligence: 3 years (Preferred)Contract Renewal:PossibleWork Location:Fully RemoteThis Job Is Ideal for Someone Who Is:Dependable -- more reliable than spontaneousPeople-oriented -- enjoys interacting with people and working on group projectsAdaptable/flexible -- enjoys doing work that requires frequent shifts in directionDetail-oriented -- would rather focus on the details of work than the bigger pictureWork Remotely:YesCOVID-19 Precaution(s):Remote interview process",Remote,Data Engineer/BI Developer
Daybreak IT Solutions,/company/ABC/jobs/Data-Engineer-574d56171abbcfef?fccid=6823712f35e7e312&vjs=3,"Job detailsSalary$95,000 - $110,000 a yearJob TypeFull-timeContractNumber of hires for this role5 to 10Full Job DescriptionPosition:  Data EngineerLocation:  California/ Currently RemoteDuration:  12+monthsResponsibility: · Experience of data modelling and data engineering tools and platforms such as Kafka, Spark, and Hadoop· Built large-scale data pipelines and data-centric applications using any of the distributed storage platforms such as HDFS, S3, NoSQL databases (Hbase, Cassandra, etc.) and any of the distributed processing platforms like Hadoop, Spark, Hive, Oozie, and Airflow in a production setting· Hands on experience in MapR, Cloudera, Hortonworks and/or cloud (AWS EMR, Azure HDInsights, Qubole etc.) based Hadoop distributionsJob Types: Full-time, ContractPay: $95,000.00 - $110,000.00 per yearSchedule:8 hour shiftMonday to FridayFull Time Opportunity:YesWork Location:One locationWork Remotely:YesCOVID-19 Precaution(s):Remote interview process",San Jose CA,Data Engineer
DISH Network,/rc/clk?jk=dcd5cfb4688c819d&fccid=aaf3b433897ea465&vjs=3,"Job Title: Java/Big Data Application Developer About J.P. Morgan Corporate &amp; Investment Bank | J.P. Morgan's Corporate &amp; Investment Bank is a global leader across banking, markets and investor services. The world's most important corporations, governments and institutions entrust us with their business in more than 100 countries. With $18 trillion of assets under custody and $393 billion in deposits, the Corporate &amp; Investment Bank provides strategic advice, raises capital, manages risk and extends liquidity in markets around the world. | Securities Services |  | Securities Services, is a global industry leader with $22 trillion in assets under custody. J.P. Morgan provides innovative custody, fund accounting and administration and securities services to the world's largest institutional investors, alternative asset managers and debt and equity issuers. J.P. Morgan's Securities Services business is comprised of Custody, Accounting, Transfer Agency, Middle Office outsourcing, Foreign Exchange, and other activities related to servicing assets for traditional and alternative funds; it leverages its scale and capabilities in more than 100 markets to help clients optimize efficiency, mitigate risk and enhance revenue through a broad range of investor services as well as securities clearance, collateral management and alternative investment services. | Data Platform Product Development |  | The Securities Services Data Platform team is responsible for formulating and implementing the Data Strategy for the Securities Services business. We set strategic direction, identify and govern Data best practice, manage end-to-end data delivery across asset classes from Systems of Record through to consumers and own the presentation of our data across multiple distribution channels. |  | Job Description |  | Senior, hands-on development experienced in developing high performance real time transactional and operation data store systems. | The candidate will have responsibility for design, implementation and testing of a highly scalable data layer using Java, Hadoop, HBase, Phoenix, HortonWorks, Spark, and related tools. The candidate must be highly proficient in Java with expertise in core java technologies. The candidate will have responsibility for unit-level design, coding, unit testing, integration testing and participating in the full SDLC. The candidate must have a sound grasp of development best practice and system architecture. She/he will be expected to produce high-quality that can pass critical peer review, and to work under a high-pressure and timeline-driven environment. | Qualifications |  | Minimum 5 years' experience building mission-critical enterprise applications, with a proven delivery track record. | Minimum 5 years' core Java experience | Minimum 2 years implementing Big Data tools (HBase, Phoenix, Hortonworks/Cloudera, Hadoop) in a production environment (proof of concept and lab work do not quality). | Bachelor of Science in Computer Science or equivalent degree. | Demonstrates exceptional analytical and problem-solving skills. | Strong communication, organizational, and collaboration skills. | Ability to follow complex design and development standards. | Experience working in multi-time zone development team. | Experience with Agile development. | Deep understanding of Java, data structures, concurrency and multi-threading, run time VM, garbage collection, web-based request handling and distributed architecture. | Expert OO analysis and design skills. | Experience building high volume systems with real-time performance and read and write capabilities. | Experience with real-time, event-driven systems and service-oriented architectures. | Experience with enterprise messaging systems such as Kafka, including message queues and pub/sub.JPMorgan Chase &amp; Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management. | We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs. | Equal Opportunity Employer/Disability/Veterans",Jersey City NJ,Java/Big Data Software Engineer
IBM,/rc/clk?jk=6cdc0a2f3abde223&fccid=1230acb7e56c6df5&vjs=3,"Data Engineer | Cambridge, MA, USA  Virtual Req #12191 |  Wednesday, November 4, 2020 | Gannett Co., Inc. (NYSE: GCI) is an innovative, digitally focused media and marketing solutions company committed to strengthening communities across our network. With an unmatched reach at the national and local level, Gannett touches the lives of nearly 140 million people monthly with our Pulitzer-Prize winning content, consumer experiences and benefits, and advertiser products and services. Gannett brands include the USA TODAY and more than 260 daily local newspaper brands, digital marketing services companies ReachLocal, WordStream, and ThriveHive and U.K. media company Newsquest. There's never been a better time to join our talented team. Visit Gannett.com/Careers to learn more about our opportunities, and visit Gannett Privacy Policy to understand our privacy policy. | About Reviewed | Buying stuff is easy. Buying the right thing is hard. That’s where we can help. Reviewed, a part of the USA TODAY Network, brings consumers the most scientific and trustworthy reviews available, written by our team of experts. We believe that tough, objective reviews are the best way to analyze products. We provide readers with insightful, practical, and entertaining stories that help them make informed decisions. |  | As a Data Engineer, you will be responsible for building and maintaining the data pipelines and creating a data visualization strategy. In addition, you will work closely with other members of the data team and organization to build out our holistic data platform and help use data to drive company-wide decisions. | Reviewed is based in Cambridge, MA. This role can be in the office or remote, with a preference for candidates able to work in the Eastern time zone. | Responsibilities: |  Create data models and processes for measuring affiliate performance, advertising, and product development efforts. Develop and produce KPI dashboard using APIs to pull data from Google Ads, affiliate platforms, Google Analytics, and more. Automate daily, weekly, and monthly reports, and forecasts for core KPIs for all traffic sources. Work closely with the product engineering team to make sure all new features are properly capturing data. Define best practices and new processes to collect, analyze, and manage data. |  Troubleshoot data discrepancies; resolve and articulate them. |  | Requirements: |  Bachelor's or master's degree in Computer Science, Data Science, Engineering, MIS or related technical field or equivalent combination of education and experience. You have 2+ years working as a Data Engineer. Expertise in JavaScript, Python, Java or another high-level programming language. Experience building and maintaining data pipelines. Experience working with SEM, affiliate marketing platforms, and web analytics. Mastery of SQL. You have a working knowledge of BI tools like Google Data Studio and Tableau. You have a logical, problem-solving mentality. You have a strong understanding of data modeling concepts and methodologies. Strong communication skills and a proven ability to communicate not just raw data but what the data means to various non-technical audiences. You are creative, flexible, eager to learn, and able to work productively in a multi-project, deadline-driven environment. Experience in the digital media industry is a strong plus. |  #content | Gannett Co., Inc. is a proud equal opportunity employer. We are a drug free, EEO employer committed to a diverse workforce. We will consider all qualified candidates regardless of race, color, national origin, sex, age, marital status, personal appearance, sexual orientation, gender identity, family responsibilities, disability, education, political affiliation, or veteran status. | Other details | Job Family Business Intelligence Job Function General Administration Pay Type Salary",Remote,Data Engineer
JPMorgan Chase Bank N.A.,/company/Primesoftinc/jobs/Senior-Data-Engineer-df117c9ed78085ab?fccid=0540b5b387d37378&vjs=3,"Job detailsJob TypeFull-timePart-timeContractNumber of hires for this role2 to 4QualificationsAzure: 5 years (Preferred)Full Job DescriptionRole: Data EngineerExp: 10+ YearsLocation: USA/CanadaVisa: Other than Opt’sMOI: Phone + SkypeSkills Required: *Extensive work experience in SQL and JAVA SCRIPT (Must Have)*Working Experience in SnowFlake (Must have)*Working Experience in Azure Data Factory (Nice to have)*Asset Management experience (Nice to have)*Ability to Apply Snowflake best practices to Snowflake Data Warehouse. Strong development background on snowflake stored procedures using java script, SQL, data modeling, Snow flake DB setup, configuration and deployment*Hands-on experience with Azure cloud data services*Implementation experience with column-oriented database technologies, NoSQL database technologies (i.e. Cosmos DB) and traditional database systems (i.e. SQL Server, MySQL)*Experience in implementing data pipelines for both streaming and batch integrations using tools/frameworks like Azure Data Factory, Spark, Spark Streaming and python scripting etc.*Ability to handle module or track level responsibilities and contributing to tasks “hands-on” experience in data modeling, warehouse design and fact/dimension implementations*Experience working with code repositories and continuous integrationRegardsSundeep.B732-790-5650sundeep.b (at) primesoftinc (dotcom)Job Types: Full-time, Part-time, ContractSchedule:8 hour shiftExperience:Azure: 5 years (Preferred)Total IT: 10 years (Preferred)Snowflake: 5 years (Preferred)Work Remotely:YesCOVID-19 Precaution(s):Remote interview process",California City CA,Senior Data Engineer (Rem0te)
Sun Energy Services Llc,/rc/clk?jk=36687f0a09967fef&fccid=3c371b712ace44cf&vjs=3,"Job detailsJob TypeFull-timeContractFull Job DescriptionDepartment Summary: |  | DISH is a Fortune 250 company with more than $13 billion in annual revenue that continues to redefine the communications industry. Our legacy is innovation and a willingness to challenge the status quo, including reinventing ourselves. We disrupted the pay-TV industry in the mid-90s with the launch of the DISH satellite TV service, taking on some of the largest U.S. corporations in the process, and grew to be the fourth-largest pay-TV provider. We are doing it again with the first live, internet-delivered TV service – Sling TV – that bucks traditional pay-TV norms and gives consumers a truly new way to access and watch television. | Now we have our sights set on upending the wireless industry and unseating the entrenched incumbent carriers! | We are driven by curiosity, pride, adventure, and a desire to win – it’s in our DNA. We’re looking for people with boundless energy, intelligence, and an overwhelming need to achieve to join our team as we embark on the next chapter of our story. | Opportunity is here. We are DISH. | Job Duties and Responsibilities: |  | We are looking for a data engineer to help us maintain and implement our machine learning initiatives. The position entails working within an agile development team to build big data processing and transformation pipelines. It will also involve training, implementing, and deploying machine learning models. The position will also include utilizing cloud technologies to provide an optimal machine learning solution. | Skills, Experience and Requirements: |  | The successful data engineer will have: | Bachelor's or master's degree, preferably in computer science, computer engineering, or other technical (STEM) field, or equivalent combination of education and experience. | 3+ years of big data/machine learning experience | 2+ years of Python experience |  Preferred qualifications: | Java | AWS | MySQL | Elasticsearch | Nice to have: | EMR | Docker/Kubernetes | Sagemaker | TensorFlow | Kafka",Englewood CO 80112,Data Engineer
Accenture,/rc/clk?jk=84dee327aabe373b&fccid=2e5a195d27f5250c&vjs=3,"Who We Are: |  | We reward shoppers for digitizing their shopping experience. | Our mission is to delight the world's shoppers with a free smartphone app that is easy, smart and fun. |  | Why Join the Fetch Family? |  | We make it better for users even when that's difficult for us | We empower people with information and trust | We challenge ideas, not people | We think bigger and keep building | We find ways to bring the fun to Fetch! |  | We're committed to building an empowered and inclusive community of innovative and passionate people. As a growing organization, we need team players who can go above and beyond their individual responsibilities to help our company build towards its vision. If you are a creative, hard-working, and fun-seeking person interested in working with a close-knit group of highly talented people, this is the right place for you. |  | Fetch Rewards is an equal employment opportunity employer. |  | The Role: | REQUIRED: Python programming skills | Solid SQL skills | Familiarity with Unix systems, shell scripting, and Git | Experience with relational (SQL), non-relational (NoSQL), and/or object data stores (e.g., Snowflake, MongoDB, S3, HDFS, Postgres, Redis, DynamoDB) | Interest in building and experimenting with different tools and tech, and sharing your learnings with the broader organization | The desire to work with other teams in the organization (e.g., Development, Business Intelligence, Data Science) to build tools and solutions that support and help manage data within the Fetch ecosystem | Bachelor's degree in Computer Science (or equivalent) | At least 3 years of relevant full-time work experience | Bonus points for: | Excellent written and verbal communication skills | Familiarity with open source software and dependency management | ETL process, data pipeline, and/or micro-service development experience | Cloud engineering and DevOps skills (e.g., AWS, CloudFormation, Docker) | Familiarity with messaging and asynchronous technologies (e.g., SQS, Kinesis, RabbitMQ, Kafka) | Big data development skills (e.g., Spark, Hadoop, MPP DW) | Experience with visualization tools (e.g., Tableau) | Love of Dogs! . . . Or just tolerance. We're a very canine-friendly workplace",Illinois,Data Engineer
Inspire,/rc/clk?jk=64dc4eb8db33e1b8&fccid=adea62ae96d0c2c1&vjs=3,"The Opportunity: |  | We are seeking a Data Engineer to design, develop, deploy, and maintain a sophisticated data platform and data pipelines that collects, ingests, cleans, transforms, and maps data from various sources | This position will be eventually based in our Yonkers, New York office so only candidates who currently reside in the tri-state area will be considered for this role. |  | What you will do: | Design, build, operationalize, and maintain the data platform. | Develop and document data architecture artifacts - data dictionaries, data flows, data models (Conceptual / Logical / Physical) etc. Good judgment will be required to balance various factors (ACID, CAP, etc.) in database design | Develop data pipelines needed to integrate variety of internal and external data sources through efficient ETLs | Develop and manage ELT processes for application data processing. | Work with other software engineers, database developers, infrastructure, operations ,QA and other cross functional teams to design and implement data solutions | About You: | You have a Bachelor’s in CS, IT or other related field or equivalent work experience in addition to 3 years in backend development, with a focus on Data Engineering | You have 3 years of experience with SQL,Python, and Unix Shell Scripting | You are skilled in building large scale data warehouses and data lakes (Snowflake), Data Modeling (Relational, Dimensional etc), developing ETL &amp; ELT processes and complex data migration strategies | You have experience working with Cloud Technologies (AWS) - Specifically services used in Big Data ecosystem (AWS Glue, Step Function, Lambdas, S3 , EC2 etc) | Big Bonus Points For | Exposure to Tableau or data visualization tools | Exposure to Business Process Management suites. (Appian etc) | Diversity, Inclusion, and Belonging at Consumer Reports: At CR, we believe our continued ability to understand and advance the interests of all American consumers is possible only if our staff fully reflects the full cultural, racial and ethnic diversity of those consumers. Consumer Reports is committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, national origin, gender, sex, gender identity or expression, sexual orientation, age, citizenship, marital or parental status, disability, veteran status, or other class protected by applicable law. We are proud to be an equal opportunity workplace.",Yonkers NY 10703,Data Engineer
Consumer Reports,/rc/clk?jk=5513ab470944211f&fccid=aac3cf2de368e654&vjs=3,"Role &amp; Responsibilities: | Have all round experience in developing and delivering large-scale business applications in scale-up systems as well as scale-out distributed systems. | Responsible for design and development of application on Big data platform. | Should implement complex algorithms in a scalable fashion. Core data processing skills are highly important with tools like HIVE/Impala | Should be able to write MapReduce jobs or Spark jobs for implementation. Ability to write Java-based middle layer orchestration between various components on Hadoop/spark stack. | Work closely with product and Analytic managers, user interaction designers, and other software engineers to develop new offerings and improve existing ones. |  |  | Desired Skills &amp; Experience: | Technical | B.Tech or Master’s degree from a reputed university in Computer Science or equivalent disciplines. | 1-3 years’ experience building software or web applications with object-oriented or functional programming languages. Doesn’t matter what language, just a focus on writing clean, well designed and scalable code on MapReduce. | Experience with Big Data technologies such as Hadoop, Hive, Spark, or Storm | Experience with streaming technologies like Kafka, Spark, and Flink | Experience with scalable systems, large-scale data processing, and ETL pipelines | Experience with SQL and relational databases such as Postgres or MySQL | Experience with NoSQL databases like Dynamo DB, Cloud Search, or open source variants like Cassandra, HBase, Solr, or Elastic Search | Experience with DevOps tools (Git Hub, Travis CI, and JIRA) and methodologies (Lean, Agile, Scrum, Test Driven Development) | Experience building and deploying applications on on-premise and AWS cloud-based infrastructure. |  |  | To apply, send your cv at hr@valiancesolutions.com",New York NY 10019,Big Data Engineer
KPMG,/company/Able-Infotek/jobs/Data-Engineer-b53bc911a225484f?fccid=6738eb22b90d4777&vjs=3,"Job detailsJob TypeFull-timeContractFull Job DescriptionTitle : Data EngineerEmployment Type: ContractLocation :  Austin,TX******US Citizens/GC are encouraged to apply. We are unable to sponsor at this time *****Job Description: -Key Qualifications:Strong skills in SQL, Java and/or PythonExperience with Apache Big Data Frameworks (Hadoop, Spark, Hive)Familiarity with workflow scheduling/orchestration tools (Oozie, Jenkins)Experience with Docker and Kubernetes preferredExperience with AWS a bonusStrong documentation and technical writing skillsAttention to detail and excellent communication skillsDescriptionAs a Data Engineer, your day-to-day tasks will include:Helping us leverage large-scale data stores by building out ETL pipelines and utilities in Spark and HiveDeveloping robust, low latency, and fault-tolerant pipelines to support business-critical systemsAggregating key metrics for business partners to inform key decisionsWorking with cloud technologies to deploy your applicationsEnvironmentCan work effectively on a small and nimble team, no trouble context-switchingEducationB.S./M.S. in Computer Science or Computer EngineeringJob Types: Full-time, ContractPay: $0.00 per hourSchedule:Monday to FridayWork Location:One locationWork Remotely:No",Austin TX 78758,Data Engineer
NBCUniversal,/rc/clk?jk=5d88b343e4789372&fccid=2dd390c3a48a7ed0&vjs=3,"The KPMG Advisory practice is currently our fastest growing practice. We are seeing tremendous client demand, and looking forward we don't anticipate that slowing down. In this ever-changing market environment, our professionals must be adaptable and thrive in a collaborative, team-driven culture. At KPMG, our people are our number one priority. With a wealth of learning and career development opportunities, a world-class training facility and leading market tools, we make sure our people continue to grow both professionally and personally. If you're looking for a firm with a strong team connection where you can be your whole self, have an impact, advance your skills, deepen your experiences, and have the flexibility and access to constantly find new areas of inspiration and expand your capabilities, then consider a career in Advisory. | KPMG is currently seeking a Director in our Deal Advisory practice. | Responsibilities: | Assess, capture, and translate complex business issues and solution requirements into structured technical tasks for the data engineering team, including rapid learning of industry standards and development of effective work stream plans | Manage the development of big data analytics and cloud-based applications to facilitate the work of data scientists and application developers | Design, build, launch, optimize, and extend full-stack data and business intelligence solutions spanning extraction, storage, complex transformation, and visualization layers | Support build of big data environments that enable analytics solutions on a variety of big data platforms, including assessing the usefulness of new technologies and advocating for their adoption | Serve as the primary point of contact for question and escalations related to data architecture and solutions applicable to business problems or use cases | Qualifications: | Minimum of eight years of experience working as a data engineer | Bachelor's degree from an accredited college/university in a numerate subject (Computer Science, Mathematics, Electrical Engineering, Statistics, or Science) or equivalent practical experience | Experience creating and maintaining ETL/ELT pipelines that operate on a variety of sources, such as APIs, FTP sites, cloud-based blob stores, databases (relational and non-relational), unstructured data, and GIS data | Experience performing operational programming tasks, such as version control, CI/CD, testing and quality assurance and within cloud platforms such as AWS, Azure, or GCP with preferred experience in Azure | Experience with Apache data projects (Hadoop, Spark, Hive, Airflow), or cloud platform equivalents (Databricks, Azure Data Lake Storage, Azure Data Factory) | Experienced in one or more of the following programming languages: Python, Scala, R, Java, Golang, Kotlin, C, or C++ and with SDLC methodologies, particularly Agile, and project management tools, preferably Azure DevOp | KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-equal opportunity employer. KPMG complies with all applicable federal, state and local laws regarding recruitment and hiring. All qualified applicants are considered for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and hiring laws. No phone calls or agencies please.",New York NY,Specialist - Director Data Engineer
Performix,/rc/clk?jk=3b1f172ab527c63e&fccid=b9ca7ec1464dfa21&vjs=3,"Job detailsSalaryUp to $124,000 a yearFull Job DescriptionHomesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations. | One thing that's stayed the same since our founding: our commitment to our customers, partners and employees. | Join us on our journey as we continue to grow into a powerful contender in the field of insurance. | Compensation may vary based on the job level and your geographic work location. | Compensation Minimum:$100,000 | Compensation Maximum:$124,000 | Senior Digital Data Strategist / Senior Product Data Engineer II |  | Focus Areas: Data Warehousing, Data pipelining, Data architecture, Data Transformation, Data Analytics, Data Consultation |  |  |  | Are you excited about creating innovative data products customer centric advanced analytics tools? Are you able to lead and leverage cutting edge product data, design thinking and innovative technologies? Are you passionate about driving data driven business decisions through product, human-centered experiences and innovative technologies? |  |  | If so, we would love to speak to you about working for a premier insurance company. We are seeking a data and human-centered experience leader with a desire to lead and learn from others and who knows how to execute and deliver on big ideas. As part of the digital products insights and integration team, we build scalable data solutions to enable insight generation in addition to providing analytical support for flagship initiatives within the enterprise. Example data solutions include 1) Question Set Optimization: a tool which enables business and product leaders identify the optimal balance of questions a user answers on our online insurance quoting platforms and has direct impact on conversion. You will work with a team focused on helping customers, internal stakeholders and business partners build compelling data driven business cases to grow, compete, innovate and lead new products and human-centered experiences in the insurance marketplace. With your leadership, data expertise, product and human-centered experience acumen, you and your team will help drive maximum value from our enterprise’s investments and determine, measure and ensure our enterprise’s product data strategy and human-centered experiences are fully realized. |  |  | As a Senior Product Data Engineer at Homesite, you will implement the product data strategy, lead product data engineering efforts, and data technologies that will power the future of insurance. You do not shy away from new challenges. You are intensely focused on identifying and resolving analytics bottlenecks through deep data knowledge. You are willing to take risks and face setbacks along the way. You thrive in a fast-paced and results-oriented environment. You are data and design-oriented, and you think in terms of Product intelligence, ROI and impact. You quickly establish relationships with key stakeholders such as Sales, Business Development, Operations and Technology partners to build innovative internal analytics products, develop meaningful KPIs and provide actionable data driven insights. You will be an educator and translator across business and technology partners to ensure teams have a shared understanding and mindset of both the true business needs and the technical possibilities available to solve them. You will support the team in defining the product data strategy and help to deliver product and human-centered experiences that increase acquisition and loyalty, improve productivity and accelerate delivery of our insurance products across channels. | Role and Responsibilities: | Develop and manage technical product architecture, design and implementation of big data platforms and business analytics solutions to empower product management to solve their data driven analytics and reporting needs. | Contribute to the design and implementation of homesite’s first digital product data warehouse / data lake infrastructure using AWS big data stack, Python, Redshift, Athena, SQL etc. and take ownership of critical reporting/insights development. | Drive better decision making by creating decision support tools that range from BI dashboards to interactive applications | Manage ETLs to source data from various commercial, sales and operational systems and create unified data model for analytics and reporting | Provide oversight of data delivery and management through the complete lifecycle from collection through data retirement, coordinating with business, data science, analytics, and reporting users to understand their data needs, and coordinates the delivery of those requirements in the enterprise data lake and data warehouse | Work with integration developers to provide guidance for data movement and mapping into and within the enterprise data lake and data warehouse. | Facilitate fact-finding interviews and brainstorming workshops with cross-functional business partners, and data users to identify analytics bottle necks | Collaborate closely with product, partner, analytics and engineering teams to ship and iterate internal tools (including crafting data strategies and specs, driving alignment and reviews, managing in-flight work, and understanding outcomes) | Create analytics tools that drive product integration and work collaboratively to innovatively transform the customer and agent experiences guiding long-term product strategies | Collaborate and communicate across the ecosystem including data science, analytics, design, distribution and more to ensure the team is positioned as the best in class analytics service provider to partners | Create value for customers and agents while keeping the Product P&amp;L top of mind | Seek new perspectives and challenge what is possible | Pose the right questions and share best practices to allow partners to develop long-term strategies | Speak up, be candid and authentic | Break barriers and connect the dots across the business and technology | Flexible, agile and comfortable with ambiguity | Motivate and can lead the team to do the best work of their careers | Required Qualifications: | Degree in Analytics, Data Science. Computer Science, Engineering, Mathematics, or a related field with 8+ years industry experience in enterprise technologies | 5+ years of experience in data architecture and 3+ years of experience in cloud technologies | Have advanced skills and experience working with the following technologies: AWS Redshift, AWS Athena, AWS LAMBDA, ETL processes and tools, SQL, Python (Data transformation), MS SQL, Tableau and/or R shiny, Streamlit.io, Python and/or R | Should be skilled in the architecture of data warehouse solutions for Enterprise using multiple platforms (EMR, RDBMS, Columnar, Cloud) – AWS LAMBDA, Athena, API gateways. | Data modeling, extensive experience with SQL, Python (preferable), Scala or Java and exposure to cloud computing (AWS, Azure or Google, Google Cloud preferable). | Experience with at least one relational database technology such as Redshift, Oracle, MySQL or MS SQL | Experience working on at least one parallel processing data technology such as Redshift, Teradata, Netezza, Spark or Hadoop based big data solution | Proficiency in at least one programming language (e.g., Python (preferred, Java, Scala) for data transformation and rock-solid SQL skills | ETL Processes &amp; Tools knowledge (e.g., SSIS, Talend); OLAP technologies | Experience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologies | Experience building data products incrementally, and integrating and managing datasets from multiple sources | Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets | Preferred Qualifications: | Proven ability to recognize structural issues within the organization, functional interdependencies and cross-silo redundancies | Master's degree preferred | Insurance experience preferred | Proficiency in at least one modern programming language (Python , Ruby, Java, etc.) | Experience providing technical leadership and mentor others on the data engineering space | Linux/UNIX including to process large data sets/or bash | Experience with AWS Stack | Prior experience and knowledge in writing complex queries and SQL Performance tuning. | Experience of developing visualizations using at least two reporting tools such as Tableau, Streamlit etc. | Google analytics knowledge |  Any knowledge/skill on Data Science, AI/ML will be plu s | Insurance Industry Experience | #LI-Remote | When you work at Homesite you can expect benefits that support your physical, emotional, and financial wellbeing. You will have access to comprehensive medical, dental, vision and wellbeing benefits that enable you to take care of your health. We also offer a competitive 401(k) contribution, a pension plan, an annual incentive, and a paid-time off program. In addition, our student loan repayment program and paid-family leave are available to support our employees and their families. Interns at Homesite are eligible for the paid time off program. Contingent workers are not eligible for American Family Enterprise benefits. | Stay connected: Join Our Enterprise Talent Community !",Remote,Senior Product Data Engineer II
Transcendent Solutions LLC,/rc/clk?jk=1d9af0a3a2136a06&fccid=5987b21ba94af195&vjs=3,"Description: |  | The Mission of the Role | Transcendent is looking for a Data &amp; Reporting Engineer to be a key member of the Data &amp; Reporting team. Performing a critical role in agile software development, this team member will focus on data acquisition, transformation, &amp; visualization. Working closely with the customer and the development team, the Data &amp; Reporting Engineer will ensure we are providing our internal &amp; external customers with high-quality actionable insights and features. | Responsibilities | Work as a member of an Agile development team following Scrum software development methodologies | Extract, Transform, and Load data from a transactional database to a dimensional Data Warehouse | Works with the Reporting Team to design &amp; architect tables within a Data Warehouse | Create and perform MS SQL queries for a variety of reporting requirements | Develop, implement, &amp; optimize stored procedures, views, and functions using T-SQL | Work with complex datasets to investigate key business trends, performance, issues, &amp; behaviors | Suggest new queries to support business intelligence analysis | Investigate, mitigate, and prevent exceptions in reporting output | Design of dashboards and report layouts | Map existing or new queries to reports | Personal Characteristics | Analytical – finds meaning and action in numbers, and is a data-driven decision-maker | Self-starter, open to and proactive in learning new skills | Excellent communicator – written and oral | Stellar time management and organizational skills | Resourceful – figures out how to achieve results creatively with finite resources | Strong attention to detail | Thrives in a nimble, lean, fast-paced startup environment | Works collaboratively and energetically | Impeccable integrity and ethical standards | Self-aware, accepting, honest, open, and respectful of others | Ability to create and execute on processes for a growing SaaS startup using best practices | An undying commitment to personal growth and development | . Requirements: |  | Experience | Development Languages / Tools | Microsoft SQL Server (3+ years) | T-SQL | Writing and tuning Stored Procedures, Functions, and Views | Data Warehousing experience (1+ years) | Star Schema – Fact &amp; Dimension tables | ETL process (3+ years) | SSIS and/or DBT preferred | Business Intelligence Tools | Sisense preferred | Report Development (3+ years) | Multiple data formats | JSON | CSV | XML | API | Working with large datasets; multiple terabyte databases | Agile Scrum methodologies | An understanding of the software development lifecycle, application design, user interface design, functional design, technical characteristics of relational databases and client server systems | Ability to create technical documentation | Experience working with Product Owners, Business Analysts and Project Managers to plan and execute software development projects to timely and successful completion | Navigate through a web and mobile user interface | Microsoft Office (3+ years)",Oldsmar FL 34677,Data & Reporting Engineer
Homesite Insurance,/rc/clk?jk=1372f50772270903&fccid=510aa29fcf8f87d9&vjs=3,"Provides advanced data solutions by using software to process, store, and serve data to others. Tests data quality and optimizes data availability. Ensures that data pipelines are scalable, repeatable, and secure. Utilizes a deep dive analytical skillset on a variety of internal and external data. | Core Responsibilities | Writes ETL (Extract / Transform / Load) processes, designs database systems, and develops tools for real-time and offline analytic processing. | Troubleshoots software and processes for data consistency and integrity. Integrates large scale data from a variety of sources for business partners to generate insight and make decisions. | Translates business specifications into design specifications and code. Responsible for writing complex programs, ad hoc queries, and reports. Ensures that all code is well structured, includes sufficient documentation, and is easy to maintain and reuse. | Partners with internal clients to gain an enhanced understanding of business functions and informational needs. Gains expertise in tools, technologies, and applications/databases in specific business areas and company-wide systems. | Leads all phases of solution development. Explains technical considerations at related meetings, including those with internal clients and less experienced team members. | Tests code thoroughly for accuracy of intended purpose. Reviews end product with the client to ensure adequate understanding. Provides data analysis guidance as required. | Designs and conducts training sessions on tools and data sources used by the team and self provisioners. Provides job aids to team members and business users. | Tests and implements new software releases through regression testing. Identifies issues and engages with vendors to resolve and elevate software into production. | Participates in special projects and performs other duties as assigned. |  | Qualifications | Minimum of five years data analytics, programming, database administration, or data management experience. | Undergraduate degree or equivalent combination of training and experience. | About Vanguard | We are Vanguard. Together, we’re changing the way the world invests. | For us, investing doesn’t just end in value. It starts with values. Because when you invest with courage, when you invest with clarity, and when you invest with care, you can get so much more in return. We invest with purpose – and that’s how we’ve become a global market leader. Here, we grow by doing the right thing for the people we serve. And so can you. | We want to make success accessible to everyone. This is our opportunity. Let’s make it count. | Inclusion Statement | Vanguard’s continued commitment to diversity and inclusion is firmly rooted in our culture. Every decision we make to best serve our clients, crew (internally employees are referred to as crew), and communities is guided by one simple statement: “Do the right thing.” | We believe that a critical aspect of doing the right thing requires building diverse, inclusive, and highly effective teams of individuals who are as unique as the clients they serve. We empower our crew to contribute their distinct strengths to achieving Vanguard’s core purpose through our values. | When all crew members feel valued and included, our ability to collaborate and innovate is amplified, and we are united in delivering on Vanguard's core purpose. | Our core purpose: To take a stand for all investors, to treat them fairly, and to give them the best chance for investment success.",Malvern PA,Data Engineer
Fetch Rewards,/rc/clk?jk=5ebbb8d8f4f206b9&fccid=bd7c229675b4b45a&vjs=3,"Role Overview: |  | The Data Engineer supports the implementation of Signant Health's products and services to meet customer’s needs. This position is responsible for working with other data management staff and internal project team members to ensure on time and accurate deliverables; implementing assigned programming/analytical tasks; performing unit testing; documenting his or her work according to accepted quality principles; and supporting testing. | Key Accountabilities/Decision Making &amp; Influence: |  | KEY ACCOUNTABILITIES | Assume responsibility for the design, development and on-time delivery of project specific configurations and customizations | Work directly with the external and internal clients to prepare, adapt, or agree on all specifications | Create and document system design specifications and other technical documents as required | Understand and follow all coding standards | Complete unit testing and peer review documentation as required | Support test script development and performance of user acceptance testing | Support all phases of testing by efficiently diagnosing and resolving defects | Recommend, design and implement on-going application and architectural improvements |  Performs all work in accordance with documented Standard Operating Procedures (SOPs), Working Instructions. | Adheres to Good Clinical Practices (GCP), 21 CFR Part 11 and other regulatory requirements as required. | Diversity and Inclusion Competencies: | Dedication and commitment to promote diversity, multiculturalism and inclusion in all work activities | Ability to collaborate in diverse teams to foster productive outcomes. |  | Knowledge, Skills &amp; Attributes: | 2-4 years of professional experience programming | Experience with some or all of the following technologies: | Microsoft SQL Server2008/2012/2017 MySQL or other relational databases including stored procedures, views and triggers | Microsoft SQL Server Reporting Services or other reporting tool | Team Foundation Server, Visual Source Safe, Subversion or other source control product | Experience developing, enhancing and customizing configurable applications is desirable | Domain experience in any of the following is desirable: | Mobile device applications include Smartphone and Tablet | Electronic Data Capture applications | Clinical Trial Management Systems | Experience estimating development and support tasks | Familiarity with 21 CFR Part 11 or experience in a regulated environment desirable | Â : |  | At Signant Health, accepting difference isn’t enough—we celebrate it, we support it, and we nurture it for the benefit of our team members, our clients and our community. Signant Health is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or veteran status.",Blue Bell PA 19422,Data Engineer
Aruba Networks,/rc/clk?jk=477cd20d9e24c5fb&fccid=216eb700022de6f6&vjs=3,"Hewlett Packard Enterprise advances the way people live and work. We bring together the brightest minds to create breakthrough technology solutions, helping our customers make their mark on the world. | Aruba is redefining the “IT EDGE”. Creating new customer experiences by building intelligent spaces and digital workspaces. We are focused on campus, branch, mobility and the IoT to transform business models with the combined power of compute, context, control and secure connectivity. | As a Consulting Systems Engineer – Data Center Networking, the incumbent will be responsible for managing pre-sales technical &amp; functional support to sales teams, partners, prospective clients and customers while ensuring customer satisfaction. With a proven track record of successful sales support activity, the Consulting Systems Engineer will present and articulate the capabilities and values of an HPE Aruba Solution versus that of our competitors. The incumbent will work with HPE Aruba’s Territory Managers, Systems Engineers and Sales Leadership to qualify opportunities and convert leads into successful engagements. | Roles and Responsibilities: | Possess expert competitive knowledge including solutions, technology, and product offerings of HPE Aruba | Perform an expert level of technical presentations for customers, partners, and potential prospects. | Actively practice and disseminate HPE Aruba SE culture in your territory. | Provide Technical Interviews and assessment to the Systems Engineering Manager &amp; Operations Director in the process of SE recruiting and hiring. | Provide input to the Systems Engineering Manager(s) with the yearly SE Focal Review process and associated tasks. | Provide training as Subject Matter Expert (SME) to assist with SE new hire on-boarding and mentor new hire SE’s in your territory. | Active participant in recurring WW CSE meetings at headquarters. | Active participant in recurring CSE team calls with the SE Director. | CSEs primary engagements should be strategic in nature &amp; reserved for our largest customers and prospects (i.e. Page 2 new opportunities &amp; existing Page 3 / Page 4 Accounts). | Own and assist with winning strategic Page 2-3-4 deals and building ongoing customer technical relationships. | Develop and build strategic corporate relationships with Aruba TAC, HPE TS, Engineering, ACE, Product Management, and Technical Marketing. | Provide input to the Product Management team(s) with input on customer needs/requirements, new features requested (RFEs) and overall product functionality including identification of competitive gaps. | Qualifications and Education Requirements: | Typically requires a minimum of 8 years of related experience with a Bachelor’s degree; or 6 years and a | Master’s degree; or equivalent experience. | Minimum 8 years of infrastructure engineering and 4+ years’ experience with successful pre-sales support. | Expert level oral &amp; written communication, organizational, presentation, prioritization, and interpersonal skills. | Must be customer focused and have the ability to design and manage solutions to complex technical issues using the full range of HPE Aruba products and solutions. | Ability to interface with senior levels of internal / external organizations is required. | Expert level knowledge with routing (BGP, EBPG, OSPF, etc) | Expert level knowledge with core switching | Deep knowledge with network and server virtualization. | CCIE, CWNE, ACMX and/or ACDX expert level certifications are preferred. | Join us and make your mark! | We offer:A competitive salary and extensive social benefitsDiverse and dynamic work environmentWork-life balance and support for career developmentAn amazing life inside the element! Want to know more about it? | Then let’s stay connected! | https://www.facebook.com/HPECareers | https://twitter.com/HPE_Careers | HPE is an Equal Employment Opportunity/ Veterans/Disabled/LGBT and Affirmative Action employer. We are committed to diversity and building a team that represents a variety of backgrounds, perspectives, and skills. We do not discriminate and all decisions we make are made on the basis of qualifications, merit, and business need. Our goal is to be one global diverse team that is representative of our customers, in an inclusive environment where we can continue to innovate and grow together. | 1079061",New York NY,Consulting Systems Engineer – Data Center Networking
Signant Health,/rc/clk?jk=6ff9f78e64f7b451&fccid=0f9b540b37a1726e&vjs=3,"POSITION SUMMARY |  | As a Data Engineer on Inspire's Data Platforms and Services team, you will build, maintain, and improve the infrastructure and architecture that flows critical data from internal and external sources to where it creates value for the business. We take a product-driven, agile approach to our platform, driving measurable growth and meaningful outcomes every single sprint. We build efficient, scalable processes in a service-oriented ecosystem leveraging powerful code frameworks and repeatable patterns to solve real problems for stakeholders and customers. |  | THE DATA ENGINEER HAS FOUR MAIN RESPONSIBILITIES |  | Data querying and processing in SQL | Data processing and task management in Python | Communication skills, and ability to translate between the domains of business problems and technical implementations | Team-oriented development: building modular &amp; re-usable tools, writing maintainable code, owning technical and business documentation |  | SOME YEAR 1 DELIVERABLES |  | Refactor operating model code into scalable, transparent processes leveraging Airflow and DBT as core frameworks | Expand the capabilities of Inspire's core data platform to support incremental product lines and product features | Partner with Analytics to systematize and scale high-integrity value-oriented analysis | Partner with Sales, Operations, and other business stakeholders to design and deliver new data-driven integrations | Partner with other engineering teams to guide refactors of existing data infrastructure to improve data quality and features. |  | SUCCESS METRICS |  | Cultivated familiarity with Inspire's frameworks &amp; operating model | Delivery of high-quality pull requests in DBT and Airflow, evidencing strong code standards &amp; testing practices | Comfort with self-directed project management: requires minimal oversight to assess a problem, formulate a solution, deliver code, and document changes. | Positive interactions with department stakeholders: can offer guidance and input that creates business value for non-technical personnel. |  | DESIRED TRAITS |  | Technical competency - comfort on a command line, a good grasp on the fundamentals of programming, a general understanding of Git/source control, and a willingness to read the docs, search stack overflow, and test it until it works | Problem-Solving Mentality - gets excited about digging into complexity, wants to ask questions and learn more, and isn't put off by problems they've never been explicitly told how to solve. Especially troubleshooting: ability to break down a chain of steps to narrow and locate a problem. | Number Sense - Strong background in mathematics or physics, comfort with quantitative measurement and estimation. Ability to work in establishing boundaries and orders-of-magnitude to make informed judgements without fussing over exactitude. | Big-picture awareness - Understanding of the importance of context, and willingness to understand the business problem in addition to the technical one. Focus on people &amp; impact. | EXPERIENCE |  | Must Have | 1 or more years in a data analytics, engineering or science role | Strong SQL experience working with large datasets, ideally in cloud-based data warehouses | Software development in Python3 | Experience automating data processing, cleaning and/or preparation | Software development lifecycle familiarity in GitHub (ie environment management, testing, deployment) | Nice to Have | Experience with key frameworks: Snowflake, Apache Airflow, dbt, AWS services, Docker, Kubernetes | Experience at a similar scale of data processing (Multi-TB/billions of rows) | Work with real-time event stream data | Contextual work in the energy industry | Data consultancy experience a plus | Proven ability to break down a chain of steps to narrow and locate a problem | Strategic approach to problem solving and understand the why behind a problem",Philadelphia PA 19103,Data Engineer
Kaiser Permanente,/rc/clk?jk=a0c27e572d000dc1&fccid=de7e38f18c290e23&vjs=3,"About Mercato |  | Mercato is the leading online marketplace empowering independent full category and specialty grocers – a $131B sales category in the US. The Mercato platform supports thousands of merchants through the entire e-commerce lifecycle and offers consumers a way to easily shop from their favorite local stores. |  | Mercato was founded in 2015, inspired by experiences Founder and CEO Bobby Brannigan had growing up working at his family's independent grocery store in New York. He realized first-hand that the 40,000+ independent grocery stores in the US lacked the technology and infrastructure to keep up with the modern demands of consumers driven by e-commerce experiences. |  | Today, Mercato has hundreds of thousands of consumers using the platform as well as thousands of merchants. Mercato is backed by investors Greycroft Partners and Loeb.nyc. |  | Responsibilities |  | Collaborate with Product Management, Engineering, and Analytics teams to understand data needs, solve problems, and identify trends and opportunities. | Design, build and launch new data extraction, transformation, and loading processes in production. | Manage data warehouse plans for a group of products | Build data expertise and own data quality for allocated areas of ownership. | Support existing processes running in production and optimize the operational aspects of our data infrastructure, including testing, deployment, and monitoring | Collaborate with application engineering teams to develop end-to-end features that will feed into the data pipeline | Qualifications | 2 years or more of data engineering or related experience | Expertise with SQL and relational databases (i.e. MySQL, Postgres) | Experience with custom ETL design, implementation and maintenance, including schema design and dimensional data modeling. | Significant experience with ETL and workflow management engines (i.e. Airflow, AWS Step Functions, AWS Glue, dbt). | Experience with programming languages such as Java and Python | Experience working with cloud or on-prem analytics platforms (i.e., AWS Redshift, Snowflake, or similar). | Ability to analyze data to identify deliverables, gaps, and inconsistencies. | Strong communications skills, ability to identify and communicate data-driven insights to both technical and non-technical stakeholders | Managing and communicating data warehouse plans to internal clients. | Bonus Points | Experience with AWS tools and ecosystem, especially those related to data and analytics | Experience with Kafka, Kinesis, or similar systems | Experience with BI/Reporting tools such as Metabase, Looker, and Tableau | Experience with building data pipelines in a service-oriented and CI/CD environment | Previous experience at a fast-growing start-up or internal team | Hands-on experience with building full-stack (front-end/back-end) web applications in a modern tech stack | Experience with test automation tooling/techniques (specifically as they relate to data infrastructure components) | Experience with Customer Data Platforms such as mParticle and Segment | Benefits | Competitive salary | Stock options | Medical, dental, and vision insurance | Generous vacation policy; paid holidays | Get in on the ground floor and shape the strategic direction and culture of a growing company | Ongoing training and growth opportunities | Work with top-notch team where your voice will always be heard | Upbeat work environment at a company with a huge vision |  |  | Please click here for our Worker Privacy Notice.",California,Data Engineer (Remote Opportunity)
Square,/rc/clk?jk=1d8654301ba00f07&fccid=09abad886b83c501&vjs=3,"Company Description | Square builds common business tools in unconventional ways so more people can start, run, and grow their businesses. When Square started, it was difficult and expensive (or just plain impossible) for some businesses to take credit cards. Square made credit card payments possible for all by turning a mobile phone into a credit card reader. Since then Square has been building an entire business toolkit of both hardware and software products including Square Capital, Square Terminal, Square Payroll, and more. We’re working to find new and better ways to help businesses succeed on their own terms—and we’re looking for people like you to help shape tomorrow at Square. |  | Job Description | From the cozy local wine store to the trendy sneaker pop-up, the Retail team at Square is finding new ways to equip retail sellers and their staff with the tools they need to succeed. Working to further Square's mission of economic empowerment, Data Engineering seeks to understand, empathize with, and empower our customers to build a remarkable product experience. | You will: | Develop and take ownership of Retail data mart, related data pipelines, and comprehensive reporting dashboards to empower data access and self-service, and to instrument critical business metrics. | Partner with business stakeholders to understand and translate business requirements into technical design of building scalable data pipelines. | Perform heuristic analysis of drivers of business metrics, insight requests, and data extractions to resolve business issues |  | Qualifications | You have: | 5+ years of data engineer experience in a successful data engineering or business intelligence team with expert knowledge of data warehouse architecture and hands on experience of data modeling design, ETL pipeline implementation and building reports with Looker or similar BI visualization tools. | Hands on experience in cloud-based computing services and data warehouses like Snowflake, Redshift, Azure, or similar technologies. | Strong communication, business intuition, and empathy partnered with the ability to understand complex business systems | An openness to learning new languages | Additional Information |  | At Square, we value diversity and always treat all employees and job applicants based on merit, qualifications, competence, and talent. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance. Applicants in need of special assistance or accommodation during the interview process or in accessing our website may contact us by sending an email to assistance(at)squareup.com. We will treat your request as confidentially as possible. In your email, please include your name and preferred method of contact, and we will respond as soon as possible. |  | Perks |  | At Square, we want you to be well and thrive. Our global benefits package includes: |  | Healthcare coverage | Retirement Plans | Employee Stock Purchase Program | Wellness perks | Paid parental leave | Paid time off | Learning and Development resources",Atlanta GA,Data Engineer Square for Retail
Vanguard,/rc/clk?jk=c84ad69bef5faaae&fccid=a4e4e2eaf26690c9&vjs=3,"We are: |  | Applied Intelligence, the people who love using data to tell a story. We’re also the world’s largest team of data scientists, data engineers, and experts in machine learning and AI. A great day for us? Solving big problems using the latest tech, serious brain power, and deep knowledge of just about every industry. We believe a mix of data, analytics, automation, and responsible AI can do almost anything—spark digital metamorphoses, widen the range of what humans can do, and breathe life into smart products and services. Want to join our crew of sharp analytical minds? Visit us here to find out more about Applied Intelligence. |  | You Are: | A Data Engineering pro—someone who thrives in a team setting where you can use your creative and analytical prowess to obliterate problems. You’re passionate about digital technology, and you take pride in making a tangible difference. You have communication and people skills in spades, along with strong leadership chops. Complex issues don’t faze you thanks to your razor-sharp critical thinking skills. Working in an information systems environment makes you more than happy. |  | The Work: | Lead a team that is in charge of building end-to-end digital transformation capabilities and lead fast moving development teams using Agile methodologies. | Design and build Big Data and real-time analytics solutions using industry standard technologies and work with data architects to make sure Big Data solutions align with technology direction. | Lead by example, role-modeling best practices for unit testing, CI/CD, performance testing, capacity planning, documentation, monitoring, alerting and incident response. | Keep everyone from individual contributors to top executives in the loop about progress, communicating across organizations and levels. If critical issues block progress, refer them up the chain of command to be resolved in a timely manner. | Pinpoint and clarify key issues that need action, lead the response and articulate results clearly in actionable form. | Show a strong aptitude for carrying out solutions and translating objectives into a scalable solution that meets end customers’ needs within deadlines. | Collaborate with research teams working on a variety of deep learning and NLP problems. |  | Here's what you need: | Bachelor's degree in Computer Science, Engineering, Technical Science or 12 years of experience in programming and building large scale data/analytics solutions operating in production environments. | Minimum of 1 years of expertise in designing, implementing large scale data pipelines for data curation using Spark/Data Bricks in combination with Scala. | Minimum of 1 year experience with cloud platforms-Azure,AWS,or GCP | Minimum of 1 year of experience with using SQL in relational database management |  | Bonus points if: | Minimum 1 year working with Machine Learning technologies. | Cloud platform certification |  | Important Information: |  | Equal Employment Opportunity Statement |  | Accenture is an Equal Opportunity Employer. We believe that no one should be discriminated against because of their differences, such as age, disability, ethnicity, gender, gender identity and expression, religion or sexual orientation. Our rich diversity makes us more innovative, more competitive and more creative, which helps us better serve our clients and our communities. |  | All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law. |  | Accenture is committed to providing veteran employment opportunities to our service men and women. |  | For details, view a copy of the Accenture Equal Opportunity and Affirmative Action Policy Statement |  | Requesting An Accommodation |  | Accenture is committed to providing equal employment opportunities for persons with disabilities or religious observances, including reasonable accommodation when needed. If you are hired by Accenture and require accommodation to perform the essential functions of your role, you will be asked to participate in our reasonable accommodation process. Accommodations made to facilitate the recruiting process are not a guarantee of future or continued accommodations once hired. |  | If you would like to be considered for employment opportunities with Accenture and have accommodation needs for a disability or religious observance, please call us toll free at 1 (877) 889-9009, send us an email or speak with your recruiter. |  | Other Employment Statements |  | Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture. | Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration. |  | Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process. |  | The Company will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. Additionally, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the Company's legal duty to furnish information. |  | Unless expressly indicated, this role is not open in the state of Colorado.",San Francisco CA 94105,Data Engineer
Olo,/rc/clk?jk=174ec2fea6b0e678&fccid=93d9161fbb29d190&vjs=3,"Job detailsJob TypeContractFull Job DescriptionJOB DESCRIPTION | The data engineer designs and builds platforms, tools, and solutions that help the bank manage, secure, and generate value from its data. The person in this role creates scalable and reusable solutions for gathering, collecting, storing, processing, and serving data on both small and very large (i.e., Big Data) scales. These solutions can include on-premises and cloud-based data platforms, and solutions in any of the following domains ETL, business intelligence, analytics, persistence (relational, NoSQL, data lakes), search, messaging, data warehousing, stream processing, and machine learning. | Responsible and accountable for risk by openly exchanging ideas and opinions, elevating concerns, and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite, achieves results by consistently identifying, assessing, managing, monitoring, and reporting risks of all types. |  | ESSENTIAL DUTIES AND RESPONSIBILITIES | Responsible for design, Development, and Support of data solutions, APIs, tools, and processes to enable rapid delivery of business capabilities. | Work closely with IT application teams, Enterprise architecture, infrastructure, information security, and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank. | Act as a technical Expert addressing problems related to system and application design, performance, integration, security, etc. | Conduct research and Development based on current trends and technologies related to the banking industry, data engineering and architecture, data security, and related topics. | Work with developers to Build CI/CD pipelines, Self-service Build tools, and automated deployment processes. | Evaluate software products and Provide documented recommendations as needed. | Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents. | Participate in the planning process for hardware and software. | Plan and work on internal projects as needed, including legacy system replacement, Monitoring and analytics improvements, tool Development, and technical documentation. | Provide technical guidance and mentoring for other team members. | Manage and prioritize multiple assignments. |  | MINIMUM KNOWLEDGE, SKILLS, AND ABILITIES REQUIRED | Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience. | Must be able to communicate ideas both verbally and in writing to management, business and IT sponsors, and technical resources in language that is appropriate for each group. | Fundamental understanding of distributed computing principles | Knowledge of application and data security concepts, best practices, and common vulnerabilities. | Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions, metadata management products, commercial ETL tools, Bi and reporting tools, messaging systems, data warehousing, Java (language and run time environment), major version control systems, continuous integration/delivery tools, infrastructure automation and virtualization tools, major cloud, or rest API design and development. |  | TECHNICAL SKILLS | Must Have | IBM DataStage, SQL |  | Nice to Have | SAS is a VERY NICE TO HAVE skill",Cincinnati OH 45242,Data Engineer
Kar Global,/rc/clk?jk=63159028b47e56e4&fccid=e064eb2c7274ff10&vjs=3,"Frontdoor is looking for a very strong data engineer who will bring a mindset of automation and innovation to the table. In addition to sharp technical skills, this person must be a strong communicator and collaborator who can partner up with business stakeholders to understand their needs and solve their data problems. As a data engineer at Frontdoor, you will be working in a fast-paced environment and using cutting-edge cloud technologies to develop a scalable data platform that will support years of company growth. |  | Responsibilities: |  | Build and maintain scalable data pipelines for both batch and stream processing in a cloud-computing environment. | Apply dimensional modeling to design tables and views that map business processes into an enterprise data model. | Optimize database architecture by trading off storage and computation to achieve low cost and high performance. | Build and support complex ETL infrastructure to deliver clean and reliable data to the organization. | Support the development of new products and services via ingestion, processing, and formatting data for reporting and analytics. | Interact face-to-face with business stakeholders, develop cooperative relationships, and acquire domain knowledge of the business. | Proactively automate manual processes throughout the business for higher efficiency, robustness, and speed. | Enforce production standards and governance best practices in the management of enterprise-level data, metrics, and reports. |  | Qualifications: |  | Bachelor’s in Computer Science, Engineering, Data Science, or related field (Masters or PhD preferred) | Excellent communication and inter-personal skills. | Versatile and quick learner with ability to pick up any new skills necessary to get the job done. | Demonstrated strength in data modeling, ETL development, data warehousing, data pipeline and data lake creation. | Extensive experience with cloud infrastructure and tools for AWS and GCP. | Demonstrable proficiency in Python development and advanced SQL querying. | Strong grasp of business elements and ability to convert requirements into database models and full-data pipeline systems. | Experience with visualization and reporting tools, such as Looker and Tableau. | 3-4 years of experience in data engineering or similar work. |  | Desired skills: |  | Snowflake experience (highly desirable). | Full stack experience, including microservice and web app development. | Knowledge of big data platforms, such as Hadoop and Spark. |  | Frontdoor is a company that’s obsessed with taking the hassle out of owning a home. With services powered by people and enabled by technology, it is the parent company of four home service plan brands: American Home Shield, HSA, Landmark and OneGuard, as well as AHS Proconnect , an on-demand membership service for home repairs and maintenance, and Streem, a technology company that enables businesses to serve customers through an enhanced augmented reality, computer vision and machine learning platform. Frontdoor serves more than two million customers across the U.S. through a network of more than 16,000 pre-qualified contractor firms that employ over 45,000 technicians. The company’s customizable home service plans help customers protect and maintain their homes from costly and unexpected breakdowns of essential home systems and appliances. With nearly 50 years of experience, the company responds to over four million service requests annually (or one request every eight seconds).For more details, visit frontdoorhome.com. |  | Job Category: Engineering |  | ID: R0015252",Denver CO 80238,Data Engineer
Ascendum,/rc/clk?jk=e54a4003b3e4c3e4&fccid=0c2152635a01a82e&vjs=3,"Overview: | DecisionPoint is seeking a passionate, and driven Data Engineer to support DHS’ Data Services Branch under the Office of Chief Information Officer that provides innovative and complex data analytics services and platforms to users. As a Data Engineer, you will be integral to data operations for the development and integration of a variety of data sources from customers across DHS. You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important. |  | Duties &amp; Responsibilities: | Work closely with software engineers and architects to extract, transform, and standardize data to prepare for ingest into target sources | Design and develop data services and/or pipelines as part of an Agile/Scrum team | Support continuous process automation for data ingest | Work with program management and engineers to implement and document complex and evolving requirements | Perform multiple tasks simultaneously and successful perform under changing requirements and deadlines | Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork |  | Qualifications: |  | Required Skills: | Must be able to obtain a Public Trust Clearance | BS degree in Computer Science or related IT field/equivalent experience | 5+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models. | Experience handling multiple tasks, changing priorities, and timely action; | Experience with developing data pipelines from many sources for structure and unstructured data sets in a variety of formats | Proficiency developing data extraction, transformation, and loading (ETL) processes, and performing test and validation steps | Proficiency with Python, R, and SQL languages, as well as various command line interfaces (Linux, AWS, Git Bash, etc.) | Technical proficiency with various database architectures, designs, and modeling | Familiarity with Hive, Hadoop, Kylin, and other big data analytic tools | Excellent communication, and presentation skills with the demonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiences with an impeccable attention to detail | Desired Skills: | Experience with DHS and knowledge of DHS standards a plus | Demonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutions | Mid-level expertise in developing and managing data technologies, technical operations, reusable data services, and related tools and technologies | Demonstrated ability to adequately plan and meet delivery objectives and maintain adequate service levels in a highly dynamic, complex environments |  What you’ll get! | Immediately- vested 401k with employer matching | 100% Employer Paid Dental and Vision coverage | Comprehensive Medical | Competitive PTO | Tuition Assistance | Professional Development opportunities | The ability to influence major initiatives | Our Equal Employment Opportunity Policy | The company is an equal opportunity employer. The company shall not discriminate against any employee or applicant because of race, color, religion, creed, sex, sexual orientation, gender or gender identity (except where gender is a bona fide occupational qualification), national origin, age, disability, military/veteran status, marital status, genetic information or any other factor protected by law. We are committed to equal employment opportunity in all decisions related to employment, promotion, wages, benefits and all other privileges, terms, and conditions of employment.",Washington DC 20520,Data Engineer
Salesforce,/company/VHLTechnologies-Inc./jobs/Senior-Data-Engineer-446756936e8a2767?fccid=90bd3f095885e713&vjs=3,"Job detailsSalary$60 - $70 an hourJob TypeContractNumber of hires for this role1Full Job DescriptionJOB DESCRIPTION:Required - Snowflake, SQL/Python ETLDesign, develop and maintain data ETL processes, ensure the processes are fulfilling business needs and SLAsDesign the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologiesCollaborate with Developers, Designers and Data Analyst to build optimum data pipelineStrong project management and organizational skillsExperience in Agile development methodologiesDevelop processes to answer recurring business questions and identify opportunities for improvementFacilitate technical planning and optimise data infrastructure, model and pipeline to improve data platform stability continuouslyEnsure Data Quality through continuous improvement and monitoringWe are looking for a strong candidate with multiple implementation experience in a Data Engineer role involving tools like snowflake, python and AWS cloud servicesSnowflake SnowPro cerfitication is desirableNice to Have - AWSContract length: 12 monthsJob Type: ContractSalary: $60.00 - $70.00 per hourSchedule:8 hour shiftWork Remotely:Temporarily due to COVID-19",Burbank CA,Senior Data Engineer
Gap Inc.,/rc/clk?jk=b95ae27a6f5ce220&fccid=b8aaf191c532bb48&vjs=3,"Job detailsJob TypeOtherFull Job DescriptionFearless is looking for a Data Science Engineer to add to our diverse team of 130+ employees (and counting!). |  | What you'll be doing: |  | We're looking to change the world by building software with a soul, and we want your help. |  | The Data Science Engineer works with a variety of data sources and stakeholders to design and implement solutions that solve business problems. The Data Science Engineer drives the data formatting, analysis, and optimization to build solutions using tools like statistical analysis and machine learning models. They are experts in algorithms, data modeling, and asking good questions. |  | They help build the data pipelines used to manage and analyze data. They work with the team to support product innovation and implementation. |  | What you should know: | This position is 100% remote until after COVID-19. | Once COVID-19 is over, this position is located in the Downtown Baltimore office and will have the flexibility to support some remote work/telecommuting. | Why we're excited about you: |  | We need your data science engineering skills! |  | What other skills will help you succeed at Fearless? Glad you asked! We're excited about candidates who have the following skills: |  | Required: | Develop custom data models, algorithms, and predictive models to perform multifaceted analysis | Strong understanding of Python with the ability to quickly pick up new libraries. Optional: react, cypher | Build and maintain predictive models and machine learning algorithms from the ground up to solve real-world business challenges | Understands how to ETL new data sources along with mining for insights | Familiarity with Machine Learning and Deep Learning Tools | Strongly Proficient in statistics, data processing, or data annotation | Experience with various types of databases such as relational, key-value, document, graph | Experience with OLAP data storage technologies like data lakes, and data warehouses | Experience working with at least one data analysis tool like Hadoop, Apache Spark, or cloud-provider equivalents | Understand, monitor QA, translate and collaborate with teams to ensure ongoing data quality. | Stitch, calibrate, and optimize sparse and noisy data across various data sources | Partner with various stakeholders and teams of stakeholders to understand business problems, help define them into KPIs and then deliver insightful analysis in reports or visualizations | Good to have: | Strong analytical and problem-solving skills with attention to detail | Understand, monitor QA, translate and collaborate with teams to ensure ongoing data quality. | View data through the lens of what questions to ask, what assumptions to make, what algorithms to use and how to get the biggest impact from it | Support regular ad-hoc data and analysis needs to better understand customer behaviors. | Support teams in running growth programs and A/B tests through analyzing results and communicating findings and insights. | Effective organizational and time management skills with the ability to work independently, as well as with remote teams, under strict project deadlines |  | At Fearless, we believe in sharing knowledge, fresh perspectives, and unique interests as individuals and as a company, so we're also interested to know what makes you tick. We want to know where your interests and passions lie so we can all grow together. |  |  | Compensation: |  | We believe in paying people fairly, so we've established a compensation model that ensures everyone at Fearless — regardless of race, ethnicity, gender, sexual orientation, disability, religion, age, nationality, or negotiation skills — is given equal pay for equal work. |  |  | So, what's next? |  | Over the years, we've honed a 3-step interview process that helps ensure that every employee we hire is the right fit for us and that we're the right fit for them. If we think you're a good fit, we'll get in touch and start scheduling your interviews! |  | Cultural Interview - We're a people-first company, so we always start off by getting to know more about you, how you work, what your career goals are, and what you're passionate about. This is your opportunity to ask questions and get a feel for Fearless, so don't be shy! | Technical Interview - This is where we get into the nitty gritty of the project. During the Technical Interview, you'll be interviewed by our Passion Coaches and/or the team's Project Lead to make sure your skills align with the project requirements. | Business Interview - At this point, you've made it to the final frontier! The Business Interview is when you'll meet with Fearless leadership to dot the i's, cross the t's, and determine whether or not we'll be moving forward with the hiring process. |  |  | Why Fearless? |  | Our people make us who we are. We believe that every member of the Fearless team has something to share, and we value the unique viewpoint you'll bring to our community. But we value your community, too, so we offer fulfilling work that stays in balance with the rest of life. Because everyone has different needs, desires, and goals, our benefits offer the choices and flexibility that our team members need to live well and succeed. Here are a few highlights of our benefits package: |  | Flexible schedule | Family-friendly workplace | 3 weeks accrued PTO + 1 week sick leave + 10 federal holidays + your birthday off | 100% coverage of the employee-only premium for HSA, HMO, or PPO plan and Employee Wellness Plan | Tech, education / training, and snack allowances | [Free parking in downtown Baltimore / public transit coverage] | Safe Harbor 401(k) plan with employer contributions |  |  | About Fearless |  | Fearless is a full-stack digital services firm in Baltimore that delivers sleek, modern, and user-friendly software designed to push the boundaries of possibility. It's our mission to build software with a soul — tools that empower communities and make a difference — so we can create a world where good software powers the things that matter. |  | That's not our only goal, though. We also strive to create a purple culture that makes our employees excited to come to work every day. That's why we encourage our employees to pursue their passions, both in and out of the office. With built-in company mentoring, continuing education support, flexible schedules, and a family-friendly work environment, we've created a culture that allows our team to thrive professionally and personally. |  | Fearless believes in equal opportunity employment. We won't discriminate against any employee or applicant on the basis of race, gender, nationality, age, religion, disability, military status, or sexual orientation. As a company and as individuals, we're committed to providing an inclusive and welcoming environment for our team, our family members, our clients, our subcontractors, and our vendors.",Baltimore MD,Data Science Engineer
Prime Consulting,/rc/clk?jk=0f94b6f7f53978a6&fccid=1639254ea84748b5&vjs=3,"Facebook is seeking an entry-level engineer with graduate level experience looking to apply their technical skills in a fast-paced and complex environment. A working knowledge of server hardware and the desire to participate in projects at a large-scale data center is central to this role. This position will work to resolve and diagnose compute issues at scale, escalate issues, and work with remote engineering teams. Additionally, this role will support rack lifecycle processes with a focus on helping build out and support cloud scale compute and storage environments. Solid communication skills are a requirement for this role. This person should enjoy working in a fast-paced environment where adaptability and flexibility will be key to their success.The candidate should also have working knowledge and experience in at least one of the following core areas: Networking, Programming/Scripting, Hardware, or OS repair. | Work within Facebook's ticketing system | First point of contact for break fix technicians | Responsible for assisting with projects (retrofits, new process details, etc.) and repairs throughout the data center | Understand and debug hardware and Linux OS related issues | Identify and help create documentation for the global data center knowledge base | Assist with process improvements and best practices in data center operations | Participate in on-call rotation (once a month on call for a week, after hours, first point of contact) | Currently has, or is in the process of obtaining, a Bachelor's or Master's degree in technical field, or equivalent experience/certification | Knowledge of Linux and server hardware support | Experience modifying and developing in Python, SQL, and/or shell scripting | Must obtain work authorization in the country of employment at the time of hire and maintain ongoing work authorization during employment | Working conceptual knowledge of technologies such as HTTP, DNS, RAID, and DHCP | Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started. | Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",Covington GA,Data Center Production Operations Engineer (University Grad)
Exelon Corporation,/rc/clk?jk=180433b5b76e62c3&fccid=4027cfd917e1ee29&vjs=3,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts. | Job Category | Products and Technology | Job Details | Salesforce is embarking on our own Digital Transformation to deliver customer success for our customers and accelerate our growth. A key pillar of this transformation is building a strong data foundation to support all of our core marketing needs through intelligent data sets as a service. |  |  | We’re looking for an experienced and committed data engineering lead who would deliver Salesforce Marketing’s data foundation and automated datasets. This lead will work closely with product management, data science, data visualization, and analyst teams to support them by building data architecture and data pipelines using Google Cloud Platform(GCP), who would consume this data for producing business-driven insights and AI solutions. |  |  | The right candidate will have strong experience with data infrastructure, data architecture, ETL, SQL, automation, data frameworks, and processes to rapidly integrate disconnected and disparate data sources into automated datasets for analyst's consumption. The candidate will also have a proven track record working with enterprise metrics, strong operational skills to drive efficiency and speed, expertise building repeatable data engineering processes, strong project management skills, and a vision for how to deliver data products. Experience with marketing and sales data sets is a huge plus. |  |  | Responsibilities: | Design and Implement of data pipelines, both batch and real-time, that produces reliable data for various data consumption use cases. | Help manage Marketing data platforms which include the Google Cloud stack, Amazon Web services, and traditional data-warehouses such as Oracle. | Manage all aspects of dataset design, creation, and curation; including the frameworks to derive metrics to deliver data products for KPI’s, visualization, data science, analyst and stakeholder teams | Build and own the automation and monitoring frameworks that showcase reliable, accurate, easy-to-understand metrics and operational KPIs to stakeholders for data pipeline quality and performance. | Drive design, building, and launching of new data models and data pipelines in production systems | Be the subject matter expert for data, pipeline design, and other related big data and programming technologies. | Proactively identify reliability &amp; data quality problems and drive triaging and remediation process | Partner with data producers in understanding data sources, enable data contracts, and define the data model that drives analytics | Partner with Analysts and Data Scientists on delivering reliable data that powers actionable insights | Evaluate various technologies and platforms in open source and proprietary products. Execute proof of concept on new technology and tools to pick the best tools and solutions. | Understanding data governance practices such as metadata management, data lineage, and data glossaries a huge plus. | Foster strong collaboration between globally distributed team members | Harness operational excellence &amp; continuous improvement with a can-do attitude. | Qualification and Skills: | You have to love data - this is what we do. We are looking for people who are excited about different and unique data sets, and all the ways that they could be used in order to improve user experience. | B.S/M.S. in Computer Sciences or equivalent field, with 13+ years total experience and 8+ years of relevant experience within the big data/data warehousing domain. | Solid understanding of RDBMS concepts, data structures, and distributed data processing patterns. | Expertise in programming pipelines in languages like Python, Scala, Java, etc. | Expertise in big data technologies like Hadoop, Spark, Hive, Snowflake, etc. | Experience with Cloud technologies like AWS, GCP, Containers, Kubernetes. (GCP Preferred) | Experience with version control systems (Github, BitBucket, etc..) and CI/CD tools. | Experience in data orchestration &amp; scheduling tools like Airflow, Control-M, Autosys, Tidal, etc. | Proven track record in building products on some production-grade systems. | Partner with Product Managers and Data Scientists to understand customer requirements and design prototypes and bring ideas to production. | Passionate, curious, self-starter and innovative mentality. | Hands-on knowledge of salesforce products and functionalities a plus. | Experience with Sales and Marketing data is a huge plus | To understand coding skills better, include below along with your resume: | Github account and an example of source code worked | A brief description of your skills, interests, and work style | Link to blog or webpage | Accommodations - If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form. | Posting Statement | At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits. | Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesfore.com or Salesforce.org. | Salesforce welcomes all. | Pursuant to the San Francisco Fair Chance Ordinance and the Los Angeles Fair Chance Initiative for Hiring, Salesforce will consider for employment qualified applicants with arrest and conviction records.",New York NY,Lead Data Engineer Data Engineering - Marketing
Facebook,/rc/clk?jk=3bc87d5f3a30dc06&fccid=48ecd526e3aa3225&vjs=3,"This senior level employee is responsible for guiding and influencing the development of a formal Global Quality Services Framework and for leading the development of software quality assurance test strategies. |  |  |  | Essential Responsibilities: | Conducts or oversees business-specific projects by applying deep expertise in subject area; promoting adherence to all procedures and policies; developing work plans to meet business priorities and deadlines; determining and carrying out processes and methodologies; coordinating and delegating resources to accomplish organizational goals; partnering internally and externally to make effective business decisions; solving complex problems; escalating issues or risks, as appropriate; monitoring progress and results; recognizing and capitalizing on improvement opportunities; evaluating recommendations made; and influencing the completion of project tasks by others. | Practices self-leadership and promotes learning in others by building relationships with cross-functional stakeholders; communicating information and providing advice to drive projects forward; influencing team members within assigned unit; listening and responding to, seeking, and addressing performance feedback; adapting to competing demands and new responsibilities; providing feedback to others, including upward feedback to leadership and mentoring junior team members; creating and executing plans to capitalize on strengths and improve opportunity areas; and adapting to and learning from change, difficulties, and feedback. | Leads the development of quality assurance (QA) test project strategies, methodologies, and standard processes for large-scale, complex IT initiatives spanning all QA domains by analyzing business and technology requirements to ensure testability and traceability, and determining testing scope and approach. | Oversees and addresses critical issues, dependencies, and risks related to testing. | Ensures quality assurance projects are appropriately staffed, work plans are followed, and milestones are met. | Reviews and signs off on testing scope and approach, and partners with cross-functional IT and business stakeholders to review and approve the overall testing approach. | Manages the development test scenarios and execution of test cases across all testing phases (e.g., SIT, Performance, UAT, Automation, Production, Validation). | Ensures quality metrics are tracked across testing phases (e.g., SIT, Performance, UAT, Automation, Production, Validation). | Manages the review and validation of testable business processes, test data, and test environment requirements. | Develops quality assurance (QA) project plans for moderately to highly complex projects by identifying project scope, work plans, schedules, milestones, and critical paths, and ensuring proper staffing. | Develops guidelines and best practices to ensure test plans and timelines are aligned with project/program milestones. | Defines and ensures adherence with entry and exit criteria according to Kaiser testing standards. | Reviews project status and milestones reports, provides justification for and first-level authorization for exceptions and waivers, and meets with IT and business stakeholders to address contingency plans, as appropriate. | Generates scheduled reports (e.g., test execution, defects, ad hoc reports) and provides daily test execution metrics to IT teams and management, as appropriate. | Initiates and evaluates required business process improvements in order to achieve business results and appropriate solutions for customers. | Minimum Qualifications: | Minimum two (2) years SQA experience working across multiple IT domains or business units in a corporate setting. | Minimum two (2) years in a leadership role working with technical teams. | Bachelor's degree in Computer Science, CIS, or related field and Minimum eight (8) years experience in SQA, software testing or related field. Additional equivalent work experience may be substituted for the degree requirement. | Preferred Qualifications: | Two (2) years of work experience in a role requiring interaction with senior leadership (e.g., Director level and above) | Three (3) years experience in a leadership role of a large matrixed organization. | Three (3) years experience working with IT vendors. | Four (4) years experience working on project(s) involving the implementation of software development life cycle(s) (SDLC). | Three (3) years experience writing technical documentation in a software testing or quality assurance environment. | Four (4) years experience working with integration technologies (e.g., web services, MQ). | Two (2) years experience working in and defining the requirements for virtual testing environments. | Two (2) years experience working in and defining the requirements for healthcare testing environments. | Two (2) years experience testing and defining the requirements testing business system applications. | Two (2) years experience working with and defining the requirements for data mart/data warehousing. | Master's degree in Computer Science, CIS, or related field. | Quality Assurance Institute (QAI) or American Society for Quality (ASQ) or similar certification. | PrimaryLocation : Colorado,Greenwood Village,Greenwood Plaza IT | HoursPerWeek : 40 | Shift : Day | Workdays : Mon, Tue, Wed, Thu, Fri | WorkingHoursStart : 08:00 AM | WorkingHoursEnd : 05:00 PM | Job Schedule : Full-time | Job Type : Standard | Employee Status : Regular | Employee Group/Union Affiliation : NUE-CO-02|NUE|Non Union Employee | Job Level : Individual Contributor | Job Category : Information Technology | Department : Edge Cyber Security | Travel : No | Kaiser Permanente is an equal opportunity employer committed to a diverse and inclusive workforce. Applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy), age, sexual orientation, national origin, marital status, parental status, ancestry, disability, gender identity, veteran status, genetic information, other distinguishing characteristics of diversity and inclusion, or any other protected status. |  | External hires must pass a background check/drug screen. Qualified applicants with arrest and/or conviction records will be considered for employment in a manner consistent with federal and state laws, as well as applicable local ordinances, including but not limited to the San Francisco and Los Angeles Fair Chance Ordinances.",Greenwood Village CO 80111,Data Engineer/Wrangler
The Hartford,/company/Rapid-Value-Solutions/jobs/Data-Engineer-550b507dc6dcd84d?fccid=67e05c3d1dfa6238&vjs=3,"Job detailsSalary$70 - $85 an hourJob TypeFull-timeContractNumber of hires for this role2 to 4QualificationsBachelor's (Preferred)Full Job DescriptionPosition Responsibilities: Work on cloud foundry platformUnderstands business requirements and applies them to complex programming and analysis.Communicates (oral and written) recommendations and mentors/provides guidance to less experienced colleagues.Partners with various departments to understand and incorporate standards information and requirements into work procedures.Identifies, analyzes and provides feedback to departmental standards, norms, and new goals/objectives.Analyzes existing applications and systems and formulates logic for new systems, devises logic procedures, logical database design, performs coding and tests/debugs programs with an operational mindset.Works on complex data &amp; analytics-centric problems having broad impact that require in depth analysis and judgment to obtain results or solutions.Designs and deploys new complex Enterprise systems and enhancements to existing systems ensuring compatibility and inter-operability.Resolves application programming analysis problems of broad scope within procedural guidelines. May seek assistance from the supervisor or more skilled programmers/analysts on unusual or especially complex problems that cross multiple functional/technology areas.Delivers best-in-class software as part of a software delivery team.Conceptualizing and generating infrastructure that allows big data to be accessed and analyzedPlans work to meet assigned general objectives; progress is reviewed upon completion and solutions may provide an opportunity for creative/non-standard approaches.Desired Skills: · Experience in data acquisition, data set process, improving data reliability, quality and efficiency· Must have good experience developing in Python using PySpark and other data tools· Must have experience with large data sets to address business issues· Good to have experience using Cloud Foundry· Good to have expertise in AWS technologies like Glue, S3 and RedShift· Experience with modern software delivery practices, including source control, testing, continuous delivery· Experience delivering product with Agile methodologiesMinimum Education/Skills: Bachelors Degree in Computer ScienceJob Types: Full-time, ContractPay: $70.00 - $85.00 per hourSchedule:Monday to FridayEducation:Bachelor's (Preferred)Experience:Data Engineering: 5 years (Required)Pyspark: 3 years (Required)Cloud Foundry: 2 years (Required)Work Location:One locationWork Remotely:Temporarily due to COVID-19COVID-19 Precaution(s):Remote interview process",San Francisco CA,Data Engineer
Comcast,/rc/clk?jk=61bd2a2aff62ec11&fccid=25d3835187829237&vjs=3,"You are a driven and motivated problem solver ready to pursue meaningful work. You strive to make an impact every day &amp; not only at work, but in your personal life and community too. If that sounds like you, then you've landed in the right place. | JOB TITLE : Data Engineer | LOCATION : One Hartford Plaza, Hartford, CT 06155 | OPENINGS: 1 | DUTIES: | Position will be responsible for engineering data pipelines to enable creation of curated data assets and BI applications; Architect, design and implement end-to-end data solutions; Analyze complex data sets to identify affected or in-scope populations for new projects; Create functional and technical documentation for solutions delivered; Automate refreshes of stored procedures; Understand Tableau and how data solutions are best prepared accordingly; Collaborate with BI stakeholders to troubleshoot performance or brainstorm alternative approaches; Prepare data dictionaries for BI stakeholders and business end-users; Champion data quality, integrity and reliability throughout the department by designing and promoting best practices; Collaborate with business teams to identify requirements and business needs; Develop and maintain relationships with key stakeholders/ data consumers. | REQUIREMENTS: | The position requires a Bachelor’s degree, or foreign equivalent, in Computer Science, Computer Engineering, Information Systems, or a related field plus two (2) years of experience in the job offered or in a related position. Experience must include the following: Demonstrable experience with data warehousing, dimensional modeling, data virtualization and data delivery methodologies; Demonstrable experience using SQL, Oracle or Talend; Demonstrable knowledge of ETL processes across various tools and understanding of the data usage with Business Intelligence reporting tools; Demonstrable experience managing multiple concurrent projects both independently &amp; in a team setting; Demonstrable experience deploying data workflows in a production environment; Demonstrable experience translating business requirements into technical specifications. | CONTACT: | Apply online at https://thehartford.wd5.myworkdayjobs.com/Careers_Restricted/job/Hartford-CT/Data-Engineer-Hartford-CT-_R212840 referencing Req#R212840/11474.296. Hartford Fire Insurance Company is an Equal Opportunity Employer. | Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age | Data Engineer - GE08AE",Hartford CT,Data Engineer (Hartford CT)
everis USA Inc.,/rc/clk?jk=69a49cdeea88b8ec&fccid=0b74c73a7d280485&vjs=3,"Jr. Data Engineer - TAP(Job Number: 334542) | Description | Chubb is looking for individuals passionate about technology to join our Technology Associate Program (TAP). The Technology Associate Program is a two-year program designed for early career IT professionals to help them develop expertise and technical acumen as a technologist. By joining the Technology Associate Program, you will be offered the following: |  | On-the-job and educational technical training (offered through Pluralsight) to enhance skills within your chosen discipline | Business acumen and professional development training specific to IT and an understanding of our broader industry | Networking opportunities with IT and Business leaders and TAP associate peers | Hackathons |  | Position Summary: | The Data Engineer will work with the business to understanding data requirements and will become a data platform expert in designing and building data solutions focused on Cloud-based Big Data ecosystems. You will work closely with other data science teams and take ownership of your projects and deliver high-quality data-driven advanced analytics applications. You will solve diverse business problems by utilizing a variety of different tools, strategies, algorithms and programming languages. |  | What about you? | You are highly collaborative, creative, and intellectually curious individual who is passionate about data engineering and supporting cutting-edge computing capabilities. | You are able work well, both individually and within a team. | You are adaptable and able to overcome technical challenges. | You are a self-starter and motivated to learn and succeed. | You are data driven and are able to identify and solution problems as they arise. | Responsibilities: | Collaborate and work with global data management stakeholders to identify requirements for complex business problems that may be loosely defined. | Work with the business, applications owners, solutions architects, and with technical architects to understand the implications of respective data architectures to maximize the value of information across the organization. | Build the enterprise conceptual and logical data models for analytics, operational and data mart structures in accordance to industry best practices models. | Identify, evaluate and implement leading edge data management frameworks required to address complex large-scale data challenges. | Work within multi-functional agile teams with end-to-end responsibility for product development and delivery. | Provide architectural support by building proof of concepts &amp; prototypes. |  | Desired Qualifications: | Working knowledge of Excel, PowerPoint, and Word is required. | Energetic, able to build and sustain long-term relationships across a multitude of stakeholders in a fast paced, multi-national work environment. | Strong time management and organizational skills | Possess strong verbal and written communication skills and ability to present, persuade and influence peers. | Internship or Job experience in software development | Bachelor's degree in Information Systems or related field with GPA of 3.0+ required | Excellent data analysis skills | Experience in performing analysis and design for data management and data driven projects. | Exposure or knowledge of tools such as T-SQL on Spark SQL, ANSI SQL etc. | Experience or exposure Python, Jupyter Notebook, etc. | Experienced in programming languages such as Python, Sscala or Java | Familiarity with data science and analytic tool sets such as Jupyter hub |  | Bonus Skills: |  | Exposure or experience with Cloud Platforms | Experience in designing and leading the conceptual, logical and physical design for distributed databases. | Experience with operating system command languages such as bash or ksh | Experience with development tools such as git and integrated development environments | Understanding of the SAFe Agile development methodology |  | EEO Statement | At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion, and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin, ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law. Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliation against any individual who reports discrimination or harassment. |  | Work Locations - Jersey City Jersey City 07302 |  | Job - Information Technology |  | Travel - No |  | Job Posting - Nov 2, 2020, 4:32:37 PM",Jersey City NJ,Jr. Data Engineer - TAP
Bankers Healthcare Group,/rc/clk?jk=a64e7aba4b2be0d8&fccid=5a91fd675c45515f&vjs=3,"Are you ready to join a growing team that puts a premium on productivity and has an award-winning culture, centered around transforming talented employees into effective business leaders? |  | Then Bankers Healthcare Group is the place for you. We offer innovative financial solutions to licensed and highly-skilled professionals, representing the best of both traditional lending and fintech, and are looking for passionate, impact players to help take our company to the next level. | At BHG, you’ll become immersed in the finance industry—with a variety of loan solutions, credit cards, patient financing, bank programs, and collections services, which have helped BHG become one of the leading providers of finance solutions. | With over 18 years in business, we have the stability of an established company with the speed and agility of a startup, where ingenuity and risk-taking are encouraged, and every employee has the opportunity to learn, grow and thrive. | Who You Are | You are a motivated Data Engineer professional who is passionate about data. You excel at architecting data solution, and have experience building out the next generation of data tools. You are articulate, and thrive in a fast-paced environment where you can change the data infrastructure of the company. | What You’ll Do | Ensure high throughput of development teams by identifying potential issues, removing impediments or guiding the team to remove impediments by collaborating with the appropriate resource | Enabling real-time analytics and event driven architecture | Develop pipelines real-time streaming from different sources like FTP, Windows Blob Storage, SQL Server, Cosmos or Mongo DB and scheduling the pipelines as per requirement using Kafka and Databricks | Work with cross functional teams such as IT and Marketing | What You’ll Need | 2+ years’ experience in data engineering and custom coding using Python, Java, or Scala utilizing tools such as Databricks. | Experience working with NoSQL databases such as MongoDB or Cosmos DB | Experience with real-time analytics and event driven architecture | Ability to build Python scripts for scheduling, monitoring, &amp; management | Experience managing Spark and/or Kafka clusters on-prem or in the cloud | Experience with Data lake or Delta Lake | Understanding of big data pipelines | Familiarity with docker | Life at BHG | At BHG, we work hard and aren’t afraid to take risks. Since the beginning, our core values of PMA (positive mental attitude), team player and loyalty have been the driving force behind every interaction we have between each other and our customers. We have a healthy respect for the daily grind, yet we value work/life balance. We believe that all employees should have the opportunity to lead and that good ideas can come from anyone. From the top-down, our leaders are actively involved not only in strategic oversight and running the business, but also in the wellbeing and growth of all employees. We consider people our #1 asset, and help employees realize their full potential, set and exceed their goals, and explore new opportunities for personal and professional development. | Why You Should Join BHG | We strive to offer amenities, opportunities, events, and programming that support the interests of our teams, while furthering the culture that makes us Great Place to Work® certified. Some of the benefits you can expect when you join BHG include: |  100% coverage of monthly health insurance premiums Competitive PTO and vacation policies Company 401(k) plan with employer contributions after one year On-site gym access and memberships, with personal trainers, and certified nutritionists on staff Company-sponsored training and certification opportunities Monthly award ceremonies where top achievers are celebrated and receive additional bonuses Ongoing volunteer opportunities to give back to the community through our BHG Cares program |  | If you’re ready for a career where you can exercise your passions, be surrounded by co-workers who are relentlessly committed to service, and have a team-player mindset, apply today! |  | Bankers Healthcare Group is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender, race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law. Bankers Healthcare Group is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation. | #LI-Remote",Remote,Big Data Engineer
Fearless,/rc/clk?jk=759e230c4e4a986a&fccid=d26901b7a9b35c6c&vjs=3,"BlockFi's mission is to provide liquidity, transparency and efficiency to digital financial markets by creating products that meet the needs of consumers and corporations across the globe. We build bridges between traditional finance and digital markets that enable growth for all participants. |  | As a Senior Data Engineer your responsibilities will be focused on the creation, maintenance, and scalability, of all data sources that BlockFi currently has, or will have in the future. You will be expected to not only meet your direct day to day responsibilities detailed below, but also assist with helping to shape the future of the tech stack within the BlockFi Analytics department. This role provides the opportunity to get in on the ground floor of BlockFi's Analytics and Data Science department. |  | Responsibilities |  |  | Assist the Engineering and Data Science teams in the maintenance of current databases and warehouses, while seeking opportunities to improve efficiency and automation | Be the owner of the BlockFi Analytics code base. Take the existing tech stack, and prepare it to be capable of being performant during exponential growth | Own the end to end process of onboarding, integrating, scaling, and maintaining all external data flows into the BlockFi Analytics' team databases. | Organize and reconcile large amounts of data to ensure timely, and accurate reporting while delivering a top notch client experience | Review data for discrepancies among systems to ensure back-end data matches client-facing data | Create and maintain documentation on key internal data processes that helps to enable the team to train new team members, detail existing obscure data pipelines, and lay the foundation for automation | Provide guidance to the organization surrounding how current processes, and changes to those processes will effect BlockFi Analytics in the short and long term | Be a self starter that finds opportunities for development and can deliver solutions with minimal oversight | Recognize that no job is too small and egos must not delay deliverables | Display intellectual curiosity and seek opportunities for personal and professional improvement |  | Qualifications |  | SQL (Advanced), Python (Advanced), Web Scraping (Python), Data modeling for Reporting Relational DB (Postgres) and Warehouses (Amazon Redshift), Amazon Web Services (AWS), Scaling, Optimization, Tableau Server Management/Automation (Bonus) | Experience building backend systems to support ""Real Time"" Analytics | Experience building scalable systems from the ground up, to account for exponential growth of underlying data | A degree in a technical field such as Finance, Data Science, Statistics, Computer Science or similar field | Direct experience building and managing an Analytics Code Base | 7+ years of professional experience | Excellent problem solving skills, as well as the ability to work independently without need for constant oversight | Strong critical thinking and analytical capabilities | High attention to detail | Adaptable to various environments and sudden changes | Ability to embrace new technology, applications and solutions | Desire to often work with people as well as be a self-motivated team player |  | Why BlockFi? |  | BlockFi has experienced incredible growth since our launch in August 2017. From raising over $100MM in debt and equity capital to helping thousands of clients (and growing!) do more with their crypto assets, we have established a dominant position as the debt and credit crypto market leader in the U.S. As we expand our product suite and geographic footprint, our addressable market will grow exponentially. |  | BlockFi's leadership team has decades of experience in the traditional financial services and banking world, and we take a conservative approach to regulation that will position us well for sustainable long-term growth and expansion. |  | Our team is comprised of highly motivated professionals from diverse backgrounds. We are aiming to become the leading lender in crypto and are poised to redefine the global financial ecosystem for the better. In addition: |  | BlockFi is one of the first companies to ever offer crypto-backed loans and the only company whose founding team has an institutional understanding of the debt capital markets and regulatory landscape in the U.S. | $4.3MM of seed funding and $50MM in capital for funding loans | $100 MM of Series A, B, and C funding led by Valar Ventures with participation from Susquehanna, Winklevoss Capital, Fidelity, Galaxy Digital, Akuna Capital, and Morgan Creek | We are moving quickly and have already deployed substantial capital into the space, proving our ability to execute and capture customer demand |  | We offer |  | Competitive salary | Unlimited vacation / sick days | Employer paid health coverage (vision, dental, 401K) | Work alongside an enthusiastic, collegial, and driven team in a highly meritocratic environment",New York NY,Senior Data Engineer
JumpCloud,/rc/clk?jk=47997c5ce3bbbeae&fccid=fa0ca3b638673d62&vjs=3,"Why We Work at Dun &amp; Bradstreet | We are at a transformational moment in our company journey - and we’re so excited about it. Each day, we are finding new ways to strengthen our award-winning culture, and to accelerate creativity, innovation and growth. Our purpose is to help customers improve business performance with Dun &amp; Bradstreet’s Data Cloud and Live Business Identity, and we’re wildly passionate and committed to this purpose. So, if you’re looking to make an immediate impact at a company that welcomes bold and diverse thinking, come join us! | Brief Description: | The Data Engineer role involves developing applications designed to accumulate, derive meaning from, and apply stewardship to, large datasets. | The Data Engineer will have to both be able to work with the Global People Data team’s internal datasets and tools, as well as be able to coordinate with outside groups to continuously meet their needs. | The Data Engineer is expected to be a key developer of Global People Data tools and products , both in maintaining and upgrading existing tools and in developing new products using state of the art techniques and programming concepts. | Requirements | Bachelor’s degree (preferable in computer science or a related field | Experience with Machine Learning and/or Artificial Intelligence Technologies | 2 - 5 yrs. experience with SQL for data analysis | 2 - 5 yrs. experience with Python for data analysis | 1 - 3 yrs. experience with ETL pipeline development | Ability to work closely with others and problem solve complex situations | Experience with hosted environments (AWS, and Azure recommended) | Dun &amp; Bradstreet is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, age, national origin, citizenship status, disability status, sexual orientation, gender identity or expression, pregnancy, genetic information, protected military and veteran status, ancestry, marital status, medical condition (cancer and genetic characteristics) or any other characteristic protected by law. |  | We are committed to Equal Employment Opportunity and providing reasonable accommodations to applicants with physical and/or mental disabilities. If you are interested in applying for employment with Dun &amp; Bradstreet and need special assistance or an accommodation to use our website or to apply for a position, please send an e-mail with your request to TalentAcquisitionTeam@dnb.com. Determination on requests for reasonable accommodation are made on a case-by-case basis. | Please note that all Dun &amp; Bradstreet job postings can be found at https://dnb.wd1.myworkdayjobs.com/Careers and all communication from Dun &amp; Bradstreet will come from an email address ending in @dnb.com.",Waltham MA,Data Engineer
Square,/company/ThoughtStorm-INC/jobs/Junior-Data-Engineer-Gc-Usc-545d5afeccee9999?fccid=6d2345d934bb817f&vjs=3,"Job detailsJob TypeFull-timeContractNumber of hires for this role1QualificationsBachelor's (Preferred)JavaScript: 2 years (Preferred)Full Job DescriptionThe company built a custom app in Google Sheets.The app collects data for their company.Need someone to develop features in google sheetsCreate dashboards, reports.They have data stored in Excel, moving towards google sheets.*They would like someone to clean up the app.Setup the ability to generate reportsRestructure dataHave the app provide reports about the company projects (materials being used in each project, cust, employee hourly rates, etc..)**Must be skilled in Google Sheets, DashBoards, Reports, JavaScript.*Job Types: Full-time, ContractSchedule:8 hour shiftEducation:Bachelor's (Preferred)Experience:Data Analyst: 2 years (Preferred)Google Sheets: 2 years (Preferred)Reports: 2 years (Preferred)JavaScript: 2 years (Preferred)Work Location:One locationWork Remotely:Temporarily due to COVID-19",Ventura CA,Jr. Data Engineer (Locals Only)GC/USC
Dun & Bradstreet,/rc/clk?jk=70c5bd21e61086fd&fccid=b0ee184b9e1cf9bb&vjs=3,"As a data engineer on our team, you’ll help build our data acquisition and ingestion system, efficiently aggregating billions of data points across thousands of diverse sources. Our underlying data platform is the foundation for our customer-facing research and risk products, making public records from thousands of different sources easily and readily available. |  |  | In this role, you will: |  | Write, maintain, and improve Python code for collecting and processing data from thousands of different third-party sources | Analyze new data sources to understand how to best acquire and model/catalogue the data | Develop a strong understanding of the ways our users use our products to help inform how we can best present records and data to meet their needs | Work with many different kinds of data, both public and proprietary, in many different structured and unstructured formats, ingested from sources such as APIs, databases, websites, files, cloud storage, etc. | Curate and monitor existing source integrations to ensure data in our platform is accurate, consistent, available | Analyze our data to deliver insights that improve our platform and power new features and products |  | About you: |  | You LOVE (or are at least intensely interested by) data and the idea of collecting, organizing, and making it more accessible and usable | You have worked programmatically with data in a role such as a software engineer, data analyst, digital archivist, scientist, researcher, or data programmer | You can quickly profile and understand a data set and implement an appropriate process in code for working with it | You’re experienced and proficient with Python, and have a strong working knowledge of web technologies such as HTTP, HTML, and JSON | You have experience ingesting data from varied and complex real-world sources including websites, files in multiple formats, databases, and APIs | You have a strong understanding of data types, schemas, and normalization and how to work with “dirty” data | You are a fast, motivated learner and are willing to pick up new tools and technologies on the fly to solve a problem | You’re excited by open-ended problems and are comfortable owning and delivering a solution from start-to-finish | You have a VERY strong attention to detail and documentation | You enjoy working collaboratively and really care about the work you do, the people you do it with, and the customers who ultimately use the product |  | Some of the technologies you’ll work with: |  | Python, ElasticSearch, Postgres, Redis, Linux, Celery, Docker, GCP",Remote,Data Engineer
Thermo Fisher Scientific,/rc/clk?jk=71b67e7a1c48022a&fccid=4d9339102788fdca&vjs=3,"The role |  | As a Data Engineer at Blue State, you'll play an integral role on a smart and vibrant analytics team servicing a wide range of progressive organizations. You'll design, build, and manage the systems and processes which form the underpinning of Blue State's analytics work, supporting and working alongside data analysts and campaign strategists. But you'll also work directly with Blue State's clients to help solve their data integrity and integration challenges, serving as a trusted advisor to your counterparts within client organizations. |  | Day-to-day responsibilities: |  | Create and support systems and processes for managing, compiling, manipulating, and analyzing data for client and internal projects | Work with Blue State's client organizations to solve difficult data migration, management, and integration challenges | Build data pipelines, data warehouses, reporting dashboards, automated exports, and synchronization processes | Automate workflows and look for further opportunities to improve efficiency in our work | Always maintain a high level of data security and privacy | The team |  | You will be a part of the global Data &amp; Technology team working primarily with our creative agency on client projects. You'll work in either the NY or DC office. |  | Top things we're looking for | Good foundational understanding of statistical analysis | Extensive experience working with SQL databases in an analytics or business intelligence context | Familiarity with common marketing technology platforms like Google Analytics, Google Ads, Facebook Ads, email marketing tools, and other marketing automation tools | Experience with ETL/ELT tools, processes, and best practices | Strong Python experience: | Python should be your go-to tool for solving problems. If the first thing you want to do when you have to do the same thing twice is write a Python script to automate it - we want you! | Experience with task automation in a Python context - experience with AirFlow, Prefect, Dask a big plus | Experience working with restful APIs - you can competently navigate unfamiliar API documentation and figure out how to accomplish tasks | Strong working knowledge of Google BigQuery and the Google Cloud Platform data product ecosystem including: | Designing data warehouse schemas for cross-channel marketing analytics | Utilizing the suite of Google Cloud Platform tools for the purposes of extracting, processing, manipulating and analysing data | Building and running automated tasks within the GCP environment - e.g. Cloud Compute, Cloud Functions, Cloud Run, Cloud Scheduler | Comfortable managing GCP IAM policies across projects and teams | Comfortable working within a spreadsheet (even if you prefer a database) - preferably in Google Sheets - bonus points if you've extended Google Sheets using Google Apps Script | Familiarity with Git and maintains good habits around code maintenance | Able to build repeatable and well-documented processes and tools that can be used by other technically-savvy but non-Python developer analytics team members (think easy to use command-line scripts - not GUIs) | Good at teaching others what you know. |  | At Blue State, diversity is a necessity, not a nice-to-have. We encourage those from underrepresented communities — women, people of color, LGBTQIA+, immigrants, indigenous folks, those with disabilities and people at all the intersections in between — to apply. Even if you don't think your current skill set checks every box below, but this role seems to align with your strengths, we want to hear from you. |  | The company |  | Blue State is a values-led creative and campaigns agency that partners with leading causes, companies, and campaigns to build better organisations for a better world. We drive real change, make good trouble, put people first and are constantly curious. |  | We believe that there is no force more powerful than people taking collective action on the things they care about. We don't pretend to have all the answers, but we know where to find them: in people. We listen, learn, and uncover new insights that often surprise us and our clients — and move us toward better results. Across clients including UNHCR, Amnesty International, Google, Tesco, Nesta, and Tate. We have offices in New York City, Washington DC, London, Oakland and Chicago.",New York NY,Data Engineer
ActiveCampaign,/rc/clk?jk=a1de91a46d079f86&fccid=c9019ff6e219ad1a&vjs=3,"The Opportunity | The Data Engineer will be the ‘go to person’ on our rapidly expanding engineering team solving all our data challenges for billions of data points related to the world’s business contacts and company profiles. The primary role of the data engineer is to improve the accuracy of our data recommendations that predicts every professional’s emails, phone numbers and websites with the highest level and degree of accuracy possible. This person will have an opportunity to solve significant data problems that are positively impacting millions of business professionals and helping the world connect to opportunity and each other faster than ever before. | Apply today if you have: | 5+ years of hands-on professional (not academic) experience in pioneering data engineering and/or big data.Working experience in an AGILE environment with modern SDLCExpert knowledge of data engineering, predictive modeling, statistics and machine learning in general. Deep experience with algorithms, statistics, and data analytics.Experience/knowledge in our more major technical &amp; programming languages/cloud computing including but not limited to: Aurora PostgreSQL, AWS Redshift, AWS Glue, AWS EMR, AWS Spark (Scala/Python/PySpark), AWS QuickSight, Presto, DynamoDB, Redis, ELK stack, Node.JS, Pandas (Python Library) | PM19 | Seamless.AI has been delivering the world's best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohio’s fastest growing companies and won 2020 Best Places to Work and LinkedIn’s Top 50 Tech Startups 2020. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.",Columbus OH,Data Engineer - Remote US
Peloton,/rc/clk?jk=2e8ee51d4cdfd34b&fccid=8a97d8070e7b31f8&vjs=3,"Job detailsJob TypeInternshipFull Job DescriptionFrequently cited statistics show that women and underrepresented groups apply to jobs only if they think they meet 100% of the qualifications on a job description. IMO wants you to apply, even if you don’t think you meet 100% of the qualifications listed. We look forward to receiving your application! | Work that is meaningful. A job that has impact. Colleagues that inspire. That’s what you’ll find at Intelligent Medical Objects (IMO), a growing health IT company creating clinical terminology and insights solutions that are used by more than 500,000 US physicians and 4,500 US hospitals to power better patient care and support meaningful analytics. | At IMO, a core team of clinicians, software developers, and data scientists combine computer science and medical expertise to help patients and healthcare professionals access high quality health information quickly and easily to improve total patient health. We are currently in need of Data Engineer Interns to join this team! | The Data Engineer Intern will support our software developers, database architects, data analysts, and data scientists on data initiatives. Gain experience writing clean, quality, and testable code in a fast-paced collaborative environment. Contribute to innovative approaches to complex problems. Work and learn in a small cross functional agile team. Lead and participate in design and architecture decisions. Be passionate and continue to hone your craft in real world scenarios. | Join our growing Software Engineering department as a Data Engineer Intern to help design, create, and support high quality solutions that support 80% of US clinicians and build Data Engineering solutions at IMO! | What You'll Do: | Develop working knowledge on our tech stack for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies | Learn to manage existing data pipelines to implement source ingestion, gathering and logging access data | Maintain analytics tools that utilize the data pipeline to provide actionable insights | Learn about structured, semi-structured data, unstructured data and involved technologies | Work cooperatively with team members to manage conflict constructively and fosters trust, commitment, accountability. | Deliver quality product leveraging the values of transparency, inspection and adaptation in an agile way | Take ownership to proactively anticipate the implications and consequences of situations | Implement creative solutions to technical challenges and embraces out-of-the-box thinking | What You'll Need: | Diverse Sophomore or Junior level classroom experience and a passion within the field of Math, Statistics, Informatics, Information Sciences, Engineering, Computer Science or related. | Aptitude and motivation to learn cloud-native Data Engineering practices. | Basic understanding of SQL knowledge and experience working with relational and NoSQL databases | Aptitude and motivation to working with unstructured datasets, processes supporting data transformation, data structures, metadata, dependency, and workload management. | Aptitude and motivation to develop code with object-oriented/functional scripting languages, such as Python, Scala, etc. | Exposure to ETL and BI Dashboard tools, such as Talend, Tableau, etc. | Exposure to AWS cloud based Big Data stack services, such as EC2, EMR, RDS, Redshift. | Curiosity for technical challenges and an eagerness to explore new approaches. | Your voice (whether it’s a loud one or a quiet one). We believe in communicating. Ideas, needs, questions, concerns, good jokes.. Remember when we said we welcome your point of view? We meant it. | An autonomous attitude. Of course, we’re here to teach, support and guide you through the internship, but ultimately, you will own projects and your development. | What We'll Give You: | Training from the best. Collectively, we’ve got years of expertise in Healthcare and IT. | Extensive team-specific training that will drive your trajectory (classroom &amp; beyond) | Coaching on the non-technical stuff like presentation skills, business etiquette, résumé workshop, etc. | A challenge. We’re here to push you. Our goal is to fill your summer with experience and connections you won’t forget. Probably a little fun, too. | The opportunity to directly influence our products. | #LI-LF1 | At IMO, we celebrate diversity and are committed to creating an inclusive environment for all employees. IMO is proud to be an equal opportunity workplace and is an affirmative action employer.",Rosemont IL,Internship - Data Engineer
Chubb,/rc/clk?jk=6f675f2608706b58&fccid=09abad886b83c501&vjs=3,"Company Description | Square builds common business tools in unconventional ways so more people can start, run, and grow their businesses. When Square started, it was difficult and expensive (or just plain impossible) for some businesses to take credit cards. Square made credit card payments possible for all by turning a mobile phone into a credit card reader. Since then Square has been building an entire business toolkit of both hardware and software products including Square Capital, Square Terminal, Square Payroll, and more. We’re working to find new and better ways to help businesses succeed on their own terms—and we’re looking for people like you to help shape tomorrow at Square. |  | Job Description | As a Data Engineer on the Marketing team, you will join an organization whose mandate is to develop foundational data and reporting infrastructure to driving Square's revenue growth and accelerating seller acquisition. You will collaborate and work with teams across Square to build outstanding data pipelines, dashboards and processes that stitch together complex sets of data stores and guide large investment decisions. Your work will have an impact on hundreds of partners at Square. | You Will: | Develop data foundation and reporting infrastructure to ensure accurate and reliable business reporting | Partner with business leads and cross-functional teams to understand their data and reporting requirements and translate them into Product Requirement Definition (PRD), technical specifications and scalable implementation | Be the expert on end-to-end data flow for Marketing | Implement data model and ETL code improvements to improve pipeline efficiency and data quality |  | Qualifications | You Have: | 2+ years experience in Data Engineering or similar role | Expert knowledge in writing complex SQL and ETL development with experience processing extremely large datasets within cloud-based data warehouses such as Snowflake, Google BigQuery, and Amazon Redshift | Experience working with business teams on complex problems and can translate them to efficient, scalable and easy to maintain data engineering solutions and data visualization | Knowledge in data modeling concepts and implementation | Expertise in visualization technologies such as Tableau and/or Looker | Experience with Python | Experience with Linux/OSX command line, version control software (git), and general software development | BS degree in Engineering, Computer Science, Math or a related technical field | Technologies we use and teach: | SQL and Python | Looker, or other data visualizations technologies | ETL scheduling technologies with dependency checking such as Airflow | Linux/OSX command line, version control software (git) | Additional Information |  | At Square, we value diversity and always treat all employees and job applicants based on merit, qualifications, competence, and talent. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance. Applicants in need of special assistance or accommodation during the interview process or in accessing our website may contact us by sending an email to assistance(at)squareup.com. We will treat your request as confidentially as possible. In your email, please include your name and preferred method of contact, and we will respond as soon as possible. |  | Perks |  | At Square, we want you to be well and thrive. Our global benefits package includes: |  | Healthcare coverage | Retirement Plans | Employee Stock Purchase Program | Wellness perks | Paid parental leave | Paid time off | Learning and Development resources",San Francisco CA,Data Engineer Marketing
Farm Credit Services of America,/company/Solvenyle/jobs/Data-Engineer-Aws-c916182aaba88da7?fccid=31a27e3bd6590f2f&vjs=3,"Job detailsSalary$38 - $42 an hourJob TypeFull-timePart-timeContractNumber of hires for this role2 to 4QualificationsMaster's (Preferred)AWS: 5 years (Preferred)Terraform: 4 years (Preferred)Full Job DescriptionHi ,Hope you doing Good!!This is Lokesh from Ibrain, Here Please find the below opportunity which fits your profile. Please let me know your Interest. References are highly appreciated.Title : AWS Data EngineerLocation :  Bloomington, ILTHIS IS A W2 POSITION - WE NEED SOMEONE WHO CAN WORK ON OUR W2 - 2+ YEARS CONTRACT - ONE ROUND OF INTERVIEWEXP: 3-5 YEARS - IF SPONSORSHIP REQUIRED WE CAN SPONSOR FOR VISAJob Description &amp; Key Skills : MUST HAVE Skills and Experience:- CI/CD- Terraform- EC2, S3, Athena, Glue, Lambda. CloudWatch, IAM- Python, Pytest- Unix/Linux (centOS), Bash scripting- Deep knowledge of Glue ETL (jobs, catalog, etc.)Nice To Have Skills and Experience:- Terratest, Go programming language- QuickSight- Lake Formation- Knowledge of ingestion patterns (Kafka, Kinesis, etc.)- GlacierThank You !Lokesh I IT RecruiterM: 636-486-4431Job Types: Full-time, Part-time, ContractPay: $38.00 - $42.00 per hourSchedule:8 hour shiftEducation:Master's (Preferred)Experience:AWS: 5 years (Preferred)AWS Certification: 4 years (Preferred)Terraform: 4 years (Preferred)Data Engineer: 5 years (Preferred)Glue: 3 years (Preferred)Work Location:One locationWork Remotely:Temporarily due to COVID-19COVID-19 Precaution(s):Remote interview process",Piscataway NJ,Data Engineer with AWS
UnitedHealth Group,/rc/clk?jk=d8a1aaa0a0dd1033&fccid=1021f02f59bb1214&vjs=3,"This is a great opportunity to grow and advance your career at the federal level. | Currently Remote, this position will be located in Rockville, MD once the office reopens. Successful candidates must be able to complete a Public Trust background check. | Job Description: | Precise is seeking candidates for the position of Junior Cloud Data engineer supporting the FDA CFSAN Data Warehouse (CDW) project in Rockville, MD. | FDA’s Center for Food Safety and Applied Nutrition (CFSAN) is embarking upon a path to establish a modern data warehouse for the center’s need to store and access data from varied applications and databases. The newly developed CFSAN Data warehouse will provide the foundation for business intelligence and analytics for the stakeholders to make quick informed decisions and reporting. The Data Warehouse is built on AWS leveraging variety of AWS data and AI/ML services | Job Duties: | Implement application migration from on-premises to Cloud in support of the Data Warehouse objectives for FDA CFSAN program | Participate into other cross-function design, development activities. | Ensure consistency, quality, and accuracy of the data mapped from source to the target data warehouse environments | Work closely with the PM, Architect, customers, and end users to translate business requirements into technical designs to map and load data from source systems to the data warehouse | Communicate and accurately document all architectural decisions, plans, goals, and functional requirements | Maintain metadata with data definitions, relationships, sources, and lineage | Works with business users to translate business requirements into technical designs to map and load data from source systems to the data warehouse | Designs solutions that leverage enterprise tools and have reusable components and services | Ensures data designs meet business requirements | Develops strategies for data acquisitions, archive recovery, and implementation of database | Ensures the realization of the solution architecture, making sure it is operable &amp; cost-effective | Ensures technical requirements are delivered with quality and completeness through peer reviews | Essential Skills: | Experience in developing Python programs | Experience with AWS cloud service environment | Experience in design and implementation of stored procedures, views and other application database code objects to aid complex mappings. | Experience in maintaining SQL scripts and complex queries for analysis and extraction. | Understand of Infrastructure as a server platforms | Understand logical design of Databases, Data Warehouses and Multidimensional Databases including data flows, conception model and logical models | Experience on premise to cloud data migration in enterprise systems | Experience in analyzing and processing structured or unstructured data sets. | Experience with enterprise BI tools such as QuickSight, Tableau is preferred. | Strong analytical and problem-solving skills | Strong understanding of data governance | Data Security knowledge | Data Analysis with a degree of creativity and lateral thinking is expected | Attention to detail, creative problem-solving abilities, and coaching and influencing skills are a must | Ability to assesses risks, anticipates bottlenecks, provide escalation management, and collaborates to drive toward mutually beneficial outcomes | Preferred Skills: | Forward thinking on new technologies and patterns | Participated in providing the project estimates for development team efforts | AWS certification | Education: | Bachelors or MS in related field | Work Location: Rockville, MD | Successful candidates must be able to complete a Public Trust background investigation | ABOUT US | Precise Software Solutions, Inc., an SBA 8(a) program participant, is an innovative small business with a proven record of success delivering quality services and solutions to government organizations. A CMMI Level 3 company, Precise serves as a trusted advisor to senior technology executives and helps government agencies enhance and expand their information technology capabilities. Precise helps their customers capitalize on the efficiencies offered by technological advancements and ensures the integrity of their IT systems and programs so they can perform their public mission more effectively. The company is known for delivering agile and innovative solutions and specializes in strategic consulting, system modernization and integration, digital transformation and experience, infrastructure and cloud implementation, and data management and analytics. | BENEFITS AND PERKS: | Health Benefits (Medical, Dental and Vision) | Flexible Spending Accounts (FSA) &amp; Health Savings Account (HSA) | Retirement Plan | Paid Time Off | Parental Leave | Life Insurance | Training and Development | Two Innovation Days | Employee Referral Program | Annual Charity Donation Match | Awards and Recognition | Standing Desks | Our Equal Employment Opportunity Policy | Precise is an equal opportunity employer. The company shall not discriminate against any employee or applicant because of race, color, religion, creed, sex, sexual orientation, gender or gender identity (except where gender is a bona fide occupational qualification), national origin, age, disability, military/veteran status, marital status, genetic information or any other factor protected by law. We are committed to equal employment opportunity in all decisions related to employment, promotion, wages, benefits and all other privileges, terms and conditions of employment. The company is dedicated to seeking all qualified applicants. | Powered by JazzHR | M2k6EBHIs5",Rockville MD 20850,Jr. Data Engineer (Cloud Python Datawarehouse)
Precise Software Solutions Inc.,/rc/clk?jk=c9e00f996cf6bab0&fccid=8899ecbdba2d0a5e&vjs=3,"Peloton is looking for a Data Engineer to build our Data Warehouse and Data Pipelines. You will work with multiple teams of passionate and skilled data engineers, architects, and analysts responsible for building batch and streaming data pipelines that process terabytes of data daily and support all of the analytics, business intelligence, data science and reporting data needs across the organization. |  | Peloton is a cloud first engineering organization with all of our data infrastructure in AWS leveraging EMR, AWS Glue, Redshift, S3, Spark. You will be interacting with many business teams including marketing, sales, supply chain, logistics, finance and partner to scale Peloton's data infrastructure for future strategic needs. |  | Responsibilities |  | Understand the data needs of different stakeholders across multiple business verticals including Finance, Marketing, Logistics, Product etc. | Develop the vision and map strategy to provide proactive solutions and enable stakeholders to extract insights and value from data. | Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions. | Design best practices for big data processing, data modeling and warehouse development throughout the company. |  | Requirements |  | Familiar with at least one of the programming languages: Python, Java. | Comfortable with Linux operating system and command line tools such as Bash. | Familiar with REST for accessing cloud based services. | Excellent knowledge about databases, such as PostgreSQL and Redshift. | Has experiences with GIT, Github, JIRA and SCRUM. | 2+ years in building a data warehouse and data pipelines. Or, 3+ years in data intensive engineering roles. | Experience with big data architectures and data modeling to efficiently process large volumes of data. | Background in ETL and data processing, know how to transform data to meet business goals. | Experience developing large data processing pipelines on Apache Spark. | Experience with Python or Java programming languages. | Strong understanding of SQL and working knowledge of using SQL(prefer PostgreSQL and Redshift) for various reporting and transformation needs. | Excellent communication, adaptability and collaboration skills. | Experience running Agile methodology and applying Agile to data engineering. | Experience with Java, JDBC, AWS, SDK |  | Nice to have |  | Familiar with AWS ecosystem, including RDS, Glue, Athena, etc. | Has experiences with Apache Hadoop, Hive, Spark and PySpark. |  | ABOUT PELOTON |  | Peloton is the largest interactive fitness platform in the world with a loyal community of more than 2.6 million Members. The company pioneered connected, technology-enabled fitness, and the streaming of immersive, instructor-led boutique classes for its Members anytime, anywhere. Peloton makes fitness entertaining, approachable, effective, and convenient, while fostering social connections that encourage its Members to be the best versions of themselves. An innovator at the nexus of fitness, technology, and media, Peloton has reinvented the fitness industry by developing a first-of-its-kind subscription platform that seamlessly combines the best equipment, proprietary networked software, and world-class streaming digital fitness and wellness content, creating a product that its Members love. The brand's immersive content is accessible through the Peloton Bike, Peloton Tread, and Peloton App, which allows access to a full slate of fitness classes across disciplines, on any iOS or Android device, Fire TV, Roku, Chromecast and Android TV. Founded in 2012 and headquartered in New York City, Peloton has a growing number of retail showrooms across the US, UK, Canada and Germany. For more information, visit www.onepeloton.com.",New York NY 10011,Data Engineer
Intelligent Medical Objects,/rc/clk?jk=ee6c5fa297200812&fccid=2ae98fa5cb3d52c1&vjs=3,"We all depend on healthcare throughout our lifetimes, for ourselves, and our families and friends, but it is notoriously difficult to navigate and understand. As an industry that comprises 20% of the US economy we think healthcare should work better for all of us. At Collective Health we believe it's time for a new day in healthcare where as members we are informed and empowered to make the right care choices when the decisions are urgent and critical. |  | We deliver a connected healthcare experience for over a quarter million members and 60+ companies across the nation who want the best for their employees. We've got a ton of interesting problems to solve around data pipeline design and implementation, data architecture and modeling, distributed systems, and more. If you're passionate about tackling hard problems while making a real difference in the world, we'd love to talk! |  | What you'll do: | Data Pipelines - Create new pipelines and improve/maintain existing pipelines using Spark (Scala, Pyspark, Spark SQL) | Data Modeling - Partner with analytic consumers to design logical and physical schemas, improve existing data models and build new ones | Cross-functional Collaboration - Interface with Product, Engineering, Data Science, Analytics/BI, and Operations to understand their data needs, providing both consultative and data engineering solutions for consumers | Build data expertise and own data quality across various business domains including healthcare claims and member experience | Your skills and qualifications include: | BS degree in Computer Science or related technical field, or equivalent practical experience | 2+ years proven work experience as a data engineer, working with at least one programming language (e.g. Scala, Python/PySpark) plus SQL expertise | 2+ years experience with schema design, dimensional data modeling, and large-scale data warehousing architecture | Expertise in building data pipelines through efficient ETL design, implementation and maintenance | Background working with distributed data systems such as Spark, Presto, Hive, and Redshift. Experience with schedulers/workflow management tools (e.g. Airflow) a plus | Excellent communication skills to collaborate with stakeholders in Engineering, Product, Data Science, Analytics/BI, and Operations |  | Collective Health is a technology company simplifying employer healthcare to make health insurance work for everyone. With more than 200,000 members and over 45 enterprise clients—including Pinterest, Red Bull, Restoration Hardware, Activision Blizzard, and more—our technical and customer experience teams are reinventing the healthcare experience for forward-thinking employers and their people across the U.S. |  | Collective Health is headquartered in San Francisco, CA, with additional offices in Chicago, IL, and Lehi, UT. Founded in 2013, Collective Health is backed by the SoftBank Vision Fund, DFJ Growth, PSP Investments, NEA, GV, G Squared, Founders Fund, Maverick Ventures, Mubadala Ventures, Sun Life, and other leading investors. For more information, visit us at https://www.collectivehealth.com |  | We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Illinois,Data Engineer
Collective Health,/rc/clk?jk=d16398ec3fadf618&fccid=99611b002ebf86da&vjs=3,"About OnePointOne |  | OnePointOne is an indoor farming automation company solving pressing issues of global food production to provide radical new solutions for plant-based research and health applications through automation, data science, and brainpower. We plan to feed developed and developing nations, city by city, by taking an innovative and strategic stance against the problems facing our urbanizing world and growing populations. We also plan to deploy our unique hyper-clean and precision-automated plant production capabilities in a variety of novel ways outside of simple food consumption. Working with OnePointOne offers a unique opportunity to develop a technology-driven agricultural solution with societal, environmental, medical, and economic profits, all intended to improve lives around the world. |  | About the Data Engineer role: |  | OPO's plant production facility is enabling and empowering the collection of data on production quality and optimization that is extraordinary in the history of horticulture. Our Data Engineer is tasked with architecting and developing secure collection, transport and storage processes for the data as well as designing access to the data to support operational and research requirements. It's a highly technical position, requiring experience and skills in areas like programming, mathematics and computer science. In addition to the technical skills noted, our Data Engineer also needs soft skills to communicate with data stakeholders that range from plant production operations to doctorate level plant scientists. |  | The Skillsets OPO is looking for in our Data Engineer: | In depth experience architecting, designing, developing and managing petabyte per year scale solutions for reliability, efficiency and quality. | Demonstrated experience supporting intensive scope and scale data access and visualization | Working knowledge of competing database and data processing technologies for a hybrid cloud / on-prem solution and how to evaluate and select technologies for our solutions | Demonstrated experience managing data reliability and quality and dataset solutioning for ML initiatives and predictive analytics and prescriptive modeling. | Experience with multi-discipline engineering projects | The Attributes OPO is looking for in our Data Engineer: | Passion for OnePointOne's mission and for developing new and exciting technologies. | Proven track record of results driven planning and execution | Hands-on development experience and leadership | Small company and/or startup experience a plus | Clear demonstration of internal and external cross-functional partnership and strong collaborative approach; a natural partner advocate with penchant for developing win-win solutions | Must be able to perform under pressure, short deadlines and in high visibility settings with a bias for action and sense of urgency | Comfort with change and ambiguity in a fast-paced environment; experience working in a dynamic, rapidly changing environment is a plus | Exceptional work ethic and sense of ownership and accountability | Obsessive attention to detail | Good presentation, oral and communication skills |  | Job Type: Full-time | Job Location: Phoenix, Arizona, or Remote |  | Visit our website at www.onepointone.com for more information.",Arizona,Data Engineer
Comcast,/rc/clk?jk=6b93e7d5eb5339d3&fccid=ea25315ee9da22e5&vjs=3,"Job detailsJob TypeCommissionFull Job DescriptionJob Summary | • Position will be responsible for the design, development, testing, and support of the Big Data Analytics solutions on cloud (AWS) in collaboration with cross functional teams. • Collaborate with key stake holders and translate business requirements to technical requirements and implement solutions under the guidance of technical leads. • Dive into large, noisy, and complex real-world customer TV and Digital Ad viewing data to do pre-campaign planning and post campaigns performance measurement using Big data Platform (DataBricks, Pyspark, Glue, EMR). • Responsible for building automated data pipelines to ingest data, integrate data from multiple data sources (On-Premise &amp; Cloud) and create aggregated data sets for reporting needs. | Job Description | Functional Competencies: | Strong understanding of database structures, query languages (e.g. SQL), fundamentals of mathematics, distributed systems (Hadoop), data science, and statistical concepts. | Experience consolidating and integrating data from multiple sources (On-Premise &amp; Cloud). | Ability to analyze, transform and aggregate large data sets (Big Data) using BI tools (Hive QL, Pyspark, Jupyter Notebooks, AWS Athena, AWS Glue). | Ability to automate PySpark Jobs using Lambda/Glue/EMR/Python and fine tune for performance. | Ability to Architect and Design Big Data Analytics solutions on cloud (AWS). | Knowledge of Media/Advertising industry. | Interpersonal Competencies: | Experience sharing insights and recommendations to audiences with varying levels of analytic understanding | Ability to communicate complex concepts in easy-to-understand terminology | Team player with a “can-do” attitude | “Lean forward” bias to find opportunities and drive results | Ability to work effectively across functions, disciplines, and levels | Preferred Educational Level: | BS or MS degree in Mathematics, Computer Science, Statistics, or related field of study | Experience: | 5+ years of Hands-on experience in Big Data Analytics geared towards BI insights. | 3+ years of Hands-on experience working on data pipelines, automation of jobs using big data technologies (Spark, Python, Pyspark, Glue). | 3+ years of experience working with Linux, DataBricks, and Azkaban or similar tools. | Strong knowledge of SQL, Python and relational databases. | Knowledge of AWS services such as Glue, Athena, Lambda, EC2, IAM, CloudWatch, EMR, S3 and Big data Query engines like Hive, Presto, Spark. | Hands-on experience working with Big Data and building Data Analytics solutions on Cloud (AWS). | Employees at all levels are expected to: | Understand our Operating Principles; make them the guidelines for how you do your job. | Own the customer experience - think and act in ways that put our customers first, give them seamless digital options at every touchpoint, and make them promoters of our products and services. | Know your stuff - be enthusiastic learners, users and advocates of our game-changing technology, products and services, especially our digital tools and experiences. | Win as a team - make big things happen by working together and being open to new ideas. | Be an active part of the Net Promoter System - a way of working that brings more employee and customer feedback into the company - by joining huddles, making call backs and helping us elevate opportunities to do better for our customers. | Drive results and growth. | Respect and promote inclusion &amp; diversity. | Do what's right for each other, our customers, investors and our communities. | Disclaimer: | This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications. | Comcast is an EOE/Veterans/Disabled/LGBT employer. | Education | Bachelor's Degree | Relevant Work Experience | 5-7 Years | Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That’s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality – to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.",Philadelphia PA,Data Engineer 3
Seamless.AI,/rc/clk?jk=769d5d0b24ec4209&fccid=d3d3520998346837&vjs=3,"We are currently seeking a Sr Data Engineer to perform data analysis for a data warehouse/operational data store, data marts, and other data stores in support of the Optum business. The new hire will define and maintain business intelligence/data warehouse methodologies, standards, and industry best practices. You will work with Development and QA team to develop data delivery/processing solutions and to create Data Dictionary with full description of data elements and their usage. | You’ll enjoy the flexibility to telecommute* from anywhere within the U.S. as you take on some tough challenges. | Primary Responsibilities: | Gather business requirements for analytical applications in iterative/agile development model partnering with Business and IT stakeholders | Create source-to-target mapping based on requirements | Create rules definitions, data profiling and transformation logic | Gather and prepare analysis based on requirements from internal and external sources to evaluate and demonstrate program effectiveness and efficiency, and problem solving | Support Data Governance activities and be responsible for data integrity | Developing scalable reporting processes and querying data sources to conduct ad hoc analyses/detailed data profiling. | Research complex functional data/analytical issues | Assume responsibility for data integrity, data quality among various internal groups and/or between internal and external sources | Provides source system analysis and perform gap analysis between source and target systems | You'll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in. | Required Qualifications: | Undergraduate degree or equivalent experience | 5+ years of data analysis experience | 5+ years of systems analysis/requirement gathering experience | 3+ years of Health Care/Claims data experience | Working experience with HealthCare data standards like HL7 or 834/837 or SMART | If you need to enter a work site for any reason, you will be required to screen for symptoms using the ProtectWell mobile app, Interactive Voice Response (i.e., entering your symptoms via phone system) or a similar UnitedHealth Group-approved symptom screener. Employees must comply with any state and local masking orders. In addition, when in a UnitedHealth Group building, employees are expected to wear a mask in areas where physical distancing cannot be attained. |  Preferred Qualifications: | Data modeling and data architecture experience | Knowledge of ETL tools | Organizes sorts and filters data in order to distinguish patterns and recognize trends | HealthCare Industry/Claims experience | Develops innovative approaches | Technology Careers with Optum . Information and technology have amazing power to transform the health care industry and improve people's lives. This is where it's happening. This is where you'll help solve the problems that have never been solved. We're freeing information so it can be used safely and securely wherever it's needed. We're creating the very best ideas that can most easily be put into action to help our clients improve the quality of care and lower costs for millions. This is where the best and the brightest work together to make positive change a reality. This is the place to do your life's best work.(sm) | *All Telecommuters will be required to adhere to UnitedHealth Group’s Telecommuter Policy. | Colorado Residents Only: The salary range for Colorado residents is $79,700 to $142,600. Pay is based on several factors including but not limited to education, work experience, certifications, etc. As of the date of this posting, In addition to your salary, UHG offers the following benefits for this position, subject to applicable eligibility requirements: Health, dental, and vision plans; wellness program; flexible spending accounts; paid parking or public transportation costs; 401(k) retirement plan; employee stock purchase plan; life insurance, short-term disability insurance, and long-term disability insurance; business travel accident insurance; Employee Assistance Program; PTO; and employee-paid critical illness and accident insurance. | Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity / Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law. | UnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment. | Job Keywords: Sr. Data Engineer, Telecommute, Telecommuter, Telecommuting, Work from home, Work at home, Remote",Eden Prairie MN 55346,Sr Data Engineer - Telecommute
Capgemini,/company/SPR-Software-systems-INC/jobs/Senior-Data-Engineer-Role-15d486889440a3e5?fccid=3dc512e352224a0b&vjs=3,"Job detailsSalary$55 - $60 an hourJob TypeFull-timeContractNumber of hires for this role1Full Job DescriptionGreetings from SPR Software systems Inc. We have a Super Urgent! requirement and my client wants to do interview ASAP and close the position this week.I noticed your professional profile on the job boards and is a good match for this super high Priority! job. So thought of connecting with you.Please send us your updated CV and Contact number. So that we can go through with your updated skill set and get in touch with you.Title: Data EngineerLocation: San Antonio, TX (Remote)Duration: 12+ months (DBT ,Python) we are Looking for strong data person with hands on python with experience in building CI/CD pipeline.Experience in DBT(Data Build tool) and Snowflake PreferredThanks,Sai KumarSPR Software SystemsPhone: 469-322-4455 x 110Expected Start Date: 12/2/2021Job Types: Full-time, ContractSalary: $55.00 - $60.00 per hourSchedule:8 hour shiftCOVID-19 Precaution(s):Remote interview processSpeak with the employer+91 469-322-4455 x 110",San Antonio TX,Sr Data Engineer role in San AntonioTX
Facebook,/company/Nota-Bene-Global-Services-Pvt-Ltd/jobs/Data-Proc-Engineer-Gcp-ef66acb1179a15d5?fccid=6993cec364d5c59d&vjs=3,"Job detailsJob TypeFull-timePart-timeInternshipNumber of hires for this role2 to 4Full Job DescriptionJob Description:Role :: DataProc Engineer with GCPLocation :: Anywhere in US or Canada // Remote1.a. Must have Experience with Dataprocb. Strong experience with Spark/Hadoopc. Experince with google cloud (GCP)d. Should be good at Dataproc optimizatione. Should be immediately available to work in US Pacific time zoneJob Types: Full-time, Part-time, InternshipCOVID-19 Precaution(s):Remote interview process",Remote,Data Proc Engineer with GCP
BJ's Wholesale Club Inc.,/company/Vitalyze-Inc./jobs/Cloud-Data-Engineer-cb259aaafa75377d?fccid=580c75820d31d138&vjs=3,"Job detailsSalaryFrom $50 an hourJob TypeFull-timeContractQualificationsBachelor's (Preferred)Full Job DescriptionVitalyze Inc. - Cloud Data Engineer – AWS Job Description*Job Responsibilities &amp; DutiesThis is a role within the Platform Data and Analytics team, focused on creating a competitive advantage through novel data platform, infrastructure, metrics, insights and data services.Build and implement data solutions in the cloud (AWS)Experience with AWS big data technologies: S3, Glue, EMR, Kinesis, RDS, Redshift, AthenaStrong software development and programming skills with focus on data using Java, Python or other object-oriented languages.Must Have PythonStrong software development and programming skills using Python (PySpark)· Experience creating and driving ETL pipelines in AWS based environmentExperience with integration of data from multiple data sources – API IntegrationsCloud Data Engineer – AWS Qualifications and SkillsUndergraduate degree, preferred3-5 years of experience as an Cloud Data EngineerExcellent verbal and written communication skillsDetail- and deadline-oriented multitaskerAbility to make strong decisions under pressureExcellent time management and organizational skillsCompany ProfileAt Vitalyze, we bring together the latest clinical technologies and innovations to deliver true Value-Based Healthcare. Our cutting-edge technology is equipped to help accelerate medicine that truly works. Vitalyze seeks to revolutionize health and wellness globally for individuals, clinicians, and partners. If you are ready to be a part of a growing company that seeks to change the face of healthcare and help you grow professionally, we encourage you to apply today!*Job Types: Full-time, ContractPay: From $50.00 per hourSchedule:8 hour shiftMonday to FridayEducation:Bachelor's (Preferred)Experience:Cloud Data Engineer: 3 years (Required)Contract Length:3 - 4 months5 - 6 monthsContract Renewal:LikelyWork Location:Fully RemoteThis Job Is:A job for which military experienced candidates are encouraged to applyCompany's website:vitalyzeinc.comWork Remotely:NoCOVID-19 Precaution(s):Remote interview processVirtual meetings",Remote,Cloud Data Engineer - Amazon - AWS
DUFF AND PHELPS LLC,/rc/clk?jk=9f457f7f59f7ff55&fccid=0be2ca2cdeae413b&vjs=3,"Data Engineer |  | Cooler Screens is backed and led by some of the most prominent Chicago &amp; Silicon Valley leaders, advisers, and investors and has developed a digital solution to create and transform a multi-billion dollar industry and positively affect the buying experiences of consumers in the US and beyond. If you’re looking to get on the ground floor of the next digital revolution and Chicago tech success story – this opportunity may be for you. |  | As the Data Engineer at Cooler Screens, you will design and maintain an enterprise data warehouse platform architecture that structures data for analytical processes to support BI and data integration management, which will be foundational in building out our data processing pipeline. You will be responsible for constructing how we identify, absorb, store, and query data across the Cooler Screens platform that scales to the rising growth of the company. |  | As we transform the retail customer experience with our digital media platform, we invite you to join Cooler Screens in enabling the journey. |  | Who we are: | Cooler Screens was founded on the core idea that consumers deserve a far better experience than what is available today in brick-and-mortar retail. We call this concept Consumer Experience (“CX”) and it guides all that we do. We have begun the CX journey by bringing what consumers love about shopping on-line to the frozen and refrigerated food aisle. |  | Cooler Screens is powered by proprietary innovation and a global network of industry-leading partners. Global technology, CPG, and retail leaders have partnered with Cooler Screens to be part of this important and rapidly growing industry initiative. We are backed and led by some of the most prominent Silicon Valley and Chicago leaders, advisers, and investors. | Who You Are: | You’re excited to contribute to building a robust scalable and robust data platform and shaping an early stage startup | You’re passionate about building scalable data processing systems and turning data into insights | You know how to develop solutions to transform and optimize data and have expertise in solutions to build robust data pipelines and repositories | You’re comfortable working with a variety of BI tools and helping users to develop solutions to gain key data insights | You’re a critical thinker and enjoy measuring, analyzing, and solving complex problems | You’re experienced and comfortable in managing data from a variety of sources and formats and using best practices to ensure consistency and quality | You thrive working both independently and in a team environment | What You Need to Succeed | You have 4+ years of data engineering development experience | You have a deep understanding of the data architecture and relevant technologies | You have hands-on experience with Big Data technologies such as Spark and Hadoop | You have an advanced understanding of cloud data warehousing solutions | You have extensive experience with database modeling and design | Cooler Perks | For the right candidate, we have a competitive compensation and ownership package with an excellent benefits package that includes medical, dental, vision, 401(k), life insurance, paid time off, and many other perks. Come join our fast-growing team. You can be based in Chicago or possibly elsewhere. |  | We are proud to be an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, native origin, sexual orientation, age, citizenship, marital status, disability, gender identity or veteran status. | Powered by JazzHR | wz8mTPh8Go",Remote,Data Engineer
Intellectual Ventures,/company/Mammoth-Media/jobs/Data-Analytic-Engineer-807dc5260af9f052?fccid=b81e21c458801fe4&vjs=3,"Job detailsSalary$90,000 - $110,000 a yearJob TypeFull-timeNumber of hires for this role1QualificationsBachelor's (Preferred)SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Full Job DescriptionTitle: D **ata Analytics EngineerReporting to: VP, EngineeringOverview Mammoth Media is seeking to add a Data Analytics Engineer to the team.The Data Engineering team is passionate about developing systems that ingest, analyze and validate data for a growing user base on several mobile applications. Data engineers are responsible for maintaining highly scalable data pipelines used to generate dashboards for our business team, providing near real-time insights into our products and their user acquisition.The person for this role will showcase strong communication, strong data analysis, teamwork and leadership skills. A key proponent of this position is eagerness to learn and improve on an existing codebase and infrastructure. The Data Engineering team frequently reports on and evaluates data in our dashboard, helping to provide necessary visibility to our business team and our top KPIs.About the Job: Design, develop and maintain data processing pipelines using modern technologies within AWS.Monitor and support data pipelines to honor internal and external SLA’s.Build complex workflows and orchestrate data dependencies.Improve analytics, modelling and data design.Help maintain code quality, organization, and testing.Work closely with product owners to understand business requirements.QualificationsBachelor's degree or higher in a quantitative/technical field (e.g. Computer Science,Economics, Finance, Mathematics, Statistics, Engineering) OR equivalent experience.2-4 years building and maintaining data and analytics systems.2-4 years programming experience in at least one of the following mainstream languages: Python, Java, Scala, Go, Node.Proficient in SQL and experience with RDBMS/NoSQL databases.Strong understanding of architecting, maintaining and developing cloud technologies within AWS.Demonstrated ability to deliver and communicate progress.Proficient understanding of code versioning tools ( Git ).Experience or strong interest in delivering data products and serverless architecture.Bonuses, but not required: Experience with any of: Redshift, S3, Firehose, Kinesis, Kafka, EMR, Snowflake, Lookr,PeriscopeStartup experienceDemonstrated coding skills (github, open source)Benefits and Perks: Competitive compensationUnlimited PTO + 11 company holidaysMedical, dental and vision coverageEmployer sponsored 401kFlexibility to work from home or in-officeAbout Mammoth: Mammoth Media is the social entertainment studio for Generation Z. We build, publish, and monetize interactive short-form content through multiple apps – Yarn, Wishbone, and CatchUp– which collectively serve over 32 million MAUs and are consistently ranked in the Top 100U.S. Apps charts.Powered by a team of technology &amp; gaming experts, engineers, and content strategists,Mammoth is actively defining a new category in mobile entertainment through the intersection of hypercasual gaming. Across our apps, we’ve partnered with legendary media &amp; technology companies including Marvel, Archie Comics, Snapchat, TikTok and many more.Mammoth Media is backed by Greylock Partners and was incubated by Science Inc., theLA-based startup studio and investment firm behind Dollar Shave Club (acquired by Unilever),FameBit (acq. by Google/YouTube), HelloSociety (acq. by The New York Times Company),DogVacay (acq. by Rover) and more.Job Type: Full-timePay: $90,000.00 - $110,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceHealth insuranceReferral programVision insuranceSchedule:8 hour shiftDay shiftMonday to FridayEducation:Bachelor's (Preferred)Experience:SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Work Location:Fully RemoteVisa Sponsorship Potentially Available:No: Not providing sponsorship for this jobThis Job Is Ideal for Someone Who Is:Dependable -- more reliable than spontaneousPeople-oriented -- enjoys interacting with people and working on group projectsAdaptable/flexible -- enjoys doing work that requires frequent shifts in directionDetail-oriented -- would rather focus on the details of work than the bigger pictureAutonomous/Independent -- enjoys working with little directionInnovative -- prefers working in unconventional ways or on tasks that require creativityHigh stress tolerance -- thrives in a high-pressure environmentThis Company Describes Its Culture as:Innovative -- innovative and risk-takingAggressive -- competitive and growth-orientedTeam-oriented -- cooperative and collaborativeCompany's website:www.mammoth.laCOVID-19 Precaution(s):Remote interview processVirtual meetings",Remote,Data Analytics Engineer
Verizon Media,/rc/clk?jk=5551dcfe92783a11&fccid=b44c7339866c4605&vjs=3,"Working with William Hill, you will be at the heart of the technological revolution with one of the world's most trusted betting and gaming companies. William Hill deals with projects ranging from desktop or mobile casinos and betting sites, to name a few. We process 500 online Sportsbook bets per second each Saturday, that's the same as the number of orders processed by Amazon UK, on its busiest day of the year. We deal with more than 20 million users daily; impressed? You can be sure there are many more challenges waiting for you. |  | When we say cutting edge, we mean it. Here, you can work on highly reliable systems with low latency, much like the transactional systems of the best financial institutions, but…with the fun included. |  | You will have access to development opportunities, including IT conferences, internal training, and lunch and learn sessions. You will be part of a great working atmosphere, performing complex work in a collaborative team of amazing people, with forward-thinking managers. You will have the opportunity to make an impact. |  | What You Will Do: |  | We're looking for a Data Quality Assurance Specialist/ QA Engineer to join our growing data team! |  | As the Data Quality Tester/ Engineer, you will: |  | Lead the collection, analyzing and testing of data for our data warehouse | Be an expert in writing and executing test cases, managing and analyzing large sets of data, creating automation scripts and communicating relevant insights to the team | Love to collaborate with other data professionals, engineers, and business operation experts to understand and address reporting needs | Experience with both traditional and big data streaming | Create and maintain complex SQL queries such as; type 2 triggers, various joins, aggregations, groupings and more | Execute ETL jobs using orchestration tools (airflow/scheduler/SQL agent) | Have a working knowledge and testing experience of ETL | Be proactive with leadership abilities with a strong ""can do attitude"" | Possesses heavy experience with automation testing for ETL | Highly detailed oriented | Strong willingness &amp; comfort taking on and challenging development approaches | Conduct and facilitate UAT | Previously worked in agile and scrum |  |  What You Will Need: |  | 5+ years' experience Data Warehousing/Business Intelligence concepts from QA perspective | Expert Level Cloud Experience | Experience with EDW Master Data | Fluency in EDW source to target testing (SQL scripting) | Hands-on experience with Data transformation/ manipulation testing (SQL scripting) | Working Knowledge of data quality/ completeness validation (SQL scripting) | Experience in running the scheduler (Mainframe ESP) | Firm grasp on testing technics (ie: black box testing, white box testing) | Expert level SQL skills | Expert level Python skills | Experience with creation of Test Plan/ completion documents |  |  | William Hill provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, creed, national origin, ancestry, sex, age, physical or mental disability, pregnancy, veteran or military status, genetic information, sexual orientation, gender identity or expression, marital status, civil union/domestic partnership status, familial status, domestic violence victim status, or any other legally recognized protected basis under federal, state or local laws. William Hill complies with applicable federal, state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training |  |  | Essential Functions/Exposures: |  | Essential Functions/Exposures: | Must be able to sit for extended periods of time | Must be able to type and talk on the phone for extended periods of time |  |  | #LI-OG1",Jersey City NJ,QA Engineer | Data Quality
William Hill US,/rc/clk?jk=d7da5be46b1325e7&fccid=f32a31323adb5fda&vjs=3,"*Allows for a Remote schedule* |  | As a Data Engineer here at Carilion you will work with a talented engineering group to design, develop and test both ETL and ELT data pipelines which ingests and analyzes large datasets. Leveraging technologies/tools like Talend, IBM DataStage, python, java and javascript you will implement data integration projects on both on-prem and cloud platforms such as AWS and Snowflake. This role involves building the data pipelines which feed the cutting edge of population health and analytics for Carilion Clinic and working to solve complex problems to ensure the best quality data is available to support reporting and patient care initiatives. | Be at your best with an organization that equips you to do so. Whose people unite in making lives better. At Carilion Clinic, we help communities stay healthy and our region grow stronger through compassionate care, medical education and research, and neighborhood outreach. As part of our team you can expect professional stability, strong technological resources, and the advancement potential of a regional leader. | Based in Roanoke, VA, we are an award-winning, community-based network of hospitals, primary and specialty physician practices, and affiliations with prestigious academic medical institutions. Explore how joining a regional leader can enhance your ability to learn, grow, and succeed. | Minimum Qualifications | Education: Bachelor’s degree in computer science, information systems, health care business or equivalent experience required. Master’s degree preferred. | Experience: Minimum: 6+ years of relevant experience required. Minimum: 3-5 years working with enterprise data sources and ETL tools such as IBM DataStage or Informatica. Preferred: Financial and healthcare knowledge. Experience supporting technical service delivery relationships. Experience translating business needs into designs and specifications. Experience managing projects, testing and quality control. Experience with metadata management. Experience with data profiling and data quality analysis. Experience working with a coordinated data management environment (CDME) preferred. Experience facilitating requirements definition meetings/sessions using structured approaches such as Joint Application Development (JAD). Experience with IBM InfoSphere and IBM data models a plus. Proficient with dimensional databases, cubes and columnar data structures. Proficient with Microsoft Office applications such as Office, Project, Visio, SharePoint, | Licensure, certification, and/or registration: Maintain application vendor certifications as needed. Valid VA driver’s license. Preferred certifications: ITIL V3, CBAP, CBIP, CDMP, DGSP, PMP. | Other Minimum Qualifications: Strong listening skills. Strong verbal and written communication skills demonstrating proper business use of grammar and punctuation. Strong presentation skills clearly conveying information, ideas, concepts, and instructions to a variety of audiences. Meeting management skills including agendas, facilitation, minutes and time management Negotiation skills to resolve conflict and build cooperation. | Our Values | Below are our core values that we strive to embody and expect of all our team members: |  | Collaboration: Working together with purpose to achieve shared goals. | Commitment: Unwavering in our quest for exceptional quality and service. | Compassion: Putting heart into everything we do. |  | Courage: Doing what’s right for our patients without question. | Curiosity: Fostering creativity and innovation in our pursuit of excellence. | Requisition Number: 64735 | Employment Status: Full time | Location: Technology Services Group | Shift: Day | Shift Details: M-F 8-5 with on-call rotation | Recruiter: MARK A MISKOVIC | Recruiter Phone: | Recruiter Email: mamiskovic@carilionclinic.org |  | This is Carilion Clinic ... |  |  | An organization where innovation happens, collaboration is expected and ideas are valued. A not-for-profit, mission-driven health system built on progress and partnerships. A courageous team that is always learning, never discouraged and forever curious. |  | Headquartered in Roanoke, Va., you will find a robust system of award winning hospitals, Level 1 and 3 trauma centers, Level 3 NICU, Institute of Orthopedics and Neurosciences, multi-specialty physician practices, and The Virginia Tech Carilion School of Medicine and Research Institute. |  | Carilion is where you can make your own path, make new discoveries and, most importantly, make a difference. Here, in a place where the air is clean, people are kind and life is good. Make your tomorrow with us. |  |  | Equal Opportunity Employer |  | Minorities/Females/Protected Veterans/Individuals with Disabilities/Sexual Orientation/Gender Identity |  | Carilion Clinic is a drug-free workplace.",Roanoke VA 24016,Data Engineer (REMOTE)
Carilion Clinic,/rc/clk?jk=1939b4c2e8f58d56&fccid=105ecfd0283f415f&vjs=3,"Job Description: | Job Title: Data Engineer / NiFi Engineer | Location: Remote | Travel/Relocation: | Experience with high velocity high volume stream processing Apache Kafka Apache NiFi StreamSets etc | Experience configuring and administering NiFi installations | Experience in building data ingestion workflows pipeline flows using NiFi NiFi registry | Experience creating custom NiFi processors | Experience with NiFi performance tuning Strong proficiency in Java and at least one other programming language such as JavaScript or PowerShell | Knowledge of security principles and methods | Experience with Agile methodologies such as Scrum ideally in a DevOps environment | Solid judgment and analytical skills strong follow up and organizational skills | Ability to explain technical concepts and adjust messaging based on the audience including non-technical groups | Ability to influence through outstanding interpersonal skills collaboration and negotiation skills | Ability to work well within a team environment as well as independently | Ability to optimize workflows based in NiFi Desired Skills | Experience with NiFi supported scripting languages Python Groovy | Experience creating custom NiFi processors Full Software Development Life Cycle SDLC experience | Experience with performance tuning of NiFi processing Familarity with AWS Snowflake Wherescape | About Capgemini: | Capgemini is a global leader in consulting, digital transformation, technology and engineering services. The Group is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50-year+ heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. Today, it is a multicultural company of 270,000 team members in almost 50 countries. With Altran, the Group reported 2019 combined revenues of €17billion. | What we offer: | Your career matters to you and is important to us too. Because your goals and needs are constantly evolving, we offer visibility, leeway and support to help you grow and progress in your career. This approach builds notably on our comprehensive competency framework, our personal development, training and career management programs, and our University innovative and business-focused learning curriculums. | We promote a culture of diversity. We believe working with talented individuals from different backgrounds and points of view is a strategic advantage and an ongoing opportunity. Diversity enriches our creative solutions and adds value for our clients. | With the digital tech sector growing at a rapid pace and women significantly underrepresented in the industry, we are determined to inspire and recruit more women into technology and build diverse teams that reflect the clients we serve. | Our Shared values have been at the heart of the group since our formation. They are honesty, boldness, trust, freedom, team spirit, modesty and fun. These values influence the way we meet client needs while respecting the regulatory requirements of each country in which we operate, and the way we promote ethically sound practices within Capgemini and in our partnerships. | Capgemini is committed to building a workforce of employees with diverse backgrounds and work experiences. We strongly encourage women, veterans and active military service personnel to apply. | Disclaimer: | Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law. | This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship. | Click the following link for more information on your rights as an Applicant - http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law | Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.",New York NY,Senior Data Engineer / NiFi Engineer
Tinder,/rc/clk?jk=1376ad13631bc7f5&fccid=43014b1412e0a7b6&vjs=3,"About Pinterest: |  | Millions of people across the world come to Pinterest to find new ideas every day. It’s where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you’ll be challenged to take on work that upholds this mission and pushes Pinterest forward. You’ll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. |  | The People Insights and Analytics team is looking for a data engineer to build and maintain a data platform to help Pinterest make better decisions about its employees using data. You’ll own the flow of data between multiple systems that track data for our employees and employee data warehouse systems, which fuels analytics, dashboards, surveys and other tools that help our leaders and managers build a world-class people experience at Pinterest. The successful candidate will have experience working with enterprise data warehouses, be passionate about empowering people to make data driven decisions and be excited to own and build a data platform. |  | What you’ll do: |  | Architect the data strategy for the People Team. We’re just getting started building a system that can scale with our business and analytics needs | Establish relationships across People team, Engineering and the Business to understand our data landscape, and create a strategy to bridge the gaps | Utilization of data from disparate sources, making it available to data scientists, analysts, and other users using scripting and/or programming languages (Python, Java, C, etc) | Evaluate structured and unstructured datasets utilizing statistics, data mining, and predictive analytics to gain additional business insights | Design, develop, and implement data processing pipelines at scale | Present programming documentation and design to team members and convey complex information in a clear and concise manner | Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes | Maintain suite of data tools platforms and dashboards in Tableau, Django, d3, Jupyter, R and other analytical tools. |  | What we’re looking for: |  | Hands-on experience in data modeling, data visualization and pipeline design and development | Hands-on experience with data platforms (Hadoop, Hive, Presto, Snowflake, Cloudera ) and familiarity with data visualization (Tableau, D3) technologies | Strong in at least one of these programming languages: Python, Java, Go | Comfortable working across a wide array of technologies, project types, and business requirements. Strength in AWS and cloud data technology strongly preferred | Experienced at organizing and executing against sprints | Ability to handle confidential material discreetly | #LI-MJ1",San Francisco CA 94103,Data Engineer People Insights & Analytics
Blue Shield of California,/rc/clk?jk=34dfd9e54dec15d8&fccid=f5f38e6ca00f718e&vjs=3,"Modern Health- |  | Modern Health is a mental health benefits platform for employers. We are the first solution to cover the full spectrum of mental well-being needs through both evidence-based digital content and professional support from a global network of certified coaches or therapists all in one comprehensive app. Whether someone wants to proactively manage stress or treat depression, Modern Health guides people to the right care at the right time. We empower companies to help all of their employees be the best version of themselves, and believe in meeting people wherever they are in their mental health journey. |  | We are a female-founded company, backed by investors like Kleiner Perkins, Founders Fund, John Doerr, Y Combinator, and Battery Ventures. We partner with 220+ global companies like Lyft, SoFi, Pixar, Gusto, Okta, and Udemy that are taking a proactive approach to mental health care for their employees. Modern Health has raised more than $170 million in less than two years with a valuation of $1.17 billion, making Modern Health the fastest entirely female-founded company in the U.S. to reach unicorn status. We are looking for driven, creative, and passionate individuals to join in our mission. |  | In 2020 we tripled our employee headcount, doubled our customer base and grew our revenue 10x. As a hyper-growth company we have maintained our people-first culture winning awards including Fortune's Best Workplaces in the Bay Area 2021 and maintaining an employee retention rate of 99.97% and an overall Glassdoor rating of 4.9 out of 5. | An inclusive and diverse culture are key components of mental well-being in the workplace, and that starts with how we build our own team. If you're excited about this role, we'd love to hear from you! |  | The Role- |  | Using data to make informed decisions is a strategic imperative and competitive advantage for Modern Health. As a Data Engineer, your role is integral in ensuring we have the infrastructure and systems in place to make this possible across the company. You will own our data pipelining end-to-end and work cross-functionally to help us achieve our goals. The ideal candidate should be comfortable using Looker, SQL, Postgres, AWS (RDS, ECS, EC2), and Terraform. |  | What You'll Do- | Partner with people across the company (PM, engineering, data analysts, operations, care, customer success) to understand data needs and pain points. | Build systems, tools, and documentation to enable and empower data customers from around the company to confidently build accurate dashboards, run data research projects, instrument features, and support outcome studies. | Define and iterate on our data models and pipelining to best support engineering, product, and business goals. | Own data integrity for our key product and company health KPIs. | Experience using SQL to explore data and build dashboards. | Who You Are- | Experience in ETL design and implementation. | Experience in data modeling and schema design. | You are a self-starter and see projects through from inception to completion with minimal oversight. | You have experience working cross-functionally and partnering with stakeholders across departments of varying levels. | You care deeply about product experience and quality. | Health tech experience is preferred, but not required. | Bonus points if you have experience with AWS. | Benefits- | 100% coverage for Medical / Dental / Vision | Stipend towards mental health benefits | 401k plan | Flexible PTO | Passionate team dedicated to making a positive impact | Awesome office with snacks and catered lunch in the Financial District | Generous parental leave policy | Unlimited career growth opportunity",San Francisco CA,Data Engineer
Tesla,/rc/clk?jk=37372050238eb8bc&fccid=64f09402c2ec40d0&vjs=3,"The Data Engineer, Baseball Systems position will be a member of the baseball operations software development team, and is responsible for integrating, collecting, processing, and storing many sources of baseball data, as well as designing and building new data solutions. This position must be comfortable with on-premises and cloud solutions, and take the initiative to explore new optimizations and cutting-edge data technologies. This individual will work closely with our data architect, analysts, developers, and other members of baseball operations. | Responsibilities: | Build leading-edge baseball solutions together with the software development team, analysts, and others on new and existing baseball systems | Build and maintain integration pipelines, often via an API or file-based, while also identifying areas of improvement and spending time to re-architect when required  Build and maintain infrastructure to optimize extraction, transformation, and the loading of data from various sources | Design, build, and maintain data warehousing solutions for the software development and analytics teams  Build and maintain tools for the analysts to enable more efficient and extensive data modeling and simulation efforts | Participate in key phases of the software development process of critical baseball applications, including requirements gathering, analysis, effort estimation, technical investigation, software design and implementation, testing, bug fixing, and quality assurance | Actively participate with software developers and data architects in design reviews, code reviews, and other best practices | Work closely at times with baseball analysts to design and implement data solutions | Respond to and resolve technical problems and issues in a timely manner | Technical Skills &amp; Qualifications: | Bachelor’s degree in Computer Science, Software Engineering, Computer Engineering, Statistics, Information Systems, or a related field | 2-3 years of experience in a Data Engineer role | Strong SQL skills, including query optimization and database design | Experience building custom API integrations, interfacing with JSON, XML, and custom data structures | Experience with AWS, GCP, or Azure cloud services, such as Cloud SQL, RDS, Redshift, Azure SQL, Azure SQL DW, or others | Experience building data solutions using Python, C#, C++, Ruby, or other languages | Experience with scheduling and workflow management platforms, such as Airflow | Experience with big data frameworks such as Hadoop or Spark is a plus | Experience with R and RStudio is a plus | Experience with engineering and productionizing statistical/ML models a plus | Professional experience as an analyst/data scientist (or extensive coursework) a plus | General Skills: | Ability to work autonomously and as a team in a fast paced environment | High level of attention to detail with the ability to multi-task effectively | Comfortable working remotely using Zoom, Teams, Slack, Trello, and other tools to communicate with all team members | High degree of professionalism and ability to maintain confidential information | Excellent organizational and time management skills | An understanding of baseball is a plus | Prospective employees will receive consideration without discrimination based on race, religious creed, color, sex, age, national origin, handicap, disability, military/veteran status, ancestry, sexual orientation, gender identity/expression or protected genetic information.",Boston MA,Data Engineer Baseball Systems
ClientSolv Technologies,/rc/clk?jk=0124d6f789172412&fccid=663fcf4fb2ab17e0&vjs=3,"Data Engineer – Data Integration Architect (FTE) (New York, NY) |   | We are looking for a Date Engineer – Data Integration Architect person for a leading Global Growth Equity company. The person will work as aSr Associate team member for the Data Strategy team. This position will report directly to the VP, Data Strategy. The Data Engineer will work as a full stack data developer and architect with primary focus on data integration &amp; preparation, data as a service using API Management Services, and self-service data platform to provide a distinctive user experience for mobile and global team of professionals, investors, and other stakeholders. |  |  | This is a Full-time employee position with the client onsite in their New York City offices. (once shelter in place is lifted) |  |  | The client offers a competitive salary, full benefits package, and relocation assistance. |  | US citizen, and Green Card Holder are accepted. |  |  | Responsibilities: |  |  Responsible for architecture, design, administration, and development of Data Ingestion Layer as part of Data Strategy, especially using service-based architecture. |  |  Define and develop standards, configurations, and operational/administration guidelines for data integration layer to provide it as a service to Data Science and AppDev teams. |  |  Create integration jobs connecting to heterogeneous data sources like SaaS applications, hosted applications, private cloud applications, databases etc. using web-api’s, rest/soap api’s or varying data connectors. |  |  Develop ETL, ELT and other service-based integration jobs for overall Data application delivery |  |  Contribute towards all development of SQL and NOSQL scripts, tables, Views, stored procedures, extended functions in .Net/python etc in data layer. |  |  Interact with data team members in design and development on overall data application delivery, including data ingestion &amp; preparation, data design &amp; development, and data analytics &amp; reporting. |  |  Work with Security &amp; Infrastructure, PMOs and App Dev teams to assist in the design and development of custom solutions that streamline business processes. |  |  |  | Required Qualifications : |  | 5+ years as a full stack data developer experience with 2+ years in an integration architect seat |  | Preferred experience working in the Finance industry |  | Experience in design and development of cloud-based data integration tools, especially Talend Cloud, and/or Azure Data Factory, Data Bricks, or Alteryx. |  | Proficient in heterogeneous data sources, cloud providers like Salesforce, Workday, Box etc., data aggregators, web sources or relational/analytical data sources providing structured, semi-structured or unstructured data. |  | Experience in data development tools like SQL Server, Snowflake, and Azure data services like Azure SQL, Azure Synapse, Azure Data Lake Services, CosmoDB, Redis Cache et al. |  | Working knowledge of coding tools and their libraries like python, .Net or Java |  | Bachelor’s degree",New York NY,Data Engineer - Data Integration Architect (FTE)
Spotify,/rc/clk?jk=4654a87e81116958&fccid=fe404d18bb9eef1e&vjs=3,"Engineering | Data | The Platform team creates the technology that enables Spotify to learn quickly and scale easily, enabling rapid growth in our users and our business around the globe. Spanning many disciplines, we work to make the business work; creating the frameworks, capabilities and tools needed to welcome a billion customers. Join us and help to amplify productivity, quality and innovation across Spotify. | Location | New York, NY | Job type | Permanent | We are looking for SRE’s to join our team of hardworking engineers that share a common interest in distributed backend systems, big data, scalability and continued development. By joining the Data and Insights organization, you’ll be a key contributor in making the systems that power our large scale data processing, insights, and machine learning efforts more reliable. Above all, your work will impact the way the world experiences music. | The Insights Platform teams within Data and Insights have a wide range of responsibilities. The teams enable the consumption of Insights by helping anyone at Spotify answer their questions using data. Additionally, the teams build tools to support Data Scientists in their end to end workflow, querying data in BigQuery, exploring data in Jupyter notebooks, and visualizing data with Tableau or QlikSense. Join us and help to amplify efficiency, quality and innovation across Spotify! | What you'll do: | You’ll design, develop and help operate our data warehouse, BigQuery, and help make it more reliable, accessible, and scalable as our usage grows and becomes more sophisticated | Drive reliability on our Jupyter notebook platform running on Kubernetes such that we can scale it and provide a stable platform for Data Practitioners to iterate and explore data | Be a technical leader within the team you work with and within Spotify in general | Be a valued member of an autonomous, multi-functional agile team | Build, automate, maintain, scale, and supervise user-facing systems using standard methodologies, with reliability and scalability in mind | Work with the other engineers to debug and fix issues | Implement high-quality release engineering practices to facilitate rapid development, safe changes, and engineer productivity | Maintain system architecture documentation and runbooks | You’ll collaborate with other engineers, product managers, and designers to identify and tackle exciting problems, creating an awesome engineering experience within Spotify | You’ll initiate, influence and drive technical projects across teams within Spotify | You’ll use industry standard, cloud native tech, which means easily transferable skills and a focus on your professional development | Hack on what you want during regular hack days and bi-annual hack weeks | Work from our awesome office in New York City where we solve problems in Data, Monetization, Music Recommendations, Features for Artists, and Social Networking | Who you are: | You are an experienced site reliability engineer | Know how to write distributed, well designed services in Java | Have experience writing SQL, including writing and analyzing complex joins, reading explain plans, optimizing indexes, etc | You know how to work with large scale data systems | You know how to use Docker | Know how to use automation tools like Terraform, Puppet, or Ancible | You will be able to apply your knowledge of the Java runtime environment to help us improve our backend service and data landscape. | Are experienced with deploying and operating services on Linux. | Have a deep understanding of system design, data structures, and algorithms. | You care about quality and you know what it means to ship high quality code. | Google Cloud Platform experience is a bonus. | As a great influencer with great communication skills, you love sharing your knowledge with others and helping them grow. | Perks of being in the band | Extensive learning opportunities, through our dedicated team, GreenHouse. | Flexible share incentives letting you choose how you share in our success. | Global parental leave, six months off - fully paid - for all new parents. | All The Feels, our employee assistance program and self-care hub. | Flexible public holidays, swap days off according to your values and beliefs. | Spotify On Tour, join your colleagues on trips to industry festivals and events. | Learn about life at Spotify | You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens. | Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service with a community of more than 345 million users.",New York NY,Site Reliability Engineer - Data & Insights
Vera Institute of Justice,/rc/clk?jk=ce6b1e48b74038d7&fccid=d25525f95db539b9&vjs=3,"Job detailsJob TypeContractFull Job DescriptionCompany Description | ClientSolv Technologies is an IT solution firm with over a decade of experience serving Fortune 1000 companies, public sector and small to medium sized companies. ClientSolv Technologies is a woman-owned and operated company that is certified as a WMBE, 8a firm by the Federal government's Small Business Administration. |  | Job Description | We are seeking an exceptional Big Data Developer for a 6 month contract-to-hire role in Englewood, CO. This role will be onsite in Englewood, CO (no telecommuting). If you have expertise with either with Spark or Map Reduce development and enjoy working with cross-functional teams, let's talk! | Job Duties and ResponsibilitiesEvangelist for data engineering function leveraging Bigdata frameworkOptimizing data and data pipeline architectureSupport software engineers and data scientistsContribute to our cloud strategy based on prior experienceUnderstand the latest technologies in a rapidly innovative marketplaceIndependently work with all stakeholders across the organization to deliver enhanced functionality |  | Qualifications | Top skills (Technical): | Core Big Data development hands on experience using Spark or Map Reduce | Java, Scala ,Python | Data Engineering | AWS knowledge is a nice to have skill set | Additional skills: | Minimum of 5 years of experience delivering data solutions on a variety of data warehousing, big data and cloud data platforms. | 3+ years of experience working with distributed data technologies (e.g. Spark, Kafka, Flink etc) for building efficient, large-scale ‘big data’ pipelines; | Strong Software Engineering experience with proficiency in at least one of the following programming languages: Spark, Python, Scala or equivalent | Experience with building data ingestion pipelines both real time and batch using best practices | Experience with building streaming ingestion pipleline using Kafka streams, Apache Flink, or others | Strong experience in writing SQL and PLSQL | Experience with Cloud Computing platforms like Amazon AWS, Google Cloud etc. | Experience supporting and working with cross-functional teams in a dynamic environment | Experience with relational SQL and NoSQL databases, including Postgres, and Mongodb. | Experience with change data capture tools (CDC) preferred such as Attunity/goldengate | Experience with scheduling tools preferrable Control-M,Airflow or AWS Step functions. | Strong interpersonal, analytical, problem-solving, influencing, prioritization, decision- making and conflict resolution skills | Excellent written/verbal communication skills. | Additional Information | This 6 month contract-to-hire role will be onsite in Englewood, CO. There are not telecommuting options available for this role.",Meridian CO,Big Data Engineer
Modern Health,/rc/clk?jk=2cad2950c2940592&fccid=86e9be6ce380173e&vjs=3,"Role: | Tesla is currently seeking a Senior Engineer to join our data center team. This role will provide network design, implementation and operational support for Tesla's data centers. | Responsibilities:Help design, build and maintain new and existing data centersWork closely with other team members on design and initiatives; maintain and grow existing data center networksWork with Tesla’s key application teams to support their growthProvide high availability &amp; reliability to network.Requirements gathering, analyze, and propose solution to networking needsMonitor, analyze, and report metrics of network servicesDevelop automation methods to rapidly deploy, configure, and update network equipmentAssist with network troubleshootingConduct product POC evaluationDocument network knowledge base and operational “Run-Book”Must be able to work occasional weekends, after hours, and holidaysParticipate in on call rotationMay require unscheduled after-hours work. 10-20% travel required as necessary | Qualifications:8+ years’ experience mid-large global enterprise networking infrastructureDesigning and managing mid/large-scale networks in a global environmentJuniper, Arista and Palo Alto Networks hardwareExtensive experience in IP networking, L2/L3 network protocols (spanning-tree, OSPF, BGP), TCP/IP, DHCP, DNS, end to end QOS, VLAN, VRRP, LACP, MC-LAG, EVPN with VXLAN, ACL and infrastructure cablingExperience with various tools such as Protocol Analyzer, SNMP, flow, IPAM, RADIUS, Splunk, network taps, and load/stress testingExperience with developing scripts to automate infrastructure deployment and collect metricsProficient in Microsoft Word, Excel, PowerPoint, and VisioCCNP, JNCIP preferred | Skills and Competencies:Excellent organizational skills and solid team playerSolid analytical skills to troubleshoot high level, complex technical problemsExcellent verbal, written, and communicationMaintain a high degree of professionalism and integrity",Austin TX,Sr Network Engineer-Data Center
Pinterest,/rc/clk?jk=7ae1f74560dff228&fccid=104dac531ec80260&vjs=3,"Job Summary | The Data Engineer (Consultant) will be an integral member of the Data Engineer Team. This is a hands-on role which will be responsible for design, development, and implementation of cutting-edge sophisticated data integration, data warehouse and data mart (ELT/ETL) solutions using Netezza, Oracle, Informatica, and Denodo. An ideal candidate will have extensive knowledge of the data warehouse architecture, capabilities, system setup, data integration, data modeling concepts. |  | The primary responsibility of this role is to support development operations, implementation, design, delivery and support of large enterprise data warehouse/BI environments, incident resolution, build reusable automation procedures and frameworks. This position requires strong collaboration with solution leads, data modelers, analysts, business partners and other developers to understand business requirements and provide appropriate data integration solutions in alignment with solution implementation architecture. |  | Qualifications | 8+ years of proven hands-on experience in data integration, data warehouse, BI solution design, development, and implementation. | 5+ years of hands-on experience in at least one relational data source such as Oracle, SQL Server and Data warehouse MPP applications such Netezza, Teradata etc. | Experience working in a cloud environment – AWS, Azure or GCP, related cloud specific flavor of big data/Hadoop/MPP platforms and other technology components. | Experience with AI-ML tools like RapidMiner, Databricks or H20.AI. | Experience with NOSQL databases like MongoDB, Cassandra or MarkLogic | Hands-on experience in Python coding and familiarity with Python libraries used in data engineering and machine learning like Pandas, NumPy etc. | 5+ years of hands-on experience with Data Integration, Data Quality and Data Virtualization tools like Informatica PowerCenter, Informatica Data Quality and Golden Gate Replication. | Proven expertise in writing efficient advanced and analytical SQL and Unix shell scripts. | Hands-on experience with source version control, continuous integration, and experience with release/change management delivery tools /methodology. | Extensive experience in triaging data issues, analyzing end to end data pipelines and in working with business users in resolving issues. | Prior experience in leading a technical team, providing technical direction and mentoring junior data integration developer. |  | Knowledge and Experience | Expert grasp of data warehouse design techniques including slowly changing dimensions, aggregation, partitioning and indexing strategies. | Excellent communications and client interfacing skills as with an ability to work in a highly collaborative environment. | Prior experience in healthcare Payor/provider space. | Design and develop reusable components, code, and document custom automation frameworks, maintain scripts, and update these items as needed to build continuous delivery pipelines, ensuring rapid availability of the product. | Work closely with Operations Production Support team in resolving escalated high priority incidents and the development coding issues. |  Minimum Bachelor’s Degree in Computer Science or other related disciplines.",Oakland CA,Data Engineer Consultant
Boston Red Sox,/rc/clk?jk=f9cabd4f4efeb1f3&fccid=4e6d7e83ad8b9362&vjs=3,"The Security Analytics Engineering group is looking to hire a Senior Data Engineer who loves working on complex problems and getting things done. The teams focuses on using analytics, machine learning, and threat research on petabyte-scale data to deliver security value to millions of business users across thousands of businesses. | The product portfolio is a part of the rapidly growing Carbon Black Cloud platform that delivers next-generation endpoint protection capabilities from the cloud. Now with the full resources of VMware, you have the opportunity to make an impact and build upon Carbon Black’s success. | Responsibilities include: | Design, implement, and maintain end-to-end data pipelines with an understanding of ML lifecycles | Drive development of data products in collaboration with data scientists and machine learning engineers. | Build and scale data platform infrastructure that powers analytics both batch and real-time. | Responsible for overall system architecture, scalability, reliability, and performance of one or more data pipelines and data platform components. | Provide Architecture guidance and work closely to uplevel the engineering organization. | What you bring: | 6-8 plus years of experience in relevant areas in building large scale, distributed systems | Experience in big data technologies. (Spark, Flink, MapReduce, HDFS, Hive, Presto, Avro, Parquet, Airflow, etc) | Experience in large scale messaging system (Kinesis/Kafka) | Experience in the public cloud (e.g. S3, RDS, Athena, VPCs, etc) | Practical experience in building solid, distributed, internet-scale enterprise-class solutions/services in Java/Python | Experience with big data design, ETL, technology, efficient designs for distributed systems, and big data environments. | Bonus: | Experience in building data platform infrastructure, tooling, and automation | Experience in the Security domain | Category : Engineering and Technology | Subcategory: Software Engineering | Experience: Manager and Professional | Full Time/ Part Time: Full Time | Posted Date: 2021-02-10 | VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com. |  | Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.",New York NY,Senior Data Engineer - Opportunity for Working Remotely New York NY
Bosch Group,/rc/clk?jk=2ab55f9f5fbed1fa&fccid=a2faf1301ac6ad4b&vjs=3,"Company Description |  | Bosch Building Technology | We are currently building up a team and searching for a high-potential Data Engineers to create Bosch's next-generation of cloud-based software products for intelligent traffic systems. This team delivers analytic products to our customers via online channels and smart connected devices. We work together with Bosch Research and Development centers globally to use the latest technologies to create powerful and compelling experiences that will change the way that our users experience Bosch products. | Data Engineer | You are an expert in Big Data analysis and know how to apply Machine Learning (ML) techniques to solve challenging data driving problems? Can you build end-to-end scalable data processing pipelines? Are you passionate about building robust data pipelines to support model-training, deployment, and monitoring in production? Do you want to apply your skills to solve real-world problems? | You will contribute to the development and deployment of next generation algorithms to support behavioral classification, forecasting and prediction, data fusion, or in general improving our current algorithms using advanced data driving models. This role demands deep understanding of algorithm design, ML, and software stack to support end-to-end data analytics and ML products. |  | Job Description |  | Your Responsibilities:Develop next generation data driven models and algorithms for intelligent transportation use-casesOptimize and scale algorithms to real-world scenarios and squeeze out the best predictive performanceArchitect and deploy robust data infrastructure to support ML model training, evaluation, deployment, and monitoring in production (e.g., MLflow, Airflow, Luigi, Metaflow, Prefect)Support tooling for data persistence, transformation, exploration and visualization (e.g., Spark, Hive, Django, Dash, Streamlit)Build and deploy data infrastructure on cloud (e.g., AWS, Azure) |  | Qualifications |  | Basic Qualifications: | MS in Computer Science, Computer Engineering, Software Engineering, Electrical Engineering, Computational Science, Mathematics, Machine Learning, Physics or Data Science | 2+ years of work experience in architecting data infrastructure in production environment | 2+ years of work experience with Machine Learning techniques (e.g. Deep Learning, SVM, clustering, prediction, time serial analysis) solving new and challenging problems in a product development environment | Preferred Qualifications: | PhD in Data Mining, Traffic Optimization | Proficient in Python, C++ or Java | Familiar with agile development processes | Statistical background in Big Data analysis | Additional Information |  | By choice, we are committed to a diverse workforce - EOE/Protected Veteran/Disabled. | BOSCH is a proud supporter of STEM (Science,Technology, Engineering &amp; Mathematics) Initiatives | FIRST Robotics (For Inspiration and Recognition of Science and Technology) | AWIM (A World In Motion)",Pittsburgh PA 15222,Data Engineer
Zillow,/rc/clk?jk=275ace404840f952&fccid=913e1b259c8d65e2&vjs=3,"About the team | The Data Governance engineering team builds data platforms to drive effective data management at Zillow. We develop and own systems for data discovery, quality, privacy, and security that are central to Zillow's data strategy. |  | Our team enjoys creating elegant solutions to complex problems and celebrates our successes together. In this role you will be part of a collaborative work setting that promotes ownership, growth, and creativity. | About the role | Are you passionate about data quality and creating world-class tools that improve products and decisions? Zillow’s success as a leader in the real estate space relies on our customers’ continued trust in our data and our ability to immediately pinpoint and resolve any issues that arise. In this role, you will create scalable systems to uphold this promise and work closely with engineering, analytics, and AI teams to ensure that Zillow’s most critical data is of exceptional quality. You will: | Develop microservices, libraries, large-scale batch and streaming processes and self-service components to enable data producers to seamlessly incorporate the right data health evaluations into their development lifecycle. | Connect our central metadata catalog with real-time information about data and service health to drive automated action and visibility. | Build shared infrastructure abstractions that plug in to data processing and ML models for every line of business (Zillow Offers, Premier Agent, Rentals, Zillow Home Loans, New Construction, Transaction Management, etc.) | This role has been categorized as a Remote position. “Remote” employees do not have a permanent corporate office workplace and, instead, work from a physical location of their choice which must be identified to the Company. Employees may live in any of the 50 US States, with limited exceptions. In certain cases, an employee in a remote-designated job may need to live in a specific region or time zone to support customers or clients as part of their role. | Who you are | Experience developing robust, high-throughput backend services | Experience building complex, highly scalable and reliable data pipelines, using EMR, Spark, Kafka, Flink, Beam, Parquet, Hive, Avro, Airflow, or equivalents | Proficient in Python, Java, and/or Scala | Experience with frameworks for CI/CD, monitoring, testing, and alerting for data and services | Familiarity with cloud platforms (AWS, Azure, or GCP) and large data lakes | Ability to translate between business requirements and technical solutions | (plus) Familiarity with Machine Learning and AI concepts and technologies | (plus) Familiarity in front-end technologies (JS, HTML, CSS) | (plus) Interested in contributing to open source community | A degree in Computer Science or a related technical discipline or equivalent work experience | Get to know us | Zillow Group, the largest portfolio of real estate brands on mobile and the web, is building a safe, on-demand real estate experience. Whether selling, buying, renting or financing, customers can turn to Zillow's businesses to find and get into their next home with speed, certainty and ease. | We are on a mission to help people unlock their next chapter and are building transformational tools and services that create an on-demand real estate transaction experience. Millions of people visit Zillow Group sites every month to start their home search, and now they can rely on Zillow to help them finish it — and no matter what job you're in, you will play a critical role in making this vision a reality. | At Zillow Group, we're powered by our inclusive work culture, where everyone has the support and resources to do the best work of their careers. Our efforts to streamline the real estate transaction is supported by our passion to empower people and enrich lives around everything home, a deep-rooted culture of innovation, a fundamental commitment to Equity and Belonging, and world-class benefits. But, don't just take our word for it. Read our reviews on Glassdoor and recent recognition from multiple organizations, including: Human Rights Campaign (HRC) Corporate Equity Index, Fortune Best Workplaces for Technology, Fortune Best Workplaces for Millennials, Fortune Best Workplaces for Parents, Fatherly's Best Workplaces for New Dads, JUST Capital 100 Company, and the Bloomberg Gender Equality Index constituent. | Zillow Group is an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. Therefore, we provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable law. If there are preparations we can make to help ensure you have a comfortable and positive interview experience, please let us know. | See what information we collect about you.",Seattle WA 98101,Software Development Engineer Big Data - Data Quality - Remote Position
ICF,/rc/clk?jk=fc3bb32913590faf&fccid=e290923a98afee94&vjs=3,"This position is responsible for working and maintaining databases, as well as developing and maintaining system integrations and components, including but not limited to application to application integrations, internal and external API, file transfer, etc. This is a position within the Corporate IT team and requires excellent organizational skills and the ability to collaborate effectively across all levels of the organization. | Duties and Responsibilities: | Design, implement, and manage database systems, and ensure their availability, stability, reliability, and performance. | Design and develop integrations using SSIS packages, PowerShell, T-SQL, PL/SQL, or other appropriate technologies. | Participate in requirements sessions with the stakeholders and convert functional requirements into technical solutions, along with supporting materials such as use cases, designs, flowcharts, models, specifications, and reports. | Ensure the data quality and integrity in databases, as well as compliance of standards and conventions in developing programs. | Troubleshoot and resolve complex issues, including root cause analysis along with recommending and implementing preventive techniques. | Manage interfaces and automated jobs to ensure efficiency and lead effort to migrate interfaces to current technologies. | Design, develop, and implement comprehensive test and evaluation plans to ensure integrations perform efficiently and meet full requirements. | Provide and maintain technical documentation for new system processes or enhancements. | Partner with business users to provide guidance and support as necessary to help drive best practices to meet their integration needs. | Confidently present possible solutions to audiences of various sizes and technical knowledge levels weighing pros and cons. | Work with the Information Security team to ensure security standards are included, applied and maintained across all product sets within the domain. | Be part of an on-call rotation to support mission-critical production database systems. | Required Qualifications: | B.S. in information technology/computer science. | 5+ years of experience with SQL Server administration and support. | 5+ years hands-on experience developing and implementing integration solutions. | Extensive experience working with SQL, Stored Procedures, data extraction, data transformation, and data loading. | Good experience with PowerShell and SSIS packages. | Experience in SQL Server database development and troubleshooting, including typical database installation and configuration, as well as implementing database security controls to properly manage access to the data. | Experience in Agile environment - Must be able to learn and apply new technologies quickly and work within a dynamic work environment. | Preferred Skills/Experience: | Data integration experience with business systems such as Costpoint, Workday, Dynamics, OneStream, and Hyperion. | Experience with Azure DevOps, Visual Studio, TFS or VSO. | Working knowledge of other related database technologies such as: Oracle, Azure SQL, Big Data, Azure Data Factory, Azure Synapse, and with other programming languages like Python and/or C#. | Professional Skills: | Excellent written and verbal communication skills. | Work well independently as well as part of a team. | Strong analytical and problem-solving skills. | Self-starter with ability to work efficiently with minimal direction or guidance. | Data, and with other programming languages like Python and/or C#. | Working at ICF | Working at ICF means applying a passion for meaningful work with intellectual rigor to help solve the leading issues of our day. Smart, compassionate, innovative, committed, ICF employees tackle unprecedented challenges to benefit people, businesses, and governments around the globe. We believe in collaboration, mutual respect, open communication, and opportunity for growth. If you’re seeking to make a difference in the world, visit www.icf.com/careers to find your next career. ICF—together for tomorrow. | ICF is an equal opportunity employer that values diversity at all levels. (EOE – Minorities/Females/ Protected Veterans Status/Disability Status/Sexual Orientation/Gender Identity). For more information, please read our EEO &amp; AA policy . | Reasonable Accommodations are available for disabled veterans and applicants with disabilities in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: EEO is the law and Pay Transparency Statement . | Virginia Remote Office (VA99)",Fairfax VA 22031,DBA and Data Integration Engineer (Remote)
Carbon Black,/rc/clk?jk=69694e3018d50154&fccid=c84fc6b7ec7ae769&vjs=3,"Who we are | The Vera Institute of Justice, founded in 1961, envisions a society that respects the dignity of every person and safeguards justice for everyone. Vera is committed to securing equal justice, ending mass incarceration, and strengthening families and communities. | We study problems that impede human dignity and justice. | We pilot solutions that are at once transformative and achievable. | We engage diverse communities in informed debate. | And we harness the power of evidence to drive effective policy and practice. | We work with others who share our vision to tackle the most pressing injustices of our day—from mass incarceration, racial disparities, and the loss of public trust in law enforcement to the unmet needs of people who are vulnerable, marginalized, and harmed by crime and violence. Vera has offices in New York City, Washington, DC, Los Angeles, and New Orleans. Vera is an equal opportunity employer with a commitment to diversity in the workplace. We hire employees that reflect our values: respect, independence, collaboration, commitment, and race equity. If you want to learn more about life at Vera click here. | What you'll do | The Data Engineer will be responsible for building out the data infrastructure that will enable the Center on Immigration and Justice’s (CIJ) research team to effect change through data-informed insights, research, and interventions. They will iteratively progress toward the implementation of an ideal solution while accommodating the demands of an applied research setting by creating and supporting systems and processes for collecting, compiling, manipulating, and analyzing data. | The Data Engineer's responsibilities includes but are not limited to: | Create and support systems and processes for collecting, compiling, manipulating, and analyzing data to support CIJ’s program &amp; research | Work with immigration research and program management staff to identify &amp; solve for difficult data ingestion, migration, management, and integration challenges | Design data models &amp; set up data environments to support reporting &amp; analysis | Build and test data ingestion, migration, and ETL processes | Automate processes and schedule jobs within the data environment | Write documentation of systems &amp; processes for collaboration within CIJ’s research and programs | Ensure data ecosystems are security compliant and properly integrated with Vera’s IT systems where applicable | Work closely with IT to ensure compliance &amp; integration of systems | Other technical program &amp; research support as assigned | What qualifications you will need | The Data Engineer candidate will have the following qualifications: | 2-3 years of full-time experience in a data engineering capacity | Experience collaborating directly with data scientists and data analysts who develop analyses in any combination of R, Python, SQL, Tableau, Stata, etc. preferred | Prior experience designing and developing data models, building out and testing ETL data pipelines, and automating scheduled workflows using SQL &amp; Python | Fluency in collaborating with Git &amp; Github, with dedication to using these tools to conduct peer code reviews and uphold coding standards | High comfort level with ingesting messy source datasets that are prone to manual data entry errors and integrating these into a live database | Ability to build repeatable and well-documented processes and tools that can be used by other research &amp; analytics team members, regardless of the languages they use to perform their analyses | Excellent oral and written communication skills, including ability to present and teach the use of data infrastructure to a range of audiences in a variety of formats, and work effectively on a large team to advance shared priorities. | Strong social and emotional awareness with your team and external partners. | Experience with Linux and bash scripting highly preferred | Experience with Docker and deploying Docker images highly preferred | Experience with automating bespoke tasks (e.g. basic web scraping, using 3rd party APIs) highly preferred | Familiarity with government security compliance standards is a plus | Working knowledge of AWS data product ecosystem is a plus | This position may require low-level government security clearance in the future to handle secure data. However, this is not a current requirement for hiring. | The Nitty – Gritty | This is a full-time position located at Vera’s Brooklyn, New York office | Salary is competitive plus excellent benefits | How to apply | Please submit a cover letter and resume. Applications will be considered on a rolling basis until the position is filled. Online submission in PDF format (through Vera’s career page) is preferred. No phone calls, please. Only applicants selected for interviews will be contacted. | ATTN: Human Resources / Data Engineer | Vera Institute of Justice | 34 35th St, Suite 4-2A, Brooklyn, NY 11232 | Fax: (212) 941-9407 | Please use only one method (online, mail or fax) of submission. | No phone calls, please. Only applicants selected for interviews will be contacted. |  | Vera is an equal opportunity/affirmative action employer. All qualified applicants will be considered for employment without unlawful discrimination based on race, color, creed, national origin, sex, age, disability, marital status, sexual orientation, military status, prior record of arrest or conviction, citizenship status, current employment status, or caregiver status. | Vera works to advance justice, particularly racial justice, in an increasingly multicultural country and globally connected world. We value diverse experiences, including with regard to educational background and justice system contact, and depend on a diverse staff to carry out our mission. | For more information about Vera, please visit www.vera.org | Powered by JazzHR | e9hC6vvlIl",Brooklyn NY 11232,Data Engineer - CIJ (NYC)
Onix Networking Corp,/rc/clk?jk=588614b3f69c5b47&fccid=cdf5f442bc9a18df&vjs=3,"Summary: Onix is currently seeking a dedicated, self-motivated technical expert to fill the role of Data Engineer. IMPORTANT: Will need to work out of the office in Chicago for up to 6 months as part of a business-required program. | Scope/Level of Decision Making: This is an exempt position operating under limited decision-making and supervision. Position performs a variety of assigned activities, referring more complex issues to the manager. |  | Location: Remote, Illinois |  | Primary Responsibilities | The Onix Cloud Data Team helps customers transform and evolve their business through the use of Google’s extensive cloud services. As part of an entrepreneurial team in this rapidly growing business, you will work with cutting-edge cloud technologies and help shape the future of how data is used in the Enterprise. | Use Google Cloud Platform to build Enterprise-grade Big Data solutions. | Architect and build new cloud-based data pipelines. | Bring together multiple data sources into a unified data warehouse. | Apply analytics and visualizations to customer data sets. | Help customers understand the right technologies for their use case. | Establish strategic customer relationships and become their go-to trusted advisor for Big Data needs. | Assist in strategic direction and planning for the growth of the Cloud Data Team | Required Skills and Experience | Bachelor's Degree in Computer Science, Data Science, or a related discipline. | 5+ years or more of enterprise-level consulting. | Experience with large data sets and Enterprise-grade databases (structured and unstructured). | Experience architecting and building data pipelines. | Deep understanding of the ETL (extract, transform, load) process. | Experience extracting data from multiple sources via APIs and scripting. | Experience transforming data through field mapping, programmatic rulesets, and data integrity checking. | Able to expertly convey ideas and concepts to others. | Excellent communication skills (verbal, written, and presentation) | Creative problem-solving skills and the ability to design solutions not immediately apparent. | Ability to participate in multiple projects concurrently. | Customer-oriented and shows a bias for action. | Able to function in a highly dynamic team that moves rapidly from idea to planning to implementation. | Highly adaptable with the ability to learn new technologies quickly without direct oversight. | Preferred Skills and Experience | Google Certification - Professional Data Engineer | Experience with BigQuery | Experience with SQL (architecture and queries) | Experience with Tableau / other enterprise visualization and BI tools | Education: Bachelor's Degree | Travel Expectation: 20% travel |  If you meet the requirements above and desire an exciting career with a growing, dynamic organization, this position is for you! |  It is the policy of Onix to ensure equal employment opportunity in accordance with the Ohio Revised Code 125.111 and all applicable federal regulations and guidelines. Employment discrimination against employees and applicants due to race, color, religion, sex, (including sexual harassment), national origin, disability, age (40 years old or more), military status, or veteran status is illegal. |  | Onix will only employ those who are legally authorized to work in the United States or Canada. This is not a position for which sponsorship will be provided. Individuals with temporary visas such as E, F-1, H-1, H-2, L, B, J, or TN or who need sponsorship for work authorization now or in the future, are not eligible for hire.",Lakewood OH 44107,Data Engineer
Zoom Video Communications Inc.,/rc/clk?jk=fb6d0fa5960f9d20&fccid=dc2af3cb1af58a67&vjs=3,"Overview: |  | The Data Engineer will be part of a team that is responsible for supporting, maintaining and extending a large data warehouse, built using Snowflake schema, that aggregates Healthcare data on patients. This role will support a specific customer-facing business solution and dedicated business team, while collaborating with other data engineers to create a cohesive data model and data warehouse implementation that can support the full enterprise vision across multiple business products and teams. The Data Engineer's primary role will be data modeling, data model implementation within Snowflake, data analysis and data integration (importing/exporting data) using both batch and real-time processes. | Responsibilities: | Data analysis of business needs and new data sources to drive design of data warehouses and data integrations | Data model design and implementation using a Snowflake Schema | Create and maintain technical documentation surrounding data integrations | Collaborate with other Data Engineers to harmonize cross-system data designs | Implement data integrations (inbound/outbound) with multiple systems using ETL tools and manually scripted routines | Qualifications: | 5+ years of experience with database development, data modeling and data warehousing | 2+ years of experience building tables within a Snowflake schema | Proficiency with migrating data using ETL tools such as Matillion, Rhapsody, Talend, Jitterbit, SSIS or similar ETL tools | Strong communications and ability to work with business users to extract requirements and communicate design plans | Strong awareness of data security best practices within a regulated industry | Bonus points if you have specific experience with: | Matillion | Healthcare data and/or healthcare industry standards | Salesforce | About SOLTECH: |  | SOLTECH is Atlanta’s top software development and technology staffing company. Our mission (and passion) is to match job seekers with outstanding career opportunities. | Over the last 20 years, we have been successful in finding the best opportunities for people like you who are interested in seeking jobs that ramp up and further careers in technology. We call it “the catered approach” because of the in-depth and thorough methods we use to find the best win-win employment situations. | We're looking for innovative individuals who are enthusiastic about shaping the future of technology alongside the region's most successful companies. We look forward to learning about your expertise and interests so we can match you with an exciting position where you can thrive. | When you apply through SOLTECH we will make sure you land where your career growth will be nurtured and valued.",Denver CO 80210,Data Engineer
Goldman Sachs,/rc/clk?jk=46eea248c9034c48&fccid=2e7fb2f475ab63dd&vjs=3,"General information | Agency: Kinesso | Job Function: Technology | Location: New York, United States | Job Ref#: 1511 | Description &amp; Requirements | Position Summary | The Kinesso Research &amp; Development Team’s mission is to produce novel and disruptive solutions in the media, advertising and marketing technology space. We pursue this mission by identifying key industry opportunities and trends and designing new algorithmic tools using concepts from game theory, information theory, probability &amp; statistics, reinforcement learning, nonlinear dynamics and various other areas of mathematics, data science and technology, to extract commercial value. At any given time, we will be developing two to four major projects with one- to two-year time horizons and a small number of faster-cycle projects with more narrow focus. We are a small team whose existence consists mainly of pushing at high-speed down a path while simultaneously drawing the map for that path. | In pursuing its mission, the Research &amp; Development Team constantly identifies new data sources from both inside and outside the existing business and incorporates them into local platforms for use in its projects. The R&amp;D Data Engineer will be responsible for efficiently architecting, building and maintaining (1) the data infrastructure to enable the R&amp;D Team’s machine learning and data science specialists to build applications that use this data, and (2) the application infrastructure to optimize the R&amp;D process and eventual handoff of R&amp;D tools for full productization elsewhere in the company. | The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed, comfortable working in the presence of ambiguity, and confident enough to respond to ambiguity by making a reasoned judgment, trying an optimal approach, evaluating the success or failure of that approach, and trying again if necessary. | Job Responsibilities | Create and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements.Architect application infrastructure to contain and support complex AI tools connecting reinforcement learning agents, deep neural networks, proprietary algorithms, sensitive data sources and various external partners.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build and manage the infrastructure required for optimal automated extraction, transformation, and loading of data from a wide variety of data sources using SQL, Snowflake and AWS ‘big data’ technologies, for use in speculative machine learning and AI applications.Collaborate with data science and machine learning specialists to produce analytics tools that utilize the data pipeline and application infrastructure to provide actionable insights into customer acquisition and implement closed-loop optimization of media buying and customer acquisition systems.Integrate R&amp;D Team tools with internal and external platforms via API, FTP and other means.Work with data science and machine learning/artificial intelligence experts to strive for greater functionality in our data systems. | Desired Skills &amp; Experience | Experience with cloud and container systems including AWS cloud services: EC2, EMR, RDS, S3, Redshift; Docker and KubernetesExperience with object-oriented/object function scripting languages: Python required; C++ a bonus, etc.Experience designing, programming, testing and maintaining reliable connections to RESTful APIsExperience creating database stored procedures and functionsExperience architecting fast-cycle development for speculative applicationsExperience with reinforcement learning environments a bonusAbility to communicate, collaborate and work in ambiguous and unmapped contexts a necessity. |  | Employment Transparency | It is the policy of Kinesso, division of the Interpublic group, to provide equal employment opportunities to all employees and applicants for employment without regard to race, color, ethnicity, gender, age, religion, creed, national origin, sexual orientation, gender identity, marital status, citizenship, genetic information, veteran status, disability, or any other basis prohibited by applicable federal, state, or local law. | Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice. | The employer will make reasonable accommodations in compliance with the American with Disabilities Act of 1990. The job description will be reviewed periodically as duties and responsibilities change with business necessity. Essential and other job functions are subject to modification. Reasonable accommodations may be provided to enable individuals with disabilities to perform the essential functions. | For applicants to jobs in the United States: In compliance with the current Americans with Disabilities Act and state and local laws, if you have a disability and would like to request an accommodation to apply for a position with Kinesso, please email accommodation@mbww.com | About Us | Kinesso brings together the best minds in data, technology and media activation to deliver seamless data-driven marketing solutions. As a technology-enabler, Kinesso powers marketing systems through a proprietary suite of applications that integrate ad tech and martech. Kinesso’s application framework spans audience, planning, and activation, delivering insights and business outcomes for leading global brands.",New York NY,Data Engineer (Research and Development)
Capital Group,/rc/clk?jk=f1a3fc19b6d56aef&fccid=c8e07f37dc242ad2&vjs=3,"Entera, where residential real estate investing is made simple |  | At Entera, we are on a mission to transform the way investors find and buy properties. Powered by machine-learning, Entera's end-to-end residential real estate platform modernizes the real estate buying process. Entera's property source aggregation platform, discovery algorithms, intelligent tools and expert real estate service team help our clients access and evaluate more properties, make data-driven investment decisions, and win more - 100% online. |  | Entera is based in San Francisco, New York &amp; Houston, with satellite service offices in 12 additional markets across the US. We're always looking for talented, creative and passionate people to join our team. If you're interested in opportunities at Entera, we'd love to hear from you! |  | Job Description |  | As a Data Engineer at Entera, you'll contribute to our best-in-class data pipeline and data-driven culture. You'll work with multi-discipline experts with hard-science backgrounds in a tight knit team to deliver on our efforts around data curation and management. You'll work with modern frameworks to ETL and massage data for preparation, and then utilize BI tools to develop visualizations and deliver data to both our internal business users and customers as you surface brand new depths of our vast dataset. You'll write Python, SQL, and R in shared notebooks that you and the data science team have ownership of. Within our team, you'll be able to further develop your skills and work with a team of experts to deliver on massive improvements to our data pipe and associated systems. |  | Successful candidates will thrive in Entera's unique operating environment and culture: high-growth, innovative, lean, and values-driven. As such, successful candidates must be highly capable in each of the following dimensions (among others): adaptability, curiosity, resourcefulness, analytical thinking/problem solving, pro-activity, collaboration, technological savvy, and operating in a dynamic environment. |  | Job Responsibilities |  | Use Python, SQL, and R to improve upon a best-in-class data pipeline and develop our workflows | Contribute to cloud-first services that improve our reporting, analysis, and metrics collection efforts | Use agile software development processes to iteratively make improvements to our back-end systems | Mold front-end and back-end data sources to help draw a more comprehensive picture of user flows throughout our system | Deliver on detailed specifications for business intelligence and reporting needs | Contribute and further develop our data-driven culture | Work with product and engineering in cross-functional teams to deliver on improvements to our systems |  | Preferred Qualifications: |  | MS or PhD in Computer Science, Mathematics, Statistics, Physics, Economics, or similar hard-science | 3+ years hands-on experience in Data + Analytics at growing product-driven tech companies | Proficiency in cloud services and modern ETL workflows | Advanced capabilities across Python, R, and SQL | Understanding of Spark | Strong analytical and problem solving skills | Working knowledge of Python web frameworks like Flask | Software development background",New York NY,Data Engineer
Citi,/rc/clk?jk=508de1e888b2dcb4&fccid=181ae1a6390e21b5&vjs=3,"-COVID-19 Hiring Update- |  | Dataminr is still actively hiring. |  | As the health and safety of our candidates and our employees come first, | we're excited to provide virtual experiences for interviews and new hire on-boarding. |  |  | Who we are: |  | Dataminr puts real-time AI and public data to work for our clients, generating relevant and actionable alerts for global corporations, public sector agencies, newsrooms, and NGOs. Our leading AI platform detects the earliest signals of high-impact events and emerging risks from vast amounts of publicly available information. Our real-time alerts enable tens of thousands of users at hundreds of public and private sector organizations to learn first of breaking events around the world, develop effective risk mitigation strategies, and respond with confidence as crises unfold. |  | Dataminr is making its mark for growth and innovation, recently earning recognition on the Deloitte Technology Fast 500, Forbes AI 50 and Forbes Cloud 100 lists. We also earned accolades for 'Most Innovative Use of AI' from the 2020 AI &amp; Machine Learning Awards. |  | The opportunity: |  | If you enjoy working on a collaborative, high growth team with best-in-class cloud-native technologies, this is the role for you! Our tech stack includes Snowflake, Kafka &amp; Kafka Connect, Spark, SCALA, Java, Python, and our business model is 24/7 streaming data. We support both streaming and batch processes within an integrated and resilient architecture that delivers high impact results for teams across the company. We are passionate about Data Infrastructure as a Service, and we find meaning in enabling others to work faster by building better tooling. In this role, you will enjoy greenfield big data engineering projects that provide highly-performant and easy-to-maintain data infrastructure. You'll partner with other teams across research, AI and engineering, and the likelihood is high that you'll make their day with data. You'll contribute to expanding our methods for ensuring the validity and quality of the company's datasets, and you'll help develop systems that accurately monitor and measure the impact of releases to our production systems. In the first month, you'll |  | start off by learning the ropes, spending time with different parts of the company to understand how Dataminr works. | get up to speed on our data infrastructure and our roadmap with overview sessions and deep dives with your team. | contribute code to production systems. |  | Within 3 months, you'll: |  | share responsibility for data infrastructure with members of your team. | help to plan new infrastructure features and improvements. | begin to take more of a role in helping others understand our data platform strategy. |  | Within 6 months, you'll: |  | own an area of the data platform, depending on your interests | design and implement pipelines that impact multiple teams across the company. | be influential in helping plan the next iteration of our data platforms. | bring new ideas to our engineering and analytics processes to help us continuously improve. |  | Who you are: |  | You love all things data. You share knowledge and love to learn. You have some experience designing, building and maintaining data infrastructure and are proficient in cloud-native dev ops. You enjoy writing and maintaining ELTs and their orchestration in order to produce meaningful and timely insights. Ideally you excel at integrating data from different sources, using SQL and Python, or Java, for exploratory analyses and data validation, and are well-versed in the advantages and limitations of various big data architectures and technologies. You are intellectually curious and you understand the importance of mindful communication in engineering. You have a history of mentoring other engineers and you give your time and support to help others. |  | Why you should work here: |  | We recognize and reward hard work with: | company paid benefits for employees and their dependents, including medical, dental, vision, disability and life insurance | 401(k) savings plan with company matching | flexible spending account for out-of-pocket medical, transit, parking and dependent care expenses | We want you to be your best, authentic self by supporting you with: | a diverse, driven, and passionate team of coworkers who want you to succeed | individual learning and development fund and professional training | generous paid time off; including sick leave and 100% company paid parental leave | in-office perks such as a kitchen stocked with snacks and beverages, and catered meals | remote working friendly perks such as expanded telehealth options for mental and physical well being, virtual yoga, meditation and health and fitness app reimbursements |  | …and this is just to name a few! |  | Dataminr is an equal opportunity and affirmative action employer. Individuals seeking employment at Dataminr are considered without regards to race, sex, color, creed, religion, national origin, age, disability, genetics, marital status, pregnancy, unemployment status, sexual orientation, citizenship status or veteran status. |  | #LI-BM | #LI-Remote",New York State,Big Data Engineer
Zillow Group,/rc/clk?jk=2abc1a8bdf9b6268&fccid=a892c8c946e25608&vjs=3,"Req ID: 39144 | Location: San Antonio, TX | Other location(s): N/A | Relocation benefits offered: No | Travel required: None | “I can be myself at work.” | You define yourself by more than just a job title, and we want you to feel comfortable bringing your true self to work. We value your talents, your traditions and your take on the world ̶ everything that makes you unique. We’re working hard to advance diversity, equity and inclusion in our organization and our communities because we know that what makes us different makes us better. | We want you to feel a strong sense of belonging. We value and welcome your experiences, ideas and identity. Over 40 employee resource groups unite our people and help to develop our collective empathy through unfiltered conversations about race, ethnicity, gender, gender identity, sexual orientation, faith, disabilities, mental health and so much more. | “I can influence my income.” | You want to feel recognized at work. Your performance will be reviewed annually, and your compensation will be designed to motivate and reward the value that you provide. You’ll receive a competitive salary, bonuses and benefits. Your company-funded retirement contribution will be the equivalent of 15% of your annual pay (including bonuses). | “I can lead a full life.” | You bring unique goals and interests to your job and your life. Whether you’re raising a family, you’re passionate about where you volunteer, or you want to explore different career paths, we’ll give you the resources that can set you up for success. | Enjoy generous time-away and health benefits from day one, with the opportunity for flexible work options | Receive 2-for-1 matching gifts for your charitable contributions and the opportunity to secure annual grants for the organizations you love | Access on-demand professional development resources that allow you to hone existing skills and learn new ones | COVID-19 HIRING: Our recruiting and onboarding activities are virtual during the pandemic and we’ve transitioned to a work-from-home environment until further notice. We are offering generous work-from-home benefits to improve our associate’s ability to work remotely. | “I can succeed as a Data Engineer at Capital Group.” | As Data Engineer in Client Data Platform you will be responsible for all aspects of the delivery of data and analytics solutions that consolidate, store and retrieve structured and non-structured data from multiple applications and sources. You will partner with the business to influence ways to achieve business priorities and provide leadership through innovation, critical thinking, collaboration and proactive risk management. You will lead the analysis and design of features across multiple layers of the data platform, according to data platform standards. Your scope will also include executing and taking accountability for quality assurance activities for the entire product; performing testing of both functional and non-functional aspects , leveraging and promoting automation and a test-early and often approach. | “I am the person Capital Group is looking for.” | You have a bachelor's degree in Computer Science, Engineering or a related technical field | You have 3-5 years of experience delivering data solutions, including cloud technologies like AWS | You write and optimize advanced SQL queries with large-scale, complex datasets | You have experience with integration of data from multiple sources and a knowledge of ETL/ELT technologies and tools like Informatica and AWS solutions | You have a solid background with data analysis and modelling | You have coding proficiency in at least one modern language such as: Python, R and Spark | You have experience in cloud-first design, preferably AWS or Azure | You’re well-versed in Machine Learning and data mining | Complimentary Experience: | Excellent technical knowledge of performance tuning and query optimization across large data sets, and exposure to bottlenecks at the storage, network or compute layers | Working experience with data cataloging and enablement through APIs | Excellent understanding of traditional RDBMS | You have strengths in leadership, interpersonal, and problem solving skills with the ability to continually learn new concepts and technologies and effectively apply them | “I can apply in less than 4 minutes.” | You’ve reviewed this job posting and you’re ready to start the candidate journey with us. Apply now to move to the next step in our recruiting process. If this role isn’t what you’re looking for, check out our other opportunities and join our talent community. | “I can learn more about Capital Group.” | At Capital Group, the success of the people who invest with us depends on the people in whom we invest. That’s why we offer a culture, compensation and opportunities that empower our associates to build successful and prosperous careers. Through nine decades, our goal has been to improve people’s lives through successful investing. We know that our history is a testament to the strength of the people we hire. More than 7,800 associates in 30+ offices around the world help our clients and each other grow and thrive every day. Find us on LinkedIn, Glassdoor, FairyGodBoss, DiversityJobs and Instagram. | We are an equal opportunity employer, which means we comply with all federal, state and local laws that prohibit discrimination when making all decisions about employment. As equal opportunity employers, our policies prohibit unlawful discrimination on the basis of race, religion, color, national origin, ancestry, sex (including gender and gender identity), pregnancy, childbirth and related medical conditions, age, physical or mental disability, medical condition, genetic information, marital status, sexual orientation, citizenship status, AIDS/HIV status, political activities or affiliations, military or veteran status, status as a victim of domestic violence, assault or stalking or any other characteristic protected by federal, state or local law.",San Antonio TX 78251,Data Engineer
Q2ebanking,/rc/clk?jk=1c5563d53ae9aae6&fccid=16a97ed26c75bf2d&vjs=3,"MORE ABOUT THIS JOB: |  | CONSUMER AND INVESTMENT MANAGEMENT DIVISION (CIMD) | The Consumer and Investment Management Division includes Goldman Sachs Asset Management (GSAM), Private Wealth Management (PWM) and our Consumer business (Marcus by Goldman Sachs). We provide asset management, wealth management and banking expertise to consumers and institutions around the world. CIMD partners with various teams across the firm to help individuals and institutions navigate changing markets and take control of their financial lives. |  | GSAM | Goldman Sachs Asset Management (GSAM) is one of the world’s leading investment managers. GSAM provides institutional and individual investors with investment and advisory solutions, with strategies spanning asset classes, industries, and geographies. We help our clients navigate today’s dynamic markets, and identify the opportunities that shape their portfolios and long-term investment goals. We extend these global capabilities to the world’s leading pension plans, sovereign wealth funds, central banks, insurance companies, financial institutions, endowments, foundations, individuals and family offices. |  | The GSAM Quantitative Investment Strategy (QIS) team manages across a variety of mandates including institutional portfolios, mutual funds and hedge funds, using sophisticated quantitative models that have been developed in an innovative research environment. The group is one of the largest direct quantitative managers in the world, and is recognized as an industry leader in quantitative portfolio management techniques. The team manages exposures to global stock, bond, currency and commodity markets to generate alpha and advanced beta strategies for our Clients’ portfolios. As one of the longest-running quantitative teams in the industry, QIS has developed a strong reputation for innovation, excellence and teamwork. | The QIS Engineering team works in a close-knit environment with Portfolio Managers and senior revenue generators. We design and develop the proprietary platforms that drive our QIS business, spanning alternative Data Acquisition, Quantitative Research, Model Generation, Portfolio Construction, Trading and more. |  | Primary Responsibilities: | This role will focus on leading the effort to advance data strategies for portfolio management teams across QIS | Manage any necessary development team | Evolve state-of-the-art data infrastructure to drive the investment process and thus the firms bottom line | Work closely with Senior and Executive leadership, to ensure QIS Infrastructure platform remains best in class | Designing, developing, and maintaining a world-class, high-performing data platform to enhance research and portfolio management | Developing rigorous and scalable data management/analysis tools to support the data-intensive quantitative investment process | RESPONSIBILITIES AND QUALIFICATIONS: |  | REQUIREMENTS | 5+ years acting as a Tech Lead for a team focused on Data Engineering, Data Pipelines, etc. | Hands-on expertise in Hadoop and Hadoop ecosystem such as Spark/Scala | Data ingestion and management, Data APIs (multi-language support), and Big Data processing | Working experience with Enterprise Java development, JSI web stack, various visualization toolkits. | Knowledge of leading technology trends and best practices | Proven to in ETL and data processing, know how to transform data to meet business goals. | Meaningful experience in using AWS stack is a plus ideal, but not required | ABOUT GOLDMAN SACHS: |  | The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world. |  | © The Goldman Sachs Group, Inc., 2020. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet.",New York NY 10282,Lead Data Engineer – QIS Technology
Paradigm Inc,/rc/clk?jk=c067f6d2f5698094&fccid=756b56d5afcb7d62&vjs=3,"Description |  | Understand data migration requirements of the client. Create new migration scripts to migrate one format to another using object oriented programming concepts to meet the customer requirements. |  | Troubleshoot customer issues and help in resolving those. |  | Efficiently manage the inbound migration queue and set the right expectations with stakeholders. |  | Continue to refactor the code to improve the productivity and reduce the redundancy |  | Help enhance the overall migration process from the Engineering point of view. |  | Participate in code reviews |  | Help in automating tasks to test the migrations and improve overall productivity |  | Collaborating with other developers to create applications |  | Document the migrations |  | Participating in execution and documentation of tests. |  | Qualifications |  | 1+ year of experience of Programming/Scripting experience |  | 1+ year of experience of data transformation, data pipeline, data analytics |  | Proficiency with server side languages such as PHP, Python, Ruby, or Java |  | Proficiency with operating system such as Ubuntu, Linux |  | Familiarity with relational database technology such as MySQL, PostgreSQL or Oracle |  | Self motivated and passionate in learning and adopting new technologies |  | Strong attention to detail, time management and organization skills |  | Solid written and verbal communication skills |  | Proficiency with Microsoft Word, Excel, Jira is a Plus! |  | Enjoy working on small teams |  | Familiarity with Project Management fundamentals is a plus! |  | Bachelor's degree in computer science, software engineering, computer engineering, electrical engineering or a related field is a plus! |  | More About the Role |  | Location: Oakland, CA or remote |  | Compensation: Competitive |  | Other Benefits: 401k matching, competitive health care coverage + FSA, equal maternity/paternity leave. |  | Culture: Fast-moving, high-energy and passionate team in a casual, laid-back environment",Oakland CA,Data Migration Engineer
Facebook,/company/SageBeans/jobs/Data-Engineer-8d01c091a526bbce?fccid=9eef152ff4f035fc&vjs=3,"Job detailsSalary$60 - $80 an hourJob TypeFull-timeNumber of hires for this role2 to 4QualificationsBachelor's (Preferred)SQL: 3 years (Preferred)ETL or ELT: 3 years (Preferred)Cloud data warehousing: 3 years (Preferred)Full Job Description1-7 years of experienceExperience with SQLExperience with ETL or ELTExperience with Cloud data warehousing / data lake environment (Snowflake, AWS, Azure, Google Cloud; client is using Snowflake on Azure)Degree: Bachelors degree at a minimumJob Type: Full-timePay: $60.00 - $80.00 per hourEducation:Bachelor's (Preferred)Experience:Data Engineer: 4 years (Preferred)SQL: 3 years (Preferred)ETL or ELT: 3 years (Preferred)Cloud data warehousing: 3 years (Preferred)data lake environment: 3 years (Preferred)Work Location:Multiple locations",Chicago IL,Data Engineer
Bluelock,/rc/clk?jk=5fcec442ff11b82d&fccid=32fcbf701b3f9e3a&vjs=3,"It has never been more important that students around the world have the tools and support to learn through digital instruction — and that they be able to harness the power of the written word. NoRedInk now helps students in more than 60% of middle and high schools in the U.S. become better writers. Our adaptive curriculum deeply engages learners by personalizing exercises to their interests, guides them step-by-step through the writing process, and boosts their skills through differentiated practice. We're relentlessly focused on our mission to unlock every writer's potential and to create a future where all students can harness the power of the written word. |  | To further advance our mission, we are looking to find a talented data engineer excited to help build a data team from the ground up, collaborating with engineers, product designers, and salespeople across the organization to improve NoRedInk's ability to help teachers and students hone their language and writing skills every day. |  | What You Will Achieve: |  | Help build a data team and pipeline from the ground up for an impactful organization that has never had a data team before | Collaborate across departments to help different people access and get the most out of NoRedInk's data | Experience personal growth through a management relationship which prioritizes support, empathy, and clearly communicated expectations. | Deliver impactful solutions at scale, building on the foundations of software that handles billions of new records every year. |  | About You |  | You have 3+ years of professional experience as a software engineer | You have experience writing and optimizing complex SQL queries | You have experience working on data pipelines | You value code quality and well tested code | You are a capable Python programmer | You're comfortable with the idea of working with Ruby on Rails and Haskell (even if you don't know one or both of them yet, or haven't used them professionally before) | You're comfortable with macOS or Linux | You're comfortable working remotely |  | Why NoRedInk? |  | NoRedInk offers a range of benefits to help you thrive in and out of the office, including flex PTO, a relaxed WFH policy, and paid parental leave. Our team members care deeply about our core values: |  | Put teachers and students first | Relentlessly improve | Invest in and take care of each other | Act with humility | Delight in our work |  | We work to model and promote these daily, helping to foster an environment that's fun, collaborative, and highly engaged. Check out our 2-minute pitch on NBC or read articles about us in The Washington Post, Wall Street Journal, and Forbes. |  |  NoRedInk is an equal opportunity employer. We know that a diverse workforce is the strongest workforce, and are committed to building and supporting an inclusive environment for all. | Note: Agencies or other third-party recruiters may not submit unsolicited candidate resumes or their information to any NoRedInk employee, including a NoRedInk Recruiter, unless a contract is signed and you are given permission by the Talent Acquisition team to work on a job opening.",California,Data Engineer
Splunk,/rc/clk?jk=06349b64444291e2&fccid=cd7f8301f56f5142&vjs=3,"Company Description | Oxfam is a global organization working to end the injustice of poverty. We help people build better futures for themselves, hold the powerful accountable, and save lives in disasters. |  | Job Description | PURPOSE OF POSITION | The Data Engineer will be responsible for developing and enhancing data pipelines across constituent data platforms (CDP) to enable growth in fundraising and public engagement activities. Resource Development and Public Engagement business systems include constituent relationship management (CRM), advocacy and fundraising marketing products, research, reporting analytics, and other related supporter engagement platforms. | PRIMARY RESPONSIBILITIES | Design, develop, and maintain data pipelines across platforms to strengthen marketing operations and analytics capabilities. | Design data integrations across constituent data platforms (CDPs) and reporting solutions. Implement automation via connectors or API connections. | Implement and manage data warehouse or alternative solution to consolidate data from disparate platforms. | Support data pipelines from CDPs to finance Enterprise Resource Planning (ERP) system. | Develop and support development of dashboards and reports from Revolution Online (ROI), Salesforce, and additional CDP datasets. | Support development of CDP architecture to enable effective marketing and engagement activities. | Collaborate with product team, internal stakeholders, and external vendors to understand, document, and analyze business requirements, support product backlog, and lead on data solutions. | Translate complex business, marketing, and analytics requirements into functional data models. | Develop and lead initiatives to maintain and improve data quality across platforms. Perform quality assurance testing. | Support CDP systems administration with product team. | Apply advanced analysis methods, including machine learning, to large volumes of data in order to provide actionable marketing insights. | Review and lead improvement plan for data management process enhancements. | General Expectations | Will stay abreast of professional standards, trends and issues affecting this set of responsibilities, demonstrating continuous learning of the field. | Will work effectively and collaboratively in support of building a team-based culture of work, will perform all duties appropriately for a multi-cultural environment, treating all persons with dignity and respect, and will be familiar with and committed to Oxfam America mission and goals. | Will have an understanding of gender justice and diversity within key areas of responsibility and a commitment to promoting gender justice and diversity in our workplace and programs. | Within key areas of responsibility, will have a commitment to providing a safe environment for staff, partners, and beneficiaries, and a commitment to promoting safeguarding in our workplace, programs, and the communities we serve. |  | Qualifications | Education | Bachelor's or Master’s degree in computer science, business or a related discipline or equivalent combination of education, experience, and training. | Experience and Core Competencies | Minimum five years of experience in data engineering for fundraising, marketing operations or related field, ideally in a public interest or cause-related organization. | Experience collaborating with a technical team and coordinating with non-technical stakeholders to manage highly complex system enhancements from start to finish. | Minimum three to five years’ experience with CDP administration and operational support across an ecosystem of platforms such as Revolution Online (ROI), Engaging Networks, Salesforce.com, Blackbaud, Hubspot and ERPs. | In-depth experience using SQL with an Oracle, or Microsoft SQL Server database. | Strong coding skills, preferably Python. | Highly organized, excellent problem-solving skills, detail oriented, and self-directed. | Experience managing data pipelines and warehouses. | Excellent interpersonal and communication skills across departments and levels. | Experience programming API connections to manage data integrations. |  | Additional Information | Oxfam America is a Gender Just organization and an equal opportunity employer. We have a zero tolerance policy for any sexual harassment, exploitation, and/or abuse. We welcome all persons to apply and do not discriminate. We take measures to prevent discrimination against any employee or job applicant on the basis of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. | We are an E-Verify employer. | For more information, please click on the following links: | E-Verify Participation Poster: English / Spanish | E-Verify Right to Work Poster: English | Spanish |  | Oxfam America is a Gender Just organization and an equal opportunity employer. We have a zero tolerance policy for any sexual harassment, exploitation, and/or abuse. We welcome all persons to apply and do not discriminate. We take measures to prevent discrimination against any employee or job applicant on the basis of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. | We are an E-Verify employer. | For more information, please click on the following links: | E-Verify Participation Poster: English / Spanish | E-Verify Right to Work Poster: English | Spanish",Boston MA 02114,Data Engineer
The New York Times,/rc/clk?jk=ca7ae7d7e24cb436&fccid=848e72c84ce4a7a7&vjs=3,"NYU Langone Health is a world-class, patient-centered, integrated academic medical center, known for its excellence in clinical care, research, and education. It comprises more than 200 locations throughout the New York area, including five inpatient locations, a children's hospital, three emergency rooms and a level 1 trauma center. Also part of NYU Langone Health is the Laura and Isaac Perlmutter Cancer Center, a National Cancer Institute designated comprehensive cancer center, and NYU Grossman School of Medicine, which since 1841 has trained thousands of physicians and scientists who have helped to shape the course of medical history. For more information, go to nyulangone.org, and interact with us on LinkedIn, Glassdoor, Indeed, Facebook, Twitter, YouTube and Instagram. |  | Position Summary: | We have an exciting opportunity to join our team as a Enterprise Data Governance Analyst. |  |  | In this role, the successful candidate NYU Langone Health's Information Management and Governance team is responsible for data architecture, data management, data governance solutions and big data solutions. The healthcare data engineer/analyst will be a key technical member of this team, and will be responsible for big data engineering, data wrangling, data analysis and user support primarily focused on the Cloudera Hadoop platform, but in future extending to the cloud. The data engineer must have strong hands-on technical skills including conventional ETL and SQL skills with programming as well as data science languages such as Python and R, using big data techniques. |  | Job Responsibilities: |  | Requirements analysis, planning and forecasting for Hadoop data engineering/ingestion projects | Requirements analysis, planning and forecasting for Hadoop data engineering/ingestion | Operational support for data ingestion and engineering, including job monitoring; issue resolution; user support | Coordinate with infrastructure and offsite/offshore teams | Design and implement optimized Hadoop and big data solutions for data ingestion, data processing, data wrangling, and data delivery | Share subject matter expertise on Hadoop-related concepts and use | Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. | Build the infrastructure required for efficient extraction, transformation, and loading of data from a wide variety of data sources | Assist users with technical issues related to their use of Hadoop. | Build data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. | Create high-quality technical and user documentation |  | Minimum Qualifications: | To qualify you must have a Bachelors Degree or higher in Computer Science or a related field Minimum 10 years of total IT experience including 5+ years of experience with similar responsibilities and 3+ years of Hadoop experience Strong knowledge of Cloudera Hadoop fundamentals, including HDFS, Hive, Impala, Parquet, Sentry, Sqoop, Pig, Flume, Oozie, Yarn, Zookeeper, Spark, Hive Metastore, Spark, Solr, Kudu Strong ETL and data engineering/ingestion experience with ingesting diverse data from various sources including relational databases and files (text, csv) Strong knowledge of SQL and databases, particularly Oracle and Microsoft SQL Server Strong knowledge of querying Hadoop data using Hive and Impala, as well as query optimization Experience with at least one data integration (ETL) tool in a conventional or big data setting Strong knowledge of Unix/Linux including scripting Good knowledge of Java; knowledge of Groovy helpful Knowledge of Python or R, and willingness to learn languages Some experience with cloud data management with AWS or GCP Ability to lead resources with greater technical skills Creative and innovative approach to problem-solving Excellent communication and interpersonal skills at all levels of the organization Experience with agile development and tools Willingness to explore new alternatives or options to solve data engineering and data mining issues, and utilize a combination of industry best practices, innovations and experience to get the job done Experience performing root cause analysis on internal and external data and processes to answer specific business questions and find opportunities for improvement |  | Preferred Qualifications: | Experience with healthcare data, particularly with providers and academic medical centers is a strong plus Knowledge of or familiarity with natural language processing (NLP) and machine learning Experience with agile development and tools Basic knowledge of statistical techniques Exposure/experience with Cloud data management and services (AWS, GCP) |  | Qualified candidates must be able to effectively communicate with all levels of the organization. |  | NYU Langone Health provides its staff with far more than just a place to work. Rather, we are an institution you can be proud of, an institution where you'll feel good about devoting your time and your talents. |  | NYU Langone Health is an equal opportunity and affirmative action employer committed to diversity and inclusion in all aspects of recruiting and employment. All qualified individuals are encouraged to apply and will receive consideration without regard to race, color, gender, gender identity or expression, sex, sexual orientation, transgender status, gender dysphoria, national origin, age, religion, disability, military and veteran status, marital or parental status, citizenship status, genetic information or any other factor which cannot lawfully be used as a basis for an employment decision. We require applications to be completed online. |  | If you wish to view NYU Langone Health's EEO policies, please click here. Please click here to view the Federal ""EEO is the law"" poster or visit https://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm for more information. To view the Pay Transparency Notice, please click here. | Required Skills |  | Required Experience",New York NY,Big Data Engineer
Amazon Dev Center U.S. Inc.,/rc/clk?jk=bdb052b6da488d0d&fccid=183ac3d6e0c659db&vjs=3,"Passionate about development in leading technologies? Looking to become a major player on a diverse team in your next career adventure? Koddi Data Engineers drive innovation by embracing challenges and leveraging decision science to solve complex problems in adtech. |  | As a Senior Data Engineer at Koddi, you will work with our technology team to build and maintain our suite of data pipelines, stores, and databases that power sophisticated adtech products used by many of the world’s largest advertisers. We are looking for smart and hardworking individuals who love to build world-class software. The right candidates will be creative thinkers who can design and deploy professional applications using the newest technologies to solve real business problems. | What skills or experience will you bring? | Bachelor's or Master's degree in CS or other technical, science, or math fields | 4+ years of experience as a data or software engineer | You have experience working with Spark and AWS technologies | You’ve got experience working with Relational Databases such as MS SQL Server, Postgres, MySQL | Your tech stack includes Scala, Java, GoLang, and/or Python | You have extensive experience working within distributed architectures and building horizontally scalable infrastructure | High level of experience with SQL, with ability to design and optimize objects | Experience with Extract, Transform, and Load (ETL) | Strong communication and teamwork skills | Willingness to learn and utilize emerging technologies | What will you do? | Work within robust data systems and develop custom solutions while consulting with external customers | Design, deliver and implement data engineering solutions for optimized downstream impact, with end-to-end input and oversight of data engineering projects | Recommend and implement improvements to data processes and warehouses that improve supportability, usability, and scalability | Optimize and refactor existing codeImprove the efficiency, scalability, and reliability of applications | Koddi's award-winning ad technology platform provides a robust network for brands to connect with | consumers and drive revenue through native sponsored placements, metasearch, and programmatic media campaigns. |  | Based in Fort Worth, Texas, Koddi has additional office locations in New York, Ann Arbor, Austin, San Francisco, and Düsseldorf. We’ve been embracing remote work since 2020 and plan to transition to a hybrid remote model when it’s safe to return to our offices. |  | Koddi has been ranked by Forbes, Deloitte, and the Inc. 5000 as one of the fastest-growing companies in the nation. We’re on a mission to forge a better path to discovery through integrity, insights, inclusivity, and innovation. Come join us!",United States,Data Engineer
Dataminr,/rc/clk?jk=dbfcd123061c217c&fccid=1639254ea84748b5&vjs=3,"Our Data Centers are the foundation upon which our rapidly scaling infrastructure efficiently operates to deliver our innovative services. Facebook is looking for talented, highly motivated engineers for our Facilities Engineering team. As a member of the team, you will work alongside experienced Systems, Network, and Controls engineers to scale our services as Facebook grows. Your primary function will be to ensure that our core infrastructure systems, the systems our Data Centers depend on to do their day to day activities, are reliable, robust and can meet the challenges the business demands. | Work on a team of highly technical and talented Windows engineers chartered with maintaining the global virtual desktop infrastructure for our Data Centers. | Support day-to-day operations and escalations of the overall virtual desktop environment and network. | Improve and optimize system administration and management with coding and automation. | Troubleshoot firewall issues and modifying access control lists. | Understand metrics for measuring success and performance of services to ensure service performance, scalability, stability and security for both existing and new service deployments. | Contribute to meeting KPIs and SLAs through management of task queues. | Support large-scale global disaster recovery exercises while working closely with other infrastructure, production and business teams at Facebook. | Bias to action around communication | self-motivated to socialize ideas, and communicate idea/progress/concerns, up, out, and across the organization. | Provide QA/QC support of newly deployed infrastructure. | 5+ years of experience in Windows OS and core infrastructure support including: Active Directory, DNS, DHCP, NTP, SMTP, IIS, .NET, and Remote Desktop infrastructure. | 1+ years experience working with TCP, IPv4/IPv6, subnetting. | 3+ years of experience in managing virtual desktop environments (VMware, Hyper-V, AWS, etc.). | 3+ years of scripting experience in PowerShell or Python. | Understanding of server hardware and sizing, hypervisor technologies (preferably VMware), and cross-platform integrations (i.e. heterogeneous operating system support). | Experience working in a Linux/UNIX environment. | Experience and demonstrated talent in avoiding or minimizing business impact despite existing technical challenges. | 3+ years of experience with networking principals, firewalls, and load balancers. | 3+ years of experience managing geographically dispersed infrastructure and DR environments. | 3+ years of experience in configuration management (Chef, Puppet, SCCM). | Experience with controls system communications protocols, e.g. BacNet, Modbus, SNMP, OPC. | Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started. | Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",Fremont CA,Data Center Facilities Engineering Systems Engineer
Karat,/company/PCS-GLOBAL-TECH/jobs/Data-Engineer-Integration-3768620ee7a2a7f7?fccid=252beafbd99a6037&vjs=3,"Job detailsSalary$60,000 - $70,000 a yearJob TypeFull-timeInternshipContractQualificationsBachelor's (Preferred)SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)US work authorization (Preferred)Full Job Description** The job descriptions will vary depending on the individual project but here are the primary roles you could be working on after internship **Descriptions of some of them: ****SQL DeveloperHelp write and optimize in-application SQL statements.Ensure the performance, security, and availability of databases.Prepare documentation and specifications.Write TSQL scripts and objects such as stored procedures, user-defined functions, views, indexes per business requirements.Handle common database procedures such as upgrade, backup, recovery, migration, etc.**Business Intelligence DeveloperMaintain, support, and enhance the business intelligence data backend, including data warehouses and data lakes.Collaborate and work with data analysts in various departments to ensure that data meet their reporting and analysis needs.Build interfaces between the business intelligence systems and other college information systems to maintain a timely and accurate integration of data.**Data Analyst/EngineerProvides a plan with data, reporting, and analyses that enable data-driven decision-making.Develops system test cases and documents results, researches system issues, and documents findings.Provides summary analyses in written and oral presentation settings.Builds a database from scratch. And prepares complex presentations.**Junior Data Analyst/Engineer- Provide quality assurance of imported data, working with quality assurance analysts if necessary.- Commissioning and decommissioning of data sets.- Managing master data, including creation, updates, and deletion.- Managing users and user roles.Job Types: Full-time, Contract, InternshipSalary: $60,000.00 - $70,000.00 per yearBenefits:Dental insuranceHealth insuranceLife insuranceVision insuranceSchedule:8 hour shiftMonday to FridayCOVID-19 considerations:Due to COVID-19, the internship will be remoteAbility to Commute/Relocate:Wayne, PA (Preferred)Education:Bachelor's (Preferred)Experience:SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Willingness To Travel:100% (Preferred)Work Location:Multiple locationsInternship Compensation:PayVisa Sponsorship Potentially Available:Yes: H-1B work authorizationWork Remotely:Temporarily due to COVID-19",Poway CA,Data Engineer and Integration
NYU Langone,/rc/clk?jk=1f077ad733e2c4a2&fccid=2794a3b4e0c4be57&vjs=3,"Overview: |  | NewWave is an information technology company helping businesses and government agencies modernize and thrive by applying the power of technology. NewWave began making a mark in the federal healthcare space in 2004, where we continue to actively modernize systems to improve healthcare’s value for millions of Americans. Since then, our work has expanded across various sectors and industries, where we help our customers stay ahead of the new and make the world in which we live, better. | Responsibilities: |  | We are looking for a savvy Data Engineer to join our growing team. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. | Create and maintain optimal data pipeline architecture | Assemble large, complex data sets that meet functional / non-functional business requirements | Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. | Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies | Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics | Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs | Keep our data separated and secure across national boundaries through multiple data centers and AWS regions | Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader | Work with data and analytics experts to strive for greater functionality in our data systems | Qualifications: | Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases | Experience building and optimizing ‘big data’ data pipelines, architectures and data sets | Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement | Strong analytic skills related to working with unstructured datasets | Build processes supporting data transformation, data structures, metadata, dependency and workload management | A successful history of manipulating, processing and extracting value from large disconnected datasets | Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores | Strong project management and organizational skills | Experience supporting and working with cross-functional teams in a dynamic environment | We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field | Experience with big data tools: Hadoop, Spark, Kafka | Experience with relational SQL and NoSQL databases, including Postgres and Cassandra | Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, | Experience with AWS cloud services: EC2, EMR, RDS, Redshift | Experience with stream-processing systems: Storm, Spark-Streaming, | Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala | Residency Requirement | Must have lived in the United States at least 3 out of the last 5 years | FSLA Status | Exempt | Interpersonal Skills | Excellent interpersonal, verbal and written communication, and organizational skills - must be able to communicate fluently in English both verbally and in writing | Facts and data oriented | Deadline and closure oriented | Strong persuasion, facilitation and influencing skills | Self-driven | Strong analytical, organizational and project management skills | Demonstrated ability to lead and work with cross functional teams including senior level individuals | Must be able to thrive in a fast-paced, rapidly evolving environment with varying priorities, based on a team building culture | NewWave is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. NewWave is a proud Veteran friendly employer.",Woodlawn MD,Data Engineer
Avalon Healthcare Solutions,/rc/clk?jk=207b8c43be9465ac&fccid=656a6eb3d1d4a020&vjs=3,"Are you looking for a challenging role as a Data Center Storage Engineer? | As a leading strategic service provider (SSP), InterVision assists IT leaders in solving the most crucial challenges they face by solving for the right technology, deployed on the right premises, and managed through the right model to fit their unique demands and meet their long-term goals. | InterVision’s mission is to transform business through the evolutionary power of technology, and we’re committed to unlocking value by delivering innovative technology through a consultative approach. Our people are the best in their field! | If you are looking for a career changing opportunity, we want to meet you! | InterVision Company is seeking a Data Center Storage Engineer who is well versed in a variety of datacenter technologies with strong experience in storage. This position will facilitate the day to day operations of our infrastructure supporting our multitenant cloud offerings. This position will be part of a team that supports and maintains applications and infrastructure for our storage, compute, and virtualization platforms. The qualified Data Center Storage Engineer will have experience working in large to medium datacenter environments. This position will focus on the following technologies: | Fiber based storage. Experience with Infinidat a plus. | Network attached storage. | MDS switch technology experience a plus. | Exposure to VMware (vsphere, ESXi, vCloud Director, NSX) | Scripting (bash, powershell, python, PHP, ansible, chief, puppet) | Exposure to Cisco UCS compute platform | Linux exposure | Microsoft Server Technologies |  | We welcome your experience and talents to our team! | 5-7 years of hands on experience supporting SAN storage. (EMC, IBM SAN, PureStorage) | 5-7 years of hands on experience with SAN switches (Brocade, Cisco MDS) | 5-7 years scripting and automation. Demonstrated abilities in one or more of the following would be a plus: PowerShell, bash, ksh, python, PHP, ansible, chief, puppet. | A working knowledge of ITSM practices | Some of the exciting work you will encounter: | Working in a environment with other highly skilled peers. | Working with some of the best storage, compute, networking on the market. | Working from home but having a good team connection. | Hands off Management. | A team that is open and shares information | A well-funded and mature company with the feeling of a startup. | Access to training and help with certification acquisition. | How Do We Back Our Strong Reputation? | GREAT PLACE TO WORK: If you thrive in an environment of growth and individual impact, InterVision is the place for you! | CUSTOMER SUCCESS: | When it comes to technology, our 25+ year history has guided some of the largest and most influential companies solve their problems with a broad range of innovative technologies ranging from network infrastructure to collaboration to cloud migrations. | InterVision can not only help clients refine their strategy with the right technology and the right cloud strategy, but also bring the resource models to take it to the finish line, assuring them a powerful combination of vision and capabilities. | BROAD CAPABILITIES: InterVision offers a wide range of services and solutions that organizations need to thrive in today’s dynamic IT market, including a broad range of innovative solutions for datacenter and cloud transformation, IT resiliency, modern communications, remote workforce and advanced data analytics. We’re competitive at all levels of engagement. | WORLD-CLASS FACILITIES: InterVision has headquarters in Santa Clara, Calif. and St. Louis, Mo., and offices, operations centers and datacenters in Missouri, Indiana, Nevada, Massachusetts, Colorado, California and Washington. We operate multiple Tier 3 and Tier 4 level data centers, which are SOC II Type 2 compliant specific and certified in Confidentiality and Security. | INDUSTRY ACCOLADES: InterVision has received some of the technology industry’s most prestigious awards and acknowledgements – repeatedly by Gartner® and Forrester™ plus Inc. Magazine’s 5000 Fastest Growing Private Companies. InterVision President and CEO Aaron Stone received accolades as one of the Top 100 St. Louisans to “Know How to Succeed in Business.” | TOP VENDOR CERTIFICATIONS: InterVision holds the highest certifications and partner levels with leading technology vendors, and we have teams of trained, certified engineers supporting their solutions. Here are just a few of the certifications from our list of 80+ vendors: Amazon Partner Network (APN) Premier Consulting Partner (with seven competencies), NetApp Star Partner, Cisco Gold Certification, Cisco Cloud and Managed Services Master, Juniper Elite Partner, Microsoft Gold Partner, AT&amp;T Premier Partner, Palo Alto Networks Diamond Partner, and VMware Premier Partner to name a few.",Indianapolis IN,Data Center Storage Engineer
Oxfam America,/rc/clk?jk=f80b8846eac1967c&fccid=10c1f2532daaed15&vjs=3,"As a member of the Finance Products team, your primary responsibility will be to oversee all of the software that powers our Finance teams, which include Accounting, FP&amp;A and Tax. You will also be collaborating with and providing crucial data to our Data Science teams. The data sets, pipelines, and tools that you build are mission critical for many company-wide decisions and will be for years to come. Additionally, you will work on a team where continuous learning and team-wide knowledge sharing is a core part of the day-to-day culture. You will report directly to the manager of the Finance Products team in our NYC Headquarters. |  | RESPONSIBILITIES |  | Build and maintain data processing services built with Airflow | Write, test and review microbatch or streaming ETL powered by Kafka | Write, test and review complex SQL queries and stored procedures for Postgres | Continuously improve our system, tests, and Data Quality Indicators | Stay knowledgeable about the ever changing technology landscape. | Work with the team to guide technical and architectural changes. | Be a technical leader, mentor and encourage the technical growth of your teammates |  | QUALIFICATIONS |  | You love working directly with the people whose problems you're solving | You are experienced at data modeling, storage, security, and retrieval | You're motivated and inspired. You naturally lead a project and ask for help when needed | You enjoy working with dynamic programming languages, relational databases, and distributed systems. Our platform is ever-evolving, but is a combination of Python, Java, Postgres, Kubernetes, Spark, Presto, Kafka, and MongoDB | You have a deep understanding of the products and tools you work with | 5+ years of experience |  | About Squarespace |  | Squarespace makes beautiful products to help people with creative ideas succeed. By blending elegant design and sophisticated engineering, we empower millions of people — from individuals and local artists to entrepreneurs shaping the world's most iconic businesses — to share their stories with the world. Squarespace's team of more than 1,000 is headquartered in downtown New York City, with offices in Dublin and Portland. For more information, visit www.squarespace.com/about. |  | Benefits &amp; Perks |  | Health insurance with 100% premium covered for you and your dependent children | Flexible vacation &amp; paid time off | Up to 20 weeks of paid family leave | Equity plan for all employees | Retirement benefits with employer match | Fertility and adoption benefits | Free lunch and snacks at all offices | Education reimbursement | Dog-friendly workplace in New York office | Commuter benefit in the form of reduced tax (Ireland) and pretax (US) |  | Today, more than a million people around the globe use Squarespace to share different perspectives and experiences with the world. Not only do we embrace and celebrate the diversity of our customer base, but we also strive for the same in our employees. At Squarespace, we are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, national origin, gender, sex, gender identity or expression, sexual orientation, age, citizenship, marital or parental status, disability, veteran status, or other class protected by applicable law. We are proud to be an equal opportunity workplace.",New York NY 10014,Senior Data Engineer Finance Products
Kalibri Labs,/company/Shark-analytics/jobs/Senior-Data-Engineer-Yrs-4a46772ea91019fa?fccid=b957326e0d359f41&vjs=3,"Job detailsSalary$34 - $70 an hourJob TypeFull-timePart-timeContractNumber of hires for this role2 to 4QualificationsBachelor's (Preferred)Data Warehouse: 5 years (Preferred)AWS: 5 years (Preferred)Full Job DescriptionPosition: Sr Data EngineerLocation: Atlanta, GADuration: 12 MonthsMust Have Skills: Python, Snowflake, Airflow, AWS and Data Warehousing/ETLJob Description :Hands-On experience in Snowflake, Airflow orchestration, Java script, Python, Spark SQL , LambdaMust have 10+ years of experience in delivering data engineering projectsHave strong knowledge in ETL, Data warehousing, Business intelligenceShould be able to configure pipelines to ingest data from data sources to the data platform. This will include configuration of airflow ingestion pipelines and/or Snowflake external tables/ snowpipe.Should be able to configure pipelines to ingest and process data from data sources to the data platform. This will include configuration of airflow ingestion pipeline and SnowflakeMonitor and respond to scheduled workloads that feed data from and to the data platform.Closely follow the run schedules and changes to schedules during maintenance windows to make sure workloads are executed after the maintenance window is complete.Communicate and work with Suppliers on schema drift and/or other Supplier issues.Communicate to stakeholders, delays or impacts of failures and escalate as needed to Leadership, EOC.Create and execute quality scripts to monitor and maintain the accuracy of our data.ThanksNaveenJob Types: Full-time, Part-time, ContractSalary: $34.00 - $70.00 per hourSchedule:8 hour shiftMonday to FridayEducation:Bachelor's (Preferred)Experience:Snowflake: 5 years (Preferred)Data Warehouse: 5 years (Preferred)Airflow: 5 years (Preferred)AWS: 5 years (Preferred)Work Location:One locationWork Remotely:Temporarily due to COVID-19",Atlanta GA,Sr. Data Engineer 10+ yrs Required
Squarespace,/rc/clk?jk=6bd788fc95a48e52&fccid=3748c93dadbb2e71&vjs=3,"About the Company: | Avalon Healthcare Solutions, headquartered in Tampa, Florida, is a clinical services and information technology company using evidence-based medicine to develop and deploy medical policies and protocols in the high-volume, dynamic and complex diagnostic lab environment. The company manages the appropriate use of thousands of existing lab tests and researches new tests to determine efficacy and impact on patient care. | Studies show that 30% of clinical laboratory testing is unnecessary or overused. Inappropriate testing or missing a key screening can lead to complications and expense arising from unwarranted care, or not obtaining proper care when needed, leading to increased health risks and costs. Avalon helps ensure delivery of the right test, at the right time, and in the right setting. We seek to ensure the most effective patient treatment, improve clinical outcomes, and optimize cost and affordability. | Avalon is a portfolio company of Francisco Partners, a global private equity firm that specializes in investments in technology and technology-enabled service companies. Since its launch 15 years ago, FP has raised approximately $10 billion and invested in more than 150 companies. | Avalon is a high growth company where every associate has an opportunity to make a difference. You will be part of a team that shapes a new market and business. You’ll enjoy seeing the results of your work as we rapidly implement our plan. Most importantly, you will help Avalon to achieve its mission and improve clinical outcomes and health care affordability for the people we serve. | For more about Avalon, please visit our web site at http://www.avalonhcs.com. |  | About the Data Science Engineer Position: | Within the analytics department, the Data Science Engineer will aid the Data Scientist in prepping data for modeling. The Data Science Engineer will construct and maintain the infrastructure used for advanced analytics and be responsible for assessing and reporting on data quality. This position entails ensuring the right infrastructure is in place to support data science projects and results from models are available for consumption by internal and external clients. |  | Avalon Healthcare Solutions is based in Tampa, Florida. This is a full-time, W-2 position. At this time, we can consider only local candidates authorized to work in the US. No sponsorship is available. No recruiter calls, please. |  | Data Science Engineer - Essential Functions and Responsibilities: | Acquire data to meet project requirements | Curate data for advanced analytics and data science | Construct, test and maintain scalable data science architecture | Generate reliable and repeatable processes to onboard data | Assess and report data integrity | Find trends in data to optimize data quality | Prepare data for modeling | Assist in developing and testing models | Develop methodology to store and transmit model outputs | Integrate model outputs with visualization and BI tools |  | Data Science Engineer - Qualifications: | Coding skills in python, SQL and R | Excellent problem solving and communication skills | Experience with data warehousing and ETL | Desire to learn new software and methods | Familiar with web service offerings such as S3 and Redshift |  | Data Science Engineer - Preferred Qualifications: | Familiar with statistics and statistical modeling | Experience in healthcare and managed care",United States,Data Engineer
Travelers,/company/Kalibri-Labs/jobs/Big-Data-Engineer-02d100ec7e3af349?fccid=257a02caa05672e3&vjs=3,"Job detailsJob TypeFull-timeNumber of hires for this role1QualificationsUS work authorization (Required)Bachelor's (Preferred)Scala: 1 year (Preferred)Spark: 1 year (Preferred)Full Job DescriptionAt Kalibri Labs, we are helping to redefine and rebuild the hotel industry. We are looking for passionate, energetic, and hardworking people with an entrepreneurial spirit, who dream big and challenge the status quo. We are working on cutting-edge solutions for the industry as they navigate the recovery process. We are using our big data coupled with machine learning and AI to help highlight the path forward. Kalibri Labs is growing, so if you’re ready to make a difference and utilize your talents across a groundbreaking organization, please keep reading!*This position is not eligible for sponsorshipWe are seeking a Software Engineer with a focus on big data to help build new pipelines using modern, cutting-edge technologies. This involves using Scala and the Spark library to process data into useful forms. We manage the continuous integration and continuous deployment using Jenkins. We use Snowflake as our main data warehouse and AWS as our cloud provider. The Scrum team adheres to Agile methodology including sprint planning/review/grooming/retrospective and daily standups.ResponsibilitiesDesign and develop new big data processing pipelinesRefine current big data processing pipelinesUpdate documentation on internal wiki for technical communicationParticipate in code reviews and design sessions during Agile processDebug, evaluate, and troubleshoot throughout application development processMaintain and operate continuous build, test, and integration pipelinesDevelop thorough understanding of technologies used and demonstrate archetypical patterns for their useSkills &amp; Requirements3+ years of experience on a development team contributing to all parts of a modern projectExtensive experience with ScalaExtensive experience with SparkExperience with Junit and MockitoAbility to work autonomously and collaboratively on a complex systemExperience with SQLExperience with Java and Spring Boot preferredExperience with Docker preferredAbility to work with and coordinate with other developers using digital tools such as Slack, Gmail, and Atlassian’s suite of products (JIRA, Bitbucket)Experience with version control tools, such as GitExperience working in Linux desiredExcellent command of written and spoken EnglishBachelor’s degree in Computer Science or related disciplineDesire to stay up to date with modern techniques and evolving technologiesJob Type: Full-timeBenefits:401(k)Dental insuranceEmployee assistance programFlexible scheduleFlexible spending accountHealth insuranceLife insurancePaid time offRetirement planVision insuranceSchedule:Monday to FridayEducation:Bachelor's (Preferred)Experience:Scala: 1 year (Preferred)Spark: 1 year (Preferred)Work Location:Fully RemoteCompany's website:kalibrilabs.comCOVID-19 Precaution(s):Remote interview processVirtual meetings",Remote,Big Data Engineer
DPR Construction,/rc/clk?jk=b015548512438aba&fccid=7da80e6d4430aa40&vjs=3,"Job Description | Senior Data Engineer | Position Summary |  | DPR is looking for an experienced Data Engineer to join our Data Solutions team and tackle exciting and impactful business problems in the domain of Construction. This position will be part of a team that uses AI, ML, modern Data Engineering techniques to bring efficiencies in the construction life cycle. Ideal candidates need to have a strong curiosity about data, and a proven track record of successfully applying analytics, data engineering techniques. This an outstanding opportunity to apply your skills and have a direct impact on the Construction Industry. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. | Responsibilities |   | Responsibilities include solutions implementation from cloud-first thinking using Agile, Scrum, Dev-Ops, Data Ops. |   | Create and maintain optimal data pipeline architecture. |   | Assemble data sets that meet functional / non-functional business requirements. |   | Build analytics models that utilize the data pipeline to provide actionable insights into the construction lifecycle. |   | Maintain data pipelines and support enhancements based on business requirements and requests. |   | Design and implement process improvements like automating manual processes, optimizing data pipeline from source to consumption, scale the data infrastructure, etc. |   | Support software developers, database architects, data analysts, and data scientists on data initiatives and ensure that the data delivery architecture is consistent throughout ongoing projects. |   | Expand and optimize data and data pipeline architecture, as well as optimize data flow and collection for cross-functional teams. |   | Work with stakeholders, cross-functional business groups, Data and Software teams to assist with data- related technical issues and support their data needs. |   | Develop data models for analytics and data scientists and them in building and optimizing data. | Position Requirements |   | 4+ years experience as a software/data engineer in a fast-paced, technical, problem-solving environment. |   | Experience with Cloud Data Warehouse technologies. |   | Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. |   | Strong analytic skills related to working with unstructured datasets. |   | Strong project management and organizational skills. |   | Knowledge and understanding of AWS and Azure data ecosystem. |   | Experience with Data Modeling and BI tools like Power BI, Looker |   |  | Experience with SQL, JSON and XML, LookML |   | Knowledge of API’s, REST, GraphQL. |   | Experience with programming languages like Python. | DPR has been nationally recognized for its strong company culture, based on a well-defined purpose “We Exist to Build Great Things,” and four core values: integrity, enjoyment, uniqueness and ever forward. A flat, title-less organization that empowers people at all levels to make decisions, DPR ranked on FORTUNE’s “100 Best Companies to Work For” list for five consecutive years. For more information, visit http://www.dpr.com .",Washington DC 20002,Data Engineer- Sr
Snapchat,/rc/clk?jk=7cfcd1f575883e75&fccid=3ed0572c448b2368&vjs=3,"Job detailsJob TypeContractFull Job DescriptionKey Technologies: Python, SQL, NodeJs, AngularJS, ELK This person will also be asked to help with creating reports and dashboards that leadership will use. Must Haves: • Data Analytics Background with the ability to understand the logic behind the data • Programming experience with the following (Python, NodeJS, Angular, SQL) • Development/programming ability to write services and move data • Experience writing queries and developing dashboards with specific KPI’s Nice to Have: • Experience building an ELK Pipeline",Providence RI,Python Data Engineer
CBRE,/rc/clk?jk=ef14d608e2c32771&fccid=f368300325e8e8bc&vjs=3,"Snap Inc. is a camera company. We believe that reinventing the camera represents our greatest opportunity to improve the way people live and communicate. Our products empower people to express themselves, live in the moment, learn about the world, and have fun together. | We’re looking for a Data Engineer on our Data Insights and Governance team! Working from our Los Angeles, CA, headquarters, you’ll collaborate with teams across the organization (engineering, finance, sales, marketing, and strategy) to build pipelines and systems to deliver the data necessary for making the right decision in the moment. | What you’ll do: | Work closely with stakeholders in engineering, finance, sales, marketing, strategy, and governance to make high quality datasets available to consumers in a timely manner | Develop data pipelines adhering with privacy and governance principles | Become familiar with our data consumption portals and their capabilities | Build expertise and ownership of data quality for supported domains | Build tooling and implement systems to overcome limitations of the data consumption portals when appropriate | Drive adoption of the data sets you’ve produced | Knowledge, Skills &amp; Abilities: | Experience in building data pipelines to serve reporting needs | Experience owning all or part of a team roadmap | Ability to prioritize requests from multiple stakeholders in disparate domains | Ability to effectively communicate complex projects to non-technical stakeholders |  | Minimum Qualifications: | BS/BA degree in Computer Science, Math, Physics, or a related field, or equivalent years of experience in a relevant field | 3+ year experience in SQL or similar languages | 3+ years development experience in at least one object-oriented or scripting language (Python, Java, Scala, etc) | Preferred Qualifications: | Hands on experience with Google BigQuery | Experience in version control systems such as Git | Data architecture and warehousing experience | Experience leading a small team of data or software engineers | Experience with Airflow | Experience in ETL / Data application development | At Snap, we believe that having a team of diverse backgrounds and voices working together will enable us to create innovative products that improve the way people live and communicate. Snap is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. EOE, including disability/vets. If you have a disability or special need that requires accommodation, please don’t be shy and contact us at accommodations-ext@snap.com .",Los Angeles CA 90291,Data Engineer
Intone Networks,/rc/clk?jk=0a805975539bb3ef&fccid=21b50278807da744&vjs=3,"Who we are: | Grabango is the leading provider of checkout-free shopper technology for existing stores. Founded by Will Glaser (former founder &amp; CTO of Pandora Media), the Grabango team has developed the only enterprise class solution for large store chains in the market today. Grabango's system accommodates thousands of store locations, hundreds of thousands of SKUs and millions of square feet across a given retail enterprise. |  | We're a growing group of curious, self-directed people working towards a common goal. We delight in taking risks and testing hypotheses in a collaborative environment. Our ability to celebrate both our successes and failures as milestones of progress opens the door to tremendous breakthroughs. |  | Computer vision and machine learning are at the core of our patent pending technology. Our accomplished team is a powerful combination of technology and commerce professionals that are working to ""Eliminate lines and save people time!"" |  | Overview: |  | Grabango is looking for a Data Engineer to join our growing team of analytics experts. This Data Engineer will be responsible for expanding and optimizing our data pipeline architecture and managing our data warehouse, as well as optimizing data accessibility for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts, and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives. |  | This full-time role reports to the Data Science Manager and is based in Berkeley. |  |  | What you'll be doing: | Create and maintain ETL pipelines that can robustly ingest, transform, and store both external customer data and internally generated data | Build out our data warehouse and own the data flow between production databases and our data warehouse | Collaborate with computer vision and machine learning teams to create data pipelines for iteration of cutting edge computer vision models | Implement best practices to ensure data quality and integrity in our pipelines | What you should have: | 4-7 years of experience as a data engineer | Excellent SQL and Python skills | Experience with both SQL and NoSQL databases, such as Cassandra and MongoDB | Experience with data pipelining tools such as dbt, airflow, luigi or Dagster | Comfort working within systems running Kubernetes, Docker, Linux, Git | Strong communication skills to collaborate with cross-functional stakeholders | You can rapidly become familiar with and apply new tools and technologies | Educations &amp; Certifications: | BS in Computer Science, Engineering, or related field |  | Grabango Values: | Integrity: Do the right thing, particularly when you don't have to. | Bold Innovation: Think recklessly, but temper your actions with pragmatism. | People Matter: Hire the best, and treasure them. | Don't Live with Broken: Have the courage to admit mistakes, and the urgency to correct them. | Customer Focus: Deliver beyond expectations, both internally and externally. | Inclusion: Foster an environment that welcomes all. | Simplify: Everything should be made as simple as possible, but not simpler. |  |  | Grabango is proud to be an equal opportunity employer and is committed to developing a workplace where diversity and inclusion are an essential part of who we are. We strive to hire and support a workforce as diverse as our shopper base, so we can develop products and services that best suit our customers. We do not make employment decisions based on race, color, religion, ethnic or national origin, nationality, sex, gender, gender-identity, sexual orientation, disability, age, military or veteran status and we comply with all local, state and federal employment laws. |  | Grabango participates in the E-Verify Program, an internet-based system operated by the Department of Homeland Security and the Social Security Administration. It allows employers to confirm an individual's employment eligibility to work in the United States.",Berkeley CA,Data Engineer
Apple,/rc/clk?jk=5535267f2b6c2a37&fccid=c1099851e9794854&vjs=3,"Summary | Posted: Dec 9, 2020 | Weekly Hours: 40 | Role Number:200166439 | This team is more than a group of engineers — it’s a group of music lovers. That passion has made Apple Music the world’s most complete music experience, with over 60 million songs, thousands of playlists, and daily selections from music experts for 115 countries. | The team’s data-driven engineers focus relentlessly on the customer experience by running worldwide experiments and analyzing usage and latency, while collaborating with Apple’s product groups. As a result, someone can use the Shazam app to identify an intriguing song in a café in the morning, add it to their playlist from Apple Watch, listen to it through their AirPods on their commute, and share it with their family on HomePod at dinnertime. And there’s more where that came from, because personalization powered by machine learning and music science helps listeners discover more of what they love. Apple Music is a big part of Apple’s business because it’s a big part of people’s lives. Areas of work: macOS/iOS Engineering, Full-Stack Engineering, Front-End Engineering, Back-End Engineering, Quality Engineering, Machine Learning Engineering, Data Science, Data Engineering, Site Reliability Engineering, Commerce Engineering, and Engineering Project Management. | Key Qualifications | Experience in designing, implementing and supporting highly scalable data systems and services in Java and/or Scala | Experience with Hadoop-ecosystem technologies in particular MapReduce, Spark / Spark-SQL / Spark Streaming, Hive, YARN/MR2 | Experience building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components | Experience in data-modeling and data-architecture optimized for big data patterns, ie. warehousing concepts; efficient storage and query on HDFS; data security and privacy techniques) | Knowledgable about distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues | Experience with low-latency NoSQL datastores and traditional relational databases is desired | Description | Our Data Engineering team is seeking a hardworking, performance-savvy, engineer to build out the big data platform and services, which power many of these customer features — existing and new. You will be responsible for designing and implementing features that rely on processing and serving very large datasets with an awareness of scalability. This will include crafting systems to model, ingest, process and compute large-scale, mission-critical data across Apple Music. High-throughput and reliability are essential. | This is your opportunity to help engineer highly visible global-scale systems with petabytes of data, supporting hundreds of millions of users! | Education &amp; Experience | Bachelors degree in Computer Science, or equivalent experience. | We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",New York NY,Apple Music - Software Data Engineer
Citi,/rc/clk?jk=75ce63e978aa4b06&fccid=e08122e0e993cc32&vjs=3,"Job detailsJob TypeContractFull Job DescriptionThe primary responsibilities of this role, Senior Data Engineer, are to: | Work on the deployment, delivery and expansion of data pipelines; | Collaborate with interdisciplinary scientists to gather requirement for data pipelines; | Primary focus on developing data pipelines with geospatial and imaging processing data pipelines to enable batch and real-time analytics. | Optimize algorithms and data workers to scale horizontally and contribute to the development of new algorithms and capabilities that will enable connected pipeline analytics for all pipelines; | Work on all aspects of the design, development, validation, scaling and delivery of analytical solutions; | Work on the development, deployment, and support of systems computing solutions; | Collaborate with analytics and discovery teams to design and plan data engineering solutions; | Implement, configure, and maintain critical third-party solutions related to engineering work, including compute environments, BI platforms, and cloud systems; | Design and maintain ETL workflows; | Provide consulting and feedback to partner teams on data architecture and strategy; | Integrate proactive strategies and best practices to ensure security of stored data; | Design, build, and maintain integrated data solutions such as data lakes and data warehouses; | Design and maintain data storage systems and access patterns; | Collaborate and influence with cross-functional stakeholders to develop our strategic target state data infrastructure and organization model; | Actively drive skill development of others in the space through proactive coaching, feedback, and training; | Drive practices which help raise the success of the overall team, such as code reviews, integrated testing, and other practices; | Partner cross-functionally in the development of shared infrastructure where aligned with Breeding business needs; | Actively identify new technologies and practice within the domain of engineering and drive review for potential introduction to the team's infrastructure. | WHO YOU ARE | Your success will be driven by your demonstration of our LIFE values. More specifically related to this position, Bayer seeks an incumbent who possesses the following: | Required Qualifications: | Bachelor's degree in Computer Science, Electrical Engineering or a closely-related field with at least 10 years of industry experience OR Master's Degree in Computer Science, Electrical Engineering or a closely-related field with at least eight years of industry experience OR Doctorate in Computer Science, Electrical Engineering, or a closely-related field with at least four years of industry experience; | Technical knowledge with at least of seven years of experience in at least four of the following: | o SQL and NoSQL databases (data warehousing, data modeling, etc.); | o Experience with big data tools (Spark, Kafka, Flink, Hadoop, etc.); | Knowledge of algorithms and data structures; | Two plus years of experience with GIS data and relevant tools (ArcGIS Pro, ArcGIS Online, ArcMap) | Experience in Developing and supporting large scale geospatial and Imaging Processing data pipelines | Experience with tools for authoring workflows and pipelines (Airflow, AWS Step Functions, KubeFlow, etc.); | Experience with AWS cloud services (EMR, S3, RedShift, EC2, etc.); | Experience with distributed systems; | Experience with python, Java, R, or Scala. | Preferred Qualifications: | Network and Database administration; | Proven systems administration and operations experience; | Proven ability to plan, schedule and deliver quality software DevOps methodology; | Experience in running production cloud systems and diagnosing and fixing problems; | Familiarity with ML workflows including validation and hyper parameter tuning approaches, and ML frameworks such as scikit-learn and tensor flow. | BAY1JP00006365",Chesterfield MO 63017,Data Engineer (Expert) - Data & Analytics - IT - Corp - US
CNH Industrial,/rc/clk?jk=4298f7e6b4e0cad6&fccid=e2c5b29a1878dd23&vjs=3,"Overview: |  | Through its people and brands, CNH Industrial delivers power, technology and innovation to farmers, builders and drivers all around the world. Each of its brands, including Case IH, New Holland Agriculture, Case and New Holland Construction, FPT Industrial, Capital, and Parts &amp; Service, is a major international force in its specific sector. | Responsibilities: |  | This position will instrument (transducers, accelerometers and strain gages) Agricultural Equipment, collect data and analyze data on various component and sub systems of a vehicle in a lab or field environment to determine acceptability of the designs; recommend alternatives to enhance performance, reliability, quality, durability and margin improvement; and work with the product development team to resolve product issues; develop mission profiles for duty cycle development and test stand loading requirements; travel to remote field locations to collect data during key harvest seasons | Full responsibility for executing data acquisition and analysis requests in lab and field settings including planning, instrumentation set up, strain gaging, data collection, analysis, and reporting. Work with design engineers to interpret data for components to achieve desired results | The engineer will recommend alternatives to enhance performance, reliability, quality, durability, and margin improvement; and work with the product development team to resolve product issues | Prepare, organize and submit timely and concise reports which include data reduction and analysis of system/component performance, reliability, and durability. Maintain accurate and comprehensive test records for reference and analysis | Qualifications: |  | Requirements: | Bachelor’s Degree in Engineering (Mechanical, Agricultural, or Electrical) | Minimum 4 or more years of experience with data acquisition concepts and tools (DEWESoft, Somat/HBM, National Instruments, or Campbell data loggers preferred) | Ability to travel for 4 week intervals in primarily domestic but also international locations up to 30% | Preferred Requirements/Qualifications: | Experience with test equipment and analysis software (nCode Glyphworks, InField, DEWESoft X, Matlab or similar computer applications) | Experience with Noise, vibration, and harshness (NVH) testing and analysis techniques | The ability to execute tasks with minimal supervision and quickly learn new tools and techniques is required | Good communication (oral and written) and teamwork skills | The candidate is experienced with personal computers and MS Office products along with basic computer programming skills to support test equipment | Operational experience with agricultural and/or construction equipment | Ability to obtain commercial driver’s license | EEO: |  | CNH Industrial is an equal opportunity employer. This company considers candidates regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status. Applicants can learn more about their rights by viewing the federal ""EEO is the Law"" poster and its supplement here. | CNH Industrial participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. You can view additional information here. | If you need reasonable accommodation with the application process, please call 1-800-889-4422 option 1 and then option 5, or contact us at narecruitingmailbox@cnhind.com. | Read about our company’s commitment to pay transparency by clicking this link: pay transparency notice.",New Holland PA,Data Acquisition Field Engineer
Govini,/rc/clk?jk=b6bbc54883ee66e2&fccid=78f10670925c5ae8&vjs=3,"Company Description |  | Denodo is the leader in data virtualization – providing unmatched performance, unified access to the broadest range of enterprise, Big Data, cloud and unstructured sources, and the agilest data services provisioning and governance – at less than half the cost of traditional data integration. Denodo’s customers have gained significant business agility and ROI by creating a unified virtual data layer that serves strategic enterprise-wide information needs for agile BI, big data analytics, web and cloud integration, single-view applications, and SOA data services across every major industry. Founded in 1999, Denodo is privately held. | In the 2020 Forrester Wave for Enterprise Data Fabric, Forrester noted that ""Denodo is known for data virtualization, and over the years it has also evolved into a data fabric vendor. Denodo's data fabric solution integrates key data management components, including data integration, data ingestion, data transformation, data governance and security, to support new and emerging use cases including customers 360, real-time and on-demand analytics, IoT analytics, and self-service analytics."" |  | Job Description |  | The Opportunity | Denodo is always looking for technical, passionate people to join our Services Engineering team. We want a professional who will consult, develop, train and troubleshoot to enhance our clients’ journey around Data Virtualization. | Your mission: to help customers achieve and maintain success through accelerated adoption and productive use of Denodo solutions. | In this role you will successfully employ a combination of high technical expertise and client management skills to conduct troubleshooting and issue resolution, provide technical guidance and advice through remote or on-site consulting engagements, deliver timely and complete solutions to customer issues, and being a critical point of contact for getting things done among Denodo, partner and client teams. Our client’s rely on data in their daily operations. You will be called upon to creatively assist our customers in ensuring they are successful in their data processing endeavors utilizing our products. | Location New York, NY | Duties &amp; Responsibilities | As a Data Services Engineer you will successfully employ a combination of high technical expertise, research and investigative know-how, trouble shooting and problem solving techniques, and communication skills between clients and internal Denodo teams to achieve your mission. | Obtain and maintain strong knowledge of the Denodo Platform, be able to deliver a superb technical discussion, including overview of our key and advanced features and benefits, services offerings, differentiation, and competitive positioning. | Constantly learn new things and maintain an overview of modern technologies. | Provide technical consulting, training and support. | Diagnose and resolve clients inquiries related to operating Denodo software products in their environment. | Participate in problem escalation and call prevention activities to help clients and other technical specialists increase their efficiency when using Denodo products. | Be able to address a majority of technical questions concerning customization, integration, enterprise architecture and general feature / functionality of our product. | Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client’s business cases, requirements and issues. | Train and engage clients in the product architecture, configuration, and use of the Denodo Platform. | Promote knowledge and best practices while managing deliverables and timelines. | Capable of building and/or leading the development of custom deployments based on and even beyond client’s requirements. | Manage client expectations, establish credibility at all levels within the client and build problem-solving partnerships with the client, partners and colleagues.Develop white papers, presentations, training materials or documentation on related topics and contribute to knowledge management activities. | Participate in on-call support of Denodo products. | Be willing to travel as necessary to address or service customer needs. |  | Qualifications |  | Required Skills | BS or higher degree in Computer Science. | Solid understanding of SQL and good grasp of relational and analytical database management theory and practice. | Good knowledge of JDBC, XML and Web Services APIs. | Excellent verbal and written communication skills to be able to interact with technical and business counterparts. | Active listener. | Strong analytical and problem solving abilities. | Lots of curiosity. You never stop learning new things. | Creativity. We love to be surprised with innovative solutions. | Willingness to travel on occasion. | Be a team worker with positive attitude. | We Value | Experience working with GIT or other version control systems. | Experience working with Big Data and/or noSQL environments like Hadoop, mongoDB, others. | Knowledge and experience with systems and services hosted in the main cloud vendors (AWS, Azure, GCP). | Experience working with caching approaches and technologies such as JCS. | Experience in Windows &amp; Linux (and UNIX) operating systems in server environments. | Business software implementation and integration projects (e.g. ETL/Data Warehouse architectures, CEP, BPM). | Integration with packaged applications (e.g. relational databases, SAP, Siebel, Oracle Financials, Business Intelligence tools, …). | Industry experience in supporting mission critical software components. | Experience in attending customer meetings and writing technical documentation. | Experience in Java software development, especially in the web and database fields. | Foreign language skills are a plus. | Additional Information |  | Employment Practices | We are committed to equal employment opportunity. | We respect, value and welcome diversity in our workforce. | We do not accept resumes from headhunters or suppliers that have not signed a formal fee agreement. Therefore, any resume received from an unapproved supplier will be considered unsolicited, and we will not be obligated to pay a referral fee.",New York NY,Data Services Engineer
Bayer,/rc/clk?jk=38e2dce9d668b7d2&fccid=c9b9e477b3c84b4d&vjs=3,"RESPONSIBILITIES |  | Utilizes advanced skills to perform complex preventive maintenance and corrective repair of buildings, industrial systems, vehicles, equipment and grounds. Working under limited supervision, monitors building system operations and performance. Utilizes several trade skills such as carpentry, plumbing, electrical, painting, roofing, heating and cooling. |  |  | ESSENTIAL DUTIES AND RESPONSIBILITIES |  | Complies with all applicable codes, regulations, governmental agency and Company directives related to building operations and work safety. |  | Inspects building systems including fire alarms, HVAC, and plumbing to ensure operation of equipment is within design capabilities and achieves environmental conditions prescribed by client. |  | Oversees and inspects the work performed by outside contractors. Contracted work includes landscaping, snow removal, remodeling, HVAC, plumbers, and cleaning. |  | Performs assigned repairs, emergency and preventive maintenance. Completes maintenance and repair records as required. |  | Reviews assigned work orders. Estimates time and materials needed to complete repair. Orders necessary materials and supplies to complete all tasks. |  | Maintains an energy management program to ensure measures are taken to operate all systems in the most efficient manner to keep operating costs at a minimum. |  | Maintains the building lighting system, including element and ballast repairs or replacements. |  | Performs welding, carpentry, furniture assembly and locksmith tasks as needed. |  | Responds quickly to emergency situations, summoning additional assistance as needed. |  | Performs other duties as assigned. |  |  | SUPERVISORY RESPONSIBILITIES |  | No formal supervisory responsibilities in this position. May provide informal assistance such as technical guidance and/or training to coworkers. May coordinate work and assign tasks. | QUALIFICATIONS |  | To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. |  | Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. |  |  | EDUCATION and EXPERIENCE |  | High school diploma or general education degree (GED) and a minimum of four years of related experience and/or trade school training. |  |  | CERTIFICATES and/or LICENSES |  | Universal CFC certification preferred. Additional certification in one or more of the following: electrical, mechanical, HVAC and refrigeration systems, process controls, mechanical power transmissions, painting, plumbing, carpentry or engine repair. Certifications/licenses as may be required by local or state jurisdictions. |  |  | COMMUNICATION SKILLS |  | Ability to comprehend and interpret instructions, short correspondence, and memos and ask clarifying questions to ensure understanding. Ability to write routine reports and correspondence. Ability to respond to common inquiries or complaints from clients, co-workers, and/or supervisor. Ability to effectively present information to an internal department and/or large groups of employees. |  |  | FINANCIAL KNOWLEDGE |  | Requires basic knowledge of financial terms and principles. Ability to calculate simple figures such as percentages. |  |  | REASONING ABILITY |  | Ability to solve practical problems and deal with a variety of concrete variables in situation where only limited standardization exists. Requires intermediate analytical skills. |  |  | OTHER SKILLS and ABILITIES |  | Uses personal computer and / or PDA for work order system, email, ESS and training. Basic skills with Microsoft Office Outlook. Physical requirements include stooping, standing, walking, climbing stairs / ladders and ability to lift / carry heavy loads of 50 lbs. or more. |  |  | SCOPE OF RESPONSIBILITY |  | Decisions made with general understanding of procedures and company policies to achieve set results and deadlines. Errors in judgment may cause short-term impact to co-workers and supervisor. |  |  | SAFETY |  | Responsible for personal safety and the safety of those who are affected by your work. This includes but is not limited to: |  | Complete all required and assigned HSE training at a satisfactory level |  | Follow all activity policies and procedures, including all HSE-related requirements at all times |  | Participate in all HSE-related programs &amp; activities as required, including incident investigations, interviews, auditing and assessment, etc. |  | Report any conditions which you feel could result in an accident or injury and / or stop work if required",Orangeburg NY,Union Engineer Data Center Night Shift
Macquarie Group Limited,/rc/clk?jk=718baa1fa01a4a99&fccid=2bfe06fc096100f7&vjs=3,"Role Description/Responsibilities: | Support creation of a cloud based data management Infrastructure through the development of data lakes and a data warehouse. Candidate will be involved in all aspects of the modeling process, including data discovery, data quality, and logical/physical modeling. The analyst should have experience with particular Informatica tools (EDC, IDQ, and Axon) in an Azure Cloud environment, as well as other Azure based tools such as Polybase, and Synapse. | In office 2-5 days a week under non-COVID conditions. Can work remotely, but most be located in Washington DC area. | Minimum Skills: | Informtica Data Quality (3), | Informatica Axon (3), | Informatica EDC (3) |  | Required Qualifications: | Data Warehouse Tools (3 years) | Data Audit and Profiling (3 years) | Data Warehouse ETL Testing (3 years) | Informatica Axon (3 years) | Informatica Data Quality (3 years) | Informatica Enterprise Data Catalog (3 years) |  | Preferred Qualifications: | Government agency support |  | Soft Skills: | Communicating technical requirements across multiple organizations in succinct language |  | Government Requirements: | 1. Candidate must have an active Secret Clearance or ability to obtain a Secret Clearance | 2. Candidate must be U.S. Citizen (no dual citizenship) | 3. Must complete a Background investigation prior to start of work |  | This and all positions at Mount Airey Group, Inc. require the candidate to prove eligibility to work in the United States within 3 days of being employed. |  | Mount Airey Group, Inc. is an Equal Opportunity Employer M/F/D/V. | If you are interested in being considered for this opening, please send your resume to | careers@mountaireygroup.com for consideration. Please provide the following details in the email: |  | the position title and number, | your salary requirements, | your current security clearance, if any, | your availability, | your best contact phone number, | and the best time to reach you.",Rosslyn VA,Data Engineer
Reliable Software Resources,/rc/clk?jk=e25bca5f78334bab&fccid=b28dac4cff245ca5&vjs=3,"Company: Veruna, Inc. |  | Role: Data Migration Engineer | Job Type: Full-Time, Exempt | Location: Remote | Contact: Heather Leite: hleite at veruna.com; Cover Letter and Resume required |  | Help Us Shape the Future of the Insurance Industry |  | Veruna empowers insurance agencies to control the destiny of their business. We are a venture backed start-up in high growth, scale-up mode built on the Salesforce platform. |  | Veruna is seeking individuals who are passionate on building a business and want to overcome new challenges. Candidates should be knowledgeable and capable, but always eager to learn more and to teach others. Overall, we strive to create a culture that is both relaxed and focused, and we stress empathy and collaboration with our customers, users, and each other. |  | Summary: |  | Veruna is looking for a skilled Data Migration Engineer to analyze the scope of client requirements for data migration projects, design and document solid migration plans, and deliver migration services and solutions in the insurance agency management market. This individual will be part of a team, responsible for both week-to-week data transformations for onboarding new business and designing the future of Veruna’s strategy of scale. Veruna seeks to provide excellence for customers’ experience when adopting our platform. |  | You Are: |  | Dedicated, multi-faceted problem solver | Starting off your career in data analysis, with a hunger to learn and grow | Able to work independently with minimal guidance in a fast-paced, start-up environment | Able to lead and mentor junior data migration analysts | Exceptionally self-motivated with a keen attention to detail |  | Responsibilities: |  | Execute data migrations and data quality efforts focused on the completeness, standardization, and de-duplication | Act as subject matter expert (SME) for projects including analysis of legacy files, matching of data, and validating data | Design opportunities for automation of data transformation processes, working with product and development teams to create solutions and tools | With project leadership, assist in the estimation of work effort, determination of required resources, and set a realistic schedule for implementation | Design data quality analysis tools, for both pre- and post-migration activities | Create testing and automation strategies and approaches in alignment with overall organizational goals | Analyze data, mapping the data between various source systems, and ensuring the accurate importing of data into Salesforce | Maintain effective client relations and rapport – delivering timely and relevant information with the ability to quickly identify and report project risks | Develop SQL Script, analyzing data, creating data metrics, and integrating data using emerging technologies | Identifying and documenting data milestones | Additional duties and responsibilities to vary as needed |  | What You Need: |  | Bachelor’s degree (technical discipline a plus) | 3+ years of in-depth experience migrating and managing data in Salesforce | Minimum 5+ years of Microsoft SQL Server experience required | Minimum 3+ years of Scripting work experience | Interface design, documentation, and testing experience | Track record retrieving and analyzing data using Excel, Access, and Salesforce data migration tools | Experience using Salesforce cleaning data tools such as Active Prime, DemandTools, Alteryx, others | Understanding of current technologies and Software Development Lifecycle (SDLC) | Excellent problem-solving skills; strong logical reasoning and solution oriented |  | What Will Make You Stand Out: |  | Salesforce Admin Certification | Experience with ETL products | Independent and fast learner | Prior experience working in the Insurance industry (preferably on the agency management side) | Familiarity with both Python and Pearl | Understanding of AL3 and ACORD industry formats | Willing to dive in and learn as you go! |  |  | Veruna is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.",Hopkinton MA 01748,Data Migration Engineer
Pearson,/rc/clk?jk=66eca689adc0bdea&fccid=0272fddcb6fcbae4&vjs=3,"dutiesprojekt distribuované analytické platformy pro kybernetickou bezpečnost v oblasti bankovnictví, automobilového průmysluRequiredmust haveDobrý znalost alespoň jednoho z jazyků: Python, Java, ScalaApache Kafka, Apache Spark, Apache FlinkAWS, Azureplynulá angličtinaWelcomedzkušenost s platformami pro big data, např. Cloudera apod.our offerWe offer transparent conditions, non-competition clause free contract, agreed percentage of sales rate and participation in our favorable referral or affiliate program. You will be able to contribute on customer, internal and open-source projects and participate in start-ups. If you wish to work in friendly and community-based company don't hesitate and apply",Remote,Big Data Engineer (Junior/Senior)
Tyler Technologies Inc.,/company/TracFone-Wireless-Inc./jobs/Data-Engineer-79de4e1f473a87d7?fccid=fd974d294ef7390f&vjs=3,"Job detailsJob TypeFull-timeQualificationsUS work authorization (Preferred)Full Job DescriptionPosition Highlights: · Recognized leader:  Rated #1 Prepaid Wireless Provider in the U.S.· Technology driven:  Opportunity to work with state-of-the-art technology· Teamwork:  A supportive team environment that thrives on innovation.· Culture: An entrepreneurial focus, where ownership and ingenuity are expected.· Benefits: Excellent health benefits, Matching 401K, and education reimbursement.What you will do: The position is responsible for constructing and optimizing our data lake and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Incumbent is expected to be experienced as a data pipeline builder and a data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data-analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Individual must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. The incumbent will stage and make the data ready for Data Science, Analytics, Artificial Intelligence and Machine Learning purposes.The position is responsible for data analysis, trending &amp; root-cause-identification of customer impacting issues pertaining to customer impacting systems across all the enterprise on all channels for all brands. Individual must be very strong in logical &amp; analytical thinking with experience in SQL, Oracle SQL (other SQLs desired), shell script with attention to detail. Ability to understand data patterns from large sets of structured and unstructured data. Monitor success &amp; failure rates on all channels for all key provisioning process such as Activation, Reactivation, Purchases, Redemptions, Ports and Upgrades.Individual must be able to work well with others in a team environment and across departments. The incumbent should mentor junior staff in business knowledge and technical skills.Responsibilities: · Build data streams to ingest, load, transform, group, logically join and assemble data ready for data analysis / analytics/ reporting / next best action / next-best-offer, build data streams to ingest, load, transform, group, logically join and assemble data ready for data analysis / analytics/ reporting / next best action / next best offer.· Pipeline data using Cloud or on-premises technologies: AWS Big Data Services, Hadoop, HDFS, AI/Deep Learning API’s, SQL/NOSQL, Unstructured Databases, etc.· Responsible to write infra structure as code using Terraform· Design/Implement QA framework within the Data-Lake. Design and implement test strategies, write test cases, design/implement test automation· Responsible for maintaining integrity between multiple databases, including but not limiting to, CLARIFY, BI, BRM, OFS (Inbound/Outbound Process) and MAX.· Work on T-shaped skills across Data Lake/Data Pipeline to take data across from the source all the way to the consumption layer.· Maintain knowledge and proficiency of current and upcoming hardware/software technologies. Mentor junior staff in ramping up analytical and technical skills.Requirements: · A bachelor’s degree from an accredited college in Computer Science or equivalent.· Strong knowledge of AWS Cloud Systems.· Strong knowledge of AWS data related services (DMS, Glue, EMR, S3, Athena, Lambda, Redshift, DynamoDB, KMS).· Strong knowledge of Python and Pyspark (Hive).· Strong knowledge of Unix/AIX and Windows operating systems, standard concepts, practices, and procedures within the relational database field.· Strong database/relational/non-relational concepts required.· Strong analytical and problem solving skills.· Hadoop, Kafka, Spark, Scala (preferred).· Must have 5+ years of Data Sytems/Warehouse/Lakes/Equivalent sytem experience with multiple OS platforms· Must have 3 + years of Python and Pyspark.· Must have 3 + years of AWS or alternative cloud systems.· Must possess or develop business knowledge of how customer transactions reflect the business logic that drive the existing or future code.· Must possess or develop ability to converse with the business, development, operations, carriers, vendors, etc.· Shell scripting, SQL Stored Procedures IBM/other databases and JAVA skills are desired.· Strong experience of different architectural components comprising the middle-ware is requiredTracfone Wireless is an Equal Employment Opportunity employer. We embrace diversity and do not discriminate based on race, religion, color, national origin, sex, sexual orientation, age, veteran status, disability status, or any other applicable characteristics protected by law.Job Type: Full-timeBenefits:401(k) matchingDental insuranceDisability insuranceEmployee assistance programFlexible spending accountHealth insuranceLife insurancePaid time offProfessional development assistanceTuition reimbursementVision insuranceSchedule:Monday to FridayWork Location:Fully RemoteThis Job Is:A job for which people with disabilities are encouraged to applyCompany's website:https://jobs.silkroad.com/TracFone/ExternalCareerSite/jobs/2160Benefit Conditions:Waiting period may applyWork Remotely:Temporarily due to COVID-19COVID-19 Precaution(s):Remote interview processVirtual meetings",Remote,Data Engineer
Mount Airey Group,/rc/clk?jk=360c4bb50b400630&fccid=e83f337650f342f0&vjs=3,"Header: |  | Using their inquisitiveness, tenacity, and willingness to learn, the Data Engineer will help develop the GFS Data Platform consisting of the data lake, analytics, enterprise data warehouse, and visualization tools. This person will have the ability to follow the teams technical direction by writing high quality code and following modern design principles. | The Data Engineer builds data solutions in an agile and incremental manner and is an active participant in all team ceremonies. They have the ability to understand complex architecture and engineering designs and the skill to help implement them. The Data Engineer understands the business problem they are trying to solve and can appropriately apply technology to help solve it. | Responsibilities &amp; Qualifications: |  | Essential Functions | Follow the technical strategy and direction for the Data Platform environment at GFS, including tools, techniques, and processes. | Help design and build technology solutions across multiple initiatives at the same time. | Collaborate with others on the team to rapidly prototype and implement technical architectures to deliver insightful solutions. | Understand the business analytics problems the company faces and help build solutions that solve them. | Work on a high-performing team focused on results and relationships. | Knowledge / Skills / Abilities | Experience writing software at a large enterprise. | Experienced in modern data movement technologies (e.g. serverless, real-time, as needed computing, low latency data movement, high volume data management, etc...). | Experience working with devops processes to deliver code quickly with a high degree of confidence and quality. | Experience in understanding and developing complex data models | Strong personal skills to develop relationships with peers on their team. | Business Influence - Experience in empathizing with users and adjusting the technical solution to solve their problems. | Experience in working with data visualization tools (Microstrategy, PowerBI, Looker, Tableau, etc…). | Preferred languages include but not limited to Python, Terraform, and Java. | At least 3 years of experience in technical design, development, and delivery of advanced analytics technical environments. | Bachelor’s degree in Computer Science, Information Technology, Advanced Analytics, Statistics, or related field. | In lieu of the specified education and experience requirements, a proven capability to perform the essential functions of the job, as demonstrated by an equivalent combination of education, training and/or relevant work experience may be considered. | Nice to Haves | Experience with one or many cloud computing providers (e.g. GCP, AWS, Azure). | Preferred tool experience include but not limited to Apache Airflow, Kafka, Attunity, databases, queuing, Microstrategy. | Knowledge of data governance, expectations and leading practices. | Experience evaluating alternate solutions to a technical problem while considering the cost and benefit. | Our Commitment: | Find out more about our values, culture, and benefits at gfs.com/careers. Be part of it! | Gordon Food Service values our customers and understands that their success is largely dependent upon their workforce. To demonstrate our commitment to our partnership, we will require any candidate who works for a Gordon Food Service customer to provide a letter of support from their management if they are selected for the interview process. | Gordon Food Service is an equal opportunity employer. All qualified applicants and employees will receive consideration for employment, or in terms or conditions of employment, without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, status as a protected veteran, or status as a qualified individual with disability. The EEO is the Law poster is available here: http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf | If you require a reasonable accommodation for any part of the application or hiring process due to a disability, please submit your request to talent@gfs.com and use the words “Accommodation Request” in your subject line. Please keep in mind this method is reserved for individuals who require accommodation due to a disability. | All Gordon Food Service locations are tobacco-free. | Gordon Food Service is a drug-free workplace and drug tests all employees.",Wyoming MI 49509,Data Engineer
TracFone Wireless Inc.,/rc/clk?jk=2961838ee422b2e3&fccid=932bcab1f7ee18f8&vjs=3,"Company Description | Govini is a decision science firm dedicated to transforming the business of national security through data science and machine learning. Through its data and analytic platform, Govini delivers objective, decision-grade information at scale to the national security enterprise, so that leaders can better direct investments to innovation and modernization. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania. |  | Job Description | We are passionate about data, we are passionate about code, and we are passionate about the product we offer. We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships. | We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs. | As a member of the engineering team, you will be responsible for assessing and evaluating our current data capabilities, making recommendations to enhance processes and quantifying the benefit as it relates to Govini’s growth, strategy and mission. | This is a team member position, working onsite in our Pittsburgh, PA office. | In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and produce simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy. | Scope of Responsibilities | Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage | Identify data sources, assess their value and quality and estimate the level of effort required to integrate into existing data model, infrastructure and products. | Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository. | Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification. | Develop, refine and oversee master data management standards, including establishing and enforcing governance procedures and ensuring data integrity across multiple functions. Responsible for owning data quality metrics and meeting defined data accuracy goals according to industry best practices. | Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts. | Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency. | Work across functional teams to understand advanced statistical, machine learning, and text processing models. Incorporate them into Govini’s existing data engineering infrastructure. | Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns. | Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language. | Qualifications | US Citizenship is Required | Required Skills | Bachelor's degree in Computer Science, Mathematics or related technical field | Minimum of 3 years direct experience creating sustainable, automated processes for data discovery, curation and synthesis | 3-5 years experience with programmatically manipulating data Experience with PostgreSQL or similar RDBMS | Expert at advanced SQL programming | Experience utilizing open-source technologies such as Linux, PostgreSQL | Proficient usage of common data formats such as CSV, XML, and JSON | Requires strong analytical ability and attention to detailAbility to work independently with little supervision | A burning desire to tackle hard problems and create sustainable solutions | Desired Skills | Experience using Amazon Web Services | Experience in or exposure to the nuances of a startup or other entrepreneurial environment | Strong expertise with scripting languages such as Python, Ruby, Perl | 3-5 years Master Data Management experience including data consolidation, linkage, federation and dissemination | We firmly believe that past performance is the best indicator of future performance. If you believe you meet the qualifications above, and you’re prepared to perform complex, mentally rigorous work, we’re eager to hear from you. | Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.",Pittsburgh PA,Data Engineer
Indeed,/rc/clk?jk=56f7a11c0a3ba444&fccid=5f460c65e642e29d&vjs=3,"Data Analytics Engineer(Job Number: HAM0003OQ) |  | Honda of America Mfg., Inc. |  | Description | What Makes Honda The Best? | Are you an innovator? Honda’s core values are what make our company unique. |  | Dreams: The Power of Dreams mentality drives our team to create intelligent products that enhance mobility and increase the joy in people’s lives. | Joy and Passion: The joy we experience at Honda is fueled by our dreams and passion. | Challenging Spirit: Honda’s challenging spirit pushes us to set and reach ambitious goals. Honda is voluntarily reducing 50% of carbon dioxide emissions in our products by 2050 and developing technology to reduce the number of collisions to zero by 2040. | Respect for the individual: This core value pushes our associates to contribute at the highest level and work effectively on a team. | If your goals and values are at the same speed as Honda’s, we want you to join our team! |  | About this position: | North American Market Quality Department’s mission is to identify and resolve field quality problems quickly to ensure customer satisfaction and maintain quality competitiveness. Our Data Analytics Engineers review multiple data sources to evaluate, summarize and draw conclusions to identify trends that lead to countermeasure activity by our Analysis Engineers. In the Market Information Group we look for candidates who have a passion for our mission and can take responsibility to focus on Market Quality needs and strive to innovate and continuously learn. |  | Your responsibilities: | Capable of analyzing multiple data sources (warranty claims, dealer information, manufacturing data, geographical, environmental etc.) in detail to identify quality trends and problem indicators using engineering fundamentals. | Demonstrate the ability to look for data trends not previously identified through deep understanding of the data and the ability to holistically review each problem. | Transform data using SQL to identify trends that lead to actionable problems. | Effectively generate ideas and support building tools in order to automate analysis processes. | Summarize and evaluate data, make conclusions and provide direction on problems identified. | Using engineering analysis, develop a risk assessment, considering seriousness, urgency and growth of emerging market problems. | Utilize predictive analytics (Forecasting) to support future growth of problems identified to ensure action is taken at the appropriate time. | Ensure data integrity through testing of results. | Strong communication &amp; visualization of the data to articulate the findings. | Responsible to support and direct analysis engineers with developing data queries for a specifically described problem. | What you need to be successful in the role: | Strong problem solving skills to ensure data is correct and responsible areas accept the results. | Ability to combine engineering analysis with market data to identify warranty problems early. | Strong communication skills (written, verbal, and visual) – ability to communicate market data and engineering analysis regarding conclusions that were reached. Ability to create and present market data reports to support investigations. | Excellent attention to detail. | Decisive in determining the importance of a market problem and its impact to customer satisfaction and future models. | Development /programming of tools to improve department efficiency, eliminate repetitive tasks, and implement new warranty analysis tools/techniques. | Prioritization skills to manage work load and make sound decisions for assigned team's data analysis activities. | Proficient using query languages such as SQL. | Experienced in Microsoft Excel (Pivot tables, Vlookup, etc.) | Good applied statistics skills, such as distributions, statistical testing, regression, etc. (desired) | Knowledge of data gathering, cleaning and transforming techniques. (desired) | Experience with common data science toolkits such as R &amp; Python. (desired) | Excel VBA Programming (desired) | QlikView (Business Intelligence software) Programming (desired) | Basic statistical distribution knowledge (desired) |  | Required Work Experience: |  Minimum of 3 months co-op/internship or related experience required | Required Education: | BS in Computer Science, Data Analytics, Mathematics or Engineering with strong engineering aptitude or equivalent relevant experience. |  | A few other important details: | Responsible for team's engineering data analytics on automotive products built at every Honda and Acura factory in North America (Accord, CR-V, RDX, NSX, etc.) | Office environment and work from home opportunities available | Travel approximately 1 time per quarter 1-3 days |  | We care about our associates and their wellbeing. We offer a wide range of benefits including: |  | Competitive total compensation | 401k Plan with company contributions | Flexible and valuable insurance plans | Relocation assistance (if eligible) | Onsite automobile service center and wellness center | Development to further your career | Discount on Honda products | Paid time off and paid holidays",Raymond OH 43067,Data Analytics Engineer
Agama Solutions,/rc/clk?jk=ec36fe3cf4b2d558&fccid=db130263a2af3230&vjs=3,"Title: Data Engineer | Location: San Francisco, CA | Position Type: Long Term |  | Job Description: | Creating AWS infrastructure for Salesforce syncs to/from Redshift | Managing AWS infrastructure as code using Terraform | Understanding of AWS IAM roles and VPC connectivity are important. | Creating scripts to be run on said AWS infrastructure that performs the sync. | Uploading script deployment packages to AWS and Git | Managing code on a properly secured secret storage system so that passphrases/secrets are not committed directly in source code to AWS/Git | Good understanding of Salesforce APIs and platform behavior |  | Skill set: | Python (or Go Lang), AWS Lambda, AWS S3, AWS EC2, AWS Redshift, AWS Postgres, Informatica and SFDC config if not dev.",San Francisco CA,Data Engineer
mPulse Mobile,/rc/clk?jk=2d0214e50c5a8c3d&fccid=9bafb2a1aee32e86&vjs=3,"Auth0 is a unicorn that just closed a $120M Series F round of funding, with total capital raised to date of $330M and valuation of nearly $2B. We are growing rapidly and looking for exceptional new team members to add to our exceptional talent pool - and who will help take us to the next level of success. One team, one score. | Our vision is to provide people with secure access to any application in one click or less. And our promise is to make identity work for everyone—whether you’re a developer looking to innovate, or a security professional looking to mitigate. We are looking for curious, excited, boundary-pushing team members. So, if you’re a big thinker who is nimble and adaptable, Auth0 may be an ideal place for you to shine. | We are hiring interns to join our Product Delivery team for Summer 2021 (June-August). This is a paid, full-time, US-remote position. Learn more about our culture here. We are looking for interns who embody our core values, which will be our guiding principles throughout the program. | About the Product Delivery Organization: | Product Delivery’s mission is to provide people with secure access to any application in one-click or less. This entails everything from bringing ideas to life, scaling all aspects of our platform, team, and culture, and sustaining a defensible market position. Product Delivery comprises: Engineering, Data, Product Management, Security, Business Agility and Operations. |  | Internship Program Overview: | Mentorship: You will have ongoing support from leadership, a dedicated mentor, and buddy for the entire program. | Meaningful Projects: You will own meaningful projects and will contribute to our roadmap. | Continuous Learning: You will collaborate across teams to solve technical and non-technical challenges, get a well-rounded view of our business, diversify your skills, and stay ahead of the curve with a specialized curriculum to propel your career. | Development: We will challenge you to meet your personal and professional goals- and have fun along the way. In addition to your project work and mentorship, you will learn from key-members of the team through our Speaker Series, Lunch &amp; Learns, and Coffee Chats. | Remote Culture: Born as a globally-distributed organization out of necessity, Auth0 has tried and tested strategies for making remote work successful. With over half of our workforce working fully remotely before COVID-19, we’ve worked hard to establish a strong remote-friendly culture. Our unique culture will teach you to thrive as you sharpen your ability to excel in a remote-work environment. |  | About the Data Engineering Team: | The Auth0 Data engineering team is focused on providing a secure and reliable single source of truth for the company and our customers to make critical data-driven decisions. We are in the process of transforming the existing batch-based data systems into a modern real-time cloud-agnostic global data platform that can scale gracefully with the organization and its rapid growth. | Throughout this internship, you will learn in-depth the tools and techniques of processing high volume real-time data and will develop innovative ways to generate low-latency insights. | You will be a good fit if you: | Have the drive to improve on the status-quo, think outside the box and are not afraid to fail. | Have good communication and writing skills. | Enjoy problem solving. | Skilled in SQL and Python. | Are familiar with data engineering principles and related technologies. | Are passionate about working on systems that are highly-reliable, maintainable, and scalable. | You are able to communicate and collaborate effectively with a distributed team. | You excel when working autonomously, and know when to seek help from your team members. | You always strive to learn and are interested in improving your security and identity knowledge. | As part of our growth, we are committed to building an inclusive work environment where all Auziros feel welcomed as their authentic selves—inclusive of all genders, sexual orientations, ethnicities, races, education, ages, or other personal characteristics. | Requirements: | Are currently pursuing a full-time undergraduate degree program in the United States and will be returning to the program after the completion of the internship, graduating with your Bachelor's by June 2022. | Are currently pursuing a degree in Computer Science. | Able to commit to a full-time internship (40hrs / week), Summer 2021 (June - August). | Authorized to work in the United States with no restrictions. | Taken courses that highlight streaming data, analytics, distributed computing, processing large volumes of data, programming in Python, Spark, C++, Time-series databases, etc. | Have a strong WiFi connection. | This is a paid, full-time internship experience that offers exposure to a fast-paced startup. This role is categorized as a Remote position. “Remote” employees do not have a permanent corporate office workplace and, instead, work from a physical location of their choice, which must be identified to the Company. Employees may live in any of the 50 US States, with limited exceptions. | We are accepting applications until our vacancies are filled or until March 15, 2021. | Auth0 safeguards more than 4.5 billion login transactions each month and its top priorities are availability and security. | We like to think that we are helping make the internet safer. Our team is spread across more than 35 countries and we are proud to continually be recognized as a great place to work. Culture is critical to us, and we are transparent about our vision and principles. | Auth0 is an Equal Employment Opportunity employer. Auth0 conducts all employment-related activities without regard to race, religion, color, national origin, age, sex, marital status, sexual orientation, disability, citizenship status, genetics, or status as a Vietnam-era special disabled and other covered veteran status, or any other characteristic protected by law. Auth0 participates in E-Verify and will confirm work authorization for candidates residing in the United States.",Remote,Data Engineer Intern Data Engineering
Gordon Food Service,/rc/clk?jk=cf2972ef25ea1b15&fccid=35fa439a19059a40&vjs=3,"At Tyler Technologies, we exist to inspire and empower every public servant to address society's pressing issues. We do that by enabling data-driven leadership through connected data and shared insights. |  |  | Our data as a service platform and cloud-based solutions support the world’s most effective open and internal data sharing programs at every level of government. What we do matters. New York City, Virginia, the Department of Commerce, and 230+ other cities, states, counties and federal agencies use us to connect citizens and their internal teams with information that matters to their day to day life. The Data Engineer will be part of our cross-division Solution Development team, which develops data-driven analytical and transparency solutions. These solutions lower the barriers to insights and provide public servants unprecedented access to data in Tyler source systems that underpin many of the basic functions of governments including public safety, community development, and public administration. |  |  | We are looking for an experienced Data Engineer. This person will work with internal partners, clients and developers to brainstorm and evaluate technical solutions, product integration opportunities and demonstrations. This role requires creative thinking, a deep understanding of data/ the power of data, as well as empathy for partner/client challenges and pain points. This person needs to work hand-in-hand at a strategic level with their colleagues in Product Development, and other Tyler divisions as well as in other departments, around a specified set of targeted software partnerships, integrations, and solutions we are pursuing and incubating. |  |  | This is a technical role but also requires a diverse range of consultative and project management skills. The primary focus of this role is to collaborate with cross-divisional partners to design and implement data ingress strategies that support Socrata solutions. These solutions in turn enable public sector organizations to share data with their audiences, internal employees and other agencies. The work with partners includes full projects as well as proofs of concept. A proof of concept aligns Socrata solutions with a partner's vision, mission or program, which requires technical configuration, data transformation and solution creation. | Work with our transformative data solutions that help agencies address mission-critical outcomes. Our cloud-based data platform, open data solutions, and performance management solutions help agencies improve performance, transparency, and public engagement. | Location | Remote |  | Responsibilities | Develop scalable technical solutions that enable easy integrations between the Socrata platform and partner data systems. | Develop and manage quality documentation of new and existing integrations to facilitate the reuse, broad adoption and scaling of those solutions. | Create and conduct demonstrations of Socrata solutions and services around Socrata Connected Government Cloud. | Identify new opportunities and solutions that meet the needs of current and prospective partners and clients. | Collaborate with partners and clients to define the scope of work for successful proofs of concepts and pilots. Lead those projects, especially during the first month, in which the lion’s share of the technical configuration, data transformation and solution creation take place. | Help partners and their clients present their data in the most widely accessible and useful forms, including but not limited to: transforming it from its native relational architecture to a more consumption-optimized (“flatter”) architecture; decoding type codes; splitting, combining and/or concatenating data; cleaning up metadata; creating data visualizations including charts and maps; telling stories of outcome and impact with data and creating lightweight data exploration experiences. | Attend feasibility meetings with new partners and clients to assess the feasibility of using the Socrata solutions to meet their additional needs beyond their original purpose. | Qualifications | Undergraduate degree in a technical, relevant field of work or commensurate industry experience is required; Master’s degree or relevant industry experience is preferred. | Ability to perform data transformations using data extract, transform and load (ETL) tools in languages such as Scala, Java, or Python. | Familiarity and experience with cloud computing platforms, particularly Amazon Web Services. | Ability to create and optimize advanced SQL queries both for data extraction and as the basis for compelling customer-facing Socrata solutions. | Ability to communicate at both a technical and a business level, developing trust and confidence with a wide variety of individuals within a client organization. | Off the charts passion for delivering elegantly-designed and well-performing products and services that delight clients. | Ability to “think on your feet” and comfortable architecting creative solutions to data sharing problems by adapting and configuring Socrata solutions to meet unique client needs. Ability to articulately talk through and whiteboard these concepts and approaches with clients. | Familiarity working with public sector organizations is strongly desired. | Ability to create beautiful data visualizations including charts and maps using Socrata solutions. | Ability to focus, balance trade-offs, work within constraints and make good decisions, including managing multiple projects simultaneously, is required. | Comfortable conducting client trainings of data publishing workflows. | Self-directed, self-motivated and the ability to work independently. | About Us | Tyler Technologies (NYSE: TYL) provides integrated software and technology services to the public sector. Tyler’s end-to-end solutions empower local, state, and federal government entities to operate more efficiently and connect more transparently with their constituents and with each other. By connecting data and processes across disparate systems, Tyler’s solutions are transforming how clients gain actionable insights that solve problems in their communities. Tyler has more than 26,000 successful installations across more than 10,000 sites, with clients in all 50 states, Canada, the Caribbean, Australia, and other international locations. A financially strong company, Tyler has achieved double-digit revenue growth every quarter since 2012. It was also named to Forbes’ “Best Midsize Employers” list in 2019 and recognized twice on its “Most Innovative Growth Companies” list. More information about Tyler Technologies, headquartered in Plano, Texas, can be found at tylertech.com. To learn more about our Socrata solutions, visit tylertech.com/products/socrata. |  | Additionally, we aspire to be remarkable: in the culture we create, the products we build, and the services we deliver. We believe a diverse team that embodies different backgrounds and experiences is necessary for us to be the best we can be. Within the company, we pursue a culture of inclusivity by identifying and removing aspects of our culture that stop people from being able to do the best work of their lives in physical and emotional safety, while being their authentic selves. We seek diversity, equity, and inclusion across our organization and in our daily work as individuals. |  | We are committed to making continual progress in everything that we do. | Colorado Code of Regulations 7 CCR 1103-13 Disclosure | Salary will generally fall between $82,000 - $115,000 before adjustment for geographic differences. Recruiter can confirm if position is incentive eligible. |  | Taking Care of You &amp; Your Family | Your health and well-being are important to us. That’s why we invest in our team members by offering competitive benefits to support their health and financial wellness. Learn more about how we care for our people » | Tyler is subject to regulations, guidelines, and/or client requirements relating to the qualifications of Tyler personnel performing certain client work. Because of the nature of this position, it is a requirement that the candidate can successfully pass a federal background check at the time an offer is extended and over the course of employment with Tyler.",Remote,Data Engineer
Kiewit Corporation,/company/iSeeCars.com/jobs/Data-Engineer-fe13f770596f7945?fccid=ed9916d99209f153&vjs=3,"Job detailsSalaryFrom $80,000 a yearJob TypeFull-timePart-timeNumber of hires for this role1QualificationsBachelor's (Required)Data Warehouse: 3 years (Required)SQL: 4 years (Required)US work authorization (Required)Python: 4 years (Preferred)Full Job DescriptionData EngineeriSeeCars.com, a Boston-based company, is an award-winning car search engine that helps consumers find great deals on cars. Founded in 2012 by TripAdvisor and SAP veterans, it is turning car shopping on its head by using big data analytics, AI, and proprietary algorithms to objectively analyze, score and rank millions of cars and formulate insights that give consumers the upper hand. iSeeCars has been featured extensively in the media, including The Wall Street Journal, New York Times, CNN, Consumer Reports, Forbes, Fortune, USA TODAY, and Bloomberg.*What you’ll doAs a Data Engineer, you’ll be responsible for acquiring data using API, feeds, scraping, scripting, automation and software development experience to design and build high-performance automated systems, create ETL data pipelines and enhance the data aggregation systems.Work with data feeds and data APIsDesign and develop web crawling and scraping solutions with a focus on performance and accuracyBuild scalable tools that automate web crawling, scraping, and data aggregation to populate databasesOwn the creation process of the crawling and scraping tools, services and workflows to improve crawl and scrape analysis, reports and data managementTest the acquired data to insure accuracy and quality and rectify any issues with breaks as well as performance as neededRequirementsExperience conducting large scale web crawling and scrapingExperience working with data APIs and data feedsStrong troubleshooting and debugging skillsSolid python experienceSolid SQL experienceExperience with shell scriptingExperience working with Data APIsFamiliarity with statistical conceptFamiliarity with Linux/UNIX, HTTP, HTML, Javascript and NetworkingFamiliarity with techniques and tools for crawling, extracting and processing data (e.g. Scrapy, pandas, mapreduce, SQL, Selenium, BeautifulSoup, etcExperience in building systematic data quality processes and checksExperience with using data profiling tools to query the data, identify anomalies, gaps and issueExperience with version control, open source practices, and code review is a plusExperience in working with data in various forms (data warehouses/SQL, unstructured data environments/PIG,HIVE, Impala) is a plusExperience working with ETL / Data warehousingExcellent communication skills (written and spoken English)Excellent analytical and reasoning skills; able to decompose complex problems and projects into manageable pieces; comfortable suggesting and presenting solutionWorking knowledge of AWS / S3 / RDS / Lambda*Job Types: Full-time, Part-timePay: From $80,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceFlexible scheduleHealth insurancePaid time offVision insuranceSchedule:8 hour shiftMonday to FridayCOVID-19 considerations:Working remotely for foreseeable futureEducation:Bachelor's (Required)Experience:Data Warehouse: 3 years (Required)SQL: 4 years (Required)Python: 4 years (Preferred)Web scraping: 4 years (Preferred)Work Location:Fully RemoteVisa Sponsorship Potentially Available:No: Not providing sponsorship for this jobThis Job Is:A good fit for applicants with gaps in their resume, or who have been out of the workforce for the past 6 months or moreA job for which all ages, including older job seekers, are encouraged to applyCompany's website:https://www.iseecars.comBenefit Conditions:Waiting period may applyOnly full-time employees eligibleWork Remotely:YesCOVID-19 Precaution(s):Remote interview processVirtual meetings",Remote,Data Engineer
Honda R&D Americas Inc.,/rc/clk?jk=1e6e01fe129516ca&fccid=915b1c0ee87e5e8a&vjs=3,"Description | The Data Engineer is part of a team responsible for supporting corporate and customer based reporting, as well as on-demand data needs for the organization. Under the direction of the Manager of Business Intelligence Systems, this individual will participate in the planning, design, development and implementation of solutions to address ongoing business reporting opportunities and needs. | Responsibilities | Develop impactful and accurate reports, dashboards and other data visualizations. |  | Communicate effectively across multiple departments and with stakeholders to review business requirements and propose solutions. |  | Support cross-team requirement and development efforts for multiple projects. |  | Maintain data integrity and ongoing quality of delivered reports. |  | Support ad hoc reporting as necessary for outside departments. |  | Identify data quality issues and work with appropriate teams to resolve them. |  | Document design, deployment, and operating procedures for each report. |  | Other duties as assigned. |  | Pearson is an Equal Opportunity and Affirmative Action Employer and a member of E-Verify. All qualified applicants, including minorities, women, protected veterans, and individuals with disabilities are encouraged to apply. | Qualifications | Due to the nature of this position, the applicant will need the ability to work from home or during off-hours as necessary. Candidates must have: | Proven experience with report design and ability to present data in a meaningful way using Power BI, Reporting Services and/or MS Excel |  | Excellent attention to detail. |  | Good understanding of database structures and query optimization. |  | Strong knowledge of SQL with experience querying large complex data models. |  | Experience with creation and maintenance of database objects. |  | Good communication skills with the ability to understand and translate user requests into technical requirements. |  | Strong customer service orientation. |  | Creativity to discover multiple solutions to business intelligence problems. |  | High-speed Internet access at home. |  | Bachelor’s degree or equivalent experience in Computer Science, Information Systems or related disciplines. |  | Nice to have, but not necessary: | Experience with other tools in the Microsoft Business Intelligence stack (Integration Services, Analysis Services). |  | Familiarity with Sharepoint. |  | Required for the interview: | The ability to demonstrate work experience by providing report examples and source code. |  | #LI-POST | Primary Location : US-NC-Durham | Other Locations : US-MD-Columbia | Work Locations : | US-MD-Columbia-Grantchester Merriweather | 10960 Grantchester WayTwo Merriweather | Columbia21044 | Job : Technology | Organization : North America | Employee Status : Regular Employee | Job Type : Standard | Job Level : Individual Contributor | Shift : Day Job | Travel : No | Job Posting : Feb 11, 2021 | Job Unposting : Ongoing | Schedule: : Full-time Regular | Req ID: 2102298",Durham NC,Senior Data Engineer
Intel,/company/Bardavon-Health-Innovations/jobs/Data-Engineer-d63770b3af6d8d52?fccid=db5496012d1e73a5&vjs=3,"Job detailsJob TypeFull-timeFull Job DescriptionWhy Bardavon?At Bardavon, we treat our people right.We hire exceptional people who are exceptional at their jobs. We are dreamers. We are driven, idealistic, committed, and optimistic. As a company, we are irreverent about the status quo and confident in our ability to change the Workers’ Compensation industry and healthcare in America.Our unique and challenging Tech Stack and Project portfolio motivate us to be the best. This includes projects that foster hope for the Workers Compensation industry, Analytics to reduce our client’s injuries and Platforms to ease Provider process. Our purpose-built technology solution bNOTES®, collects and connects clinical treatment data for Workers’ Compensation claims. We are focused on building a state-of-the-art technical team and are looking for motivated, highly skilled Data Engineers to join our team.What You’ll Do: As a Data Engineer you are a software engineer who works with data. Your primary functions include the development, documentation testing and debugging data applications. In addition, the data engineer will participate in design meetings and interact with business stakeholders to refine, test and debug programs to meet business needs.Implement solutions as designed by Bardavon Software Architects.Utilizing technical expertise; review system requirements, performs design and analysis, and coding and unit testing of complex to highly complex system functionality and/or defect correction across multiple platforms.Partner with architects, business system analysts and project managers to create an enterprise data ecosystem for healthcare and related data.Participate in the design, development and maintenance of enterprise and departmental data models for data marts, data lakes, data warehouses, databases, and reporting.Participate in the evaluation of proposed data related technology acquisitions.Serve as subject matter expert in multiple analytical disciplines and data domains.Establish ETL and data cleansing processes.Provide technical, analytical, and business knowledge support to engineering.Ensure effective communication between internal team members and between teams on data process flow.Define and enforce data governance standards, policies, and procedures for the design and proper use and care of data within the enterprise as well as by third party business associates.Support the metadata strategy as it pertains to data governance and model metadata capture.Provides information and assistance relative to the use and understanding of data models and data stores.Engage in hands on development.Perform other duties as assigned.Must Have’s: Bachelor's Degree in Computer Science, Management Information Systems, related discipline, or a combination of education and equivalent experience.At least 3 years of experience in a data analysis or software engineering in a multi-faceted, fast paced work environment.Knowledge of a data lake ecosystem, such Hadoop, Spark, MPP databases.Hands on experience with AWS or other cloud services.Background or familiarity with Data Governance, Metadata Management, Taxonomy. Management, Master Data Management and Data Security.Expertise in Software and Data Warehouse development and best practices.Healthcare data experience is highly preferred.Nice To Have’s: Experience in one or more Data Modeling and Business Intelligence tools.Experience with Data Visualization tools such as Tableau or Power BI.Experience in Data Mining using R, Python, or other languages.Advanced SQL skills.Timeline: Our Human Resources team will review your resume and respond.If there is a match, we will give you a call.If there is not a match, we promise to let you know and will stay in touch for future roles.We do Workers’ Comp differently. Bardavon Health Innovations is a proactive Workers’ Compensation partner that connects all stakeholders to better manage claims and offer work readiness solutions through rehabilitation therapy. We share a holistic analysis of the claim so America’s injured workers can achieve optimized outcomes and return to full-duty employment.Bardavon offers a complete benefits package, including medical, dental, and vision insurance; 401(k) with company match; and generous paid time off.EOE M/F/D/VJob Type: Full-timeBenefits:401(k)401(k) matchingDental insuranceDisability insuranceEmployee assistance programFlexible scheduleFlexible spending accountHealth insuranceHealth savings accountLife insurancePaid time offProfessional development assistanceReferral programVision insuranceSchedule:8 hour shiftMonday to FridayWork Location:Fully RemoteCompany's website:www.bardavon.comBenefit Conditions:Waiting period may applyCOVID-19 Precaution(s):Remote interview processVirtual meetings",Remote,Data Engineer
Bardavon Health Innovations,/company/Unify-Consulting/jobs/Data-Engineer-e4460a1f3e422ee3?fccid=e1e5ee5e82327809&vjs=3,"Job detailsSalary$140,000 - $180,000 a yearJob TypeFull-timeNumber of hires for this role5 to 10QualificationsUS work authorization (Required)Bachelor's (Preferred)SQL: 4 years (Preferred)Data Warehouse: 4 years (Preferred)Full Job DescriptionAbout the jobWe are seeking experienced mid to senior-level data engineering consultants (5+ years) with management consulting experience, who are versatile in skillset and are passionate about bringing value to our clients. We look for technology and data analytic leaders who are experts in their fields who also want to make a meaningful impact for local clients in the Seattle/Kirkland/Bellevue area.The successful candidate will be a strong individual contributor with the ability to execute specialized data solutions and make departures from traditional approaches if needed. They will possess skills in design, development, implementation, documentation, and management of data solutions, and be expected to clearly articulate and present technical information to clients and stakeholders.*Only accepting local candidates at this time.Responsibilities: · Build complex and scalable production level data pipelines; from source to data products ready for end-user consumption (e.g. multi-dimensional model)· Understanding of how to model data for scalability and optimized performance· Drive the design, building, and launching of new data models and data pipelines in production.· Collaborate with business users to understand the need / problem, break down requirements, and maintain communication through delivery· Collaborate with BI developers, Data Scientists, Product Managers, Software Engineers, Data Modelers and/or Platform owners to define the scope, design and implement the correct solution· Produces detailed and high-quality documentation detailing design and behaviors of data pipelines· Instill comprehension of technical concepts for non-technical audiences through presentations (executive leadership and large groups), email, and meetings· Participate in architectural evolution of data engineering patterns, frameworks, systems, and platforms including defining best practices, standards, principles, and policies· Working knowledge of data quality approaches and techniques· Familiarity implementing good security practices for sensitive data· Bring a positive energy every day and work within a team to deliver the best possible solutions· Learn the business and the data that supports the business; doesn’t just implement technologyRequired Qualifications: · 5+ years experience in data integration and data warehousing (preferably cloud datawarehouses)· 5+ years experience with relational database technologies (SQL Server, MySQL, Oracle, Teradata, or similar)· 5+ years experience with ETL/ELT tools such as Apache Spark, Smartstream, Fivetran, Talend, Matillion, AWS Glue, HVR or similar· 3+ years experience Experience with custom ETL/ELT and programming/scripting language experience: Pyspark, Python, Scala (Python required)· Experience with developing dynamic data pipeline frameworks (with Apache Airflow, Control M for instance)· Current experience with developing on a Public Cloud Environment (AWS, Azure, GCP for instance)· Experience working in an Agile team and delivery capability· Familiarity with common security measures for analytical systems· Familiarity with performance tuning and optimization· BS Degree or equivalent required in Computer Science or similar area of studyPreferred Qualifications: · Experience in development, design, and application of data modeling (on star-schema model), data mining and data architecture concepts and techniques· Excellent communication skills with the ability to relay information between technical and non-technical teams· Management consulting experience strongly preferred· Demonstrated presentation skills· Experience working with large datasets and big data technologies, preferably cloud-based, such as Redshift, Snowflake, Databricks, Azure SQL Data Warehouse, MongoDB, Cosmos, or similar· Proficient with NoSQL, Object-Oriented, Distributed databases· Proficient with data preparation tools (i.e. Alteryx)· Experience with source control tools· Proficient in project management and system reliabilityUnify Consulting provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.Job Type: Full-timePay: $140,000.00 - $180,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceDisability insuranceEmployee assistance programFlexible scheduleFlexible spending accountHealth insuranceHealth savings accountLife insurancePaid time offParental leaveReferral programRetirement planTuition reimbursementVision insuranceSchedule:8 hour shiftMonday to FridaySupplemental Pay:Bonus payCOVID-19 considerations:All of our consultants are currently working remote.Education:Bachelor's (Preferred)Experience:SQL: 4 years (Preferred)Data Warehouse: 4 years (Preferred)Work Location:Fully RemoteThis Company Describes Its Culture as:People-oriented -- supportive and fairness-focusedTeam-oriented -- cooperative and collaborativeThis Job Is:A job for which military experienced candidates are encouraged to applyCompany's website:https://www.unifyconsulting.com/Benefit Conditions:Only full-time employees eligibleCOVID-19 Precaution(s):Remote interview processVirtual meetings",Kirkland WA 98033,Data Engineer
Unify Consulting,/rc/clk?jk=3d50713fce7d663f&fccid=79bd6aafbadfa288&vjs=3,"At Weber, grilling is a passion that’s reflected in everything we do. Our goal is to share this passion and spark inspiration with the people who matter most – our grilling community. Weber has been the world’s premiere manufacturer of charcoal and gas grills and accessories since 1952. If you have the desire to work for a company that is recognized for exceptional quality products and high customer satisfaction, employment with Weber may be right for you. We provide a friendly working atmosphere with an environment of growth and opportunity through innovation, pride, and excellence. | Weber is committed to inclusive, equitable and diverse Hiring practices. Our goal is to create a workforce which resembles the diverse rich communities we live, play, and support every day. | Discover What’s Possible with a career, at Weber. | Summary | Weber is looking for an Analyst, Data Science to join our Strategic Data Analytics team. Analysts work on our centralized enterprise analytics team to analyze consumer data, develop data visualizations, and perform various advanced analytics activities. The data analytics effort is growing at Weber, and we are looking for a new team member interested in learning and collaborating to help solve consumer marketing and other business problems with data-driven solutions. At this leading global grill manufacturing and marketing company you will have the opportunity to work with a wide variety of very large datasets to drive revenue growth, improve operational processes, and affect the shopping and grilling experience for our consumers worldwide. |  | An ideal candidate would be someone who has a track record of successfully delivering analytical projects, is passionate about working with data to find the story hidden within, demonstrates a continuous learning and growth mindset, has experience working at the intersection of consumer marketing and technology, and has an ability to communicate complex analytics concepts in non-technical terms to business leaders. The ideal candidate will also have demonstrable experience working with the variety of technologies contained within the Google Cloud Platform, as well as experience developing enterprise quality Python 3.7 code for analytics applications. | Primary Responsibilities: | This position is responsible for planning, managing and executing an array and analytics activities to provide insights and model results that benefit the consumer marketing and product development teams. This involves, in part, collecting data and constructing solutions to business problems. Candidates for this position must be able to write Python 3.7 and Google BigQuery scripts to manipulate and analyze data. Candidates must also be able to perform analyses on sample and real-world business datasets to validate and quantify trends or patterns of interest to the business groups worldwide. Candidates must also be able to design and construct data visualizations, i.e., interactive dashboards, for business use in Tableau. | Requirements: | Bachelor's degree or advanced degree computer science, information science, marketing analytics or other quantitative field and 3 or more years of work experience in consumer analytics, with clear examples of work performed. | Demonstrable experience with SQL, Google Cloud Platform BigQuery, Python 3.7 (particularly emphasizing Pandas, Matplotlib, Numpy, and Scikit Learn), Google Cloud APIs, Google Cloud AI Platform Notebooks, Tableau, Google Dataprep and Google Dataflow. It is a bonus to have experience with analytical modeling in SAS, SPSS and/or SAP and/or experience developing dashboards in other BI tools, e.g., Microsoft Power BI. | Must demonstrate proficiency in each of the following areas: 1) data analysis and relational-style query languages; 2) data visualization; 3) a high-level programming language; 4) understanding of consumer marketing. | Proficiency with Microsoft applications including Excel, Word and Power Point. | A track record of independently delivering or leading the delivery of multiple analytics or data science projects, including visualizations of analytical results. | Data science and/or analytics professional certifications are not required, but will be favorably considered. . Of particular interest is any certification relating to Tableau. Also of interest is any Google certification relating to data analytics, data science and/or data engineering. | Weber-Stephen Products LLC is an equal opportunity employer and considers qualified applicants for employment without regard to race, color, creed, religion, national origin, sex, sexual orientation, gender identity and expression, age, disability, or Vietnam era, or other eligible veteran status, or any other protected factor.",United States,Cloud Data Engineer
Allstate,/rc/clk?jk=28c035fd403f7670&fccid=fd3e5b9fc7b91d7c&vjs=3,"Requisition ID: 113781 | Job Level: Entry Level | Department: Information Technology | Market: Corporate Home Office | Employment Type: Full Time |  | Position Overview | Would you like to apply your skills to build one of the most advanced applications in the construction and engineering industry? Would you like to help propel our team to new heights? | Kiewit Data Services has an opening for a Content Engineer within our growing Transformation hub. This engineer will be responsible for back-end program and analytic development. | We’re looking for a hands-on programmer ready to leverage their understanding of data (i.e. analytics, data pipelines, relational databases etc.) to develop software solutions which transform business processes. They must be at ease working in an agile environment with little supervision. This person should embody a passion for continuous improvement and test-driven development. |  | District Overview | Kiewit Data Services’ (KDS) mission is to make Kiewit the premier data-driven organization in our industry. To accomplish this, our projects and districts need to have the right data, of the right quality, with the right level of analysis, available to them at the right time. KDS is a cross functional organization with employees that have expertise in data, technology, support and operations backgrounds. Our core functions are data Quality, Governance, Enablement, Analytics and Data Science. |  | Location | This position is remote within the US. | Responsibilities | Analyze business processes to develop transformational programs and analytics | Collaborate cross-functionally with front-end software engineers, data scientists, project managers, and business users to achieve business transformation | Work with various technologies including: Python, SQL, AzureDB/SQL Server | Provide ongoing enhancements, support, and maintenance to pilot content modules | Work alongside other back-end engineers to elevate technology and consistently apply best practices | Qualifications | Bachelor’s degree in Computer Science, Information Technology or similar | Proficiency in at least one modern object-oriented programming language such as Python or JavaScript | Experience with SQL and working with relational databases | Experience developing and working with data ETL processes | Experience developing software solutions to solve problems | Highly motivated and self-directed | Strong interpersonal skills to resolve problems in a professional manner | Conceptual understanding of software development process |  | We believe in equal opportunity in employment practices without discrimination, and comply with all laws regarding human rights in the provinces where we operate.",Remote,Content Engineer - Kiewit Data Services (2021)
Twitter,/rc/clk?jk=7c109b0872474e5b&fccid=f1374be6a45f4b8a&vjs=3,"Job Description | This position is associated with the sale of Intel’s NAND memory and storage business to SK hynix (You can read more about the transaction in the press release - https://newsroom.intel.com/wp-content/uploads/sites/11/2020/10/nand-memory-news-q-a.pdf.) The transaction will enhance the resources and potential of the business’ storage solutions, including client and enterprise SSDs, in the rapidly growing NAND Flash space amid the era of big data. | This is an exciting time to be at Intel – come join our team as a Data Engineer and work on one of the most advanced 3DNAND and SSD technology portfolios in the world. As the global leader in the semiconductor industry, Intel possesses many industry-leading SSD technologies including the most capable Quadruple Level Cell (QLC) NAND Flash products. As a Data Engineer, you will be part of a world-class team that will transition to lead the SSD business at SK hynix. | This position aligns to Phase 1 of the transaction, which includes the 3DNAND enterprise and client SSD business. Phase 1 is expected to close in late 2021, subject to regulatory approvals and other customary conditions. At deal close, all employees aligned to this phase of the transaction will transition to SK hynix. | We're looking for a Data Engineer to join our Data and Analytics Business Solutions team who can find creative answers to tough problems. As a Data Engineer, you'll create and manage our data infrastructure and tools, including collecting, storing, processing and analyzing our data and data systems. Your primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also know how to work quickly and accurately, using the best solutions to analyze mass data sets, and you know how to get results. Lastly, you'll be expected to help design solutions that aid in our goal of data democratization. | Big Data Engineer Roles and Responsibilities: | Help us build out an optimal data infrastructure from the ground up and assist in setting technical direction for the team. Currently, the team's primary data infrastructure is comprised of SQL servers. | Work closely with BI Experts, Data Scientists, and Data Governance Analysts to ensure infrastructure meets all needs. | Proficient understanding of distributed computing principles | Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities. Experience with NoSQL and SQL databases, such as HBase, Cassandra, MongoDB, MySQL, Oracle, PostgreSQL, etc. | Design, build, manage and optimize data pipelines for data structures encompassing data transformation, data models, schemas, metadata, data quality, and workload management. | Read, extract, transform, stage and load data to selected tools and frameworks as required and requested. | Perform tasks such as writing scripts, web scraping, calling APIs, write SQL queries, etc. | Process unstructured data into a form suitable for analysis. | Support business decisions with ad hoc analysis as needed. | Monitoring data performance and modifying infrastructure as needed. | Define data retention policies. | Participate in daily standups and lead design reviews. | Document and share knowledge through the creation of comprehensive standards. | Breakdown business features into technical stories and approaches. | Create proof of concepts and prototypes. | Mentor and coach junior engineers. | Qualifications | You must possess the below minimum qualifications to be initially considered for this position. Preferred qualifications are in addition to the minimum requirements and are considered a plus factor in identifying top candidates. | Minimum Requirements: | Minimum 5-7 years of experience in data engineering. | Bachelor's Degree or more in Computer Science or a related field. | A solid track record of data management showing your flawless execution and attention to detail. | Programming experience, ideally in SQL, Python, Spark, Kafka or Java, and a willingness to learn new programming languages to meet goals and objectives. | Knowledge of data cleaning, wrangling, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. | Deep knowledge of data mining, machine learning, natural language processing, or information retrieval. | Experience processing large amounts of structured and unstructured data, including integrating data from multiple sources. | A willingness to explore new alternatives or options to solve data mining issues, and utilize a combination of industry best practices, data innovations and your experience to get the job done. | Experience in production support and troubleshooting. | A passion for documenting work and sharing knowledge systematically is a must. | Self-driven and ability to multitask effectively. | Inside this Business Group | Non-Volatile Solutions Memory Group: The Non-Volatile Memory Solutions Group is a worldwide organization that delivers NAND flash memory products for use in Solid State Drives (SSDs), portable memory storage devices, digital camera memory cards, and other devices. The group is responsible for NVM technology design and development, complete Solid State Drive (SSD) system hardware and firmware development, as well as wafer and SSD manufacturing. | Other Locations |  | US, Arizona, Phoenix | Posting Statement |  | All qualified applicants will receive consideration for employment without regard to race, color, religion, religious creed, sex, national origin, ancestry, age, physical or mental disability, medical condition, genetic information, military and veteran status, marital status, pregnancy, gender, gender expression, gender identity, sexual orientation, or any other characteristic protected by local law, regulation, or ordinance.",Folsom CA 95630,Data Engineer
Sherwin-Williams,/rc/clk?jk=1b6b767ddb8ac842&fccid=7a3824693ee1074b&vjs=3,"Job Description | Who We Are: | As data product engineers in Revenue function, our mission is to build real-time and offline data products to make data better accessible and reliable while leveraging the largest-scale data processing technologies in the world - and then apply them to the Revenue’s most critical and fundamental data problems. | Learn more about some of the challenges we tackle on this team: | Building a Petabyte-scale Data Warehouse (Google Cloud Next '18) | How Twitter Migrated its On-Prem Analytics to Google Cloud (Google Cloud Next '18) | What You’ll Do: | As a member of the Data Product Engineering team, you will work closely with product teams to build mission-critical data pipelines and products that are ‘source of truth’ for Twitter’s fundamental revenue data, as well as modern data warehouse solutions. You will collaborate closely with Ads Data Science, Machine Learning engineers, product managers, and many other data consumers across the company. | You will be a part of an early-stage team and have a significant stake in defining its future with a considerable potential to impact all of Twitter’s revenue and hundreds of millions of users. | You will be among the earliest adopters of bleeding-edge data technologies, working directly with Revenue Science and Revenue Platforms teams to integrate your services at scale. | Your efforts will reveal invaluable business and user insights, leveraging vast amounts of Twitter revenue data to fuel numerous teams including Ads Analytics, Ads Experience, Ads Data Science, Marketplace, Targeting, Prediction, and many others. |  | Qualifications | Who You Are: | You are passionate about data and driven to take the data organization challenges at the Twitter’s data scope. | Qualification: | Strong programming and algorithmic skills | Experience with data processing (such as Hadoop, Spark, Pig, Hive, MapReduce etc). | Proficiency with SQL (Relational, Redshift, Hive, Presto, Vertica) | Data Modeling and ER models | Ability in managing and communicating data project plans and requirements to internal clients | Nice to have: | Experience writing Big Data pipelines, as well as custom or structured ETL, implementation and maintenance | Experience with large-scale data warehousing architecture and data modeling | Proficiency with Java, Scala, or Python | Experience with GCP (BigQuery, BigTable, DataFlow) | Experience with Druid or Apache Flink | Experience with real-time streaming (Apache Kafka, Apache Beam, Heron, Spark Streaming) | Additional Information | We are committed to an inclusive and diverse Twitter. Twitter is an equal opportunity employer. We do not discriminate based on race, ethnicity, color, ancestry, national origin, religion, sex, sexual orientation, gender identity, age, disability, veteran, genetic information, marital status or any other legally protected status. | San Francisco applicants: Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.",San Francisco CA 94103,Data Product Engineer Revenue Science
Weber-Stephen Products LLC,/company/Sagence-Inc./jobs/Data-Engineer-83bde43bf97ece17?fccid=51efacc1777ef239&vjs=3,"Job detailsJob TypeFull-timeQualificationsData analysis skills: 1 year (Required)US work authorization (Required)Bachelor's (Preferred)SQL: 1 year (Preferred)Python: 1 year (Preferred)Data management: 1 year (Preferred)Data modeling: 1 year (Preferred)Full Job DescriptionOur PeoplePassionate, diverse, creative, genuine, curious, hands-on…these are just a few of the words that describe who we are. We have a high-energy workplace with a focus on producing high-quality, impactful results for our clients. We build teamwork through small, dedicated teams who constantly communicate and continuously teach each other and learn from one another. We promote an entrepreneurial spirit by encouraging individual initiative and foster a collaborative culture and work environment which includes open communication and on-going learning. We are committed to equality of opportunity, fairness, work and lifestyle balance, and mutual respect. We strongly believe these characteristics enable our employees to develop to their fullest potential.Data Engineer Job OverviewSagence is actively seeking a client facing Data Engineer to help us build and sustain our client’s data capabilities and our competitive advantage. Our Data Engineers are responsible for implementing repositories and building and maintaining data pipeline systems that are used in Data Analytics. Along with the requirements below, candidates must possess deep, practical experience in one or more of the following Data Management competencies; data warehousing, data lakes, data architecture. Additionally, and equally as important, as a Sagence Data Consultant, you must have the skills and experience necessary to effectively communicate with our clients*Candidates must be currently authorized to work in the US on a full-time basis. Sagence does not provide sponsorship for work visas upon hire or once on board.*Skills &amp; Experience Required for a Sagence Data EngineerAlong with the requirements above, Data Engineer candidates must have the following experience:MUST BE HANDS ON. Candidates should have system development experience and proficiency with system development methodologies and possess the following skills and knowledge: Must have hands-on experience with various IT concepts of data management/engineering (ETL, data modeling, data warehousing, etc.)Must have hands-on experience with SQL, data profiling, and data discoveryMust have a strong understanding of data analytics and data wranglingHands on experience with cloud-based data solutions (AWS, Azure) and integrating with data sources is requiredExperience with at least one data manipulating programming language (e.g. Python)Familiar with building business intelligence, analytics, or reporting solutions – either front-end consumption tools (e.g., Microsoft Power BI, Tableau &amp; Qlik) or supply of data for these purposesKnowledge of data architecture principles/approaches, data environment infrastructure considerations. Additional knowledge in data modeling principles/approaches preferredAbility to drive out technical requirements with business and IT stakeholders for implementation of data solutionsHands-on experience with Agile delivery methodologyDemonstrated ability in effectively engaging, communicating, and presenting to stakeholders across both business and technology functionsConceptual and analytical thinker with the ability to define business requirements and extract, analyze, and synthesize complex business insightsAbility to perform knowledge transfer to support a teamPrior professional experience in a management consulting, an IT management, or client facing role strongly preferredKnowledge of the Financial Services, Insurance, Healthcare or Retail industries is preferredTools &amp; Technology Required for a Sagence Data EngineerSagence Data Engineers are proficient at leveraging tools and technology that drive value for our clients. Illustrative tools include: Database Management Tools:Relational – e.g. Oracle, MySQL, Microsoft SQL Server, PostgreSQL, or similarNoSQL – e.g. MongoDB, Apache Cassandra, Couchbase, or similarCloud-based – e.g. AWS (Redshift or DynamoDB, Azure (SQL Server or Cosmos DB), or similarETL Tools – e.g. Informatica, Talend, Microsoft SSIS, or similarPython Data Manipulation/Analysis – e.g., Pandas, NumPyFamiliar with at least one Industry Leading BI tools – e.g., Microsoft PowerBI, Tableau, Qlik, Looker, or similarGeneral Skills and Requirements Required for a Sagence Data Engineer*Candidates must be currently authorized to work in the US on a full-time basis. Sagence does not provide sponsorship for work visas upon hire or once on board.*2+ years of professional experience working in a related roleStrong strategic business acumen and a passion for solving problems with data-driven solutionsClient centric focus with strong relationship management skills and the ability to influence business decisionsIntellectually curious and independently resourceful with the ability to prioritize and execute in a fast-paced and dynamic environmentWillingness to travel* to client sites as needed (*all work is currently remote)Chicago or New York area candidates preferred, but will consider candidates in other parts of U.S.About SagenceSagence is a data focused consulting firm working with clients in information-intensive industries such as Healthcare, Financial Services, Insurance and Retail. We specialize in data management and analytics, assisting our clients with the acquisition, evaluation and development of data assets. Visit our Leadership page to get to know some of our people.Visit Glassdoor to learn more about what a career with Sagence could look like as a Data Consultant.Job Type: Full-timeEducation:Bachelor's (Preferred)Experience:SQL: 1 year (Preferred)Python: 1 year (Preferred)Data analysis skills: 1 year (Required)Data management: 1 year (Preferred)Data modeling: 1 year (Preferred)Willingness To Travel:50% (Preferred)",Remote,Data Engineer
RBC,/rc/clk?jk=3638e5774432996d&fccid=2e2f013ae900ea5a&vjs=3,"Job detailsSalary$135,000 - $145,000 a yearFull Job DescriptionFYI – For Your Information, Inc.is one of the fastest growing and most successful woman-owned Federal contractors in the country. We have won awards for being a Great Place to Work and continue to make ground-breaking advancements. We focus on career development and promotion – people are the core of everything we do. If you are looking for a career and not just a job, you're in the right place! |  | FYI – For Your Information, Inc., is a Woman-Owned Small Business GSA schedule vendor that is a premier provider of Human Capital, Training, and Information Technology services. |  | FYIattributes this capacity to deliver quality services to many factors, including FYI's in-house recruiters, senior HR professionals, corporate administrative staff, and agile, committed management resources. FYI'sfocus on the national, regional and local requirements of its customers, and a corporate organizational structure with sufficient depth of skills and authority to deploy and manage scalable teams, allows FYIto be responsive to the real-time needs of our Federal clients. |  | FYI'sBenefits/Incentives: What is in it for you? |  | Opportunity to work remotely (per contract requirements). | A knowledgeable, high-achieving, experienced, and fun team. | A diverse work atmosphere. | The chance to be part of a rapidly growing company and the next success story. | Team building and innovation. | A competitive base salary with a loaded benefits package plus 401K. | Personal computer device allowance. | Pet Insurance. |  |  | Summary of Position: |  | The Senior Data Engineer will support our software developers, database architects, data analysts and data scientists. They will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. |  | Responsibilities: |  | They will work with analytical teams to gather technical requirements for both capabilities and tools, as well as, accessing and prioritizing ingest of data assets. They will work with the infrastructure and enterprise architect teams to develop and build out the enterprise platform. The will advise the CDO on technical solutions and tradeoffs, liaise with the Enterprise Architecture to ensure standards are met, and work closely with the Security and Policy teams to ensure the organization is in compliance with their policies. In addition, they will work closely with the modernization product leads to ensure the data needs for the analytical teams is built into the modernized application. |  | In addition, they will help establish and implement metadata strategy aligned with EA strategy to ensure clear visibility into mapping of data assets to applications, and understanding of impact of changes to data models |  | Duties: |  | Oversee the buildout of the enterprise Data Warehouse/API strategy | Work with the analytics teams to prioritize data assets | Manage contractors and oversee solution delivery | Optimizing data ingest and ETL processes to ensure our products have maximum performance | Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. | Navigate conflicting priorities and provide recommendations to the CDO when necessary | Experience building and optimizing 'big data' data pipelines, architectures and data sets. | Build processes supporting data transformation, data structures, metadata, dependency and workload management. | Work with the Enterprise Architect to establish modeling standards, data quality standards, data integration patterns or transactional and analytical systems. | Prepare polished technical and non-technical status reports and presentations for senior management. |  | Minimum Qualifications: |  | 12+ years of experience in data management or data architecture | Experience managing vendors/contractors | Experience with feature/product ownership | Experience with Agile Scrum methodology | Experience writing clear and effective proposals, policies, research summaries, etc. | Development experience with MuleSoft and Informatica preferred; MicroStrategy a plus | Bachelor's degree or equivalent coursework in data management/computer science |  |  | We are an equal opportunity employer and value diversity. All employment is decided on the basis of qualifications, merit and business need.",Washington DC 20036,Data Engineer
Sherwin-Williams,/rc/clk?jk=9d7659568e95a537&fccid=94f8a3a6ac5079e2&vjs=3,"Here, we believe there’s not one path to success, we believe in careers that grow with you. Whoever you are or wherever you come from in the world, there’s a place for you at Sherwin Williams. We provide you with the opportunity to explore your curiosity and drive us forward. We’ll give you the space to share your strengths and we want you to show us what you can do. You can innovate, grow and discover in a place where you can thrive and Let Your Colors Show! Sherwin-Williams values the unique talents and abilities from all backgrounds and characteristics. All qualified individuals are encouraged to apply, including individuals with disabilities and Protected Veterans. | The Lead Data Engineer is responsible for leading a team of developers uncover enterprise insights and drive business results using smarter data analytics via the Cloud. This is a hands-on technical role and you will play an integral role in building knowledge within the team and be part of strategic cloud data initiatives for organizational and process improvements. Influence strategies and engineering capabilities for modern Data &amp; Analytics platform. |  | Essential Functions | Strategy &amp; Planning |  | Full life cycle experience (requirements gathering through production deployment) in all aspects of BI, modern data warehousing, ETL, data modeling and analytics | Ensures critical Data Warehouse and BI systems remain stable while continually evolving to meet ever-changing business needs. | Effectively identify, manage and mitigate project risks and issues | Work with the extended team to support recruitment of new developers/contractors into the team | Seek out and apply new technologies and skills for ETL improvement in daily work through conferences, online training, reading, participation in organizations and user groups, etc. | Translate business needs into technical and system design specifications | Build and promote a positive working environment. |  | Execution &amp; Deployment | Design and develop or modify ETL processes that load data for the data warehouse using ETL tools (Oracle ODI) and PL/SQL | Maintain existing ETL Oracle PL/SQL procedures, packages, and Unix scripts | Provide ETL performance tuning | Troubleshoot and resolve production issues under tight deadlines | Conduct ETL design reviews with the Data Architects and DBAs | Maintain clean and stable dev, qa, and test environments. | Further refine and ensure adherence to standards for development, code migration, version control, and all policies and procedures. | Drive change to implement efficient and effective development methodologies as advised by management. | Oversee the technologies, tools and techniques used within the team. | Actively contribute to the process of continual improvement for self, the team and the data warehouse environment. | Operational Management | Primary responsibility for the support of the Best of Breed data loads. | Execute project plans to successfully meet objectives. | Conduct staffing and resource management activities. | Train and integrate new team members and proactively support the continued development of existing team members. | Understand individual team member strengths and opportunities for improvement in order to provide clear tasks and objectives to improve performance. | Lead and work on installations, configurations, upgrades &amp; administration of the current and future technology stack. | Ensure 24/7 availability of existing data loads with the assistance of remote developers within the team. | Continue to improve team’s knowledge on new ETL tools. | Plan for maintenance support windows and assign validation steps to the team accordingly. | Identify and address performance, capacity, and data replication issues; bring recommendations to management. | Incidental Functions | Ability to manage time effectively and have awareness of various time zones. | Able to work well under pressure, flexible, positive &amp; focused during times of change. | Able to solve complex problems, participates in continuous improvement, adapts the ideas of others. | Demonstrates a proactive approach, getting things done, demonstrates accountability &amp; ownership, prioritizes own workload. | Demonstrates intellectual rigor, possesses relevant abilities and is able to pick up new skills quickly. | Attend meetings on behalf of the team. | Make presentations to management, internal and external clients, and peer groups as requested. | Train and share knowledge with the team. | Assist with Lean projects as may be required to contribute to the efficiency and effectiveness of the team. | Participate in hiring activities and fulfilling affirmative action obligations and ensuring compliance with the equal employee opportunity policy. | Minor travel may be required (domestic and international). | Work hours outside the standard office 7.5 hour work day may be required. | Position Requirements | Formal Education &amp; Certification | Bachelor degree or foreign equivalent in a related field or equivalent experience. | Knowledge &amp; Experience | 10+ years IT experience | 10+ years experience as a data engineer or related specialty (software engineer/developer, business intelligence developer, data science engineer, database administrator) with a track record of manipulating, processing, and extracting value from large datasets. | 5+ years of experience in scripting and optimizing advanced Oracle SQL &amp; PL/SQL. | 5+ years of experience with an ETL or ELT tool for data extraction, transformation and loading using one or more of the following: Informatica Power Center, Oracle Data Integrator (ODI), Azure Data Factory or a similar set of tools. | Experience with manipulating, processing, and extracting value from large datasets. | Well versed with dimensional data modeling and the construction of ETL routines in a data warehouse environment. | Strong experience with UNIX/Linux scripts. | Experience with one or more of the following BI tools: Business Objects, Tableau, Oracle BI. | Proficiency in at least one of the following languages: Python, R, Scala. | Familiarity with Cloud data solutions, architecture and their interdependencies for BI use. | Experience with Microsoft Azure &amp; Snowflake DW is a plus. | Experience with Oracle E-Business Suite is a plus. | Hands-on experience building data sources for Tableau is a plus. | Preferred Qualifications | Expertise in building, managing and migrating SaaS/PaaS enterprise applications on Cloud native applications(AWS/GCP/AZURE) at large scale; Azure is the preferred platform. | Understanding of Star schema data warehouse modeling using the Kimball Methodology. | Experience in statistical, predictive modelling and machine learning techniques | Experience developing Oracle Application Express (APEX) applications. | Hands-on experience building data sources for Tableau is a plus. | Personal Attributes | Strong written and oral communications skills. | Proven ability and initiative to learn and research new concepts, ideas, and technologies quickly. | Strong systems/process orientation with demonstrated analytical thinking, organization skills and problem solving skills. | Ability to work in a team-oriented, collaborative environment. | Ability to train and teach others. | Ability to facilitate meetings and follow up with resulting action items. | Ability to prioritize and execute tasks in a high-pressure environment. | Strong presentation and interpersonal skills. | Ability to work effectively in a multi-cultural environment, and to lead and influence cross-organizationally with and without direct authority. | Must be legally authorized to work in country of employment without sponsorship for employment visa status now or in the future. | Equal Opportunity Employer. All qualified candidates will receive consideration for employment and will not be discriminated against based on race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability, age, pregnancy, genetic information, creed, citizenship status, marital status, or any other consideration prohibited by law or contract. VEVRAA Federal Contractor requesting priority referral of protected veterans.",Cleveland OH,Lead Data Engineer II
Provectus,/rc/clk?jk=a8a753bcdf5d277f&fccid=920167529b802494&vjs=3,"BentoBox empowers restaurants to own their presence, profits and relationships. The hospitality platform disrupts third-party services that come between the restaurant and the guest. BentoBox puts the restaurant first and offers tools that drive high-margin revenue directly through the restaurant's website. BentoBox is trusted and loved by over 6,000 restaurants worldwide including The Meatball Shop, Joseph Leonard, Union Square Hospitality Group, Major Food Group, Rose's Luxury, Eleven Madison Park &amp; many more. Learn more at www.getbento.com |  | We're looking for a Senior Data Engineer to join our team. This is a unique opportunity to lead the establishment of a cross-functional data practice within a SaaS company that serves the hospitality industry. You'll play a crucial role in empowering our business to make data informed decisions by surfacing insightful data to internal stakeholders. We currently use Stitch, Snowflake, dbt, and Looker for our data stack. |  | Responsibilities |  | Lead and own the setup of cross-functional company reporting, dashboards, and analysis based on an in-depth understanding of business model, operations, product and company KPIs. | Write and maintain SQL code on an ongoing basis to provide insights and check data for accuracy and validity. | Support various team initiatives on an ad-hoc basis by fielding requests for reports and dashboards, provide advice and insight along with the data. | Analyze complex data sets, not only looking for trends, but also meaningful edge cases. Clearly communicate findings to leadership, board, and other key stakeholders as needed. | Establish a best-in-class data practice within the organization by vetting and recommending tools, workflows, and infrastructure. | Uphold BentoBox's core mission, vision, and values |  | Skills and Attributes |  | 5+ years of experience working with large data sets to surface actionable insights and inform decisions | Expert knowledge of SQL and modern data platforms. Experience building, configuring, and managing high-quality data pipelines using tools like Snowflake, dbt, and Looker to enable analysis and visualization | Confident with end-to-end data workflows including analysis, reporting, and presenting evidence-based conclusions in a compelling way | Proficient in identifying/interpreting trends and anomalies in data, and troubleshooting accordingly | Experience with at least one other tool or programming language (e.g., Python, R) and an interest in expanding your skillset | Exceptional verbal and written communication skills, with a knack for communicating your findings to both technical and non-technical stakeholders | Highly organized and able to manage multiple projects and prioritize competing initiatives |  | What We Offer |  | Competitive salary + equity | Health insurance (medical, dental, and vision) - BentoBox covers 100% of healthcare premium costs | 401k plan with company match | Life insurance and disability plans | Commuter benefits | Flexible vacation plan | Paid parental leave | Professional development opportunities, including a yearly learning stipend | Perks when dining with BentoBox customers such as tip reimbursement | Making a positive impact on the hospitality community |  | BentoBox provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",New York State,Senior Data Engineer
Sagence Consulting,/rc/clk?jk=bd3ee20afb378873&fccid=537b899e30af3338&vjs=3,"What is the opportunity? | The Trade Floor Support Analyst provides immediate Tier One, Two and Three support to US RBC Capital Markets traders. Trade Floor Support works in conjunction with Market Data Engineering, PC/LAN, Network Operations, Telecom and the entirety of CM Infrastructure to resolve hardware, application and market data issues across all US RBC Capital Market Trading Floors. | What will you do? | Field high volume of calls, email and walk-up queries to provide support for time-sensitive hardware, application and market data support for US CM traders. | Provide early on-site morning support and on-call weekend support to Global CM Traders. | Installation, configuration and troubleshooting of all Market Data applications. | Identifying and resolving vendor connectivity issues across all Market Data applications handling data from global locations. | Provide troubleshooting expertise in PC’s, laptops, Mobile devices, and their associated hardware and software. | Coordinating and implementing upgrades and changes to applications, workstation hardware and workstation operating systems in a production environment. | Coordinate and implementing desk and PC moves across all US CM Trading offices. | What do you need to succeed? | Must have | In-depth knowledge of Market Data and trading applications such as Bloomberg, Tradeweb, CME and Reuter’s applications, | Knowledge of Win 10, MS Office Suite | Possess troubleshooting knowledge of network, hardware, application and OS layers. | Possess time management skills | Good verbal and written communication skills. | Possess good interpersonal skills. | IT Troubleshooting experience | Ability to work as a team | Ability to work on multiple concurrent tasks | Ability to work under High Pressure Situations | Ability to resolve time sensitive issues quickly | Availability to work after hours and weekends | Nice to Have | Previous Customer Service experience | What’s in it for you? | We thrive on the challenge to be our best, progressive thinking to keep growing, and working together to deliver trusted advice to help our clients thrive and communities prosper. We care about each other, reaching our potential, making a difference to our communities, and achieving success that is mutual. | A comprehensive Total Rewards Program including bonuses and flexible benefits, competitive compensation | Leaders who support your development through coaching and managing opportunities | Ability to make a difference and lasting impact | Work in a dynamic, collaborative, progressive, and high-performing team | Learn more about RBC Tech Jobs |  | Join our Talent Community | Stay in-the-know about great career opportunities at RBC. Sign up and get customized info on our latest jobs, career tips and Recruitment events that matter to you. |  | Expand your limits and create a new future together at RBC. Find out how we use our passion and drive to enhance the well-being of our clients and communities at rbc.com/careers. |  | JOB SUMMARY | City: New York | Address: 200 Vesey Street | Work Hours/Week: 40 | Work Environment: Office | Employment Type: Permanent | Career Level: Experienced Hire/Professional | Pay Type: Salaried | Required Travel(%): 0-25 | Exempt/Non-Exempt: Exempt | People Manager: No | Application Deadline: 03/05/2021 | Platform: Capital Markets | Req ID: 310121 | Ad Code(s):",New York NY 10281,Trade Floor Technical Support Analyst - Desktop Engineer (Market Data Experience)
BentoBox,/rc/clk?jk=634661e5bade7c9e&fccid=9d24d9fb191f8e45&vjs=3,"Job Description: |  | Develop mathematical analysis models to use in development of Beshton’s website applications for identifying product marketing influence and other software products to assist clients in improving their business operations; | Apply knowledge of statistics and mathematics to software data analysis and provide analytical support on big data projects; | Assemble data such as product news and process the data to use in analysis, utilize analytical methods including regression analysis and hypothesis testing; | Design and analyze statistical algorithms for use in software, collaborate with application developers to ensure the mathematical accuracy of the platform; | Perform complex analysis computations to validate software models and algorithms, identify discrepancies, and develop solutions; | Disseminate analysis results through presenting reports with data visualizations to explain analysis process and results to peers and management. |  | Requirement: |  | Minimum a Master’s Degree in Applied Mathematics, or a closely related field. |  | Apply send email to career@beshton.com",San Francisco Bay Area CA,Data Engineer
Turner,/rc/clk?jk=1b66f3152e0a47df&fccid=734cb5a01ee60f80&vjs=3,"Minecraft is one of the most popular video games of all time, with more than 200 million copies sold worldwide on PC, Console and Mobile. Minecraft inspires people around the world to create together and has resulted in one of the most active and passionate player communities in history. We’re looking for an ETL data engineer to join the Minecraft Data and Analytics team and help us shape the future of the Minecraft franchise. |  | As a BI data engineer specializing in scalable ETL, you are an expert in designing, developing, and maintaining secure, scalable data warehousing, data pipelines, and back-end services for driving impactful decisions, unlocking data insights and providing timely data solutions for the studio. This person will be responsible for ensuring the timely orchestration of hundreds of data pipelines that form the basis of the Minecraft Data and Analytics environment. | Responsibilities | Work with Project Management, data scientists and business stakeholders to understand requirements and translate to technical requirements. | Design, architect and support high quality data frameworks that will provide actionable information to various teams across the studio. | Experiment with and recommend new technologies that improve the team’s ability to innovate. | Collaborate and communicate with engineers, data scientists and analysts to optimize data flows, tools, operational costs and reporting infrastructures. | Use best engineering practices to ensure security and privacy compliance across all development projects. | Build data marts for different use cases across the business, using Microsoft and other big data technologies including Azure Synapse. | Facilitate ingestion of raw telemetry, and perform cooking, joining, and aggregation to facilitate consumption downstream. | Qualifications | Bachelor’s degree in computer science or engineering, database systems, mathematics, or 5+ years of industry experience in a data engineering role. | Advanced hands on experience with Azure Cloud Services (Data Factory, Data Explorer, HDInsight. Cosmos DB, SQL) or equivalent | Experience with Data Lake infrastructures (Cosmos, Hadoop) | Experience with data warehouse technical architectures, ETL/ELT, and reporting/analytic tools | Experience optimizing code for hardened, efficient deployments | Experience designing and building data warehouse solutions | Experience building and maintaining data pipelines | Experience with production BI implementations in the Cloud | Experience with Machine Learning Model deployment | Experience building Power BI, Excel, and Reporting Services dashboards and reports | Exceptional problem solving, technical and data analysis skills | Great written and verbal communication and presentation skills | Be self-driven, and show ability to deliver on ambiguous projects with incomplete or dirty data | Ability to work in a team environment that promotes collaboration | Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form. |  | Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.",Redmond WA 98052,Data Engineer - Minecraft
Microsoft,/rc/clk?jk=287566fdcd91ebec&fccid=45010906ed8b775d&vjs=3,"Pinecone is pioneering a vector database for AI/ML applications. We provide customers with search and recommendation capabilities that until now have been in the hands of a few large tech giants. Our team includes founders of multiple startups, CS professors, and world-class engineers formerly from Amazon, Google &amp; Facebook. |  | We're looking for a distributed systems engineer who is an expert on DB internals. |  | If you're excited about hard problems at the intersection of AI/ML, search engines, and databases, if you like moving fast, and feel restless when a day goes by without attaining something impactful, it's a opportunity to join an exciting startup as one of the first employees. |  | You should have: |  | At least 4 years of experience as a Software Engineer |  | Extensive experience in database internals |  | Experience in building web-scale services and distributed systems |  | Proficiency in at least one scripting language, such as Python |  | Proficiency in at least one compiled language, such as C++ |  | Some experience with Information Retrieval and Search |  |  | It's also necessary that you are a kind, thoughtful human being who people like to work alongside. While not necessary, it would also help to have experience with either embeddings, nearest-neighbor search, productionizing ML models, or DL frameworks (such as TensorFlow, PyTorch).",New York NY,Sr. Engineer: Distributed Data Systems
Pinecone,/rc/clk?jk=9608419ef96f5f6b&fccid=1639254ea84748b5&vjs=3,"Facebook is seeking an experienced Controls Engineer to join our Data Center Facility Operations team. Our data centers serve as the foundation upon which our software operates to meet the demands of our customers. Our controls systems, direct digital controller and programmable logic controller, provides the smarts to efficiently and reliably run our data centers. The candidate will provide site support, where a Facility Operations team operates multiple data centers, for facility retrofits, optimization efforts, escalated technical issues, and new construction ‘ready for operations’ activities. The ideal candidate shall be able to work both broadly leading critical projects cross-functionally, as well as work independently through deep dives and expert analyses. | Serve as the principal liaison for all controls systems related issues, coordinating efforts of external activities necessary to support assigned site operations. | Develop and implement engineering standards/best practices. | Develop and generate specifications, RFPs, ROMs and SOWs for assigned projects. | Serve as the point of escalation for complex control system malfunctions. | Coordinating the associated troubleshooting and response. | Facilitate change control reviews activities for assigned projects. | Perform readiness review of assigned new build projects to inform overall building acceptance. | Travel to data center sites in support of pre-engineering, implementation, startup testing, and commissioning of projects at the assigned site. | 30% domestic and international travel. | 5+ years controls experience in programming development, start-up, and commissioning of electrical monitoring systems, central plants, air handling units, and evaporative cooling/humidification systems. | Associates in Engineering/Technical studies, Military Technical School, or equivalent work experience. | Working knowledge of mechanical, electrical and life safety systems associated with critical environments. | Experience auditing blueprints/CAD drawings and controls diagrams. | Communication and organizational experience. | Bachelor’s degree in engineering discipline | Experience with software programming languages such as Python, PHP, SQL | Experience beyond field technician level such as project management, project engineering, and operations management | IT/Networking Certification | Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started. | Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com. | #LI-REMOTE",Remote,Data Center Facilities Engineering Controls Engineer
Facebook,/rc/clk?jk=f6700542c507fbcc&fccid=af97857db3a698b1&vjs=3,"Job Type: | Location: LOC_ROBERTS-Roberts Ctr Pediatric Research | Req ID: 83115 | Shift: Days | Employment Status: Regular - Full Time |  | Job Summary | We are looking for a Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data warehouse and building data integrations, developing data best practices and governance, performing clinical and administrative reporting and data visualization, as well as optimizing data flow and collection for cross functional teams. |  | The ideal candidate has been exposed to all aspects of data from multiple complex sources who enjoys optimizing data systems and building them from the ground up. The Data Engineer II will support our developers, database architects, data analysts and data scientists ensuring optimal data delivery architecture is consistent throughout ongoing projects. They will also support non-technical colleagues in the collection and appropriate use of clinical and non-clinical data. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. |  | Exhibits the ability to assist in client requirements assessment, solution set designs, coding, testing and implementation. | Demonstrates advanced SQL knowledge, relational and multidimensional models, and business intelligence delivery tools. | Demonstrates the ability to modify existing architecture to solve complex problems. | Recommends and establishes conventions and standards for all technical areas related to data storage, transformation and aggregations. | Participates in selection of new technologies, and consults on requirements. | Acts as a technical escalation point or SME for a particular aspect of the environment, assisting with complex problems and solutions. | Has an advanced understanding of technical environment and integration points, including all software, hardware, and supporting environments. | Demonstrates basic knowledge of project management concepts and may act as an implementation lead for department initiatives. | Demonstrates the ability in creating detailed documentation, including project plans, requirements status reports and operations documentation. | Exhibits ability to clearly articulate problems, issues, requirements and potential solutions to team members and clients. | Works with analysts to identify and understand source data systems | Demonstrates the ability to serve as a resource to cross- functional work teams | Exhibits the ability to guide associate Information Architects and provide teaching support to clients, Operations, and Help Desk as needed. | Knowledgeable in Research Administration (Finance, Pre/Post Award, Compliance) workflow including Project Accounting, Time &amp; Effort &amp; supported technologies | Job Responsibilities | A Data Engineer II is a mid-level individual contributor and may be involved in some leadership activities. A Data Engineer II also: | Meets with clients and IS personnel to determine and define specific analytical and technical systems information requirements, objectives and solution sets for maintenance or application enhancement requests of moderate complexity. | Develops data models that describe processes and data flows related to applications of simple to moderate complexity. | Identifies and resolves interface issues between applications from the same vendor or between two applications of different vendors. | Prepares supporting documentation and specifications for BI application changes or replacements with assistance from senior staff. | Assists with gathering and codifying appropriate user, system, or application requirements. | In a rapid development environment, manages scope, implementation teams, change management, and issues related to product Data Warehouse and BI products. | Acts as a product owner, interfacing with all levels of the organization to define product scope and requirements. | Monitors planned tasks; identifies and reports changes; reports status of project to Lead IS staff and/or IS Manager. | Acts as change agent to assist and support clients' transition from old to new solution set. | Assists with project evaluation and/or installation, testing and support. | Reports project issues and staff performance issues to Lead IS staff and/or IS Manager. | Performs technical development and may lead technical development teams. | Under limited supervision, modifies existing or creates new code to meet specific user, system, or application requirements. | Prepares conversion plans for moderate level applications implementations. | Provides guidance to associate Information Architects. | Provides teaching support to team members, clients, Operations, and Help Desk as needed. | Job Responsibilities (Continued) | Job Responsibilities (Continued) | Required Licenses, Certifications, Registrations | One (1) Certifications or proficiency in appropriate Data Science/Data Integration/Data Warehousing technology or subject domain. | Required Education and Experience | Required Education: | Bachelor’s Degree in Computer Science, Computer/Software Engineering, Information Technology, Data Science or related fields. |  | Required Experience: | Minimum of four (4) years of Data Engineering/Business Intelligence/Data Warehousing experience, preferably in a healthcare, research, and/or Research Administration environment. | Experience with data relating to compliance, project accounting, IRB, ServiceNow Ticketing or clinical trials desirable. | Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. | Experience supporting and working with cross-functional teams in a dynamic environment. | Working knowledge of message queuing, stream processing, and highly scalable data stores. | Previous experience manipulating, processing and extracting value from large disconnected datasets. | Preferred Education, Experience &amp; Cert/Lic | Preferred Education: | Advanced Degree in Computer Science, Informatics, Information Systems or another quantitative field. |  | Preferred Experience: | Minimum of six (6) years of experience in a Data Engineer role | Additional Technical Requirements | General | Experience in the development or implementation of structured operational processes, conformance with SLAs, and metrics based reporting. | Intermediate knowledge in Change Control Mgt. processes | Experience with process documentation and communication tools including MS Word, Project, Excel, Visio and PowerPoint. | Moderate to advanced experience and proven use of one or more of the subject areas listed below: | SQL and Database Knowledge – Understanding SQL, Relational and Multidemensional Databases and Designs | Knowledge of relational database structures (tables, data types, data model schemas), SQL Syntax &amp; SQL Functions, develop Views and SQL Optimization |  | Analytics | Cube dimensions (creating/maintaining translations, attribute relations, hierarchies) | Dimension usage: reference dimensions, many to many relationships, fact relationships, role-playing relationships granularity | Creating and maintaining data source views and reporting models | Hypothesis development, design test/experiments, &amp; developing actionable recommendations | Statistical modeling techniques (logistic regression, log linear regression, etc...) |  | Advanced experience and proven use of one or more of the subject areas listed below: | Tableau (Preferred), Qliksense, Power PI, or any other data visualization application. | Data Warehouse Support and Design | Creating/Maintaining Tables, views, &amp; indexes |  | All CHOP employees who work in a patient building or who provide patient care are required to receive an annual influenza vaccine unless they are granted a medical or religious exemption. | Children's Hospital of Philadelphia is committed to providing a safe and healthy environment for its patients, family members, visitors and employees. In an effort to achieve this goal, employment at Children's Hospital of Philadelphia, other than for positions with regularly scheduled hours in New Jersey, is contingent upon an attestation that the job applicant does not use tobacco products. | Children's Hospital of Philadelphia is an equal opportunity employer. We do not discriminate on the basis of race, color, gender, gender identity, sexual orientation, age, religion, national or ethnic origin, disability or protected veteran status. | VEVRAA Federal Contractor/Seeking priority referrals for protected veterans. Please contact our hiring official with any referrals or questions. | CHOP Careers Contact | Talent Acquisition | 2716 South Street, 6th Floor | Philadelphia, PA 19146",Philadelphia PA 19146,IS Data Engineer II - 21468
Children's Hospital of Philadelphia,/company/Northbound-LLC/jobs/Data-Engineer-fb2a1828a6d0f87f?fccid=f60f0f1edb9350c6&vjs=3,"Job detailsSalary$41 - $42 an hourJob TypeFull-timeNumber of hires for this role1QualificationsSQL: 5 years (Required)python: 3 years (Required)Spark: 1 year (Required)Bachelor's (Preferred)Full Job DescriptionTHIS IS A W2 POSITION. NO C2C PLEASE.looking for a Data engineer for one of our client in Cupertino, CA. Data Engineer who can do dashboarding, monitoring, updating tables and maintaining the pipeline. Require skills in python, Spark, SQL and if you have Airflow will be greatJob Type: Full-timePay: $41.00 - $42.00 per hourBenefits:401(k)Dental insuranceHealth insuranceVision insuranceSchedule:Monday to FridayEducation:Bachelor's (Preferred)Experience:SQL: 5 years (Required)python: 3 years (Required)Spark: 1 year (Required)airflow: 1 year (Required)Work Location:One locationWork Remotely:Temporarily due to COVID-19COVID-19 Precaution(s):Personal protective equipment provided or requiredPlastic shield at work stationsTemperature screeningsSocial distancing guidelines in placeSanitizing, disinfecting, or cleaning procedures in place",Cupertino CA 95014,Data Engineer
AmerisourceBergen,/rc/clk?jk=80ea17ec2e8f2fa8&fccid=1bc4c70bc4fea72f&vjs=3,"Company Description: | At Lunavi, we believe in illuminating the path forward and helping our customers navigate what’s next. We are innovators who are combining the power of human ingenuity and technology to deliver unrivaled customer experience. We’re a trusted partner for companies looking to digitally transform their business, modernize business applications, solve traditional IT challenges, and extract ROI from technology. Our high performing teams, deep expertise, and proven processes help to propel businesses forward. | Our core values: | Be Great. Aspire to know more, do more, and realize your fullest potential. Keep reaching above and beyond to excel and exceed every expectation. | Reimagine Everything. Continuously create value by pursuing what's next, what's possible. Deliver a new level of awesome through relentless curiosity. | Ignite Passion. Share the very best of who you are in everything you do. Create a positive and uplifting environment that inspires others. | Own It. Know that your contributions make a direct impact. Be the difference that leads to more successful experiences and outcomes. | Job Description: | Lunavi is looking for a Data Analytics Engineer with a background and experience designing and building business intelligence, modern data warehouse, or modern data platform solution development efforts. This role would also be responsible for working in Agile Delivery teams and working with all cross functional roles to help deliver ""smart applications"" leveraging data and analytics to our customers. | This role includes a mix of project delivery and pre-sales activities including discovery, scoping, and estimating; working with a team of Data Engineers to ensure consistent delivery capability and consulting growth as well as and supporting brand leadership efforts with webinars, blogging, and workshops. | Responsibilities | Take part in discussions with client leadership explaining architecture options and recommendations | Designing and delivering Data Analytics solutions | Work with developers to build ""Smart Applications"" that incorporate data analytics to drive decision making within the application? | Advise clients on database management, integration and BI tools | Work alongside team members in design/development of data analytics implementations | Perform architectural assessments of the client's Enterprise Data Warehouse (EDW) and data analytics systems | Work with the sales and delivery teams to create and deliver client proposals and demonstrations | Present to client, industry and internal peer groups | Work within agile delivery methodology in a leading role as part of a broader Delivery Team | Work with stakeholders/ end users in the software development lifecycle – PMs, BAs, testing etc. | Requirements | 2+ years full-time professional experience in a project-based role for data platform, data warehouse, or business intelligence solution development | Background building and delivering business intelligence, data warehouse, or data analytics solutions | Ability to take part in envisioning and discovery efforts, providing detailed estimates of deliverables, timeline, and hours | Passion for data and analytics, translating data-driven insights into decisions and actions | Self-starter, self-managed, quick learner, problem-solver with a positive, collaborative, and team-based attitude | Excellent communication skills, with interests in presenting and/or blogging; ability to engage with senior business leaders | Degree in information technology, computer science, business, engineering or equivalent work experience | Ability to work remotely, at a home-office, with limited travel to client locations - as travel restrictions ease and it's safe to travel again | Preferred | Knowledge of Microsoft technologies such as Azure Data Lake,Azure Data Factory, Databricks, SQL Data Warehouse (now Synapse Analytics), Azure SQL DB, Azure Data Lake and Power BI | Desire to learn and leverage Microsoft Azure platform data technologies | Development background using C#, Python, or PowerShell | Desire to move into a consulting role | Software development background | Knowledge of Agile/Scrum principles and method",Remote,Data Analytics Engineer
Harvard University,/rc/clk?jk=f51201ad681b4071&fccid=5507a64404691526&vjs=3,"Job detailsSalary$40 per classFull Job DescriptionJob Summary | Independently perform comprehensive applications/web development for project of medium to large size and complexity; work as part of a team to implement business solutions. Write basic code of moderate complexity and maintain related documentation. | Job-Specific Responsibilities | Our group in the Department of Neurobiology at Harvard Medical School is looking for an innovative and enthusiastic team member to join a multidisciplinary research laboratory. We are interested in understanding how computations arise from the organization and connectivity of neuronal networks. To analyze neural circuit structure and function at the highest resolution and completeness possible, we combine structural and functional imaging techniques including high-throughput electron microscopy (EM) and in vivo two-photon calcium imaging. We are looking for an engineer/data scientist capable of developing and working with custom hardware and software including instrumentation for high-throughput EM, and handling and analyzing large data. |  | The main projects include: | 1) development and support of automated platforms for serial section TEM; | 2) development and application of machine learning approaches at scale for image segmentation and analyses; | 3) development and support of large data handling infrastructure; | 4) development and validation of high-throughput multi-photon microscopes for volumetric in vivo imaging. |  | Success will require a close working collaboration with biologists and the ability to independently design solutions to address biological questions. Building this novel software and hardware will play a vital role in helping us achieve our biological aims. |  | Typical duties of the Functional Connectomics Engineer include: | Developing software to analyze large-scale image datasets; | Computational systems oversight and miscellaneous programming; | Developing and maintaining custom-designed electron-optical, optical, mechanical, and electronic instruments; | Novel collection substrate preparation; | Communicating with vendors and coordinating jobs with internal and external service providers and collaborators; | Documenting projects and communicating project progress. | Typical Core Duties | Participate fully in software development life cycle | Support technical solutions to deliver business requirements | Troubleshoot problems and suggest improvements to coding practices | Act as technical liaison to internal clients | Abide by and follow the Harvard University IT technical standards, policies and Code of Conduct | Basic Qualifications | Minimum of two years' post-secondary education and/or relevant work experience | Additional Qualifications and Skills | Bachelor's Degree in computer science, engineering, life or physical sciences or a related discipline. 2 years of related experience in research or related environment. Advanced degree may count toward experience. |  | We are especially interested in candidates with - an advanced degree in science or engineering - highly proficient in Python - programming experience in other languages (e.g., C, Java, MATLAB) - experience with machine learning, deep learning, and convolutional neural networks - experience with nano-positioning systems - experience with electron microscopy and electron-optics - experience with non-linear (multi-photon) optics and laser scanning microscopy - work experience in biomedical research and/or an academic science environment - demonstrated experience teaching or training in an informal setting. The successful candidate will be self-motivated, creative, and enjoy working in a dynamic and intellectually stimulating environment. S/he will be skilled at translating research problems into well-defined technical specifications and then designing, building, and help support user-friendly software solutions and instrumentation; s/he will have excellent interpersonal skills; enthusiasm for collaborating closely with biologists; software, engineering, and data science skills; and the ability to write clearly for a broad audience. Demonstrated experience in software and instrumentation development. Proficiency in at least one programming language including, but not limited to, Python or MATLAB. | Certificates and Licenses | Completion of Harvard IT Academy specified foundational courses (or external equivalent) preferred | Working Conditions | Work is performed in an office setting | Additional Information | This is a one-year term position with strong potential for renewal. |  | Harvard offers an outstanding benefits package including: |  | Time Off: 3 - 4 weeks paid vacation, paid holiday break, 12 paid sick days, 11.5 paid holidays, and 3 paid personal days per year. | Medical/Dental/Vision: We offer a variety of excellent medical plans, dental &amp; vision plans, all coverage begins as of your start date. | Retirement: University-funded retirement plan with full vesting after 3 years of service. | Tuition Assistance Program: Competitive tuition assistance program, $40 per class at the Harvard Extension School and discounted options through participating Harvard grad schools. | Transportation: Harvard offers a 50% discounted MBTA pass as well as additional options to assist employees in their daily commute. | Wellness options: Harvard offers programs and classes at little or no cost, including stress management, massages, nutrition, meditation, and complementary health services. | Harvard access to athletic facilities, libraries, campus events, and many discounts throughout metro Boston. |  | The Harvard Medical School is not able to provide visa sponsorship for this position. | Job Function | Information Technology | Location | USA - MA - Boston | Job Code | I0756P Applications Professional II | Sub-Unit | - | Department | Neurobiology | Time Status | Full-time | Salary Grade | 056 | Union | 00 - Non Union, Exempt or Temporary | Pre-Employment Screening | Education, Identity | Schedule | 35 hrs. per week | Monday - Friday | 9:00 am - 5:00 pm | EEO Statement | We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, gender identity, sexual orientation, pregnancy and pregnancy-related conditions, or any other characteristic protected by law.",Boston MA,Data Scientist / Engineer
DCP Midstream,/company/1--800--Flowers.com/jobs/Data-Architect-Engineer-549423ae80f88a4f?fccid=2591a885cb58d5d6&vjs=3,"Job detailsSalary$120,000 - $140,000 a yearJob TypeFull-timeNumber of hires for this role1QualificationsBachelor's (Preferred)SQL: 1 year (Preferred)Full Job DescriptionDo you want to join a dedicated team on a mission to transform how 1800Flowers is managing its data? 1800Flowers is looking to build a data platform that will support rapid data discovery, delivery and enable innovative analytics.Are you well-rounded enterprise data architect who loves new challenges and ready to embrace the complexities of retail data and play a meaningful role on an innovative team? Then join us here at 1800Flowers, where we can provide you with the opportunity to do all those things! This is an exciting role for someone who has the right skills to be part of a dynamic team looking to redefine how we manage data.Job Responsibilities: Be responsible for crafting, building and maintaining our next generation data platform built on Snowflake.Continue to analyze the current state of the existing data architecture of various systems – software components, technologies, data models and business processes.Build future state of data architecture that will align and support the goals and priorities of the entire EnterpriseGenerate data management artifacts: data models, diagrams, architecture topologies, roadmaps and plans for implementation and rollout of new technologies into existing architecture.Develop robust, scalable, resilient and secure data architecture solutions that improve institutional data assets.Closely collaborate with various IT teams, Analytics team and business stakeholders in devising data strategies and solutions across the enterprise.Develop deep understanding of enterprise data flows, data sources and data consumers.Propose and enforce data management best practices, data standards, data management policies, and rules across different disciplines of the company.Identify, design, and implement internal process improvements optimizing data delivery, re-designing infrastructure for greater scalability, etc.Works on unique architectures where there might be no previous precedentJob Qualifications: 3-6 years of relevant experienceEnd-to-end data management experience, understands the complexities of a large company enterprise data architecture.Solid experience with enterprise data management solutions and EDW architecture/deployments.Good experience with data model design (conceptual, logical, physical), strong knowledge of relational databases, SQL and columnar databases.Experience in big data solutions such as Snowflake, Big Query and other non-traditional big data software.Experience in data lake architectures and solutions.Understands cloud vs. on-prem architectures and advantages vs. disadvantages.Strong in gaining knowledge in new technologies and conduct proof of concepts of well-established and innovative enterprise software tools.Experience with development and implementation of data services architectures.Data security solutions across entire data pipeline.Ability to analyze complex data flows and propose optimization strategies.Ability to communicate data architecture concepts to technical and non-technical staff.Excellent interpersonal skills, accountability, written and verbal communication skills, time management and is efficient at prioritizing deadlines and goals.Maintain healthy vendor relationships and may help drive vendor software directions.Job Type: Full-timePay: $120,000.00 - $140,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceDisability insuranceEmployee assistance programEmployee discountFlexible scheduleFlexible spending accountHealth insuranceHealth savings accountLife insurancePaid time offVision insuranceSchedule:Monday to FridaySupplemental Pay:Bonus payEducation:Bachelor's (Preferred)Experience:SQL: 1 year (Preferred)Work Location:One locationBenefit Conditions:Waiting period may applyWork Remotely:Temporarily due to COVID-19COVID-19 Precaution(s):Remote interview processPersonal protective equipment provided or requiredTemperature screeningsSocial distancing guidelines in placeVirtual meetingsSanitizing, disinfecting, or cleaning procedures in place",Carle Place NY 11514,Data Architect/Engineer
Bloom Insurance,/company/Sirrussoft-Inc/jobs/Senior-Data-Scientist-Machine-Learning-Engineer-4a75c2e293841535?fccid=81952fe782905fe5&vjs=3,"Job detailsSalary$41 - $70 an hourJob TypeFull-timeContractNumber of hires for this role2 to 4Full Job DescriptionComputer Vision experience and expertise is a MUST. We need the resume ASAPOther Details as below;Senior Machine Learning Scientist:Responsibilities:Collect image/text data and clean/organize/augment themIdentify relevant data and prepare/train team for data labellingBuild machine (deep)learning framework to train various models for scanned image-based applications for automated invoice processingBuild machine (deep)learning framework to train various NLP models for processing text documents and extract relevant informationImplement a scalable inferences processQualification:MS or Ph.D. in computer science or related real-world experienceExtensive experience in machine learning and deep learningExperience in image processing and computer visionExperience in text processing and NLPExpertise writing code in Python and any Deep learning frameworks like Pytorch/TensorflowFamiliar with object detection, text detection, text recognition, entity recognition etc.Job Types: Full-time, ContractPay: $41.00 - $70.00 per hourSchedule:Monday to FridayContract Length:1 yearFull Time Opportunity:YesWork Location:Fully RemoteCOVID-19 Precaution(s):Remote interview processPersonal protective equipment provided or required",Remote,Sr. Data Scientist/Machine Learning Engineer
Anthem,/company/Partanic-LLC/jobs/Cloud-Data-Engineer-a74e57c207e57eaa?fccid=d8f0de3e57e98ef1&vjs=3,"Job detailsJob TypeFull-timeNumber of hires for this role1Full Job DescriptionThe Cloud Data Engineer will be the Primary Data and Integration Architect, this is a remote position, the Cloud Data Engineer may be located anywhere in the United States.Cloud Data Engineer FunctionsDesign, implement and deploy scalable and fault-tolerant enterprise solutions for data platforms, integration, and analyticsIntegrate data between SaaS systems via an iPaaS (Informatica)Maintain map of all data systems and the data within those systemsServe as primary go-to” expert on our data systems and integration architectureIdentify opportunities to, develop, and implement data-driven business optimizations (e.g., automated tasks, triggered alerts, error-checking)Implement data-focused systems, process, and resourcesExperience with web services (SOAP / REST)Establish data governance strategy including:Develop and implement approaches, policies, and standards to ensure data quality and integrity is maintained and that any inaccurate data is uncovered and correctedDevelop and implement security standards for data storage and transportCloud Data Engineer Qualifications5+ years of relevant work experienceBachelor’s degree in related fieldCloud Data Engineering background strongly preferred including experience with web services (SOAP / REST)Expert SQL skillsStrong working knowledge of Informatica, PowerBI, and Azure Data Factory and ETL – or comparable systems in each of these 3 categoriesExperience with digital process automation and/or robotic process automationVery strong data analytic capabilities including experience with data collection, manipulation, cleaning, analysis, ETL, and visualization. Strong data Modeling skills (conceptual, logical, physical)Exceptional Excel skills including high level of expertise at pivot tables and various Excel functions used for data manipulation, cleaning, parsing, etc.Must have strong ability to quickly master and become expert in new data solutionsStrong communication and interpersonal skills over the phone, in person, and via video conferenceAbility to assess needs related to the job function and create new processes, documents, structures, and systems accordinglyInterested in making a positive difference in the future of behavioral healthJob Type: Full-timeBenefits:401(k)Dental insuranceFlexible scheduleHealth insuranceLife insurancePaid time offVision insuranceSchedule:Monday to FridaySupplemental Pay:Bonus payCOVID-19 considerations:100% remote positionWork Location:Fully RemoteVisa Sponsorship Potentially Available:No: Not providing sponsorship for this jobCOVID-19 Precaution(s):Remote interview processVirtual meetings",Remote,Cloud Data Engineer
Dell Technologies,/rc/clk?jk=6a6b95f12086c86a&fccid=a68644e6991a109d&vjs=3,"Job Title: Data Engineer (L1) | Location: Bloomington, IN or remote | Reports to: Database Team Manager | Department/Team: IT | FLSA: _X_ exempt | Innovate.Grow. Bloom. | The healthcare insurance industry is growing faster than any other insurance sector in the United States. If you’re looking for a tech career with a growing company where you can make a difference, earn a great salary, and be recognized for your forward thinking, then you’ve found it at Bloom. |  | Who Is Bloom? | We are the Insurance Industry's Trusted Growth Partner. We are industry visionaries. We’re solving problems and advancing the healthcare insurance business through innovative technology. We are committed to ensuring our state-of-the-art software products and services provide greater efficiency and cost savings to clients. This is what we do at Bloom and you can be part of it. | Our Technology | We provide advanced sales and enrollment automation software to the insurance industry through our Ascend™ technology platform. Bloom’s Ascend™ technology platform focuses on sales automation efficiencies and optimizing the member experience from the first moment a prospect considers a health plan membership | Our Culture | We pride ourselves on being an awesome place to work. Our office environment is fun and casual, we want our team to enjoy the space in which they work. You’ll get to know your fellow employees at our fun events and team lunches. Our community engagement events offer opportunities to give back to the community through volunteering and other activities. | Purpose and Objective | To support internal and external clients via processing and handling of data. To generate data solutions for ongoing immediate day to day business needs. |  | Essential Functions |  | Day to day functions include the following: | Design data models and develop database structures in Microsoft SQL server. | Write various database objects like stored procedures, functions, views, triggers for various front end applications. | Write SQL scripts, create SQL agent jobs to automate tasks like data importing, exporting, cleansing tasks. | Create database deployment packages for deploying changes. | Identify &amp; repair inconsistencies in data, database tuning, query optimization. | Able to generate ad hoc data on demand. | Able to identify best practices, documentation, communicate all aspects of projects in a clear, concise manner | Develop simple SSIS packages to perform various ETL functions including data cleansing, manipulating, importing, exporting. | Develop &amp; maintain client facing reports by using various data manipulation techniques in SSRS and Visual Studio. | Documentation | Optimization recommendations | Day to day troubleshooting | .NET Programming as needed |  | Education/Experience | BA, BS, or Masters in computer science/related field preferred or an equivalent combination of education and experience derived from at least 2 years of professional work experience | Solid experience with various versions of MS SQL Server and TSQL programming | Microsoft Certified DBA a plus | Skills/Knowledge | Strong experience in writing efficient SQL code | Working knowledge of SQL Server Management Studio (SSMS) | Knowledge of SQL Server Reporting Services (SSRS) | Knowledge of SQL Server Integration Services (SSIS) | Knowledge of Red Gate DBA Tool Belt (SQL Compare, SQL Data Compare, SQL Source Control) a plus | Knowledge of data science technologies is a plus | Clear, concise communication skills, excellent organizational skills | Highly self-motivated and directed | Keen attention to detail | High level of work intensity in a team environment | High integrity and values-driven | Eager for professional development | Experience and understanding of source control management a plus",Bloomington IN 47403,Data Engineer (L1)
Ascension,/rc/clk?jk=b83481c445a7225f&fccid=2a4da7fa99f4b9ae&vjs=3,"Description | SHIFT: Day Job |  | SCHEDULE: Full-time |  | ZipDrug is a proud member of the Anthem, Inc. family of companies, and delivers on our mission to provide greater access to care for our members, greater value for our customers, and greater health for our communities. Join us and together we will drive the future of health care. | This is an exceptional opportunity to do innovative work that means more to you and those we serve at one of America's leading health benefits companies and a Fortune Top 50 Company. | Engineer III | Work Location: Any Anthem location throughout the United States or can be remote | Responsible for the delivery of end to end system development and maintenance on complex technology systems within assigned client group, business unit or corporate department. | Primary duties may include, but are not limited to: | Development on medium to large complex customer technology platforms or architectures. | Maintains active relationships with internal and/or external stakeholder to develop business requirements. | Analyzes and classifies complex change request and identifies and documents possible system code enhancements. | Demonstrates knowledge of industry trends, our products and infrastructure. | Collaborates with engineers and graphic designers, analyzes and classifies complex change request and reviews and evaluates possible enhancements. | Works with development team to develop and define application scope and objectives and prepare functional and/or technical specifications. | Analyzes and evaluates detailed business and technical requirements. | Understands the product and analytics and drive requirements that take into account stakeholders needs. | Identifies problems/risks. Codes and maintains complex components of information systems. | Mentors others on coding standards and performs code reviews. | Reviews test cases and advises QA on adjacent code/regression impact. | Develops and performs system testing and fixes defects identified during testing and re-executes unit tests to validate results. | Aids in integrating activities with other IT departments for successful implementation and support of project efforts. | Provides on call support and monitors the system and identifies system deficiencies. |  | Qualifications | Requires BA/BS degree or technical institute training and 3+ years experience on one platform, multi database, multi language or multi business application, or any combination of education and experience, which would provide an equivalent background. | Incumbent should also have the ability to mentor others, lead small projects and provide troubleshooting support. | Advanced SQL knowledge preferred | Object oriented programming/scripting languages using Python preferred | Experience working with Airflow, Python, PySpark, Docker ETL Processes/Data Pipelines preferred | Experience working with Snowflake, Dbt Data Warehouses preferred | Experience with data stream-processing systems preferred | Experience working with Postgres relational databases preferred | Looker reporting models experience preferred | AWS cloud technologies experience preferred | Agile development methodologies preferred |  | We offer a range of market-competitive total rewards that include merit increases, paid holidays, Paid Time Off, and incentive bonus programs (unless covered by a collective bargaining agreement), medical, dental, vision, short and long term disability benefits, 401(k) + match, stock purchase plan, life insurance, wellness programs and financial education resources, to name a few. An Equal Opportunity Employer/Disability/Veteran. | Anthem, Inc. is ranked as one of America's Most Admired Companies among health insurers by Fortune magazine and has been named a 2019 Best Employers for Diversity by Forbes. To learn more about our company and apply, please visit us at careers.antheminc.com. An Equal Opportunity Employer/Disability/Veteran.",New Haven CT 06513,Data Engineer III- Python
CrowdStrike,/company/earlyStage-Partners/jobs/Senior-Data-Engineer-c1f0e4f07a94fffd?fccid=be77a776106c5ffc&vjs=3,"Job detailsSalary$125,000 - $170,000 a yearJob TypeFull-timeNumber of hires for this role2 to 4QualificationsBachelor's (Preferred)Full Job DescriptionAbout us: Live experiences help make us human, bringing us across today’s social and digital divides to focus on what truly connects us - the here, the now, the once-in-a-lifetime moment that we share - together. To fulfill Gametime’s vision to unite the world through shared experiences, we deliver fans an extraordinary experience for enjoying, discovering, and purchasing last-minute tickets to live events.With platforms on iOS, Android, mobile web, and desktop supporting events across the US and Canada, we are reimagining the event ticket experience in a mobile-first world.*COVID-19 UPDATEIt’s no secret that the events industry has been hit as hard by the COVID-19 pandemic. Sports, concerts, and theater were first to go back in March, and will likely be last to return. However, we are confident that the rebound is inevitable, and Gametime will be there to facilitate our nation’s need to get out there and connect with friends, family, and strangers alike. Our goal is to prepare for the upswing by developing brand new features while also strengthening our core ticketing product. Join us in helping ensure Gametime continues to be the leader in mobile ticketing.The Role: As a Senior Data Engineer at Gametime, you will have the opportunity to work not only within the data team but across the entire business. You’ll be at the forefront of building scalable systems for product, marketing, engineering, finance, and customer support to handle the high volume of data we collect as one of the fastest-growing startups in the Bay Area.What you'll do/own:Design and implement scalable data pipelines and data storage on AWS using Kinesis, Redshift, S3, and a Spark based streaming architectureCreate scalable and low latency code for data products using MongoDB, Redis, Elasticsearch or similarScale and maintain our Analytics Databases to power our dashboardsCollaborate with our data scientists to productionize their modelsImplement Comprehensive Testing and Continuous Integration frameworks for schema, data, and functional processes/pipelinesOur ideal candidate has:BS in Computer Science or equivalent experience or fieldAt least 5 years experience using Redshift or a similar Data WarehouseAt least 5 years experience programming, preferably with Python or GoHands-on experience with SQL, ETL, Data Warehousing and Data OrchestrationFamiliarity with scheduling frameworks, preferably AirflowFamiliarity with real-time/batch distributed systems like Kinesis, Kafka, Spark, professional experience with Redis and Elastic searchFamiliarity with business intelligence/analytics tools like Tableau or Periscope DataWhat we can offer you:Flexible PTOMedical, dental, &amp; vision insuranceLife insurance and disability benefits401K, HSA, pre-tax savings programsNew equipment setup providedWellness programsTenure recognitionGametime is committed to bringing together individuals from different backgrounds and perspectives. We strive to create an inclusive environment where everyone can thrive, feel a sense of belonging, and do great work together. As an equal opportunity employer, we prohibit any unlawful discrimination against a job applicant on the basis of their race, color, religion, veteran status, sex, parental status, gender identity or expression, transgender status, sexual orientation, national origin, age, disability or genetic information. We respect the laws enforced by the EEOC and are dedicated to going above and beyond in fostering diversity across our company.APPLY FOR THIS JOBJob Type: Full-timePay: $125,000.00 - $170,000.00 per yearEducation:Bachelor's (Preferred)Work Location:Fully Remote",New York NY,Senior Data Engineer
1800flowers.com,/rc/clk?jk=27701f2d62d5492c&fccid=64e4cdd7435d8c42&vjs=3,"At CrowdStrike we’re on a mission - to stop breaches. Our groundbreaking technology, services delivery, and intelligence gathering together with our innovations in machine learning and behavioral-based detection, allow our customers to not only defend themselves, but do so in a future-proof manner. We’ve earned numerous honors and top rankings for our technology, organization and people – clearly confirming our industry leadership and our special culture driving it. We also offer flexible work arrangements to help our people manage their personal and professional lives in a way that works for them. So if you’re ready to work on unrivaled technology where your desire to be part of a collaborative team is met with a laser-focused mission to stop breaches and protect people globally, let’s talk. | About the Role: | The Sales Data Engineer is responsible for building and maintaining the data foundation of the Sales business unit, automating and aggregating data from internal systems as well as external vendors and agencies. As a key member of the Sales Reporting, Forecasting, &amp; Analytics team, the Sales Data Engineer collaborates with analysts and Sales leadership to create data models that power business intelligence and analytics across business planning, pipeline analysis, bookings forecast optimization, sales analytics, and lead source attribution. | They are a self-driven individual with excellent analytical and problem solving skills, able to quickly comprehend and adapt to emerging priorities and thrive in a fast-paced, high-performing environment. | In addition to managing the data foundation, the Sales Data Engineer will work across IT, Sales Operations, Marketing Operations, Alliances Operations, and Finance to align on standardized processes, metrics, and architecture. They also will create and maintain documentation on all processes and datasets. | Responsibilities: | Design, develop, and maintain a data platform that serves as the foundation of the Sales business unit, automating and aggregating data from internal systems as well as external vendors | Design, develop, build and maintain data models to deliver insightful analytics while ensuring the highest standard in data integrity | Interface with engineers from other systems and applications to ensure proper data collection | Ensure data quality by implementing data detection mechanisms | Maintain and onboard new systems/vendors to enable data collection and analytics for the Sales business unit | Participate in reviewing the data models created by the team and change the existing data models in production as necessary | Build data pipelines and reports that enable analysts and other stakeholders across the business unit | Educate your partners: use your data and analytics experience to “see what’s missing,”identifying and addressing gaps in their existing processes | Support existing processes running in production and optimize when possible | What You’ll Need: | Proven experience as a Data Engineer, including designing, building, and maintaining data processing systems | 6+ years of experience with schema design and dimensional data modeling | 6+ years of SQL, Python, and Java experience preferred | Experience with Salesforce and Snowflake required | Experience with Domo or Tableau preferred | Experience with Sales, Alliances, and/or Marketing a plus | BS/MS in Computer Science, Data Science, Applied Mathematics, Statistics, or related field | Effective communication, at scale, through multiple media: presentations, dashboards, company-wide datasets, bots and more | Demonstrated understanding of development processes and agile methodologies | Intellectually curious and willing to learn | #LI-SL1 | #LI-Remote | We are committed to building an inclusive culture of belonging that not only embraces the diversity of our people but also reflects the diversity of the communities in which we work and the customers we serve. We know that the happiest and highest performing teams include people with diverse perspectives and ways of solving problems so we strive to attract and retain talent from all backgrounds and create workplaces where everyone feels empowered to bring their full, authentic selves to work. | CrowdStrike is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.",San Francisco CA,Sales Data Engineer – Global Reporting Forecasting & Analytics (Remote)
The Network,/company/SimWell-Inc/jobs/Industrial-Engineer-Data-Analyst-e1976449c36cd76d?fccid=dbf05b1220424db4&vjs=3,"Job detailsSalary$52,000 - $60,000 a yearJob TypeFull-timeNumber of hires for this role1QualificationsBachelor's (Preferred)Data science: 1 year (Preferred)Python: 1 year (Preferred)Industrial engineering: 1 year (Preferred)Full Job DescriptionWe are looking for an industrial engineer passionate about data science. The successful candidate will be responsible for preparation of data for simulation, AI, and optimization models. Data analyst responsibilities include transforming data and conducting full lifecycle analysis to include requirements, activities and design. Data analysts will develop analysis and reporting capabilities.ResponsibilitiesAssemble large, complex data sets that meet functional / non-functional business requirements.Interpret data, analyze results using statistical techniques and provide ongoing reportsDevelop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and qualityAcquire data from primary or secondary data sources and maintain databases/data systemsIdentify, analyze, and interpret trends or patterns in complex data setsFilter and “clean” dataLocate and define new process improvement opportunitiesSkillsProven working experience as a data analyst or business data analystTechnical expertise regarding data models, database design development, data mining and segmentation techniquesStrong knowledge of and experience with reporting packages (Business Objects etc), databases (SQL etc), programming (XML, Javascript, or ETL frameworks)Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SPSS, SAS etc)Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracyAdept at queries, report writing and presenting findingsBS in Mathematics, Economics, Computer Science, Information Management or StatisticsExperience working in supply chain field is highly valuedJob Type: Full-timePay: $52,000.00 - $60,000.00 per yearBenefits:Dental insuranceFlexible scheduleHealth insurancePaid time offVision insuranceSchedule:Monday to FridayEducation:Bachelor's (Preferred)Experience:Data science: 1 year (Preferred)Python: 1 year (Preferred)Industrial engineering: 1 year (Preferred)Work Location:One locationCompany's website:www.simwell.ioWork Remotely:Yes",Greensburg PA 15601,Industrial Engineer and Data Analyst
TWD,/rc/clk?jk=417b219d0b4bd106&fccid=401ad1536026b66d&vjs=3,"Data Center Engineer - Atlanta | Five9 is a leading provider of cloud software for the enterprise contact center market, bringing the power of the cloud to thousands of customers and facilitating more than three billion customer interactions annually. Since 2001, Five9 has led the cloud revolution in contact centers, helping organizations transition from legacy premise-based solutions to the cloud. Five9 provides businesses with cloud contact center software that is reliable, secure, compliant and scalable, which is designed to create exceptional customer experiences, increase agent productivity and deliver tangible business results. | We are actively seeking a Data Center Engineer in Atlanta for our Infrastructure team. This person will be responsible for building and installing multiple hardware components that make up the company’s infrastructure. The Data Center Engineer responds to critical customer outages, taking the necessary steps to diagnose and correct the root cause as quickly as possible. He/she exhibits a deep understanding of data center technologies, strong analytical and troubleshooting skills, and excellent communication skills. This position will be located in Atlanta, GA. | Key Responsibilities: | Build, manage and maintain company’s infrastructure in global data centers | Perform equipment installs – compute, network, storage, racks, PDUs, cabling, etc. | Closely monitor infrastructure to prevent failures and service degradation | Work with other Operations teams on troubleshooting and resolution of problems including hardware failures and physical connectivity problems | Communicate and coordinate with remote hands in remote data centers, providing technical direction when needed | Maintain asset inventory – both installed and spares | Perform all shipping and receiving duties including RMA activities | Maintain impeccable documentation around our datacenter configuration | Set standards of excellence for all datacenter operations | Maintain clean and orderly premises at all times | Update and work on tickets and tasks assigned in a timely manner | Follow change management process | Participate in off hours change windows | Provide 24x7 support as needed | Requirements: | Minimum of 4 years hands on experience with Data Center technologies – compute, network, storage, cabling, racks, PDUs, etc. | Great knowledge and experience with planning cooling, power and space resources | Linux/Unix system administration, networking and SAN connectivity skills strongly desired | Experience working with vendors (quote-purchase process, RMA processes, support interactions) | Scripting skills are a plus | Experience supporting 24x7 customer facing production environment | Ability to troubleshoot and solve hardware problems independently | Ability to lift 50 pounds | Willingness to travel – less than 10% | Bachelor of Science degree in Computer Science or equivalent experience | Five9 embraces diversity and is committed to building a team that represents a variety of backgrounds, perspectives, and skills. The more inclusive we are, the better we are. Five9 is an equal opportunity employer. | Our headquarters are located in the beautiful Bishop Ranch Business Park in San Ramon, CA. | #LI-RN1",Atlanta GA,Data Center Engineer - ATL
Komodo Health,/company/Gracemark/jobs/Senior-Data-Engineer-d02b5c6e9ceaddcf?fccid=8f5f1d7d3d3424ef&vjs=3,"Job detailsSalaryUp to $150 an hourJob TypeFull-timeContractNumber of hires for this role1QualificationsAPIs: 3 years (Required)SQL: 5 years (Required)Bachelor's (Preferred)Python: 3 years (Preferred)Tableau: 3 years (Preferred)Data warehouse: 4 years (Preferred)Full Job DescriptionWe are looking for a Data Engineering contractor to support our Corporate Operations.Responsibilities: Building new data pipelines that support various Corporate Ops teams.Providing visibility around existing costs and projected growth is critical to managing our ever-expanding business.Sourcing data from various vendor-provided and internal systems to build reliable ETL pipelines and helping craft analytic datasets that will be used for Tableau dashboards.Requirements: 5+ years of experienceExperience working with API sourcesPython programming languageStrong SQL experienceExperience building analytic datasetsJSON/YAMLGit, version controlTableauPresto(Trino)Apache SparkDockerBashModeling analytics datasetsExperience working with data from ticket tracking systems (Jira, Zendesk, etc)Job Types: Full-time, ContractPay: Up to $150.00 per hourSchedule:Monday to FridayEducation:Bachelor's (Preferred)Experience:Data Engineer: 5 years (Required)APIs: 3 years (Required)Python: 3 years (Preferred)SQL: 5 years (Required)Tableau: 3 years (Preferred)Data warehouse: 4 years (Preferred)Work Location:Multiple locationsVisa Sponsorship Potentially Available:No: Not providing sponsorship for this jobCOVID-19 Precaution(s):Remote interview processVirtual meetings",Los Gatos CA,Senior Data Engineer
Weight Watchers International,/company/Rootshell-Inc/jobs/Data-Engineer-14460764a9521a35?fccid=af202ed5967aaaae&vjs=3,"Job detailsSalaryFrom $80,000 a yearJob TypeFull-timeContractNumber of hires for this role5 to 10Full Job DescriptionWe are looking for a Senior Developer with Python and some Java skills to help us build and evolve our Data Management Platform.Your primary focus will be the development of all server-side backend data processing logic, ensuring high performance and responsiveness to requests from the front-end / API requests.The project has two modules:Data processing module developed in PythonRest API module developed in Java.Responsibilities:Writing reusable, testable and efficient codeDesign and implementation of low-latency, high-availability and performant applicationsDeveloping ETL processing in python as needed.Work with cross-functional teams to complete migration effort5+ years of experience working in the enterprise setting developing python applications in multiple projects.Hands on experience with Flask or other Python frameworksExperience writing python applications that interact with ORM (Object Relational Mapper) librariesAble to integrate multiple data sources and databases.Good understanding of the threading limitations of Python and multi-process architectureStrong unit test and debugging skillsStrong knowledgeable in XML and JSON parsing in python.Strong experience with SQL and data analysis.Knowledge in Kafka, EMS queues.Proficient understanding of code versioning, unit testing tools (e.g. Git)Contract length: 12 monthsJob Types: Full-time, ContractSalary: From $80,000.00 per yearSchedule:8 hour shift",Santa Clara CA,Data Engineer
Jockey International Inc.,/rc/clk?jk=6f6f1ee8afafcbd6&fccid=5e964c4afc56b180&vjs=3,"A career in PwC Digital Products will provide you the opportunity to design and develop products to help our clients lead the next phase of their evolution. We are industry specialists who build products connected to a purpose. Spanning across four strategic areas of transformation, our products tackle the very real challenges our clients face today, and unlock new opportunities for growth in the future. Our team leads the planning and design of the overall technical vision of our products and accelerators. |  | To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future. | As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to: | Use feedback and reflection to develop self awareness, personal strengths and address development areas. | Delegate to others to provide stretch opportunities, coaching them to deliver results. | Demonstrate critical thinking and the ability to bring order to unstructured problems. | Use a broad range of tools and techniques to extract insights from current industry or sector trends. | Review your work and that of others for quality, accuracy and relevance. | Know how and when to use tools available for a given situation and can explain the reasons for this choice. | Seek and embrace opportunities which give exposure to different situations, environments and perspectives. | Use straightforward communication, in a structured way, when influencing and connecting with others. | Able to read situations and modify behavior to build quality relationships. | Uphold the firm's code of ethics and business conduct. |  | Job Requirements and Preferences: |  | Basic Qualifications: |  | Minimum Degree Required: | High School Diploma |  | Minimum Years of Experience: | 3 year(s) |  | Preferred Qualifications: |  | Degree Preferred: | Bachelor Degree |  | Preferred Fields of Study: | Computer and Information Science, Computer Engineering, Computer Systems Analysis, Data Processing/Analytics/Science, Information Technology, Management Information Systems |  | Preferred Knowledge/Skills: | Demonstrates thorough abilities and/or a proven record of success as a team leader including the following areas: | Working with the lead data architect on research, design &amp; enhancement of data services; | Working within Cloud computing platforms such as AWS, GCP and Azure; | Applying expertise with Data modeling, quality and Transformation; | Utilizing relational databases, SQL server and writing SQL queries; | Utilizing Big Data Machine Learning toolkits such as SparkML and NoSQL databases; | Working within Columnar databases such as Snowflake, Vertica; | Architecting highly scalable distributed data pipelines using open source tools and big data technologies such as Spark, PySpark, Sqoop and ETL frameworks; | Demonstrating a general understanding of Tableau, Alteryx; | Determining the appropriate software packages or modules to run, and how easily they can be modified; | Handling large scale structured and unstructured data from internal and third party sources; and, | Leading teams in a dynamic work environment while managing stakeholder expectations and scope. |  |  | All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer. |  | For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance. |  | For positions in Colorado, visit the following link for information related to Colorado's Equal Pay for Equal Work Act: https://pwc.to/coloradoadvisoryseniorassociate.",Washington DC,PwC Digital Products - Data Engineer - Senior Associate
Travelers,/rc/clk?jk=2e0cef2c9cf2678f&fccid=b997e101c7856780&vjs=3,"This role serves as a subject matter expert in Data &amp; Analytics focused on Business Intelligence, Datamining, Statistical Modelling and Data domain areas, serves as the lead for medium to large complex activities and who understands company goals and objectives. Enables collaboration with cross-functional teams. Translates strategies into roadmaps and requirements ensuring resolution with the utmost effectiveness and efficiency. Responsible for the research and development of business cases, designs, configurations, implementation plans and overall management of project related activities. Strong communication and analytical skills are required as part of this agile team. Provides support to team members as needed. Work in a collaborative way with Business and IT resources to research, troubleshoot, resolve and implement Data &amp; Analytic services to our business. | What you will be responsible for | Leads and can drive medium to large size projects - as PM and SME to design and architect solutions, build test cases and script development and ensures projects meet business and IT objectives | Perform as the Data &amp; Analytics expert for all business needs. | Analyzes data and makes analytical recommendations for IT management to consider as components of enterprise architecture and the roadmap development for BI tools and technologies | Work autonomously with IT and Business groups to understand issues and come up with solutions to complex business challenges | Identify business requirements and map them to the Services and Applications within the Data &amp; Analytic application landscape. | Identify functionality gaps and develop solutions for them | Research and resolve questions, issues, and service opportunities. Provide recommendations, alternatives, and guidance to the business. | Advise on options, risks, and any impacts on other processes or systems | Configure, Review and Implement changes within Data &amp; Analytic application landscape. | Provides strong technical and functional business support | Work at the highest technical level of most phases of systems analysis while considering the business implications of the application technology to the current and future business environment | Qualifications | Must have a minimum of 5 years’ experience (preferably 7-12 years) in similar roles managing or supporting Data &amp; Analytics platforms and Statistical Models. | Must have experience with Master Data Management Strategies and Applications; Talend, SnowFlake, Python, Microsoft Power BI, Microsoft Development Stack | BS/MS degree in Data Management, Data Engineering, Computer Science, Mathematics, Statistics is preferred | Must have a functional and technical IT background with strong analytical skills with ability to write scripts, database queries, along with understanding the software development lifecycle and business acumen | Must have strong communication and collaboration skills articulating in clear concise messages | Strong written/verbal communications skills. Understands importance and frequency of status updates or documentation capture and can present information or ideas openly | Ability to work independently in a highly complex and challenging environment | Ability to translate between functional requirements and technical designs | Ability to work on all project phases: project preview, fit/gap analysis, configuration, design, testing and deployment | Special Demands | This job primarily operates in a professional office environment and routinely requires the use of standard office equipment such as computers, phones, copy machines, etc. Noise level is typically low. Frequently in a stationary, sitting position for prolonged periods of time. Regularly moves about inside the office to complete tasks, attend meetings or to access the copy machine or file cabinets. Periodically pulls/pushes doors open to move around the office. Occasionally may lift and carry objects up to 20 pounds. Occasional travel may be required. | Salary Range | The salary range for this job is $80,300 - 132,500 | It has been and will continue to be the policy of DCP Midstream not to discriminate against any employee or applicant for employment because of their race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender expression, veteran status, disability, or other legally protected status. | Primary Location: US-CO-Denver | Job: Information Technology / Full-time | Job Posting: Feb 27, 2021, 1:00:00 AM",Denver CO,Business Intelligence Data Engineer
PRICE WATERHOUSE COOPERS,/company/International-Software-systems/jobs/Database-Engineer-Data-Warehouse-06ca3bf9033b0037?fccid=ab03f4d0188f960f&vjs=3,"Job detailsSalary$90 - $91 an hourJob TypeFull-timeContractNumber of hires for this role1QualificationsBachelor's (Preferred)Full Job DescriptionKey SkillsExpert in the various data loading techniques to Greenplum MPP database and develop process to automate it, and troubleshoot any data load errorsPosition DescriptionExpert in the various data loading techniques to Greenplum MPP database and develop process to automate it, and troubleshoot any data load errorsDatabase development experience with strong skills in SQL and PL/pgSQL.Expert in shell scripting, regular expression and automating tasksPerform source system data analysis to understand data relationshipsInvestigate and resolve data quality, data loading failures and develop data cleansing routinesDesign and document logical and physical data models in data modeling toolGenerate Data Definition Language (DDL) scripts from modeling toolDesign and document source to target data mappingsDesign, develop and test custom ETL or data stage routinesAutomate the custom data loading process and continue to improve based on requirements.Provide ad hoc query support for ad hoc query requestsExperience with AWS, EC2, S3Provide mentorship to group members and will be expected to help develop and support mid and junior level staff.May be scheduled for after hours on-call support and will be required to support production release during non-peak hours.The selected candidate will accept other duties as directed by management Responsibilities of this position may also include: Database developer and Extract, Transform and Load (ETL) developer with custom scripting skillsDesign, development, testing, and maintenance of UNIX shell scripting and PythonSkills RequirementsRequired skillsExperience working with complex data warehouse projectsExperience designing custom ETL processes to populate the data warehouseAbility to analyze source data for potential data quality issues and address these issuesAbility to effectively communicate information, both verbally and written to end users, team members and managementHigh-level knowledge of various industry leading data ware house components (databases, ETL tools, BI/reporting tools)Experience in scripting (Shell scripting, python, Java) and automationExperience with DevOpsPrepare technical documents and presentationFamiliarity with Atlassian product suites, git, bitbucket, confluence, jiraDesired skillsAnsible scriptingExperience with cloud environment, AWSDatabase Administration (DBA) skillsExperience LevelDatabase (PostgreSQL v9.6+ or Greenplum v5+) - 4 yearsCustom ETL Scripting – 5 yearsSQL and pg/PLSQL - 4 yearsShell Scripting – 5 yearsPython – 5 yearsAWS (EC2 &amp; S3) – 1 yearEducationBachelorDegreeJob Types: Full-time, ContractPay: $90.00 - $91.00 per hourSchedule:8 hour shiftEducation:Bachelor's (Preferred)Experience:Greenplum MPP database: 5 years (Required)process to automate it,troubleshoot any data load errors: 5 years (Required)Contract Length:VariesContract Renewal:LikelyWork Location:One locationWork Remotely:Temporarily due to COVID-19COVID-19 Precaution(s):Remote interview process",Woodlawn MD,Database Engineer Data Warehouse Greenplum SQL ETL
Five9 Inc.,/company/Cigniti-Technologies/jobs/Big-Data-Engineer-903d21f40b2532f4?fccid=2d2c7485c5411f8a&vjs=3,"Job detailsSalary$70 an hourJob TypeFull-timeContractNumber of hires for this role5 to 10Full Job DescriptionHadoop/SparkAbility to navigate / debug:AWS, Kubernetes, EMR, HadoopKubernetes / ContainersGeneral infrastructure maintenance, i.e:Kubernetes version upgradeRe-sizing cluster (e.g. changing core- and task node counts)Modifying auto-scaling policiesArchitectureCloud administration, system administration, notebook environment, dev ops, production supportOwn infrastructure support. i.e: security, whitelisting, etc.LY CoreRestart leap year core and understand communication between various LY servicesData ManagementPython - general skills, ability to run code in either notebook or script form and understand errorsWeekly updates, materialized views, cachesMonitoringSystem monitoring and maintenance via AWS CloudwatchMaintaining Security posture and responding to Security vulnerabilitiesSupportProject Management / change managementUnderstanding of process flows (e.g. how and when data is updated)Managing support + triage processContract length: 12 monthsApplication Deadline: 12/2/2021Job Types: Full-time, ContractSalary: $70.00 per hourBenefits:401(k)Health insuranceSchedule:8 hour shiftCOVID-19 Precaution(s):Remote interview processVirtual meetingsSpeak with the employer+91 (408) 868-7558",Dallas TX,Big Data Engineer
International Software systems,/company/The-Network/jobs/Data-Engineer-1964657771a4c896?fccid=281e61d8fc75f97b&vjs=3,"Job detailsSalaryFrom $90,000 a yearJob TypeFull-timeQualificationsPython: 2 years (Required)AWS: 2 years (Required)English (Required)US work authorization (Required)Bachelor's (Preferred)Full Job DescriptionWe are seeking a Data Engineer to join our data team in our fight to counter-sex trafficking. The ideal candidate has prior experience directly related to building out strong data pipelines – everything from discovering and vetting new data sources to visualizing data in creative ways aligned with strategic objectives. This is a highly autonomous environment, where the employee will work to improve analytic capabilities as well as organizational impact evaluation. This individual will also work closely with the IT lead to ensure uniformity and security across resources.Duties and responsibilities DataBuild data pipelines within AWS, streamline current methods and enhance efficiency.As part of these pipelines, streamline data sharing amongst team members and partners.Clean, structure, and integrate data sources related to prevalence and program measurement: Seeking out and vetting additional data sources; Connecting the dots between complex data sets.Analytics Design tailored, professional analytic products and visualizations for internal and external use.Clearly communicate data implications and assumptions both internally and externally.Assist in fostering and managing relationships with external data partners and vendors.Position requirements Character Strong alignment with The Network’s core values (Courage, Excellence, Diversity and Humility).Strong alignment with The Network’s operating principles (It’s not about us; Partnership is scale; Fortune favors the bold). Unquestionable integrity.Personality Ambitious and initiative – this position will often involve projects without clear deliverables.Cheerful, humble, and candid in communicating with colleagues.Entrepreneurial spirit who enjoys brainstorming innovative ideas and implementing creative solutions.Collaborative and service oriented.Detail-oriented and excellent time management.Strong relational skills and can clearly communicate across all data-related topics.Required Experience BS in Mathematics, Engineering, Data Science, or similar field.At least 2-4 years of relevant work experience related to data engineering, data science and/or data architecture.Strong experience in data mining and scraping.Experience in Python and/or R, especially libraries such as Pandas, NumPy, and Spacy.Database design and management (MySQL preferred).Familiarity with AWS Services, especially related to data pipelines (S3, Athena, RDS, EC2, Lambda).Critical thinking skills, both quantitative and qualitative.Has demonstrated solid leadership.Preferred Experience Experience with data intelligence tools (e.g., Tableau) and link analysis software preferred.Experience with Elasticsearch a plus.Familiarity with Monitoring, Evaluation, and Learning (MEL) or Impact Evaluation methodologies a plus.Reports to: Director of Data and AnalyticsJob Type: Full-timePay: From $90,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceFlexible scheduleHealth insurancePaid time offVision insuranceSchedule:Monday to FridayEducation:Bachelor's (Preferred)Experience:Python: 2 years (Required)AWS: 2 years (Required)Language:English (Required)Work Location:One locationThis Company Describes Its Culture as:Innovative -- innovative and risk-takingOutcome-oriented -- results-focused with strong performance cultureTeam-oriented -- cooperative and collaborativeBenefit Conditions:Only full-time employees eligibleWork Remotely:Temporarily due to COVID-19COVID-19 Precaution(s):Remote interview processSocial distancing guidelines in placeVirtual meetings",Arlington VA 22209,Data Engineer
TCS,/rc/clk?jk=89f282a8451fecf1&fccid=4ec8d55e509a7360&vjs=3,"Vydia’s Data Engineering team plays an integral role in our success. Through our data, we nurture our artists, creators, partners, and internal users with insights to help them engage their audience. | We are seeking a highly motivated individual who loves to learn and is always keen on advancing, someone with the self-awareness to know where they need improvement. We are a close-knit team and strive to take ownership and responsibility of our work and data. | We have a wide range of data we deal with, from YouTube, Apple, Facebook, Spotify, and others. This diversity has to be met with creativity and best practices to ensure we deliver the best data quality to our clients. |  | RESPONSIBILITIES | Implement ETL/ELT DAGs using Apache Airflow. | Transform data in our Data Warehouse using AWS Redshift and Redshift Spectrum. | Collect data from REST APIs and store it in our Data Lake, AWS S3. | Work closely with product engineers to make application data available for product features. | Work closely with business analysts to orchestrate Business Intelligence reporting using Looker. | Uphold, introduce, and refine data quality standards, engineering best practices, and KPIs | Constantly strive to improve engineering workflows and efficiency, and heavily utilize automation whenever possible. | Work closely with DevSecOps to optimize cloud infrastructure performance and scalability | Writing Python unit tests to ensure high code quality and reliability. |  | REASONS TO WORK WITH US | As an Inc 500 Fastest Growing Company in America Vydia offers huge opportunities to grow with the company. | Vydia was named a Best Place to Work in NJ by NJBIZ in 2017 due to its collaborative, fast paced, and fun, thriving environment | Full medical/dental/vision package, 401k retirement and financial wellness plans | Generous vacation policy: work hard and take time when you need it | Leadership identified as 2019 Billboard Indie Power Player, 2017 Tech Innovator of the year, and highlighted by Entrepreneur Magazine as a builder of one of the most Entrepreneurial companies in America. | We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, or disability status. |  | ﻿About Vydia: | Vydia is an end-to-end platform that monetizes content and handles its own supply chain, distribution, data pipelines, complex rights management, and payments. Since its launch, the Inc. 500 video technology company has played a major role in amplifying multiple genres and further supporting the careers of Lil Pump, Post Malone, Jon Z, and Drake Bell. The company has established strategic partnerships with over 200 digital service providers and applications such as Spotify, Apple Music, TikTok, SoundCloud and networks YouTube, Vevo, BET, MTV, Music Choice, and Facebook. Vydia’s technology-first approach enables them to address the needs of the industry and proactively provide solutions with speed. | Requirements: |  | Strong Python 3 skills (classes, closures, OOP, unit testing) | Efficient SQL with an eye for readability and good style (PostgreSQL and Redshift experience is a plus. DBT or Airflow is also a plus.) | 2+ years of AWS (or related, GCP) experience. | Passionate about the latest developments in technology. | Fast at learning and seeks constructive criticism; experience with Code Reviews | Can dive into a fast-paced environment. | Collaborative team player with the ability to adapt to change quickly; Scrum/Agile methodologies | Can intuitively extract values and insights from data. | Experience interfacing with REST API’s; Oauth, and Rate limiting | Working with deeply nested complex JSON structures. | You can articulate the merits and pitfalls of the different approaches when discussing potential solutions. | Data quality is always top of mind. | You understand what it means to work at a tech startup; hopefully, that excites you more than anything about working here.",Holmdel NJ 07733,Data Engineer
Cigniti Technologies,/rc/clk?jk=3e58bbac9786b6ad&fccid=95cac7b4f6bc0c6f&vjs=3,"If you’re a creative analytical thinker who wants to do undirected research that results in the reports of the future, support all aspects of the business to the next level, and you can look beyond the data to tell a story, then keep reading! | This role is located at our global headquarters in Kenosha, WI (near Milwaukee, WI and Chicago, IL) and we are excited to meet you in person! | For more than 140 years, Jockey has been providing families with quality and comfort they can trust, and will continue to be there for generations to come. The inspirational stories of our everyday heroes exemplify our values of authenticity, courage, hope and family. | Jockey is committed to quality and innovation, and the passionate pursuit to satisfy the human need for comfort continues to be the company’s hallmark. Jockey products are sold in more than 145 countries, making us one of the most well-known apparel brands around the world. | What We Stand For: | Jockey’s culture reflects the values of our leadership. Success is achieved through hard work, mutual respect, and offering consumers products with striking quality and style from a brand they trust. Jockey associates and Jockey products are the genuine article. Retail is where the Jockey Lifestyle Brand meets the community. We were just named one of the Dave Thomas’ Foundation’s top Adoption-Friendly employers. Our non-profit 501c(3) Jockey Being Family organization helps to strengthen adoptive families in a variety of ways. We encourage our employees to take part in their community and enable them to support their volunteer passion with 40 hours of paid volunteer time annually. | The Visualization Data Engineer will use strong technical skills to do undirected research and resolve open-ended questions that results in business benefits. The engineer is responsible for building visualizations and self-service dashboards that synthesize analysis in a clear and concise manner. The engineer will also be responsible for working with other embedded analysts to improve their understanding and access to data available in the data lake. | The right individual will be responsible for: | Leverage data and analytics to uncover trends and identify opportunities to support company-wide growth initiatives across new and existing lines of business | Build visualizations and self-service dashboards that synthesize analysis in a clear and concise manner to drive business decisions and address business questions | Perform rigorous data quality and data validation to ensure accurate and reliable data products | Lead the design, development and maintenance of new datasets that enable other analysts to uncover insights | Communicate best practices with our team and other analysts across Jockey, through training, mentoring, and documenting standards and processes | Collaborate with business leaders and other analysts to identify analytic requirements | First line support for tableau requests and troubleshooting the arise from the business | Perform ad-hoc analysis to address business questions | Essential Experience: | Bachelor’s degree – Business Administration, Supply Chain, Economics, Finance, Computer Science or Statistics | Tableau, Excel pivot tables and PowerBI are necessary for this role | 5+ years of experience in a data engineer role or similar position using logic, knowledge of coding, and report generation. | SQL Server or another SQL platform is necessary | Bonus Experience: |  | Besides strong Excel capabilities, no specific analysis tools are required. | ETL experience is desired | We regret to share that visa sponsorship is not available for this role.",Kenosha WI 53141,Visualization Data Engineer
Consumer Reports,/rc/clk?jk=b5e4614751c62f4a&fccid=cc424b01c194bdf4&vjs=3,"Who are we? | Apollo Neuroscience has developed a first-of-its-kind wearable that is scientifically validated to improve people’s resilience to stress. A spinout of research between the University of Pittsburgh and the University of Pittsburgh Medical Center, our small company is already helping thousands of people all over the world. | As we’re growing, our need for building out a robust data infrastructure is a top priority both for business intelligence as well as product research and development. We’re in a unique position to both help people understand their own mental and physical health better while simultaneously using that information to offer them direct therapeutic help through our revolutionary device. | Most of our team is split between CA and Pittsburgh, PA, but like nearly everyone else, we operate remotely these days and this position can be 100% remote. From the beginning, we’ve always had remote team members and we’re always thinking about how we can improve our processes to make remote work as successful as possible. | Who are you? | As our first data engineer, you’re ready to jump in, build systems from the ground up and think strategically about how best to scale our technology stack. You will be central to establishing our central data warehouse, working with each department to develop and enhance their analytics capabilities, and be ready to scale our data platforms as Apollo Neuro and its data grows. In the beginning, you’ll be taking the existing data streams from our website, mobile app, customer service software, and the device itself and combining those into a system that can be used across the company. This will include marketing and finance as well as physical and digital product design. As our business continues to grow, there will be opportunities to scale this system and help incorporate machine learning to improve business scalability, business operations, customer retention and in the product itself for the personalization of the therapy and its delivery. | The ideal candidate will show a strong passion for diving into uncharted territory while also displaying a deep understanding of how to engineer scalable data systems and empower team members across the organization to utilize that data in meaningful ways. | In your day-to-day, you will work closely with representatives of each department from product, marketing, customer service, finance, and operations to create a system that generates meaningful data for each department as well as an overall picture for our Chief Operations Officer and Chief Executive Officer. | Responsibilities include: | The design and development of a suite of cloud-based services, stores and connectors supporting Apollo’s immediate analytics requirements while also being capable of expansion to support future needs. | Maintenance of the Apollo data stack and monitoring of data quality and accuracy. | Analysis of performance and health of the platform (e.g. reporting operational gaps). | Perform ad hoc data analyses as required to troubleshoot data-related issues and assist in their resolution. | Maintaining a deep understanding of existing cloud services supporting data processing and analysis. | Keeping up to date with the state of the art in data processing trends and technologies. | Developing standards with collaboration from other departments on best practices to maintain and improve our data sources and overall quality. | Assist personnel from other departments in developing workflow solutions that expand Apollo’s analytics capabilities and enhance internal clients’ ability to execute timely responses to analysis results. | Your qualifications: | At least 5+ years of experience with data engineering. | Should have proficient experience in Python or equivalent programming languages. | Working knowledge of SQL and NoSQL databases (especially Redshift and DynamoDB). | Experience developing scalable custom data pipelines. Airflow or other ETL tool experience is a plus. | Experience fusing data from heterogeneous data systems. | Excellent deductive reasoning ability. | Strong problem-solving skills. | Solid communication skills and a sense of customer service. | Other things we’re looking for: | Pandas experience is a plus. | AWS, Docker experience is a plus. | An eye for security as it relates to data storage and transport. | Experience interacting with personnel from all major functional areas of a business. | Previous employment with a company whose products required FDA approval. | Experience with GDPR, CCPA and HIPAA related adherence is a plus. | Powered by JazzHR | rfJS08jX4Z",Remote,Senior Data Engineer
Wells Fargo,/rc/clk?jk=92eb149499ff6d0b&fccid=78bbcd26e39621f5&vjs=3,"Job Description | Important Note: During the application process, ensure your contact information (email and phone number) is up to date and upload your current resume when submitting your application for consideration. To participate in some selection activities you will need to respond to an invitation. The invitation can be sent by both email and text message. In order to receive text message invitations, your profile must include a mobile phone number designated as 'Personal Cell' or 'Cellular' in the contact information of your application. | At Wells Fargo, we want to satisfy our customers' financial needs and help them succeed financially. We're looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you'll feel valued and inspired to contribute your unique skills and experience. | Help us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you. | Wells Fargo Technology is a team of more than 40,000 information technology and security professionals who help keep Wells Fargo at the forefront of America's diversified financial services companies. Employees execute an engineering-led IT strategy to deliver stable, secure, scalable and innovative services that provide Wells Fargo global customers ‘round-the-clock' banking access through in-store, online, ATM, and other channels. Wells Fargo Technology plays a critical role in the company's customer and employee experience, business and risk management transformation, and growth agenda. | Identity and Access Management's (IAM) vision is to provide Wells Fargo world leading cyber security risk management. Through a framework that addresses policy, process, operations, people, and technology, EIS protects Wells Fargo's infrastructure, corporate data, and customer assets, and ensures alignment with applicable regulations and laws. EIS is part of Wells Fargo's Corporate Risk organization and is led by the Chief Information Security Officer. | We are looking for an information security engineer to assist with our data driven, machine learning automation efforts for the Identity and Access Management processes. This person will provide data engineering activities, including but not limited to, the capture of key data and the development and automation of said data for automated intelligence for business processes. Prefer a candidate that has extensive knowledge in data lakes, data warehousing, data modeling and data integration for automating of analytics integration within business processes. Should have knowledge with various Identity and Access data elements and various IAM functions. | This position will support the IAM Organization in data process and engineering within the Enterprise Information Security (EIS) and IAM organizations for information needs. | Will be responsible for: | Designing, documenting, testing, maintaining, and providing issue resolution recommendations for moderately complex security solutions related to networking, cryptography, cloud, authentication/directory services, email, internet, applications, and/or endpoint security. Provides security consulting on medium projects for internal clients to ensure conformity with corporate information security policy, and standards. Leads computer security incident response activities for moderately complex events, conducts technical investigation of security-related incidents and conducts post-incident digital forensics to identify causes and recommend future mitigation strategies. Reviews and correlates security logs. Identifies security vulnerabilities/issues, performs risk assessments, and evaluates remediation alternatives. Possesses subject matter expertise in industry leading security solutions and best practices used to implement one or more components of information security such as availability, integrity, confidentiality, risk management, threat identification/modeling/monitoring, incident response, access management, and business continuity. May interface with senior management. | Deliver data process consistency across Wells Fargo EIT supporting Access Management initiatives | Works with a variety complex applications and/or application initiatives. | Utilizes an understanding of data and data design to provide solutions enabling the business. | Build and maintain the data engineering and design for the automation of IAM data capture and machine driven analytics | Build and maintain strong relationships within a variety of teams throughout the organization. | Assist with building sustainment reports to monitor EAM compliance with policies. | Successful candidate will work closely with the various stakeholders within IAM, Information Security Management, Risk, and with business lines and technology leadership across the enterprise in the execution of the WF strategies/objectives. Accordingly, critical success factors will include the ability to effectively engage in a matrixed organization, develop partnerships with many business and functional areas, and have a strong operational and delivery focus. | Required Qualifications | 5+ years of information security applications and systems experience | 4+ years of information technology applications and systems experience | 3+ years of experience with end-to-end design and delivery of data warehouse applications | 3 + years of experience supporting or designing complex ETL production environments | 2+ years of Agile experience | 4+ years of experience with databases such as Oracle, DB2, SQL server, or Teradata | Desired Qualifications | Strong verbal, written, and interpersonal communication skills | Ability to assess issues, make quick decisions, implement solutions, and influence change | Knowledge and understanding of business intelligence development and reporting | Strong customer relationship management skills | Experience with end-to-end design and delivery of data warehouse applications | Knowledge and understanding of BI platform and tools such as: SSAS, SSIS, SSRS, and TSQL | Knowledge and understanding of IAM (Identity and Access Management) | Data modeling experience | Knowledge and understanding of data visualization | Big Data experience | Other Desired Qualifications |  | 3+ years of experience with ETL tools such as NDM, Autosys, SSIS, etc | 3+ years of experience with various reporting tools, SSRS, and other Business Intelligence (BI) tools. | 5+ years of experience working with a variety of data sources to including SQL, Oracle, and various data mart's. | Ability to transform conceptual design to technical implementation | Ability to identify challenges, anticipate obstacles, influence and resolve issues. | Good communication skills (written and spoken). | Analytical skills with a keen ability to see how to translate needs of the teams into tangible deliverables. | High level understanding of various development data technologies and development environments. | Bachelor's degree, preferably in a technology related field, or four (4) plus years of experience.",New York NY,IAM Data Analytics Engineer
Glu Mobile,/company/Info-Way-Solutions/jobs/Senior-Data-Engineer-f60939aa75a7d7b2?fccid=bbc97761a95c4678&vjs=3,"Job detailsSalaryUp to $130,000 a yearJob TypeFull-timeContractNumber of hires for this role2 to 4QualificationsBachelor's (Preferred)Kafka: 5 years (Preferred)US work authorization (Preferred)Full Job DescriptionDeveloping mission-critical systems that help keep people safe is what we do. At General Dynamics Mission Systems, you’ll be part of the team that helps heroes make a true impact. The work we do is important. The challenges we face are career-defining. The opportunity we can offer is one-of-a-kind.We apply advanced technologies such as Artificial Intelligence, Blockchain, AR/VR, Cloud Native and Quantum Physics to solve our customers’ missions in cyber, RF, undersea, interstellar space and everything in between.As a Senior Data Engineer, you’ll lead model and simulation activities as you participate in requirements analysis and management, functional analysis, performance analysis, system design, trade studies, systems integration and test (verification). It’s your chance to step up to the challenge and prove you’re ready to lead the world.REPRESENTATIVE DUTIES AND TASKS:We are seeking a Senior Data Engineer to support the Insider Threat mission. Data Engineers work with various security system data owners to automate data integration and collection strategies. Work closely with the data science team to ensure data cleanliness and accuracy.Support data science team by designing, developing and implementing scalable ETL process for disparate datasets into a Hadoop infrastructureDesign, develop, implement and maintain data ingestion process from various disparate datasets using StreamSets (experience with StreamSets not mandatory)Develop processes to identify data drift and malformed recordsDevelop technical documentation and standard operating proceduresMentor new and junior data engineersLeads technical tasks for small teams or projectsKNOWLEDGE SKILLS AND ABILITIES:Working knowledge of entity resolution systemsExperience with messages systems like KafkaExperience with NoSQL and/or graph databases like MongoDB or ArangoDBAny of the following databases: SQL, MongoDB, Oracle, PostgresWorking experience with ETL processingWorking experience with data workflow products like StreamSets or NiFiWorking experience with Python RESTful API services, JDBCExperience with Hadoop and Hive/ImpalaExperience with Cloudera Data Science Workbench is a plusUnderstanding of pySpark Leadership experienceCreative thinkerAbility to multi-taskExcellent use and understanding of data engineering concepts, principles, and theoriesJob Types: Full-time, ContractSalary: Up to $130,000.00 per yearSchedule:8 hour shiftAbility to Commute/Relocate:Reston, VA (Preferred)Education:Bachelor's (Preferred)Experience:Kafka: 5 years (Preferred)Full Time Opportunity:YesWork Location:Multiple locationsVisa Sponsorship Potentially Available:Yes: Immigrant visa sponsorship (e.g., green card sponsorship)This Job Is:A job for which military experienced candidates are encouraged to applyA good fit for applicants with gaps in their resume, or who have been out of the workforce for the past 6 months or moreA good job for someone just entering the workforce or returning to the workforce with limited experience and educationA job for which people with disabilities are encouraged to applyCompany's website:Info Way SolutionsCompany's Facebook page:Info Way SolutionsWork Remotely:Temporarily due to COVID-19COVID-19 Precaution(s):Remote interview process",Reston VA,Senior Data Engineer
Premera Blue Cross,/rc/clk?jk=90189607711e5978&fccid=adea62ae96d0c2c1&vjs=3,"The Opportunity: |  | In this role, you will document test cases based on business requirements, and prepare and execute the test plan that covers functional, integration, data quality, browser and systems compatibility and automated testing. As a QE with Consumer Reports, you will also be responsible for ensuring that the application meets the business requirements and that any potential issues are identified and fixed before deployment. |  | Only local (tri-state) candidates will be considered for this position, as it will eventually be based in our Yonkers, New York office. | What you will do: | Review and analyze business requirements, functional and design specifications and story acceptance criteria for clarity and consistencies. | Develop and document detailed test cases for functional and regression testing against applications that span across various endpoints and environments. | Create and execute formal test plans to ensure validated deliverables meet business requirements, functional and design specifications, and story acceptance criteria. | Conduct Quality Assurance testing from development to product release. | Integrate changes to the automation scripts when there are functional or UI changes. | Identify, reproduce, and report defects in the defect tracking system. | Verify application defects, and validate all issues are resolved and closed in a timely manner. | Collaborate with developers, business staff or other project team members to troubleshoot issues as required. | You will work on projects that span across teams, platforms, and systems. | You will make recommendations and process improvements to the data engineering team | About You: | You have a Bachelor’s in CS, IT or other related field or equivalent work experience in addition to 1 year (excluding internships) of IT and business experience in QA/QE, with a focus on data intensive applications (Databases, ETLs, APIs etc). | You have an expertise with executing automated and manual test plans and test scripts/cases, as well as reporting and tracking defects while performing Regression, Integration, Black Box and User Acceptance Tests (UAT) as well as extensive debugging experience with data applications. | Expertise in programming using a combination of SQL,Python, and Unix Shell Scripting | You are an expert in querying Relational &amp; NoSQL databases (Oracle, MongoDB), and different formats of raw data files (csv,json,xml etc) for exploratory data analysis (EDA), data profiling and data reporting using python notebooks (Jupyter notebooks etc) | Expertise in developing code packages for automation of various data quality checks through different stages of ETLs &amp; data pipelines, Rest APIs using frameworks/libraries (ex. pytest, Pandas) | Experience working with AWS Cloud services (S3,EC2) | Diversity, Inclusion, and Belonging at Consumer Reports: At CR, we believe our continued ability to understand and advance the interests of all American consumers is possible only if our staff fully reflects the full cultural, racial and ethnic diversity of those consumers. Consumer Reports is committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, national origin, gender, sex, gender identity or expression, sexual orientation, age, citizenship, marital or parental status, disability, veteran status, or other class protected by applicable law. We are proud to be an equal opportunity workplace.",Yonkers NY 10703,Data Quality Engineer
CVS Health,/rc/clk?jk=c4e49a43cf656839&fccid=3e3eaa6aedf690a3&vjs=3,"Database Engineer II, Data Platforms - HBO Max | Location Seattle, Washington | Business: HBO | Position Type: Full Time | Job ID 180613BR | HBO Max is the future of entertainment and storytelling. |  |  | WarnerMedia’s streaming entertainment offering, HBO Max is the culmination of some of the most innovative new technology and greatest creative talent in the industry. Anchored by the entire HBO service, arguably the greatest brand in television, HBO Max also features fan favorites from the WarnerMedia library including Warner Bros., New Line, DC, CNN, TNT, TBS, truTV, Turner Classic Movies, Cartoon Network, Adult Swim, Crunchyroll, Rooster Teeth, Looney Tunes, and more. HBO Max will also be home to key third-party library acquisitions, such as Friends, South Park and Doctor Who, and more than 50 exclusive Originals in its first year, from the likes of groundbreaking creative talent such as J.J. Abrams, Ridley Scott, Jordan Peele, Steven Soderbergh, Michael Mann, Mindy Kaling, Reese Witherspoon, Anna Kendrick, Greg Berlanti, Melissa McCarthy, Robert Zemeckis, Ellen DeGeneres, and other visionaries. |  |  | The Job |  | We are the team that architects and manages the backend datastores for the streaming platforms like HBO Go, HBO Now and the future HBO Max that enables real-time targeted customer experiences and dynamic business decisions. Our current data processing infrastructure is positioned for our next generation of technology. |  | We work with Software Engineers and Site Reliability Engineers to build reliability, performance and security into our streaming products. We champion infrastructure as code and strongly believe that automation is the key to delivering reliable systems. |  | The Daily |  | Play a key role in a team that designs, defines and executes end-to-end support and performance tuning of SQL and NoSQL databases for our DTC platform | Participate in product direction and road-map planning, project execution scheduling, and hiring engineers to develop cutting-edge database services | Work with other engineers on your team to define the next generation data architecture for our DTC streaming platform, HBO Max; including identifying gaps in the existing system and designing alternatives that are scalable, robust and easy to maintain | Design, build and maintain end-to-end data platforms in an Infrastructure-As-Code environment | Achieve operational simplicity while scaling internationally through automated deployment and configuration management capabilities. | Plan capacity, and optimize system for cost while maintaining ability to handle peak load in an elastically-scaled fashion. | Improve system reliability and performance based on quantitative metrics. | Participate in cross-functional data integration efforts as Data SME | Collaborate with engineering teams and key stakeholders across the organization to continuously acquire new data, detect and fix data quality issues | Propose and architect data solutions based on requirements | Work as part of a team on scheduled and ad-hoc projects. |  | The Essentials |  | Have a demonstrated ability to work in a team of highly talented individuals, grow collaboration in a distributed environment and have fun | Have 3+ years of top-tier database development experience | Expertise with core Computer Science fundamentals, programming, algorithms, and data structures | Strong programming experience with SQL, NoSQL. | Database tuning and optimization | Strong knowledge of database architecture and administration | Have 3+ years of strong experience with Python and/or Shell scripting | Have 3+ years of strong experience with PostgreSQL and/or Cassandra |  | The Nice to Haves |  | Hands-on experience in automation processes for database creation, maintenance, and rebuild | Experience working in Cloud data center environments | Ability to identify key metrics and build resilient processes to detect and minimize issues | Strong attention to detail and ability to identify downstream effects of design on data mining, business intelligence, and analysis | Great communication and collaboration skills and a team-focused approach towards software development | Experience with version control systems (GitHub), CI/CD approaches, and build/deployment pipleline tools (Jenkins) | Media, entertainment, and/or games industry experience is a plus | Familiarity with Log Aggregation systems (ex Splunk, ELK) and processing stream-based data (ex Kafka, Lambda etc) | Experience with DynamoDB and Redis | Experience working on or with a distributed team separated by time zones | Broad exposure to AWS DB services | Prior experience setting up Prometheus or Graphite metrics as well as familiarity with Grafana for graphing and alerting | The Perks | Exclusive WarnerMedia events and advance screenings | Paid time off every year to volunteer | Access to well-being tools, resources, and freebies | Access to in-house learning and development resources | Part of the WarnerMedia family of powerhouse brands | WarnerMedia is a leading media and entertainment company that creates and distributes premium and popular content from a diverse array of talented storytellers and journalists to global audiences through its consumer brands including: HBO, HBO Now, HBO Max, Warner Bros., TNT, TBS, truTV, CNN, DC Entertainment, New Line, Cartoon Network, Adult Swim, Turner Classic Movies and others. |  | Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. | Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. |  | Warner Media, LLC and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law.",Seattle WA,Database Engineer II Data Platforms - HBO Max
Northwell Health,/rc/clk?jk=51aab7b3953958be&fccid=6f31e2074a3d8a7b&vjs=3,"Job detailsJob TypeContractFull Job DescriptionJob Description | Analyzes and implements information systems to manage financial functions for Patient Financial Services/Accounts. Ensures systems compliance to applicable laws, regulatory standards and policies. | Job Responsibility | Implements the information system initiatives to meet user requirements; communicates with user departments and project teams regarding implementation activities and system changes to ensure feasibility; develops system specifications that appropriately meet department requirements. | Assesses the patient management inpatient and outpatient registration pathways for constant improvement and enhancement; incorporates financial regulatory requirements; administers system support for patient management system users; evaluates information technology to promote operational efficiency and financial reimbursement. | Oversees managed care contracts; compiles and tracks managed care financial and statistical information related to contract development and financial impact analysis; maintains rate database and contractual terms of managed care contracts (prosthetics and implants, MR copying charges, etc.). | Develops systems to track data in comprehensive and innovative ways; collaborates with hospital departments and outside vendors to facilitate managed care contract implementation; facilitates the implementation of managed care contracts. | Responds to inquiries and interprets contract language as it relates to managed care; coordinates and maintains communication between the hospital and vendor(s) regarding system applications; coordinates communication with the vendor representative regarding enhancements, issues, etc. for the system applications. | Reviews contractual obligations, ensures status of deliverables, and resolves related issues; performs periodic post-installation reviews of system initiatives. | Reviews and evaluates post-installation processing per documented specifications; creates ad-hoc reports to ensure system builds are functioning according to specifications and ensures accurate payments. | Develops and conducts in-services for Patient Financial Services/Accounts; creates appropriate in-service training materials. | Conducts system-wide in-services with managers and staff, as necessary; ensures systems compliance to applicable laws, regulatory standards and policies. | Operates under general guidance and work assignments are varied and require interpretation and independent decisions on course of action. | Performs related duties as required. All responsibilities noted here are considered essential functions of the job under the Americans with Disabilities Act. Duties not mentioned here, but considered related are not essential functions. | Job Qualification | Bachelor's Degree required, or equivalent combination of education and related experience | Minimum of two (2) years of experience with Apache Hadoop, NoSQL, setting up cloud clusters, Apache Spark, and other advanced data science and big data technologies, required. | Experience in software development in enterprise/ web/ cloud applications, solutioning, architecture and frameworks. | Big data expertise with cloud and enterprise level design/implementation. | Experience in architecting data warehouses and/or data lakes with traditional database enterprise-class RDBMS technologies. | Strong knowledge of programming languages/tools including: Java, Python, Spark, SQL, R, and Shell Scripts.",Lake Success NY 11020,Data Engineer
M&T Bank,/rc/clk?jk=e626edca7dd646b1&fccid=0e36880b18d20c75&vjs=3,"At M&amp;T Tech , we’re a team of makers, doers, and builders, working to create the most advanced technology solutions in banking. We’re not your stereotypical suit and tie bankers: We’re an innovative team of leading tech experts, pushing boundaries, and taking risks. We’re building an agile team of the most skilled and creative, working to solve complex problems, architecting solutions, writing high-performance software, and charting our new path, all to make the lives of our customers, and the communities that we serve better. Join us and be part of something new as we build tomorrow’s bank, today. | The Enterprise Architecture, Data, and Engineering Group is responsible with enabling our technology partners across the Bank with a suite of best-in-class Platforms, Tools, and Products; allowing them to deliver rich customer solutions all while decreasing their time to market. In addition, the Group creates significant value by providing an advisory and governance function on these capabilities, so technology delivery teams can focus on the “What” not the “How”. | Overview: | This position is an opportunity to build the data framework with HDFS, Spark and Kafka technology for batch and streaming analytics. Data processes in BDM, if appropriate, include ingestion, standardization, metadata management, business rule curation, data enhancement, and statistical computation against data sources that include relational, XML, JSON, streaming, REST API, and unstructured data. BDM has provided a metadata injection layer for data ingestion across six SQL Server sources that will be leveraged for additional source ingestion. | Scope of Responsibilities: | Understand, prepare, process and analyze data to drive operational, analytical and strategic business decisions. You will create, modify and maintain both Sqoop and BDM code and complex SQL for BI/DW data flows. You will program in Spark and Python where BDM Spark co-generation is not adequate. Build end to end data flows from sources to fully curated and enhanced data sets. This can include the effort to locate and analyze source data, create data flows to extract, profile, and store ingested data, define and build data cleansing and imputation, map to a common data model, transform to satisfy business rules and statistical computations, and validate data content. | Tech Stack: | Hadoop | Informatica tools: PowerCenter, MDM, Big Data Manager | Python | Java | Scala | Spark | Kafka | Hive | HBase | Databases: SQL, NoSQL, Postgres, Cassandra | AWS cloud services: EC2, EMR, RDS, Redshift | Stream-processing systems: Storm, Spark-Streaming | The successful candidate will have: | Combined minimum of 6 years’ higher education and/or work experience in systems design, management and/or architecture | Experience with Agile Methodology | An ability to build out data products &amp; product enhancements from idea through to launch | Strong collaboration with technology partners and customers on feature requirements and prioritization | A team player mindset with an ability to thrive and effectively communicate in a fast-paced, constantly evolving environment | About M&amp;T: | M&amp;T Bank is a Top 20 US bank holding company and one of the best performing and financial stable regional banks in the country, we offer our technology employees a wide range of performance-based career development opportunities. We have a strong commitment to our customers and the communities we serve, and we continue to grow with a focus on the future. So, when looking to advance your career, look to M&amp;T. Grow with us. | Location | Buffalo, New York, United States of America",Buffalo NY,Data Engineer
IBM,/company/Human-Capital-Resource/jobs/Data-Platform-Engineer-2092791d4ab53cd4?fccid=8a408433383a9b67&vjs=3,"Job detailsSalary$120,000 - $180,000 a yearJob TypeFull-timeNumber of hires for this role1QualificationsBachelor's (Preferred)ETL Pipelines: 4 years (Preferred)Ruby: 4 years (Preferred)AWS S3 and/or AWS Glue: 4 years (Preferred)Spark: 4 years (Preferred)Kafka: 5 years (Preferred)Machine Learning: 5 years (Preferred)Full Job DescriptionThis client is looking for an accomplished, enthusiastic, and driven engineer with experience building data processing and storage systems. Our ideal candidates have architected and deployed systems to support multiple (small) engineering teams with specific needs and they enjoy a large degree of autonomy and ownership a company's data infrastructure.ResponsibilitiesDesign and develop data pipelines, ETL, storage solutions, and workflows that are optimized for speed, fault-tolerance, and scalabilityWork with Application, Machine Learning, and Site Reliability/DevOps engineers to create systems that support their varied data needs while allowing for independent manipulation and iteration of dataDefine robust data schemas for the rapid intake and processing of customer data with diverse structuresSupport product-focused engineering teams with data infrastructure, APIs, and scalable deploymentsArchitect and author internal libraries for use by fellow engineersHelp create data analytics tools for software telemetry and business intelligence purposesCultivate a better understanding of data handling best practices across engineering teamsCollaborate on security efforts for customer dataQualifications4+ years of experience writing code in one of: Ruby, Go, Python, Scala, Elixir, Java, or similar languages at a SaaS companyStrong understanding of relational and non-relational databases such as PostgreSQL, ElasticSearch, and RedisAbility to organize and model data to support varied use casesExperience creating and deploying container-based softwareFamiliarity with asynchronous data processing patterns with an added focus on monitoring and loggingPrior experience working with AWS or a similar cloud providerA BS/MS in computer science or related field of study, or equivalent experienceAbility to communicate ideas to technical and non-technical colleaguesBeneficial ExperienceExperience designing, building, and maintaining highly distributed or event-driven systemsExperience supporting Machine Learning engineers with data preparation, validation, annotation, and model evaluationPrevious work with workflow management and/or task scheduling systemsPrior use of Terraform/Ansible/Infrastructure as Code toolsAbout youYou have strong opinions about technology and the facts to back it upYou welcome healthy but respectful debateYou know the differences between: data warehouses and data lakes, schema-on-read and schema-on-write, relational and non-relational databases, batch and stream processingThe thought of code sitting undeployed for more than a week sends shivers up your spineYou want to be go-to subject matter expert for data-related questionsJob Type: Full-timePay: $120,000.00 - $180,000.00 per yearBenefits:401(k)401(k) matchingDental insuranceHealth insuranceLife insurancePaid time offVision insuranceSchedule:8 hour shiftMonday to FridaySupplemental Pay:Signing bonusEducation:Bachelor's (Preferred)Experience:ETL Pipelines: 4 years (Preferred)Ruby: 4 years (Preferred)AWS S3 and/or AWS Glue: 4 years (Preferred)Spark: 4 years (Preferred)Kafka: 5 years (Preferred)Machine Learning: 5 years (Preferred)Work Location:Fully RemoteCompany's website:https://www.hc-resource.com/Company's Facebook page:https://www.facebook.com/HumanCapResource/COVID-19 Precaution(s):Remote interview processSocial distancing guidelines in placeVirtual meetingsSanitizing, disinfecting, or cleaning procedures in place",Remote,Data Platform Engineer - Remote within US
Human Capital Resource,/rc/clk?jk=90c2962f205898f2&fccid=c594c442f5397e7b&vjs=3,"GitHub is looking for engineers to join our Data Infrastructure team. You'll be part of a team building and deploying large scale data and object storage for the world's largest code hosting platform |  | The Data Infrastructure team is highly distributed and we thrive in an environment of remote work and asynchronous communication. As a member of our team, you'll always be challenged by interesting and novel problems that have real impact on how the world builds software. |  | Responsibilities: |  | Build services and systems that empathetically and pragmatically meet real operability needs of GitHub developers | Use data to understand the availability, reliability, and sustainability of our infrastructure | You will respond to the needs of users and of other developers at GitHub. | Work closely with other teams from across the organization |  | Minimum Qualifications: |  | Experience building and deploying large, complex distributed systems with an eye toward reliability. | Proficiency in Golang, Python, and/or Ruby. | You take a pragmatic approach to decision making and design choices. |  | Preferred Qualifications: |  | Experience building highly available services at scale. | You have developed and scaled services in Go. | Experience diagnosing and resolving complex multi-system performance problems. | Experience with Docker and container orchestration systems. |  | Who We Are: |  | GitHub is the developer company. We make it easier for developers to be developers: to work together, to solve challenging problems, and to create the world’s most important technologies. We foster a collaborative community that can come together—as individuals and in teams—to create the future of software and make a difference in the world. |  | Leadership Principles: |  | Customer Obsessed - Trust by Default - Ship to Learn - Own the Outcome - Growth Mindset - Global Product, Global Team - Anything is Possible - Practice Kindness |  | Why You Should Join: |  | At GitHub, we constantly strive to create an environment that allows our employees (Hubbers) to do the best work of their lives. We've designed one of the coolest workspaces in San Francisco (HQ), where many Hubbers work, snack, and create daily. The rest of our Hubbers work remotely around the globe. Check out an updated list of where we can hire here: https://github.com/about/careers/remote |  | We are also committed to keeping Hubbers healthy, motivated, focused and creative. We've designed our top-notch benefits program with these goals in mind. In a nutshell, we've built a place where we truly love working, we think you will too. |  | GitHub is made up of people from a wide variety of backgrounds and lifestyles. We embrace diversity and invite applications from people of all walks of life. We don't discriminate against employees or applicants based on gender identity or expression, sexual orientation, race, religion, age, national origin, citizenship, disability, pregnancy status, veteran status, or any other differences. Also, if you have a disability, please let us know if there's any way we can make the interview process better for you; we're happy to accommodate! |  | Please note that benefits vary by country. If you have any questions, please don't hesitate to ask your Talent Partner. |  | #LI-POST",Remote,Data Infrastructure Engineer
Github,/rc/clk?jk=135eca8cd18989b1&fccid=cd9121ddd97de9e8&vjs=3,"As a data engineer on the BI team, you will be responsible for architecting, building and maintaining the data infrastructure for company-wide BI and cross-functional teams at Addepar. We are building the environment to unify data from different data sources, report on transformed data sets, and enable data-driven decisions. |  | You will design, build and orchestrate tools and data pipelines, integrate data from disparate data sources to be able to load the data into dimensional models in the data warehouse. You will also be responsible for building a strong data ecosystem by integrating different data systems with the data warehouse and BI tools. |  | Responsibilities: |  | Build realtime and batch data integrations to integrate data from disparate source systems into the DW | Build and maintain scalable ELT/ETL processes using workflow automation tools | Designing dimensional models and be able to cleanse and transform raw data into structured format | Administer and maintain data pipeline tools and data warehouse | Provide production support for data integration and transformation pipelines | Partner with analytics engineer and data analysts to build tooling to be able to effectively move data in and out of the data warehouse/data pond | Champion the strategy to build data security framework to store, move and access data | Establish best data practices and formalizing data governance framework | Scale and tune data pipelines, databases and SQL queries | Build and maintain data reconciliation processes |  | Requirements: |  | 5+ years of experience as a data engineer and/or software engineer in a SaaS or financial services business | 2+ years of experience working with workflow orchestration tools( Preferred: Airflow, Prefect OR Luigi, autosys, etc) | 4+ years of experience building ELT/ETL processes using one or more tools(E.g. Python, Informatica, SSIS, Pentaho, etc) | Experience working with and administering one or more columnar storage systems (Preferred: Snowflake OR Redshift, Bigquery, etc) | Knowledge and skills with Looker will be important, not critical | Must have completed at least one full cycle of building a data lake and/or a data warehouse | Exposure to data streaming tools and technologies like kafka | Experience working in cloud data environments(Preferred: AWS OR GCP, Azure, etc) | Strong software development skills in Python and SQL | Experience tuning SQL queries and data pipelines | Ability to multi-task and change priorities | Stellar communications skills for requirements gathering |  | Nice to have: |  | You have built Data Lakes in S3 or Hadoop | Experience working with CRM systems like Salesforce | Exposure to BI/reporting tools ( Preferred:Looker OR Tableau, Sisense/Periscope, Cognos, etc) | Building reports and dashboards using BI tools | Basic knowledge of statistics | Experience working with with SaaS or Fintech companies is a plus |  |  | Addepar is a wealth management platform that specializes in data aggregation, analytics and reporting for even the most complex investment portfolios. Founded in 2009 by Joe Lonsdale, who currently serves as an active Chairman of its Board of Directors and General Partner at 8VC, the company's platform aggregates portfolio, market and client data all in one place. It provides asset owners and advisors a clearer financial picture at every level, allowing them to make more informed and timely investment decisions. Addepar works with hundreds of leading financial advisors, family offices and large financial institutions that manage data for over $2 trillion of assets on the company's platform. In 2020, Addepar was named as a Forbes Fintech 50 company and honored as a member of the CB Insights Fintech 250. Addepar is headquartered in Silicon Valley and has offices in New York City and Salt Lake City. All brokerage services offered through Acervus Securities Inc., member FINRA / SIPC. |  | Addepar is proud to be an equal opportunity employer. We seek to bring together diverse ideas, experiences, skill sets, perspectives, backgrounds, and identities to drive innovative solutions. We commit to promoting a welcoming environment where inclusion and belonging are held as a shared responsibility. |  | In order to ensure the health and safety of all Addepeeps and our prospective candidates, we have instituted a virtual interview and onboarding experience.",New York State,Data/Analytics Engineer
HBO,/rc/clk?jk=9aa47191db5d1c17&fccid=a5ea7db1292ddde6&vjs=3,"The Data Engineering team at Glu builds core data infrastructure and applications in support of all areas of our business, including our studio teams, user acquisition, monetization and finance. Glu is passionate about maximizing the value that data and analytics can provide to the business and is aggressively investing in new capabilities. Our team covers a lot of ground from data ingestion through to machine learning applications. |  | We leverage a cutting-edge tech stack to build both batch systems (YARN+Spark/Hive) and stream processing applications (Kinesis/Flink/Spark Streaming/Druid) that operate efficiently at high scale. The ideal candidate has a strong engineering background and has built robust data platforms and pipelines and takes complete ownership of their area of expertise. This is a fantastic opportunity to use your engineering skills to make a material impact on a highly valued analytics platform | You'll most often be: | Taking ownership of and developing critical new features for our next-generation analytics platform, supporting Glu's worldwide studios and central functions such as marketing and finance. | Building scalable, accurate and extensible stream processing applications using cutting-edge technology such as Spark Streaming and Apache Flink. | Implementing complex and highly scalable end-to-end data pipelines, using Elastic Beanstalk, Kinesis, EMR, Spark, Hive, Druid, Cassandra. | Building data support for our personalization and experimentation efforts, solving problems from statistical test automation to building real-time M/L applications. | And your skills and experience include: | Bachelor's degree in computer science/mathematics/engineering, or other fields with proven engineering experience. | Software engineering experience, especially working on back-end data infrastructure. | Proficiency with at least one of the following languages: Java, Python, Scala. | Experience with distributed stream processing technologies such as Flink, Spark Streaming and/or Kafka Streams. | Experience with AWS Ecosystem, especially Kinesis, EMR, Lambda, and Glue. | Knowledge of NoSQL application data stores i.e. Druid, HBase, Cassandra, DynamoDB, Redis. . | Bonus Points | Experience with high-scale machine learning, I.e. Spark M/L, SageMaker, etc. | Experience with SQL and SQL-like languages, especially Hive. | Experience with CI/CD process, testing framework, and containerization technology | Experience building data-rich web applications, especially with technologies like Angular, Node.js, and Elastic Beanstalk",San Francisco CA,Associate Data Engineer
Support.com,/company/Pantheon-Inc/jobs/Senior-Data-Engineer-2074068a04bfe8c7?fccid=b15624d64f897006&vjs=3,"Job detailsJob TypeFull-timeContractNumber of hires for this role1QualificationsApache Spark: 5 years (Preferred)GitLab: 5 years (Preferred)Linear Ad sales: 6 years (Preferred)Python: 6 years (Preferred)AWS Big data lake: 7 years (Preferred)Python Plugins /operators like FTP Sensor, Oracle Operator: 6 years (Preferred)US work authorization (Preferred)Full Job DescriptionTop must have skills:1. Snowflake2. Linear Ad sales experience3. Spark, Airflow w/ linear ad sales4. GitlabOverview - This is remote anywhere - any time zone but must be cognizant of meetings. The project will be dealing with linear data- data comes in and they have to process and support stakeholders- data transfer and data analytics.Basic QualificationsStrong in Python scripting, minimum 4+ yrs,Must have hands on experience implementing AWS Big data lake using EMR and SparkStrong exp with Snowflake Database Architecture , SQL , Database performance/Optimization .Experience with Airflow tool and DAG's creation, Jobs Orchestration.Good to have Media AdSales experience .Experience leveraging open-source big data processing frameworks, such as Apache Spark.Experience developing and deploying data pipelines within a cloud native infrastructure preferably AWSExperience in using CI/CD pipeline (Gitlab)Experience in Code Quality implementation (Used Pep8/Pylint) tools or any other code quality tool.Experience of Python Plugins /operators like FTP Sensor, Oracle Operator etc.Implement Industry Standards /Best Practices.Excellent analytical and problem-solving skillsExcellent verbal and written communication skills***Applicants MUST possess the ability to work in the US without sponsorship. Visa sponsorship is not available for this position.***Job Types: Full-time, ContractPay: $0.00 per hourSchedule:Monday to FridayExperience:Snowflake Database Architecture: 5 years (Preferred)Airflow tool and DAG's creation: 6 years (Preferred)Media AdSales: 5 years (Preferred)Apache Spark: 5 years (Preferred)GitLab: 5 years (Preferred)Linear Ad sales: 6 years (Preferred)Python: 6 years (Preferred)AWS Big data lake: 7 years (Preferred)Code Quality implementation: 5 years (Preferred)Python Plugins /operators like FTP Sensor, Oracle Operator: 6 years (Preferred)Work Location:Fully Remote",Remote,Senior Data Engineer
Facebook,/rc/clk?jk=adea4d8812f4cbe5&fccid=1639254ea84748b5&vjs=3,"Facebook is seeking a Data Center Infrastructure Management (DCIM) Engineer to join our Data Center Facility Operations team. Our data centers serve as the foundation upon which our software operates to meet the demands of our customers. Our Data Center Management System platform enables monitoring and analysis of Facebook’s global data center footprint and is critical to monitoring the real-time power, cooling, and environmental data. The candidate will support the establishment and maintenance of the controls and monitoring infrastructure for new site deployments and retrofits, improve and develop quality assurance and device management methods and procedures, and resolve escalated production issues. The ideal candidate shall be able to work independently through deep dives and expert analyses. This position is fulltime and approximately 10% of the time will be on site at our data centers. | Coordination with cross functional partners to ensure DCIM data quality | Coordinate with design and construction partners to assist with the deployment, configuration, and commissioning of DCIM tooling | Analyze and resolve production issues independently and as a member of a cross functional team | Provide DCIM industry expertise in the development of new and enhanced tooling, working closely with end users and cross functional partners | Support data center controls and monitoring applications | Develop and maintain processes and procedures associated with the management of the controls and monitoring applications | Train end users on the proper operation and maintenance of controls and monitoring applications | 8+ years of experience in two or more of the following areas: | Electrical Power Monitoring System (EPMS) implementation and support | DCIM evaluation, implementation, and support | BMS implementation and support | Other technical support of electrical and mechanical equipment in a critical facility | Knowledge of critical facility networking | knowledge of database design and utilization | Associates degree in Engineering/Technical studies or Military Technical School | Effective organizational and communication skills | Experience working independently and as a member of a cross functional team | Bachelor’s degree in Math, Computer Science, or Engineering discipline | Working knowledge of Building Management Systems | Strong knowledge of a variety of commercial DCIM and data center management systems | Experience with BACnet, Modbus, and OPC-UA communication protocols | Proficiency in SQL and Python programming | Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started. | Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com. | #LI-REMOTE",Remote,Data Center Facilities Engineering DCIM Engineer
Bullhorn,/rc/clk?jk=89d4d7bd68b98445&fccid=5f645a490c5749b6&vjs=3,"HealthCare Partners, IPA and HealthCare Partners, MSO together comprise our health care delivery system providing enhanced quality care to our members, providers and health plan partners. Active since 1996, HealthCare Partners (HCP) is the largest physician-owned and led IPA in the Northeast, serving the five boroughs and Long Island. Our network includes over 6,000 primary care physicians and specialists delivering services to our 125,000 members enrolled in Commercial, Medicare and Medicaid products. Our MSO employs 240+ skilled professionals dedicated to ensuring members have access to the highest quality of care while efficiently utilizing healthcare resources. |  | HCP’s vision is to be recognized by members, providers and payers as the organization that delivers unsurpassed excellence in healthcare to the people of New York and their communities. | We pride ourselves on selecting the most qualified candidates who reflect HCP’s mission of serving our members by facilitating the delivery of quality care. | Interested in joining our successful Garden City Team? We are currently seeking a Data Operations Engineer. : |  | JOB SUMMARY: The Data Operations Engineer is responsible for the design, creation, and deployment of HealthCare Partners’ mission critical database solutions, including design, implementation and E-MPAC-TL (Extract, Monitor, Profile, Analyze, Cleanse, Transform and Load). The Data Operations Engineer is responsible for the design, implementation, maintenance and establishing the technical direction of the enterprise’s data warehouse solution and associated data marts. The Data Operations Engineer provides technical expertise in the use of technology (identification, evaluation, selection and implementation) and is accountable for the overall performance of the enterprise’s database solutions. |  | ESSENTIAL JOB FUNCTIONS / RESPONSIBILITIES: | Lead the development of ANSI SQL (DDL/DML) as it pertains to the enterprise’s data warehouse and other critical database solutions. | Serve as the resident technical expert with respect to relational database and database technologies (triggers, stored procedures, views, indexing, data partitioning and performance tuning). | Design and implement data cleansing and de-duplication methodologies/techniques. | Define and maintain database standards. | Implement and enforce data access standards as required by established information security policies. | Lead in the development of an enterprise-wide standard E-MPAC-TL process. | Peer review solutions and mentor Enterprise Architecture staff in E-MPAC-TL best practices. | Identify and maintain the enterprise’s information management lifecycle. | Contribute to the enterprise data governance and data management processes. | Adhere to existing processes as documented and actively participate in the development of new processes that adhere to best practices methodology. | Research and make recommendations on new technology products and services in support of procurement and development efforts. | Evaluate and anticipate the impact of technical solutions on environments; identify key performance indicators to quantify systems performance and deployment deliverables. | Review key performance indicator (KPI) reports to determine system and design changes. | Ensure that database solutions meet business requirements and goals, fulfill end-user requirements, and identify and resolve issues (execution of a plan that turns an idea into a solution). | Research and make recommendations on new technology products and services in support of development efforts and system solutions. | Liaise with Information Technology Operations, Business Technology Solutions and the Project Management Office to coordinate technology solutions. | Cultivate, disseminate, and enforce Information Technology policies, procedures and best practices, ensuring all Information Technology methodology and processes (Project Management, SDLC and Change Management) are followed and that code is documented (following the programming specification standards). | Possess strong skills in relationship management, communications, presentation and mentoring/coaching. | Participate in Business Continuity and Disaster Recovery Plan planning, drills, data back-up, recovery and reconciliation testing. | Attend technical training's to stay up-to-date on solution trends and new technologies. | Perform other duties as assigned. |  |  | QUALIFICATION :REQUIREMENTS:: | Skills, Knowledge, :Abilities:: : | Bachelor’s Degree in Computer Science or a related field, or an equivalent combination of education and related work experience required. | Minimum of Four (10) years’ work experience in Information Technology, incliding data warehouse responsibilities. | Demonstrate design, development and implementation of highly complex solutions. | Demonstrate logical approach to problem solving and high-level analytical capabilities, programming and related technical skills showing ingenuity and creativity, including an attention to detail and an understanding of business processes and constraints. | Work within a change management framework that incorporates distinct development, test, and production environments, solution promotion and deployment processes, as well as solution testing and defect management. | Demonstrate knowledge of application system networks, technology and concepts in an enterprise Information Technology environment, with the ability to grasp quickly new technologies, applications and concepts, and apply them as required. | Ability to work in a team environment, meet deadlines, mentor, share knowledge and take on responsibility and accountability for assigned areas. | Ability to organize and structure a growing inventory of application assets, and be able to draw and maintain diagrams that can quickly convey the overall context of the environment/solution. | Demonstrate experience in participating in multiple projects and ability to work with limited supervision, showing creativity, innovation, motivation, initiative and professionalism. Excellent interpersonal, analytical, written and verbal communication skills, including the ability to convey information to non-technical colleagues in a concise and clear way. | Demonstrate understanding and sensitivity to multi-cultural values, beliefs, and attitudes of both internal and external contacts. | Demonstrate appropriate behaviors in accordance with the organization’s vision, mission, and values. |  |  | HealthCare Partners, MSO provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, HealthCare Partners, MSO complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.",Garden City NY,Data Operations Engineer
Zachary Piper Solutions,/rc/clk?jk=96aed6c2009c3df7&fccid=1683f7cd693568b4&vjs=3,"Data Engineer | Overview: | As our Data Engineer, you'll be ready to expand your knowledge and best practice experience through an established software development methodology and database best-practices. We employ technology that connects patients and physicians and fosters a patient experience that is unrivaled in the industry, and we seek a meticulous Data Engineer that will ensure our product continues to ship with high quality data and reporting abilities. This role will report directly to the Vice President of Technology. Primary duties will include data design in our proprietary application, building ETL connections to third party systems (CRM, Billing, etc.) and exporting to a centralized data store for analytics. This key role is for a highly driven individual that can fit in well with a diverse technology team. | Expectations: |  | Architect data structures; set and monitor standards. | Great time management and self-direction and able to work independently while contributing on team efforts with clear communication | Can scope, estimate, develop, document, and test data/database/dataset functionality | Incorporate disparate data sources into a database/data lake for consumption via analytics tools such as Tableau or Looker | Ability to work with technical team to resolve data discrepancies, and work within project management tool like Jira to track work items and defects | Design, create, modify and review database objects (tables, views, indexes, keys, stored procedures, functions, DB links, etc.) to support development projects. | Troubleshoot production issues related to data and SQL code | Coordinate with VP of technology to manage projects/priorities |  | Required Technical Skills: |  | 3+ years of practical experience building and supporting Postgres databases | Expert understanding of SQL including stored procedures and functions, permissions, bulk load/export | Insistence on DRY methodology | Understanding of normalization and its tradeoffs | Documentation experience for both engineering and cross functional documentation | Ability to manage, describe maintain and document disparate data flows and sources | Experience automating and routine tasks and processes |  | Preferred Skills: |  | Familiarity with HIPAA compliance requirements | Coding and scripting experience using Python | Experience using an ORM such as Django ORM or SqlAlchemy | Experience with data operations tools (Keboola and Snowflake)",Remote,Data Engineer
Sabre Systems,/rc/clk?jk=3fc66107e2e3fda5&fccid=de71a49b535e21cb&vjs=3,"Introduction | As a Data Scientist at IBM, you will help transform our clients’ data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it’s investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live. |  | Your Role and Responsibilities | Preferred locations include: Littleton MA, Raleigh/Durham NC, Austin TX, Dallas TX, North Castle, NY, Washington DC area |  | As a Data Scientist for the IBM Enterprise &amp; Technology Security (E&amp;TS) organization, you will help transform security and compliance data for our customers into tangible business value by analyzing information, communicating outcomes and collaborating on product development. You will apply your knowledge of mathematics, statistics, and technology to transform high volumes of data in advanced analytic solutions. |  |  | We are looking for an expierenced Data Scientist to join the E&amp;TS Shared Operational Services (SOS) organization to create additional value add analysis and reporting for our customers so they can quickly and easily identify top security and compliance issues, understand trends, and be able to perform predictive analysis. |  | What you'll do as an experienced Data Scientist in the SOS team of E&amp;TS organization: | Strengthen security and compliance reporting by providing additional analytics, trend analysis, and predictive analytics | Optimize data retrieval and develop new dashboards, reports and other visualizations for stakeholders | Communicate with internal customers to understand business needs and deploy successful analytical solutions | Work in an agile, collaborative environment, partnering with other scientists, engineers, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors | Who You Are: | You have experience in data analytics, predictive analytics, statistics, or similar areas. | You have strong technical and analytical abilities, a knack for driving impact and growth, and some experience with containerization, programming/scripting | You are great at solving problems, debugging, troubleshooting, and designing &amp; implementing solutions to complex technical issues | You thrive on teamwork and have excellent verbal and written communication skills | You have an understanding of security and compliance | You pay strong attention to detail and can anticipate client needs | You are able to meet deliverable deadlines | You have an interest in, understanding of, or experience with Design Thinking and Agile Development methodologies. |  | Required Technical and Professional Expertise |  | 2+ years Cognos experience | 1+ years experience in data analytics, predictive analytics, statistics, or similar field. | 1+ years of experience implementing predictive or statistical models | 2+ years of experience using (use relevant: Java, Python) or similar | Experience writing database SQL queries | Understanding of security and compliance | Experience with data modeling | Foundational knowledge of OpenShift, dockers and containers | Strong customer facing, communications and collaboration skills |  | Preferred Technical and Professional Expertise | none |  | About Business Unit | IBM Systems helps IT leaders think differently about their infrastructure. IBM servers and storage are no longer inanimate - they can understand, reason, and learn so our clients can innovate while avoiding IT issues. Our systems power the world’s most important industries and our clients are the architects of the future. Join us to help build our leading-edge technology portfolio designed for cognitive business and optimized for cloud computing. |  | Your Life @ IBM | What matters to you when you’re looking for your next career challenge? |  | Maybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities – where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust – where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible. |  | Impact. Inclusion. Infinite Experiences. Do your best work ever. |  | About IBM | IBM’s greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries. |  | Location Statement | IBM will not be providing visa sponsorship for this position now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship. |  | IBM intends this job to be performed entirely outside of Colorado. |  | Being You @ IBM | IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.",United States,SOS Data Science Engineer
First Stop Health,/rc/clk?jk=e2191d656f68ee56&fccid=e32446a7efe045d3&vjs=3,"At Support.com we believe life is better when technology does what it's supposed to do. We want to make that happen for everyone by enabling dramatically better support for a connected world. Our offerings change the way technology support is built into products, managed and delivered. Work with us on our full-service support environment (get.support.com) and the backend at Support.com Cloud, our next-generation SaaS solution for technical support, delivering on these core values: |  | Reducing customer effort through advanced tools embeddable in mobile and web apps. We embed in customer's support sites as well as our own full-service support offering. | Optimizing the interaction of support agents with customers via intelligent, contextual guidance; data-driven, step-by-step solutions; and modern visual remote tools | Providing actionable insights into support practices and real-world product performance through support interaction analytics |  | Join us as a Data Integration Engineer consolidating data from across our enterprise as a key component to delivering amazing support experiences at scale. In Support.com engineering, we are refreshing our data stack around Snowflake. Help us grow our time series data analytic capabilities to derive insights based on agent and consumer support interactions. Analyze our support content and its usage to provide feedback into content authoring and presentation. Come and imagine a future where real time support data streams fuel machine learning algorithms that customize support to provide an optimal consumer experience. |  | Why work with us |  | Use a cutting-edge data stack built on Snowflake, Fivetran, S3, PostgreSQL, and Python. | We love collaborative, agile software development, iterative design and testing. We form tight teams, build rapid prototypes and release frequently. You won't get bored! | We encourage learning and integration of new technologies. If you're passionate about technology, we'd love to hear your story. | Support.com is a public company, with mature products established in the marketplace and continued innovation in support technology software. |  | What you will be doing |  | Build, maintain and deploy the integration infrastructure for ingesting high-volume support data from consumer interactions, devices, and apps across a myriad collection of software solutions. | Design and implement the processes that turn data into insights. Model and mine the data to describe the system's behavior and to predict future actions. | Develop and maintain the data-related code in an automated CI/CD build/test/deploy environment | Research individually and in collaboration with other teams on how to solve problems |  | What you need for the job |  | B.E/BTech/MCA/MTech/M. E from an accredited university or college (domestic or international). | 5+ years of experience working in a consumer internet or software company is required and 3+ years of relevant work experience; but, really, we are open to any developer that has the technical prowess | Excellent programming skills in Python with bonus points for Java, JavaScript, or Scala experience | Direct experience with some of the following; Snowflake, Fivetran, S3, Apache Spark, Airflow, and PostgreSQL or substantially similar tools | Ability to understand business problems and translate them into technical requirements | A reliable and fast internet connection and stable power supply to enable work from home. | Ability to benchmark systems, analyze system bottlenecks and performance issues and design solutions to eliminate them. | Ability to evaluate and clearly articulate pris and cons of various technical approaches related to data gathering. | Ability to work effectively and collaboratively from home. |  | Bonus points for these! |  | DevOps: Jenkins | Experience with a modern Big Data processing stack including Fivetran, Kafka, Kinesis or equivalent technologies | Strong knowledge of traditional Data Warehouse-related components (Sourcing, ETL, Data Modeling, Infrastructure, BI, Reporting) |  | Your benefits |  | Competitive benefits and compensation! | A flexible but challenging work from home experience! | Ability to work with exceptionally creative and talented people!",Remote,Data Integration Engineer
Pantheon Inc,/rc/clk?jk=2d1070782dd9a696&fccid=6101ba6269d76146&vjs=3,"Bullhorn is looking for a Data Engineer to join our Data Warehouse team. | Bullhorn is the leading global software provider for the staffing and recruitment industry. More than 10,000 companies rely on Bullhorn’s cloud-based platform to power their staffing processes from start to finish. Through our incredible products and services, we create raving fan customers, resulting in company growth that consistently offers new opportunities for our talent to advance their careers. 25% of our global workforce gets promoted or moves into a new role every year, expanding their skills and working with new people. Bullhorn is large enough to provide these exciting opportunities but small enough to maintain the energy of a startup, and we’re consistently ranked as a great place to work for our strong culture and rewarding career opportunities. | Our commitment to our employees: Every Bullhorn employee has a sense of belonging, a voice that is heard, and a clear path to success. Bullhorn offers unlimited planned vacation, great opportunities for career development, quarterly paid volunteer days through its philanthropic group Bullhorn Cares, and an open invitation to Bullhorn Allies groups, which celebrate and cultivate diversity and inclusion for all employees. | Our in-office employees enjoy a casual, collaborative environment with weekly catered-in lunch and breakfast, and quarterly social events. While working from the comfort of their own homes, our remote employees are provided a full equipment package with all the tools they need to perform their role. We use Zoom, Slack, and other tools to stay connected while we are remote. | Why this job is important: | As a Data Engineer you will positively impact the business by transforming operational data into analytical data for the business to derive insights and solve complex problems. | A typical day might include: | A stand-up with your team to review product specifications, requirements and progress | Assisting the data warehouse team with writing schema changes and other database design work, geared for scalability and performance. | Identifying complex data problems and reviewing related information to develop and evaluate options and design and implement solutions | Creating and maintaining ELT pipelines and Data Models for analytics/business intelligence solutions | Champion a data-driven culture across the enterprise developing methods and controls to ensure consistent application and use of data | Interface directly with internal clients and project team members to understand the needs and objectives of the business | This role may be a fit for you if you have: | Proven skills in SQL, which includes writing queries, stored procedures and views in T-SQL | Familiarity with dimensional database design | Familiarity with ETL vs ELT | Comfort working independently in a fast-paced, flexible environment. | Strong customer relations and ability to deliver business value with technical excellence |  | Bonus points for: | Bachelor’s degree in Computer Science, Information Technology, Business Analytics or related discipline | Experience with Snowflake data warehouse platform | Experience with Qlik Sense or QlikView | Experience with DataRobot | Exposure to dealing with the challenges of large relational data sets, including load performance, query performance, etc. |  | Bullhorn is committed to our core values and we are looking for people who exhibit these traits: | Service - You go beneath the surface to solve problems. | Energy - You build up your teammates and leave people positively charged. | Ownership - You take action and own up to your mistakes. | Speed &amp; Agility - You go around obstacles and demonstrate urgency. | Being Human - You consider other people's perspectives, laugh, and have fun. | #LI-TZ1",Remote,Data Engineer
Lulus.com,/rc/clk?jk=cea378fdce10a41a&fccid=a5150acad96ee1bb&vjs=3,"Job Details | Level | Experienced | Job Location | Remote - Chico, CA | Position Type | Full Time | Travel Percentage | None | Job Category | Engineering | Job Overview | As a Data Engineer reporting to the Director of Data &amp; Analytics at Lulus you will build the core data infrastructure used to feed insights to key business partners and connect ML models to customer-facing systems. You will design, develop, and optimize our data pipelines from source to target, including integration, transformation, orchestration, end-point provisioning and embeddings. You will ensure a sharp focus on the scalability of our infrastructure and strive to build and maintain resilient processes that support platforms of insight that are accurate, ‘always on’, and able to keep pace with the speed of technology. You will be a key technical liaison, collaborating with other engineers, analysts, and data scientists to solve web scale data problems, experiment with new technologies, and deliver high impact solutions that directly influence the customer experience. | About You | You are an experienced software engineer who is skilled at data extraction, transformation, and loading from a variety of large-scale sources including relational and distributed databases, event streams, and 3rd-party services | You have experience developing highly performant and robust code for mission-critical systems including monitoring capabilities, scalable interfaces, and always-on data pipelines | You are a problem solver who is passionate about data and can transform loose business requirements into impactful solutions | You can balance rigorous engineering standards with an iterative approach to delivering value to a variety of stakeholders | Thrive in an environment of flux and uncertainty, applying equal parts technical rigor and intellectual curiosity to develop sound, creative solutions for large-scale and near-real-time use cases | Revel in the challenge of overcoming complex technical hurdles, but also seek to simplify and clearly communicate your solutions to non-technical stakeholders | Display intellectual curiosity and fortitude, with a commitment to continuous learning and experimentation | Ability to work in a fast-paced, fluid environment and adapt to rapidly changing strategies and priorities | You take pride in your work and know what it takes to deliver quality data to the organization. You also understand and appreciate the value of solid engineering practices and can translate them into usable tooling and intelligence for business stakeholders | Qualifications | 2+ years progressive experience as a software/systems/data engineer required | Advanced SQL skills, including the design and optimization of complex queries, views, and functions across multiple database systems | Intermediate to Advanced skills with data manipulation languages [Python, R, etc] including solid experience with both scripted and object-oriented programming principles. Deep experience with scientific computing stacks a big plus. | Intermediate to Advanced experience developing ETL/ELT process flows for large-scale datasets | Experience developing, deploying, and maintaining complex, time-sensitive data pipelines supporting both reporting and analytics/machine learning workflows | Experience with cross-platform development in Windows/*nix/Mac environments | Experience developing and deploying in a CI/CD environment | Experience developing &amp; deploying insights on modern reporting &amp; analysis platforms | Experience developing/deploying cloud-based solutions | Experience with distributed/NoSql technologies | Experience with scalable processing/storage technologies | Bachelor’s degree in Computer Science, Computer Information Systems, or similar technical field required",Chico CA,Data Engineer
HealthCare Partners MSO,/rc/clk?jk=85bd045caa40bb4e&fccid=12f9f45cb821b955&vjs=3,"CentralReach is a leading provider of end-to-end EMR, practice management and clinical solutions that enable applied behavior analysis (ABA) clinicians and educators to produce superior outcomes for people with autism. The company is revolutionizing the ABA space with cutting-edge solutions including precision teaching, clinical data collection, scheduling, billing, learning management, fully digital evidence-based programming and more. Trusted by more than 90,000 clinicians and educators, CentralReach is committed to ongoing product improvement, market-leading industry expertise, world-class client satisfaction, and support of the ABA community to propel industry practitioners into a new era of excellence. | We are looking for experienced Data Warehouse Engineer to join our team! Reporting to our Chief Technology Officer, you will be a trusted partner, in charge of the evaluation, creation and maintenance of database systems. |  | As a key member of the Engineering team, in this “hands-on” position, you will need solid technical and analytical skills, as well as excellent interpersonal and communication skills. The ideal candidate will be able to contribute to a wide variety of data centric requirements and can be counted on to complete assignments in a timely manner with minimal oversight. |  | If you are a self-starter, who enjoys problem solving researching techniques and methodologies to produce effective, complete and reliable solutions, has an ability to collaborate and work with teams, enjoy and thrive in an agile, fast-moving, ever-changing startup environment, have a sense of humor and enjoy rolling up your sleeves and jumping in, then read on! | This role is based out of our Matawan, NJ office. Local candidates only please. | Key Accountabilities: | Principles and practices of enterprise data warehouse development, data modeling and predictive analytics | Develop and maintain processes to populate Sisense cubes | Analyze troubleshoot and remediate data integrity issues | Data warehouse system performance and optimization tuning | Identify and transform business requirements into data warehouse models. Including Creates schemas and sub-schemas in consultation with application project team | Conducts research and prepares recommendations for data warehouse tools, software, or hardware requirements | Explore Data: Perform qualitative and quantitative analysis that provides meaningful insights and synthesized recommendations to support informed decision-making | Champion for Data: Work to ensure data and product integrity by assisting in testing and validation of new and existing products. | Desired skills and experience: | Experience: 3-10 years relevant experience in enterprise data analytics, data warehouse and the Sisense product. Ideally in Healthcare industry. | Expert knowledge of SQL and Excel and advanced knowledge of PowerPoint, Word, Visio and MS Project required | Experience with many advanced data analysis, modeling and visualization tools such as Power BI, Tableau, Sisense, R, Python or similar | Proficient in querying / manipulation of large data sets (SQL, Access or similar) | Strong hands-on experience on Sisense reporting (3 years or more) | Strong experience working on BI -reporting using Sisense (3 years or more) | Proven experience working on data withcubes so that Sisense will work &amp;amp; perform well (3 years or more) | Should be able to create new Sisense cubes based on existing cubes (3 years or more) | Has experience with Agile methodologies and principles. Preferably Agile/Scrum/Kanban. | Excellent communication and organizational skills | Experience gathering business requirements and documenting in an agile environment | Personal Attributes: | Possesses and applies broad knowledge of concepts and principles or exhibits technical expertise in a specific area. | Performs moderately difficult assignments with diverse scope and complexity, requiring a great deal of originality, creativity and problem solving with initiative and independent judgment required | Some evaluation, originality or ingenuity is required | Ability to lead medium complex projects | Highly organized with superb attention to detail | Superior project management and organizational skills | Proven ability to work in an ever-changing environment, ability to connect the dots and make decisions on your own feet | Ability to produce quality materials within tight timeframes and simultaneously manage several projects | Ability to communicate clearly and concisely, verbally and in writing | Exercise sound judgment and work successfully with all employee levels, regardless of background and perspectives | Be comfortable working in a distributed, semi-virtual environment | Be capable of creating and executing on technical direction and tasks for yourself and others | Be able to take minimal direction from others and provide maximal output for yourself (and others) | Ability to performs a variety of professional tasks including, but not limited to, technology services representative on various committees or task forces | Demonstrated ability to excel both independently and as a team member in a lively, collaborative environment. | CentralReach was developed for Clinicians by Clinicians. The story of CentralReach begins in 2012 when the company’s founder, a practicing Board Certified Behavioral Analyst, decided there had to be a better way to manage her operations so she could spend more time on what mattered most — working with her clients and patients. To help ABA practices focus on what they do best, CentralReach launched the first iteration of its EMR and practice management platform. | Today, under the leadership of Chris Sullens, an award-winning CEO in the technology space, CentralReach is committed to their mission of providing cutting-edge technology and services to help clinicians and educators produce superior client and patient outcomes. Already a market leader, CentralReach is expected to grow exponentially through its four core tenets: hire and develop great people; build industry-leading products; provide exceptional service to customers and continuously invest in systems, processes and infrastructure. |  | We value our employees and offer a robust benefits package including health and dental, paid time off, life insurance, disability coverage and a 401(k) matching. We also provide comprehensive onboarding, ongoing training, mentoring and career pathing to help you develop your career. We pride ourselves on our fun and energetic environment that also provides our employees with a meaningful way to make a difference by helping clinicians produce superior outcomes for children and adults with disabilities.",Matawan NJ,Data Warehouse - Sisense Engineer
Trane Technologies,/company/SimioCloud/jobs/Senior-Cloud-Data-Engineer-b06ea52084d36e2c?fccid=8ac227a98aa1d44a&vjs=3,"Job detailsJob TypeFull-timeNumber of hires for this role1QualificationsSQL: 3 years (Required)US work authorization (Required)High school or equivalent (Preferred)Full Job DescriptionSenior Cloud Data Engineer Senior Cloud Data EngineerREMOTE OR LOCALThis position is for people looking for remote or local to our Westminster, CO office.Hello. I’d like to introduce you to our company – SimioCloud and our parent company, Moore. We help people. More specifically, we help non-profits organizations fulfill their missions by delivering best-in-class data and software solutions.Our clients range from startups to some of the largest charities in the world.We are looking for individuals who love building software.About You: You’re a proponent of creating emerging architecture - move fast for what we know now, evolve as we go, and don’t build it before we need it.You enjoy partnering with Data Scientists (who doesn’t?) to build an accessible, performant ML infrastructure in Azure.You build scalable data ingestion, ETL/ELT, and data warehousing solutions.You contribute to the design and architecture of applications, services, and data.You   agile / scrum which means you listen as much as you mentor.Experience: About 3+ years implementing large scale data projects within a cloud environmentAbout 3+ years of experience with Cloud databases – Snowflake, Azure SQL DW, AWS Redshift or similarAbout 3+ years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, and data lake solutionsAbout 3+ years of hands-on experience in technologies such as Hadoop, Spark, Hive, and Pig or similarExperience with ML platform such as Azure ML studioNoSQL / graph databases (Cosmos DB, MongoDB, Neo4j )Streaming technologies such as KafkaStrong programming skills using Python, Scala or similarAutomated tests on your code and data feels rightBonus Points: Experience in consumer marketing or non-profitsExperience with BI tools such as PowerBI, Tableau, or similarYour Zoom background is unmatchedYou’re still reading this, so I’d love to talk to you (virtually)! If you can share with me the language Spark is written in - we're on the right path!Must be located and authorized to work in the United States.We are unable to sponsor H1-B Visa’s at this time.Related keywords: cloud engineer, aws, aws cloud engineer, cloudJob Type: Full-timeBenefits:401(k)Dental insuranceDisability insuranceFlexible scheduleFlexible spending accountHealth insuranceHealth savings accountLife insurancePaid time offRetirement planVision insuranceSchedule:Monday to FridayEducation:High school or equivalent (Preferred)Experience:SQL: 3 years (Required)Work Location:Fully RemoteThis Job Is:A good fit for applicants with gaps in their resume, or who have been out of the workforce for the past 6 months or moreA job for which all ages, including older job seekers, are encouraged to applyOpen to applicants who do not have a college diplomaA job for which people with disabilities are encouraged to applyCompany's website:SimioCloud.comBenefit Conditions:Only full-time employees eligibleWork Remotely:YesCOVID-19 Precaution(s):Remote interview processPersonal protective equipment provided or requiredTemperature screeningsSocial distancing guidelines in placeVirtual meetingsSanitizing, disinfecting, or cleaning procedures in place",Remote,Senior Cloud Data Engineer
Oracle,/company/Cystems-Logic-Inc/jobs/Gcp-Data-Engineer-42721955a0af015b?fccid=be31579eb4f2c075&vjs=3,"Job detailsSalary$38 - $64 an hourJob TypeFull-timeContractNumber of hires for this role1Full Job DescriptionPosition: GCP Data Engineer Location: Buffalo NY Duration: Long term contact/ Fulltime both options are available.No Third party candidates for this role.Good To Have Skills : Google Cloud SpannerKey Responsibilities : Cloud Build, Cloud Scheduler, Cloud Tasks, Container Registry, Identity Platforms, Deployment Manager and EndpointsJob Requirements :Needs to have an understanding and ideally experience with Data (Big Query, Dataflow, Airflow, Dataflow, and NiFi) and Big Data (AWZ, Azure, Hadoop) toolsBasic understanding of concepts and workings of Data Warehousing, Data Lake, OLAP, and OLTP Applications.Experience &amp; proficiency in SQL and at least one other modern programming language (Python, Java, etc)Familiar with the execution sequence of ETL Flows - Experience with Google Cloud Platform, Big Query, Airflow, Dataflow, Composer and Ni-FiAI/ML experience and data analytics with prescriptive &amp; predictiveProvide detailed assessment of existing solutions and infrastructure to migrate to the cloud. Deliver migration strategy based on detailed analysis and implement application and data migration activities.Implement scalable, high performance hosting solutions that meet the need of todays corporate and digital applications using both private and public cloud technologies.Deliver legacy infrastructure transformation and migration to drive next-generation business outcomes.Job Types: Full-time, ContractPay: $38.00 - $64.00 per hourSchedule:Monday to FridayExperience:Cloud Build, Cloud Scheduler, Cloud Tasks: 5 years (Required)Container Registry, Identity Platforms: 2 years (Required)Google Cloud Spanner: 1 year (Preferred)Contract Length:More than 1 yearWork Remotely:No",Buffalo NY,GCP Data Engineer
MMC-LHM Management Corp,/rc/clk?jk=63311fd2112da3a7&fccid=9518af008fd804f9&vjs=3,"The Individual must demonstrate deep knowledge of an Active Data Warehousing environment. Candidate will be responsible for designing, developing, testing, implementing and supporting complex ETL processes. Must be able to understand and analyze requirements and develop, deploy and configure ETL packages. The developer will use their experience to implement robust, maintainable solutions that meet business requirements. |  |  | Essential Functions/Responsibilities: |  | Solidify and extend existing ETL Processes and Framework | Design ETL jobs and reusable components to implement specified business requirements | Troubleshoot and optimize ETL code; interpret ETL logs, perform data validation, dissect code, understand the benefits and drawbacks of parallelism, apply best practices using change data capture, expressions, scoping of variables, commonly used transforms, event handlers and logging providers, understand and optimize the surrogate key generation and inconsistent data type handlingCreate technical specifications documents and design process diagramsDevelop functional specifications for data acquisition, transformation and load processes. | Develop scripts for data file processing and process integration tasksDefine job parameters, reference lookups, filter criteriaConduct code reviews and participate in technical designConduct performance analysis and optimize scheduled jobs, custom SQL and T-SQL jobs | Prepare unit test, integration test plan documents and perform unit test, system integration test; document test resultsSupport team members during functional, UAT, regression, and performance testing.Manage data replication from various data sources | Design and develop BI reports based on business requirements | - Provide on-call support for ETL and BI processes | Create jobs that will pull data from a variety of databases and flat files |  | Desired Skills and Experience: |  | Experience implementing ETL for Data Warehouse and Business Intelligence solutions | Experience in RDBMS design and development, and performance tuning | Deep familiarity with database technologies and cloud infrastructure | - Experience in data classification &amp; handling around data types included in HIPAA, PCI, CCPA, and other types | Minimum 3 years with ETL tool (SSIS, Informatica, etc.) | Experience with scheduling ETL jobs using a scheduling tool | Experience in data streaming servicesAbility to read and write effective, modular, dynamic (parameterized) and robust code, establish and follow already established code standards, and ETL framework | Excellent troubleshooting &amp; optimization skills: interpreting ETL logs, performing data validation, dissecting code, understanding the benefits and drawbacks of parallelism, experience with change data capture, using expressions, scoping of variables, commonly used transforms, event handlers and logging providers, ability to understand and optimize the surrogate key generation and inconsistent data type handlingClear understanding of fuzzy grouping, fuzzy lookup transforms, term extraction and term lookup transforms, import and export column transforms | Experience in performance tuning, deploying and administering ETL packages, proficient in utilizing scriptingExperience designing jobs that can be easily promoted from one (Dev) environment to another (Test or Prod) seamlessly, without modification.",Sandy UT 84070,Data Engineer
Built Technologies,/rc/clk?jk=20f2dcd85e1f5d3d&fccid=60d895813d6c80b9&vjs=3,"About Gradient AI: |  | Gradient AI delivers state-of-the art artificial intelligence and Machine Learning solutions to the trillion-dollar insurance industry. AI has emerged as a disruptive force revolutionizing the way insurance professionals achieve their objectives, and Gradient AI is leading the charge. Our team is made up of Data Science and Insurance Technology experts with a history of building wildly successful technology companies. If you are passionate about making a difference for customers, and collaborating with passionate colleagues, then Gradient AI might be the right place for you. |  | About the Role: |  | We are looking for a Data Engineer to join our team to create and maintain optimal data pipeline architecture and assemble large, complex data sets. You'll have a hand in every stage of development and are comfortable thinking creatively with tools such as Python, SQL, and the AWS ecosystem. The ideal candidate is willing to learn new skills and technologies as the task at hand requires and are adaptable to fluid and evolving project requirements. |  | In this role, you will contribute materially towards shaping and realizing the vision of our business and contribute towards fundamentally changing the way an entire industry does business (really). |  | Responsibilities: |  | Design, build, and implement data systems that fuel our ML and AI models | Develop tools to extract and process client data from different sources and tools to profile and validate data | Work cross functionally with data scientists to transform large amounts of data and store it in a format to facilitate modeling | Contribute to production operations, data pipelines, workflow management, reliability engineering, and much more | Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies |  | Qualifications: |  | BS in Computer Science (or in another quantitative discipline); 5+ years of working experience | Fluency in SQL and experience with non-relational/alternative databases | Experience working in Python in a professional environment | Desire to learn new skills and tools (e.g. Redshift, Tableau, AWS Lambda, etc.) | Exposure working with a cloud-computing environment (e.g. AWS EC2) | Comfortable with Linux, including developing shell scripts | Experience working in insurtech or on AI/ML products is a bonus |  | What We Offer: |  | We are an equal opportunity employer that offers a number of benefits and perks to accommodate all types. Bring your authentic self to work in our supportive workplace where we offer: |  | A fun and fast-paced startup culture | A culture of employee engagement, diversity and inclusion | Full benefits package including medical, dental, vision, 401k, disability, life insurance, and more | Unlimited vacation days and ample holidays- we all work hard and take time for ourselves when we need it | Competitive salary and generous stock options - we all get to own a piece of what we're building | Ample opportunities to learn and take on new responsibilities |  | Work Authorization: |  | US Work Authorization Required",Boston MA 02210,Data Engineer
Spotify,/rc/clk?jk=cd13de9fa797cca0&fccid=a7befdfdf774cf24&vjs=3,"Company Description |  | dentsumcgarrybowen is a global creative agency delivering idea-led brand transformation at scale. This new entity brings together dentsu and mcgarrybowen to form a world-class creative powerhouse that now spans 33 locations in 24 markets. dentsumcgarrybowen delivers creative expertise and innovative global solutions for clients, applying big platform thinking to bring ideas to life and deliver world-class creative solutions to meet consumer touch points and work across geographies, cultures, and channels. It gives us great pride to work with the world’s most iconic companies, including Ajinomoto, American Express, Asahi Breweries, Canon, Disney, Hershey, Kao, Marriott, Shiseido, Subway, The Coca-Cola Company, Toyota, and United Airlines. | Diversity is embedded in who we are and all that we do: our mindset, our solutions, and in our teams to empower an inclusive, equitable environment. We put our people at the center, creating space for growth, understanding and learning so they can thrive. Our differences make us richer and enable stronger relationships with each other and foster greater impact for our clients. We engage with our communities to drive positive social impact by fostering equity and working to create a digital society that works for all. dentsumcgarrybowen is an agency of dentsu. |  | Job Description |  | dentsumcgarrybowen is looking for a Senior Data Engineer to contribute and strategically advance our enterprise clients’ web properties. As a Sr. Data Engineer, you will be a key contributor on a cross-functional team of strategists, designers, and creative technologists working on both strategic website initiatives and general maintenance for a large corporate enterprise. The ideal candidate will have experience in assessing, making recommendations, and implementing solutions for large organizations using WordPress and the Gutenberg Block Editor, including: | Assessing client code and integrations with a focus on security and scalability | Assisting large organizations with ongoing support of new features and functionality | Understanding an organization’s needs and being able to make recommendations around best practices, editorial workflows, tools and plugins |  | Qualifications |  | Production experience with several programming languages—most importantly advanced PHP, JavaScript, HTML and CSS | Production experience in contributing to enterprise content management system projects—specifically WordPress using the Gutenberg Block Editor | Production experience structuring and implementing components using React | Experience in working with and implementing component libraries | Thorough experience in assessing and implementing web accessibility patterns | An ability to iterate and ship ideas quickly with a high degree of autonomy | Experience working with a cross-functional team, following Agile methodologies | Highly collaborative and participative in discussions about architecture and design | Thoughtful about product design, with good user experience instincts | Additional Information |  | Dentsu is a modern marketing solutions company. Our mission is to help clients navigate, progress and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Some of our award-winning agencies include 360i, Carat, dentsumcgarrybowen, DEG, dentsuX, iProspect and Merkle. Follow us on Twitter @DentsuUSA and visit dentsu.com/us. | Employees from diverse or underrepresented backgrounds encouraged to apply. | Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsu.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.",New York NY 10001,Senior Data Engineer
mcgarrybowen,/rc/clk?jk=e4dbb35c0af99150&fccid=e87902195236c825&vjs=3,"NASHVILLE, TN /ENGINEERING – ENGINEERING /FULL-TIME |  | Built is a growth-stage company at the intersection of FinTech and PropTech. We are on a mission to change the way the world gets built with technology and services that streamline the $1.2T U.S. construction industry. We strive to empower lenders, owners, builders, and vendors with innovative software, payments products &amp; services that enable participants to manage risk, maximize productivity / collaboration and ensure better cost management as capital flows into and throughout the construction industry. Founded in 2015, Built now serves more than 110 of the top financial institutions in the US and Canada, including 25+ of the top 100 US construction lenders. In 2019, we completed our Series B raise led by Goldman Sachs bringing our total funding to $55M. Bringing on the “best talent in the world” is at the forefront of our continued growth trajectory. |  | Built’s Insights team is hard at work on the product features that enable our clients and customers to get the most out of their data within Built. This includes data warehousing, in-app reporting, secure scheduled report delivery, and ad-hoc report generation where necessary. In addition to client needs, the Insights team is also instrumental in helping Built make sense of all of its internal data, allowing internal stakeholders to make informed, data-driven product decisions. | As a member of this team, you will... | Help build the foundation for the future of reporting at Built | Ship features that enable clients and internal stakeholders to get the most out of their data | Provide data that helps drive product decisions | Participate in design and architectural conversations around data warehousing and report generation/building | Encourage and build up your teammates | Success in this role will be defined by... | Delivering the right solution at the right time with integrity | Participating in driving our data warehousing and report generation architecture forward | Communicating and collaborating with both technical and non-technical team members to arrive at negotiated decisions. | Working across languages, environments, and teams to create the best solution from the information currently available | Experience with these technologies will be helpful: | MySQL | Python (3.6+), JavaScript, and/or PHP | ETL processes | Looker or similar Business Intelligence platform (Power BI, Tableau, etc) | Rundeck | GoCD | In addition to tech skills, it's important to: | Be a good communicator | Possess a strong focus on customers, both internal and external | Be able to work across teams to accomplish goals | Have a dedication to the quality and ownership of your work product | Have empathy and support for your teammates | #LI-AG1 | Perks: | The rare opportunity to change the world and radically disrupt an industry | Competitive benefits including Unmetered Vacation; Health, Dental &amp; Vision Insurance; and 401k | Flexible Hours | Weekly Team Lunches | Kitchen loaded with all the essentials to keep you productive",Nashville TN 37211,Data Engineer
AVANGRID,/rc/clk?jk=783f0bdbcafd6c96&fccid=fe404d18bb9eef1e&vjs=3,"Engineering | Data | We are looking for a Senior Data Engineer to join our User Fraud team. Our mission is to accelerate Spotify’s growth by developing a system that detects, mitigates, and prevents unwanted behavior on our platform. | Location | New York, NY | Job type | Permanent | You will be joining a diverse team of data scientists and engineers who use machine learning, data engineering and security expertise to tackle some of the most advanced and important problems that we face. You will play an integral role in designing and implementing data pipelines with requirements for scalability and quality. You will help evolve engineering practices in the team! | What You'll Do: | Design and build batch and real-time data pipelines with data processing tools like Scio, Google Cloud Platform, Scala, BigQuery, Luigi, Styx and Docker | Drive optimization, testing and tooling choices to improve data and pipeline quality | Evolve and scale out our fraud detection platform | Collaborate with engineers, data scientists and product managers to build solutions | Learn about the domain of abusive behavior and take part in strategizing solutions to combat such behavior | Work in a multidisciplinary environment that provides opportunities for individual growth | Who You Are: | BS/MS in CS or other relevant fields of study | 5+ experience in data engineering | Strong coding skills preferably in Scala, Java, Python and SQL | Experience performing analysis with large datasets in a cloud based-environment, preferably with an understanding of Google’s Cloud Platform or similar | Experience in scheduling, developing, maintaining and orchestrating big data pipelines using state of the art technologies in the industry | Knowledgeable regarding data modeling, data access and data storage techniques | Strong analytical and problem solving ability | You are an enthusiastic learner; you see unfamiliar territories as an opportunity to grow | You value team collaboration and seek to grow the skills and knowledge of your peers | You value building strong relationships with colleagues and stakeholders, and have the ability to explain complex topics in simple terms | Working as a data engineer in the User Fraud area will challenge your design, quality, and problem-solving skills to build robust and scalable solutions. | Perks of being in the band | Extensive learning opportunities, through our dedicated team, GreenHouse. | Flexible share incentives letting you choose how you share in our success. | Global parental leave, six months off - fully paid - for all new parents. | All The Feels, our employee assistance program and self-care hub. | Flexible public holidays, swap days off according to your values and beliefs. | Spotify On Tour, join your colleagues on trips to industry festivals and events. | Learn about life at Spotify | You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens. | Spotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service with a community of more than 345 million users.",New York NY,Senior Data Engineer- User Fraud
Stash,/rc/clk?jk=d792b0ab9fcb9465&fccid=6d044ebfc8cbca1f&vjs=3,"Stash is pioneering the future of personal finance with the first financial subscription that helps people create better lives. From budgeting to saving for retirement, Stash unites banking, investing, and advice all in one app that has helped more than 5M people reach their financial goals and make progress towards financial freedom |  | At Stash, data is at the core of how we make decisions and build great products for millions of users. As a Data Engineer you will be a part of our Data Platform Team which is leading the architectural design decisions and implementation of a modern data infrastructure at scale. You will build distributed services and large scale processing systems that will support various teams to work faster and smarter. You will partner with Data Science to help productionize machine learning models and algorithms into actual data driven products that will help make smarter products for our users. |  | Tools and technologies in our tech stack (evolving): | Hadoop, Yarn, Spark, MongoDB, Hive | AWS EMR/EC2/Lambda/kinesis/S3/Glue/DynamoDB/API Gateway, Redshift | ElasticSearch, Airflow, and Terraform. | Scala, Python | What you'll do: | Build core components of data platform which will serve various types of consumers including but not limited to data science, engineers, product, qa | Build various data ingestion and transformation job/s as and when they are needed | Productionize our machine learning models and algorithms into data-driven feature MVPs that scale | Leverage best practices in continuous integration and deployment to our cloud-based infrastructure | Build scalable data services to bridge the gap between analytics and application space | Optimize data access and consumption for our business and product colleagues | Develop an understanding of key product, user, and business questions | Who we're looking for: | 3+ years of professional experience working in data engineering | BS / MS in Computer Science, Engineering, Mathematics, or a related field | You have built large-scale data products and understand the tradeoffs made when building these features | You have a deep understanding of system design, data structures, and algorithms | Experience (or a strong interest in) working with Python or Scala | Experience with working with a cluster manager (YARN / Mesos / Kubernetes) | Experience with distributed computing and working with Spark, Hadoop, or MapReduce Framework | Experience working on a cloud platform such as AWS | Experience with ETL in general |  | Gold stars: |  | Experience working with Apache Airflow | Experience working with AWS Glue | Experience in Machine Learning and Information Retrieval | _________________________________________ |  | We believe that diversity and inclusion are essential to living our values, promoting innovation, and building the best products out there. Our success is directly related to the employees that we hire, grow and retain and we believe that our team should reflect the diversity of the customers that we serve. |  | As an Equal Opportunity Employer, Stash is committed to building an inclusive environment for people of all backgrounds. We do not discriminate on the basis of race, color, gender, sexual orientation, gender identity or expression, religion, disability, national origin, protected veteran status, age, or any other status protected by law. Everyone is encouraged to apply. |  | Benefits &amp; Perks: |  | Equity in Stash | Flexible Vacation | Family-Friendly Medical, Dental, and Vision Insurance Plans | 401k | Learning &amp; Development Stipend | Commuter Benefits and Flexible Spending Account (FSA) | Employee referral bonuses | Stocked fridges &amp; kitchens and catered lunch on Fridays | Thursday happy hours | Team outings that do not involve trust falls... |  | Awards &amp; Recognition: |  | Forbes Fintech 50 (2019) | LendIt Fintech Innovator of the Year (2019) | Built in NYC's Best Places to work (2019) | Built in NYC's Startups to Watch (2018) | Wall Street Journal's ""Top 25 Tech Companies To Watch"" (2018) | MarCom Awards Double Gold &amp; Platinum Winner (2018) | Webby Award Winner for Best Mobile Sites &amp; Apps in the Financial Services and Banking category (2017) | W3 Awards Winner for Best User Experience (2017) |  | **No recruiters, please.",New York State,Data Engineer
Apple,/rc/clk?jk=47554b32dc4ba911&fccid=c1099851e9794854&vjs=3,"Summary | Posted: Feb 17, 2021 | Role Number:200223738 | The Apple Media Products Engineering team is one of the most exciting examples of Apple’s long-held passion for combining art and technology. These are the people who power the App Store, Apple TV, Apple Music, Apple Podcasts, and Apple Books. And they do it on a massive scale, meeting Apple’s high expectations with high performance to deliver a huge variety of entertainment in over 35 languages to more than 150 countries. | These engineers build secure, end-to-end solutions. They develop the custom software used to process all the creative work, the tools that providers use to deliver that media, all the server-side systems, and the APIs for many Apple services. | Thanks to Apple’s unique integration of hardware, software, and services, engineers here partner to get behind a single unified vision. That vision always includes a deep commitment to strengthening Apple’s privacy policy, one of Apple’s core values. Although services are a bigger part of Apple’s business than ever before, these teams remain small, forward-thinking, and cross-functional, offering greater exposure to the array of opportunities here. | Key Qualifications | 3+ years of proficiency in Java and functional programming paradigms, or other object-oriented languages like Scala | Proven problem solving skills | Demonstrated experience delivering re-usable tooling solutions for internal teams | Deep understanding of distributed batch and streaming data processing systems like Spark/Kafka | Experience with technologies in big data ecosystems, such as Kubernetes, Airflow | Keen data sense backed by strong SQL, and data visualization skills | Excellent collaboration and communication skills to be able to listen, influence and drive solutions cohesively | Description | As part of the AMP - Analytics Engineering Data Integrity Tools team, you will join a team that is focused on measuring and reporting data quality indicators. We are responsible for building and improving core data integrity services leveraged by all AMP Analytics Engineering lines of business. As a member of the AMP Engineering team you will have the opportunity to: | Work with a multi-functional team collaborating with several partners across product, engineering, operations and business; and closely partner with stakeholders to translate requirements to scalable engineering solutions | Help drive architectural vision to be ready for future opportunities | Design, build and maintain data pipelines and data sets serving key product features and insights protecting user privacy | Own data quality, scalability and SLAs for the allocated datasets and pipelines | Be a champion for technical excellence, and cultivate team spirit by contributing to standard methodologies, documentation, shared tools and solutions to common problems | The people who work here have reinvented and defined entire industries with the Mac, iPhone, iPad, Apple Watch, AirPods, HomePod and a multitude of groundbreaking Accessories. We have done the same with services, including iTunes, the App Store, Apple Music, and Apple TV+. The same passion for innovation that goes into our products also applies to our practices — strengthening our commitment to leave the world better than we found it. Come join us to help deliver the next amazing Apple product! | Education &amp; Experience | Bachelors or Master’s degree in computer science, engineering, similar quantitative field, and 3+ years of experience",Seattle WA,Apple Media Products - Data Integrity Engineer
CUNA Mutual Group,/rc/clk?jk=0b131a7257486d92&fccid=e2b0f357f26efe90&vjs=3,"Our team and broader organization strives to ensure that each opinion is heard and valued. We actively work to build an inclusive environment where our employees can contribute their diverse personal and professional experience to the work we do each day. | Job Purpose: | Responsible for collecting, storing, processing and building data capabilities within big data platforms for both business analytics and data science needs. Capable of leveraging both structured and unstructured data sets. Able to curate high-quality data sets using open source technologies and cloud native tools. Maintains best practices for data integration, quality, design, performance, processing, and reliability. | Job Responsibilities: | Data Modeling &amp; Analysis (75%) | Perform exploratory data analyses and provide guidance for use (both for marketing analysis and data science) | Partner with data scientists on both model rebuilds and new model development | Analyze new (possibly unstructured) data sources to determine what additional value they may bring and how to effectively make use of them | Review developed solutions to solve specific business problems | Investigate, analyze and recommend data formats and structure | Knowledge of how data entities and elements should be structured to assure accuracy, performance, understanding, operational, analytical, reporting, and data science efficiencies | Thought partner in prototyping emerging self-service technologies involving data ingestion and transformation, distributed file systems, databases and frameworks | Work on cross-functional teams to design and test data-driven applications and products | Business Communication (25%) | Communicates complex ideas, anticipates potential objections and persuades others, often at senior levels, to adopt a different point of view | Keeps up to date on industry, competitor and market trends | Communicates difficult concepts and negotiates with others to persuade a different point of view | Actively solicits and listens to feedback to determine need for change or improvement | Thought partner for development teams in applying data modeling techniques and the usage of data modeling and repository tools in consumer data domain | Job Requirements: | Bachelor's degree in computer science or related field | 6-8 years of experience technology related work | In-depth knowledge of SQL, SAS, R, and Python programming languages, and extensive hands-on data engineering experience. Experience with SparkR, Hadoop, Tableau, and graph databases is helpful. | Experience with Databricks and Snowflake | Capable of authoring robust, high quality, reusable code and contributing to the division's inventory of libraries | Expertise in big data batch computing tools, with demonstrated experience developing distributed data processing solutions | Applied knowledge of cloud computing | Applied knowledge of data modeling principles (e.g. dimensional modeling and star schemas) | Strong understanding of database internals, such as indexes, binary logging, and transactions | Experience with software engineering tools and workflows (e.g. Jenkins, CI/CD, git) | Practical experience consuming web services | Solid data understanding and business acumen in the data rich industries like insurance or financial | #LI-CH | CUNA Mutual Group’s insurance, retirement and investment products provide financial security and protection to credit unions and their members worldwide. As a dynamic and growing company, we strive to create a culture of performance, high standards and defined values. In return for your skills and contributions, we offer highly competitive compensation and benefit packages, significant professional growth, and the opportunity to win and be rewarded. | Please provide your Work Experience and Education or attach a copy of your resume. Applications received without this information may be removed from consideration.",Remote,Data Engineer (Remote)
MSC Industrial Supply,/rc/clk?jk=f8bad8bb7e22ace2&fccid=03d1cab2c90ea3c2&vjs=3,"Job detailsSalaryFrom $66,290 a yearFull Job DescriptionBuild a better career with MSC. | Serving customer and community starts with the very best people doing their very best work. That is precisely what we have at MSC Industrial Supply Co., a leading North American distributor of metalworking and maintenance, repair, and operations (MRO) products and services. Watch HERE to find out why you should work at MSC and join us as we continue to build a diverse and inclusive workplace. | Requisition ID : 5457 | Employment Type : Full Time | Job Category : Information Technology | Work Location : Davidson, NC (CSC) | State or Province : North Carolina (US-NC) | Potential Work Location : United States : North Carolina : Davidson |  |  | JOB TITLE: Data Engineer- * Remote * |  | BRIEF POSITION SUMMARY: | At MSC, we are growing our Analytics team and investing in forward-thinking BI and Data Analytics solutions. As we build out the team, we are looking for a smart and innovative leader who is self-sufficient and thrives in ambiguity, has experience in Analytics, and can drive results across MSC bridging business and technical areas. This is an opportunity to be a part of a special milestone, help us build the future of Analytics at MSC, and ignite a data-driven culture. | The Data Engineer will support building approachable and reusable data structures to access to raw data, but to do this, they need to understand company’s or client’s objectives. It’s important to have business goals in line when working with data, especially when handling large and complex datasets and databases. Develop data pipelines using industry best practices. Makes adjustments to adopt new methodologies that provide the business with increased flexibility and agility. This role requires a significant set of technical skills, including an in-depth knowledge of SQL database design, ETL/ELT design, and multiple programming languages. But data engineers also need active listening skills to work across departments to understand what business leaders want to gain from the company’s large datasets as well as communication skills to convey status or ideas to various types of audiences. |  | DUTIES and RESPONSIBILITIES: |  Provides technical leadership and guidance | Develops, constructs, tests and maintains architectures | Aligns architecture with business requirements | Data acquisition |  Develops data set processes | Designs and builds Data Service APIs | Data mining and segmentation techniques and combining raw data from different sources |  Designs and develops resilient pipelines using a variety of different technologies |  Manages our data from ingestion through ETL to storage and load. | Identifies and implements ways to improve data reliability, efficiency and quality | Uses data to discover tasks that can be automated | Participates in rotational on-call support and provide ongoing maintenance of all data infrastructure. | Stays current with latest cloud technologies, patterns, and methodologies; share knowledge by clearly articulating results and ideas to key stakeholders. May be required to present ideas to larger audience for review and buy-in. | Fosters the MSC Culture in the department and throughout the company to ensure fulfillment of MSC’s vision and unity of purpose. | Participation in special projects and performs additional duties as required |  | EDUCATION and EXPERIENCE: | Bachelor’s Degree in Computer Science, Information Technology, Systems Analysis or a related study. | Minimum of three years of experience working with high volume data infrastructure with exposure to database design/structures, ETL/ELT design patterns, and datamart structures (star, snowflake schemas, etc.) |  Minimum of three years of experience with full cycle application development (Full SDLC experience: design, development, delivery, etc.) | Minimum of one year of experience implementing modern applications using Cloud Based Solutions/Technologies (AWS Redshift, S3, Google BigQuery, Azure Data Bricks, Synapse) | Experience w/Data Lake concepts and design patterns (AWS S3, parquet, python, lambda, java, NoSQ (Preferred) | BI Technologies (such as Tableau, Jasper, Cognos, Qlik, Looker, and others) preferred. | Exposure to Spark Streaming and Machine Learning preferred. | Exposure to Graph Databases preferred. | Deploy sophisticated analytics programs, machine learning and statistical methods | Prepare data for predictive and prescriptive modeling | Experience with DevOps process (exposure to GitHub, Jenkins or other CI/CD tools) | Implementation of modern application and infrastructure design patterns, including micro-services and containers, disposable, reactive, stateless and distributed patterns |  | SKILLS: | Knowledge and proficiency in the latest open source and data frameworks but not limited to, NodeJS, OpenJDK, React, Python and NoSQL, DynamoDB database(s) | Strong comprehensive of Data Management and Data governance best practices (3NF, SCD) | Strong Knowledge about Agile techniques: User Stories, Continuous Integration, Test Driven Development, Continuous Testing, Pairing, Automated Testing, Burn Down Metrics, Velocity etc. | Strong knowledge of software development processes and procedures to understand team needs includes fundamentals of iterative and incremental development |  | OTHER REQUIREMENTS: | A valid driver’s license and the ability to travel up to 15% of the time may be required |  | Compensation starting at $66,290.00 and up commensurate with experience | Benefits may include: paid time away including holidays and parental leave, medical and dental plans, retirement savings plans with company-match, disability and life insurance, tuition reimbursement program, recognition and service award programs, wellness and workplace flexibility programs, stock purchase plan, and product discounts (Benefits may vary by location). |  |  | Why MSC | People. Collaboration. Insight. That’s how you build something that works. | Built on a foundation of trust, MSC works side by side with our customers to help them drive business results. With more than one million product offerings and 75+ years of experience across industries, MSC strives to help our customers achieve greater productivity, profitability, and growth through inventory management and other innovative supply chain solutions. |  | We care about our associates and have programs in place to help our 6,500+ team members achieve their potential. When you join our team, you will receive rewards and recognition for your contributions, training and professional development opportunities, as well as a variety of benefits to support you and your family's health, well-being, and financial future. |  | If you are inspired to learn, take risks, and succeed as a team, you can build a better career at MSC. |  | Equal Opportunity Statement | At MSC, we are committed to providing an environment of mutual respect where equal employment opportunities are available to all qualified applicants and our associates without regard to race, color, religion, age, sex, national origin, disability, protected veteran status, sexual orientation, gender identity/expression or any category protected by applicable law.",Davidson NC,Data Engineer-Remote
Red Ventures,/rc/clk?jk=03c43b3cd2870e04&fccid=b704562e07a2a03f&vjs=3,"Join SADA as a Staff Data Engineer! |  | Your Mission |  | As a Staff Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work, and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses. |  | You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. |  | You will be expected to tackle all technical challenges on whole projects and to mentor less experienced Data Engineers. You should be able to work independently, but should be a major participant in team reviews. You should be recognized as having technical mastery within the practice with an established reputation with Google and our customers. You should have demonstrable experience with public facing activities such as blogs, presentations, webinars, and OSS contributions. You will ensure the best architecture and engineering approach is applied. You will be expected to repeatedly deliver complex projects, and will be the owner of the complete customer outcome, including complex technical components of the engagement. You will actively participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects. |  | Pathway to Success |  | #BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs. |  | Your success starts by positively impacting the direction of a fast growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions. |  | As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks. |  | Expectations |  | Required Travel - 30% travel to customer sites, conferences, and other related events. Due to the COVID-19 pandemic, travel has been temporarily restricted. |  | Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives. |  | Training - Ongoing with first week orientation at HQ followed by a 90 day onboarding schedule. Details of timeline can be shared. |  | Job Requirements |  | Required Credentials: |  | Google Professional Data Engineer Certified or able to complete within the first 45 days of employment | A secondary Google Cloud certification in any other specialization. | Expert or Professional level certifications in either or both AWS and Azure. |  | Required Qualifications: |  | Mastery in at least 2-3 of the following domain areas: | Big Data: managing Hadoop clusters (all included services), troubleshooting cluster operation issues, migrating Hadoop workloads, architecting solutions on Hadoop, experience with NoSQL data stores like Cassandra and HBase, building batch/streaming ETL pipelines with frameworks such as Spark, Spark Streaming and Apache Beam, and working with messaging systems like Pub/Sub, Kafka and RabbitMQ. | Data warehouse modernization: building complete data warehouse solutions on BigQuery, including technical architectures, star/snowflake schema designs, query optimization, ETL/ELT pipelines and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive). | Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for minimizing downtime. May involve conversion between relational and NoSQL data stores, or vice versa. | Backup, restore &amp; disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale. | 5+ years of experience writing software in at least two or more languages such as Python, Java, Scala, or Go | Experience in building production-grade data solutions (relational and NoSQL) | Experience with systems monitoring/alerting, capacity planning and performance tuning | Experience with BI tool like Tableau, Looker etc. | Experience in technical consulting or other customer facing role |  | Useful Qualifications: |  | Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc) | Experience with IoT architectures and building real-time data streaming pipelines | Experience operationalizing machine learning models on large datasets | Demonstrated leadership and self-direction - willingness to teach others and learn new techniques | Demonstrated skills in selecting the right statistical tools given a data analysis problem |  | About SADA |  | Values: We built our core values on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA's values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer. |  | Make them rave | Be data driven | Be one step ahead | Be a change agent | Do the right thing |  | Work with the best: SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the 2018 Global Partner of the Year. SADA has also been awarded Best Place to Work by Inc. as well as LA Business Journal! |  | Benefits: Unlimited PTO, competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match, professional development reimbursement program as well as Google Certified training programs. |  | Business Performance: SADA has been named to the INC 5000 Fastest Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.",New York NY 10022,Staff Data Engineer
SS&C Technologies,/rc/clk?jk=072362a81d367f71&fccid=7ab93833a090100a&vjs=3,"SS&amp;C is a global provider of investment and financial services and software for the financial services and healthcare industries. Named to Fortune 1000 list as top U.S. company based on revenue, SS&amp;C is headquartered in Windsor, Connecticut and has 20,000+ employees in over 90 offices in 35 countries. Some 18,000 financial services and healthcare organizations, from the world's largest institutions to local firms, manage and account for their investments using SS&amp;C's products and services. | Job Description | SS&amp;C is a global provider of investment and financial services and software for the financial services and healthcare industries. Named to Fortune 1000 list as top U.S. company based on revenue, SS&amp;C is headquartered in Windsor, Connecticut and has 20,000+ employees in over 90 offices in 35 countries. Some 18,000 financial services and healthcare organizations, from the world's largest institutions to local firms, manage and account for their investments using SS&amp;C's products and services. | Description | SS&amp;C Advent's financial solutions are going cloud, and we're growing the team to get us there. Our best-of-breed products that span the investment firm's front-to-back office will be part of the journey to digital transformation, which started with our cloud-native, award-winning Genesis application used by portfolio managers around the globe. Now our mission and team are expanding, and we are looking for motivated people who want to be part of an exciting growth opportunity in the Fintech industry. | You will | Design and implement product cloud data warehouse schema for multiple stages of data. | Build a clean, reliable, and scalable “single-source-of-truth” cloud data store. | Work closely with data engineers and analytics engineers to build cloud data infrastructure to support our business intelligence needs. | Design and develop systems for the maintenance of the business’s data warehouse. | Quickly and thoroughly analyze reporting and analysis requirements and translate the results into good technical data designs. | Consistently give technical proposals to optimize data warehouse capacity, scalability, and performance. | Establish the documentation of reports, develops, and maintains technical specification documentation for all reports and processes. | We need this from you | Passionate about data wrangling and data analytics. | Skills to draft, analyze, and debug SQL queries and also be proficient in a scripting language, for example, Java, Python, C Sharp, Perl, R, and so forth. | Vast knowledge of database design and modeling in the context of data warehousing. | Ability to resolve data issues and business logic failures in the data warehouse. | Strong problem-solving skills with a can-do attitude. | Great software engineering best practices with git and version control systems. | Knowledge of accounting principles, financial statements, and financial modeling. | Solid communication skills to explain complex data designs to both technical and non-technical people. | Self-starter and self-motivated. | Bachelor’s Degree in Computer Science, Information Systems or other STEM field | Bonus if you have | Experience with one of the major cloud data warehouse vendors like AWS Redshift, Azure Synapse, or Snowflake. | Knowledge about one of the major business intelligence vendors like Tableau, Power BI, Quick Sight, Qlik, Sissense, Domo, or Looker. | At SS&amp;C Advent, you’ll find a rewarding career where you can thrive at every level. We invest in our employees through offering competitive compensation and benefits, progressive training opportunities, and wellness programs that empower employees to achieve total (physical, financial, social &amp; community) well-being. Advent’s culture thrives on teamwork and diversity. Plus, a Global Flexible Time Off (FTO) policy, providing the freedom and flexibility to take the time off that you need - when you need it. So, bring your unique perspective and talent to Advent and thrive in your work today! | SS&amp;C Advent is an equal opportunity employer, committed to a diverse workforce. | Unless explicitly requested or approached by SS&amp;C Technologies, Inc. or any of its affiliated companies, the company will not accept unsolicited resumes from headhunters, recruitment agencies, or fee-based recruitment services. SS&amp;C offers excellent benefits including health, dental, 401k plan, tuition and professional development reimbursement plan. SS&amp;C Technologies is an Equal Employment Opportunity employer and does not discriminate against any applicant for employment or employee on the basis of race, color, religious creed, gender, age, marital status, sexual orientation, national origin, disability, veteran status or any other classification protected by applicable discrimination laws. | Unless explicitly requested or approached by SS&amp;C Technologies, Inc. or any of its affiliated companies, the company will not accept unsolicited resumes from headhunters, recruitment agencies, or fee-based recruitment services. SS&amp;C offers excellent benefits including health, dental, 401k plan, tuition and professional development reimbursement plan. SS&amp;C Technologies is an Equal Employment Opportunity employer and does not discriminate against any applicant for employment or employee on the basis of race, color, religious creed, gender, age, marital status, sexual orientation, national origin, disability, veteran status or any other classification protected by applicable discrimination laws.",San Francisco CA 94104,Data Warehouse Engineer
SoFi,/rc/clk?jk=3985aa26838d331b&fccid=908bc786b906bc2f&vjs=3,"GEI is a leading consulting engineering firm ranked #88 in the ENR top 500 (2020). We serve Government, Energy, Industry and Institutional clients, and have completed project engagements throughout the United States, Canada, and more than 20 other countries. Founded in 1970, our privately held, employee-owned company consists of over 700 employees located in 37 offices across the U.S.A. | GEI s subject matter experts (SMEs) across North America are involved in dozens of specialty services emanating from our traditional core practices which include geotechnical, environmental, infrastructure, water resources, ecology, and transportation. GEI seller-doers are considered some of the leading SMEs in their respective fields. In order to support and improve the competitive efficiencies of all our personnel, GEI is in the middle stages of a digital business transformation (DBX) and is seeking an exceptional candidate for our Operational Development Team. | In 2014, GEI began leveraging our expertise in the design, implementation, and maintenance of client-owned and hosted data management systems to develop a cloud-based data analytics subscription service that provides advanced data storage and retrieval, automated analytics, document hosting, reporting and GIS capabilities to our clients. With management of information resources more complex than ever, GEI is deeply committed to enterprise-level, cloud-based resource management systems that promote management and ease of information sharing. GEI has now reached the point where we re adding new additions to our team to help support, maintain and develop our business information systems and data science and engineering efforts. | Job Description | GEI Consultants has an opening in our Operational Development Team for a qualified Data Engineer to support a variety of systems and data engineering tasks focused on data flow activities. The majority of our systems are based in MS SQL Server, Tableau Server, Azure, and FastField Forms. This person will primarily work closely with members of the Operational Development Team and with members of our IT staff. The ideal candidate will be focused, detail-oriented, and driven to attain and maintain very high standards for efficiency and accuracy in data acquisition and integration into our systems. The ideal candidate will have more than 10 years of data engineering experience in the AEC industry or in similar science and/or engineering environments. GEI seeks a committed, self-motivated, organized and detail-oriented individual who anticipates issues and thrives on creative, independent problem solving within a rapid, deadline-driven environment. | Essential Responsibilities &amp; Duties: | ETL of data from a wide variety of sources | Database and Data Warehouse design/expansion/backup &amp; recovery | Index management and optimization | Support data sources for Tableau Server and ArcGIS | Stored procedure development and maintenance | Identify new opportunities within GEI where existing business approaches to data can be replaced with a more efficient/automated data flow and presentation of data for analysis | Develop and optimize ETL/SSIS packages to facilitate data transfer between FTP, remote data loggers, Azure, and on-premises databases | Troubleshoot SSIS package permission issues related to execute-as/data source read/write access | SQL Agent Job development and monitoring | Develop data reporting and visualizations as specified by clients using Tableau, SSRS, etc | Perform DML and DDL via tsql/stored procedures executed directly within SSMS and remotely via SSIS | Develop test plans, implementation plans, and project timelines for various data engineering projects | Define, prioritize, communicate, and foster shared understanding of project objectives and scope | Coordinate the development of standard operating procedures (SOPs), technical training programs, and QA/QC procedures for staff and work product | Team with all staff necessary to complete assignments | Collaborate with technical team members to ensure the solution design satisfies project objectives and business requirements | Other duties as assigned | Minimum Qualifications: | 10+ years of experience in a position performing similar data engineering tasks | Proven record of ability to design, manage, and support MS SQL Server and Azure databases | Proven ability to write effective proposals for projects involving database design, data flow, and/or data management and visualization. | Ability to work with the following programming/mark-up/scripting languages preferred: VB.net, python, XML, javascript, and R | Bachelor's Degree, from an accredited college or university | MS SQL Server/Azure certification preferred | Ability to plan and meet budget, develop project plans and meet deadlines | Self-starter with attention to detail and stakeholder needs | Able to critically analyze and solve problems of a complex nature | Excellent Communication skills | Able to work on multiple projects of moderate complexity simultaneously and independently | Proficient in organization and time management skills | Familiarity with engineering and/or environmental projects and data preferred. | Able to work effectively in GEI s partnership model, including a team environment, building rapport and relationships. | GEI is open to filling this position at any of our offices nationwide but is primarily targeting select East Coast office locations including Boston, Glastonbury, Mt. Laurel, Raleigh or Atlanta. | GEI is an EEO/AA/M/F/Vet/Disability employer |  | Physical Job Requirements |  | Sedentary |  | X |  | Light | Medium | Other |  |  | Activity Level Throughout Workday |  | Physical Activity Requirements |  | Occasional |  | (0-35% of day) |  | Frequent |  | (33-66% of day) |  | Continuous |  | (67-100% of day) |  | Not Applicable |  | Sitting | X |  | Standing | X |  | Walking |  | X |  | Climbing | X |  | Lifting (floor to waist level) (in pounds) |  | X |  | Lifting (waist level and above) (in pounds) |  | X |  | Carrying objects |  | X |  | Push/pull |  | X |  | Twisting |  | X |  | Bending |  | X |  | Reaching forward |  | X |  | Reaching overhead |  | X |  | Squat/kneel/crawl |  | X |  | Wrist position deviation | X |  | Pinching/fine motor skills |  | X |  | Keyboard use/repetitive motion | X |  | Taste or smell (taste=never) | X |  | Talk or hear | X |  | Accurate 20/40 |  | Very Accurate 20/20 |  | Not Applicable |  | Near Vision | X |  | Far Vision | X |  |  | Yes |  | No |  | Not Applicable |  | Color Vision (ability to identify and distinguish colors) |  | X |  | Sensory Requirements |  | Minimal |  | Moderate |  | Accurate |  | Not Applicable |  | Depth perception | X |  | Hearing | X |  |  | Environmental Requirements |  | Occupational Exposure Risk Potential |  | Reasonably Anticipated |  | Not Anticipated |  | Blood borne pathogens | X |  | Chemical | X |  | Airborne communicable diseases | X |  | Extreme temperatures | X |  | Radiation | X |  | Uneven surfaces or elevations | X |  | Extreme noise levels | X |  | Dust/particular matter | X |  |  | Other (exposure risks): |  |  | Usual workday hours: |  | X | 8 | 10 | 12 | Other work hours |  |  |  | GEI is an EEO/AA/M/F/Vet/Disability employer",Woburn MA 01801,Data Engineer
Intone Networks,/rc/clk?jk=dfd9d8ebb1aabe5e&fccid=c39dcf34372f1519&vjs=3,"As a Data Engineer at Red Ventures, you'll build data products and create the foundation that powers our machine learning and business analytics efforts. You'll work hand-in-hand with a variety of stakeholders from functional groups across the organization to create end-to-end solutions. Red Ventures is a high-autonomy, high-ownership environment; you'll own your work from idea to production solution. Our data engineering tech stack is primarily AWS and Spark/Scala via Databricks, though we welcome strong applicants from a wide variety of technical backgrounds. We believe that diverse, inclusive teams are better teams. Think of the bullets below as guidelines: if you only partially meet the qualifications on this posting, we encourage you to apply anyway! |  |  What you'll do |  | Iterate - Red tape doesn't get in our way. We believe that ""Speed Trumps Perfection"" so we test and deploy daily. | Autonomy – Aspiring entrepreneurs succeed here because you will have full-ownership over your work from beginning to end. | Innovate – With the belief that ""Everything is Written in Pencil"", we encourage our teams to test new frameworks, learn new languages and challenge the ""status quo"" to make us better. |  | Our Data Engineers on the Financial Services vertical are responsible for building the data processing pipelines that power products like the Bankrate.com authenticated experience, The Points Guy (TPG) mobile app, Lonely Planet and new business launches like our Sage Mortgage Brokerage. This upcoming year we are planning to evolve our underlying data engineering tech stack by evaluating a data catalog product (Alation), building tools to automate common-place ETL changes, and deepening our expertise in streaming data processing. |  | What we're looking for |  | 3 years of experience doing production data engineering/ETL work | Familiarity with Spark is a plus | Scala experience is ideal but a solid knowledge of Python or Java is also acceptable | Experience with one of the major cloud providers (we use AWS but we welcome candidates with experience in Azure or GCP) | Experience with one of the major data warehousing solutions (we use Redshift but we welcome candidates with experience in Snowflake, Oracle or Teradata) | Experience with Airflow is a plus |  | Who We Are: |  | Founded in 2000, Red Ventures is a portfolio of growing digital businesses that bring consumers and brands together through integrated e-commerce, strategic partnerships and many proprietary brands including Bankrate, AllConnect.com and Reviews.com. Headquartered south of Charlotte, NC, Red Ventures has over 3000 employees in offices across the US, as well as London and Sao Paulo. For more information, visit www.redventures.com. |  | We offer competitive salaries and a comprehensive benefits program for full-time employees, including medical, dental and vision coverage, paid time off, life insurance, disability coverage, employee assistance program, 401(k) plan and a paid parental leave program. |  | Red Ventures is an equal opportunity employer that does not discriminate against any employee or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or any other basis protected by law. Employment at Red Ventures is based solely on a person's merit and qualifications. |  | We are committed to providing equal employment opportunities to qualified individuals with disabilities. This includes providing reasonable accommodation where appropriate. Should you require a reasonable accommodation to apply or participate in the job application or interview process, please contact accommodation@redventures.com.",New York State,Data Engineer Financial Services
GEI Consultants Inc,/rc/clk?jk=ab7dc81f94733315&fccid=45234f7301f08e9e&vjs=3,"Role description: | Instinet operates data centers in Secaucus and Somerset, NJ and six colocation sites in US and Canada near financial exchanges. Instinet's corporate headquarter is in New York and has seven regional offices in North America. The data center support team provides support to internal technology teams and clients co-located with Instinet. | The Data Center Operations Supervisor manages the data center support team, performing hardware installation and troubleshooting existing hardware. The supervisor is the escalation point of contact for internal teams on data center related issues and provides reports on projects in weekly meetings using Jira ticketing tools. The supervisor manages relationships with colocation vendors, review invoices, and be aware contractual obligations. | The supervisor creates and uses documentation in Confluence to establish common operational practices globally. The supervisor will champion the use of Jira Service Desk throughout the team through ticketing for incidents, workflow ticketing, generating reports to measure team efficiencies. Additionally, the team will use Jira Software Projects to manage and coordinate project work and deployments. |  | Data Center Operations Supervisor Role Requirements: | Vendor and building management skills especially HVAC, electrical and structured cabling | Familiar with network equipment installation and hardware builds installing Windows and Linux systems through imaging. | Experience with installing FPGA cards into servers is an advantage | Maintain good clear inventory records, rack elevations, and power consumption | Track work assignments and quality control in the team through Jira ticketing system. | Generate reports for management demonstrating team efficiencies and metrics using Jira | Manage remote hands for offices we do not staff directly again using Jira to track hours. | Regularly audit on access to corporate offices, data centers and co-location sites. | Adhere to and monitor the team's compliance to global operational practices, procedures and firm policies | Manage staffing schedules to maximize coverage and resource availability to meet departmental requirements. | Respond to user escalations in a timely manner providing excellent customer service | Routine travel to colocation sites and occasion travel to remote office in the US or Canada. | Flexible hours and some overtime are expected to meet project requirements or team shift coverage. | Instinet is an Equal Opportunity Employer",Somerset NJ,Senior Data Center Engineer
Instinet,/rc/clk?jk=c3907f58e5d272f1&fccid=a44f527c6aea8c1a&vjs=3,"At Verizon Media, we connect people to their passions. With brands like Yahoo, TechCrunch and more, we help people stay informed and entertained, communicate and transact while creating new ways for advertisers and partners to connect. From XR experiences to advertising and content technology, Verizon Media is an incubator of innovation and is revolutionizing the next generation of content creation in a 5G world. | Verizon Corporate Technology's Big Data Group is looking for Big Data engineers with interest or experience in building Machine Learning applications in the domain of distributed systems and NoSql datastores. You will be part of the team building worlds one of the largest Big Data Platform(s) that can ingest 100’s of Terabytes of data that will be consumed for Business Analytics, Operational Analytics, Text Analytics, Data Services. You will | This is a unique opportunity to be part of building disruptive technology where Big Data will be used as platform to build solutions for Analytics, Data Services and Solutions. | Essential Responsibilities: |  Coding skills in Java/Python that meets standards and delivers desired functionality using the technology selected |  Responsible for programming a component in distrbuted systems |  Work independently and contribute to the immediate team and to other teams across business. |  Skilled in breaking down problems, analyze problem statements |  Skilled in core data structures and algorithms | Desired Skills and Experience |  Empathize and having a positive attitude is the most important skill |  Able to work under supervision. Take directions. Listening Skills and strong interpersonal skills |  Hands on experience (coding) in developing solutions on Java/Python |  | Nice to Have: |  Familiarity with Apache Spark |  Familiarity with Machine Learning fundamentals | Verizon Media is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to, and will not be discriminated against based on age, race, gender, color, religion, national origin, sexual orientation, gender identity, veteran status, disability or any other protected category. Verizon Media is dedicated to providing an accessible environment for all candidates during the application process and for employees during their employment. If you need accessibility assistance and/or a reasonable accommodation due to a disability, please submit a request via the Accommodation Request Form ( https://www.verizonmedia.com/careers/contact-us.html ) or call 408-336-1409. Requests and calls received for non-disability related issues, such as following up on an application, will not receive a response. | At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion ( https://www.verizon.com/about/careers/work-culture ) page to learn more. | Currently work for Verizon Media? Please apply on our internal career site.",Dulles VA,big data applications engineer
SADA,/rc/clk?jk=8c3301a3aad98033&fccid=ddb831f1b75b0b0f&vjs=3,"The Data Engineer is a member of the Business Integration, Intelligence and Analytics Team. The engineer will create ETL processes, develop data pipelines, conduct data analysis, create data visualizations, and help develop performance metrics in support of any group within GTL. This work will be done as part of a team creating and supporting GTL's enterprise data lake from a large variety of data sources. |  | This position may be located at our Falls Church, VA office, or our Pittsburg, PA office |  | Responsibilities Include: | Partner with internal business stakeholders to engineer batch and real-time streaming data pipelines | Partner with DBA and other developers to harness application data and design database structures to obtain best results from the data | Develop and maintain data pipelines in Apache Airflow and Apache Spark | Write GitLab CI/CD Pipelines for infrastructure and for unit testing and deploying code | Assist in maintaining efficient operation of reports, visualizations and ETL processes within the GTL back-office | Resolve defects/bugs during QA testing, pre-production, production, and post-release patches | Have a quality mindset and work hard to prevent bugs through unit testing, test-driven development, version control, continuous integration and deployment | Troubleshoot data issues, validate result sets, and implement process improvements | Use acquired information to help build out a big data schema and infrastructure | Contribute to the design and architecture of the project | Conduct design and code reviews | Operate within Agile Development environment and apply the methodologies | Generate tracking and monitoring tools to validate data | Review and respond to system alerts in internal and external alerting and metrics systems | Document your work, system architecture and processes in JIRA, Confluence and other documentation tools. | Respond to user requests and document work done on service tickets in ticketing system | Support proper data governance processes and policies; implement and/or validate data lineage, quality checks, classification, etc. |  | Required Qualifications: |  | 4 Year College Degree in Computer Science, Mathematics, Statistics or a similar quantitative field or comparable experience in lieu of degree | A minimum of 2 years of related experience, to include a minimum of 1 year of work as a Data Engineer, BI Engineer, or Data Warehouse Engineer | Proficient to expert level skills in SQL | Proficient to expert level skills in Python | Experience with relational databases like Oracle, Microsoft SQL Server, MySQL | Experience in developing and maintaining ETL processes | Experience with AWS services like S3, EMR, EC2 or Azure equivalents | Solid analytical and organizational skills, with ability to analyze processes | Excellent presentation and communication skills | Familiarity with Business Intelligence concepts | Solid knowledge of distributed computing |  | Desired Qualifications: |  | Experience in orchestrating data pipelines using Apache Airflow, Snowflake and Spark | Knowledge of Containerization and experience in Kubernetes. | Experience building APIs and Webhooks | Experience with columnar databases like Snowflake, Redshift | Experience working with Business Intelligence software such as Looker, Business Objects and PowerBI | Experience with Informatica ETL software |  |  | GTL, an innovation leader in correctional technology, education solutions that assist in rehabilitating inmates, and payment services solutions for government. GTL leads the fields of correctional technology, education, and government payment services with visionary solutions and customized products that integrate seamlessly to deliver security, financial value, and operational efficiencies while aiding inmate rehabilitation and reducing recidivism rates. | GTL is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, pregnancy or pregnancy-related condition, age, physical or mental disability, veteran or military status, genetic information, sexual orientation, marital status, or any other legally recognized protected basis under federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for employment, verify identity and maintain employment statistics on applicants. |  | #IND123",Falls Church VA,Data Engineer
Verizon Media,/rc/clk?jk=9297bae216e3b9f8&fccid=6a524352011c6b62&vjs=3,"*While Amino is based in San Francisco, you can work remotely from anywhere within the United States!* |  | We're looking for an experienced Data Engineer with deep healthcare experience to help us make sense of the complex, yet critically important world of healthcare data. At Amino, our goal is to provide everyone with access to clear information that lets them make confident decisions about their health. |  | As an Amino Healthcare Data Engineer, you'll be responsible for collaborating with with product managers, data scientists, designers, and engineers to help us make sense of a huge mass and variety of data, in order to help us to increase healthcare transparency, lower costs, and improve outcomes. |  | Impact you will have: | You'll build and maintain a first-class data infrastructure, that supports a continuous delivery of data from many disparate sources to serve a wide-variety of analytical and product needs. | You'll influence the fundamentals of how we model and warehouse data that is accessed by most teams across the company. | What you will do: | Collaborate with a data-focused Product Manager to improve the coverage and accuracy of our data products. | Refine the conceptual model for Amino's entity, and storage patterns and policies for interacting with them. | Work closely with the Data Science team to define new entities, relationships, and insights, and help define technical patterns for how we bridge the gap from initial analyses in notebooks to reliable and automatic production processes. | Communicate with team members in Data Science, Product Engineering, Marketing, and Leadership to help understand and support their needs. | You will work with the following technologies (experience with them all is not necessary, but you should be interested in using or learning them): Python 3.6, Scientific Python, Jupyter; Snowflake, PostgreSQL, Elasticsearch, Redis; AWS, Terraform, Kinesis, S3, RDS, Elasticache, Lambda, Cloudwatch Logs; Git, Docker, Jenkins CI/CD; Spark, Databricks. | Projects you might work on: | Integrate new vendor and internal sources of data into our ETL pipelines. | Build out the next generation of our editorial tools to create and correct data about health entities. | Operationalize our healthcare cost statistical and machine learning models. | Greenfield new projects with processing streaming analytics data from our banking and transparency products. | Skills and experience you possess: | Deep experience in the healthcare domain, specifically dealing with healthcare claims data. | You're experienced with a variety of relational and non-relational databases. (We primarily use Snowflake, PostgreSQL, Elasticsearch, and Redis.) | You are comfortable writing clean, well-tested code in Python. | You have experience handling and working with large datasets and the tools to manage them. | You have worked with ETL pipelines before and are familiar with data warehousing best practices. | You value and practice collaboration and feedback. You question and seek feedback on your own work and opinions. | Your documentation and verbal communication skills are excellent. You can communicate technical vision in clear terms to your peers as well as outside of the engineering team. | Experience with Apache Airflow and DBT are a plus. | About Amino: |  | We are a small team who believes that success is a group activity. You should expect to learn from everyone at Amino, and be excited to share your knowledge. You will play a big part in influencing the shape of the product and the technology we use, and will be empowered to make technical decisions and trade-off calls between scrappy ship-it pragmatism and the long-term maintainability of your code. |  | We believe in collaboration, respect, and curiosity. We believe in having a growth mindset, and have a passion for solving problems that have never been faced before. Everyone's input is valued, be it about code, data models, business models, or product ideas. |  | We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We know the reputation and track record that the tech industry has, and work hard to be exceptional in this regard. |  | Amino values |  | Amino's diverse team members are united by our shared values. |  | Show compassion |  | Act with kindness and thoughtful intention, maintaining a constant focus on our underlying objective: to help others. We're all passionate about making a difference (big or small) for others. As a people-centric organization, we work collaboratively toward our united goals and vision. |  | Raise the bar |  | We're taking an unprecedented, unbiased approach to improve a broken system and the provider selection process. Our courageous team members take initiative, support each other, and thrive on challenges. While we're motivated by making advances, we operate without blame and learn from our mistakes to move forward. |  | Stay curious |  | Big solutions begin with asking the right questions and develop further by harnessing the power of data. At every stage of our growth, we believe it's important to examine our product and processes from all angles and find ways to continuously improve. |  | Act with integrity |  | Success for Amino revolves around our reliable reputation. Whether communicating internally or externally, we strive to be straightforward and transparent. We'll continue to build trust with both our partners and end users by being open to their feedback, keeping our promises, and letting our testimonials speak for themselves. |  | Want to learn more? | Read note from our CTO on the people-centric processes we have adopted. | Read about our Ethics. | Check to see if you know an Amino teammate on LinkedIn. | To get a sampling of the sort of data and analyses we work with, check out some of our data storytelling: | How we put a Price Tag on Hospitals | And, for fun: Strange (yet surprisingly common) holiday injuries, according to medical billing codes",San Francisco CA,Healthcare Data Engineer
GTL,/rc/clk?jk=6eb058255ed71eac&fccid=b374f2a780e04789&vjs=3,"IT Group Description: | The Equity Investment Management Technology (“EIMT”) team creates software to support the research, portfolio management, and trading activities for our institutional and private client equity products. This role is within the EIMT – Research team which is focused on delivering solutions for our buy side equity quantitative and research analysts. The solutions built include tools for quantitative analysis, business intelligence, and data ingestion. | Job Description: | We are seeking a Nashville based Data Engineer to join our Equity Investment Management Technology Research team. | Describe the role: | This is a Data Engineering position focused on enhancing the equity data architecture to provide rapid data onboarding, quality control, and accessibility. This role will focus on building out infrastructure and frameworks using cloud-based distributed compute and storage technologies, continuous integration and deployment tools, data pipeline orchestration, non-SQL and traditional data warehousing (RDMS) technology. | The key job responsibilities include, but are not limited to: | Automation of data ingestion supporting various sources and formats both external and internal. | Implementing a quality control framework for ensuring data consistency. | Cataloging new data sets to facilitate data discovery, lineage, and self-service. | Building business intelligence dashboards to provide data insights. | Assist with ad-hoc data and research requests from the investment team. | Provide support for overnight jobs. | What makes this role unique or interesting? | This role provides the opportunity to experience the full lifecycle of building investment decisions from onboarding data to evaluating factor performance while applying modern processing concepts using cloud based solutions | What is the professional development value of this role? | This role provides opportunity in the following areas: | Learning the equity investment business and engaging directly with end users. | Automating complex data loads and pipelines. | Onboard alternative datasets including learning how to web scrape. | Best practices managing large data sets. | Building technical skills including SQL, Python, and PowerBI. | Applying cloud based technologies including data lakes and data pipelines. | Qualifications, Experience, Education: | BS in Computer Science/Engineering, Finance, Mathematics/Statistics or a related major | 5+ years programming in SQL with experience in relational schema designs and optimizing query performance | 2+ years using Python or another object oriented language (C#, Java) | ETL experience is a strong plus | Working with NoSQL is a strong plus | Building visualizations using Tableau, Qlik, or PowerBI is a strong plus | Skills: | Solid analytical and technical skills | Candidate must be willing to take ownership of projects and show strong client commitment | Must demonstrate good communication skills and be comfortable working closely with business users | Self starter as well as a good team player | A strong desire to document and share work done to aid in long term support | Special Knowledge (nice to have, but not required): | Experience working in the financial industry or knowledge of basic financial statement concepts | Azure experience building data pipelines | Experience using Airflow | People of color, women, and those who identify as LGBTQ people are encouraged to apply. AB does not discriminate against any employee or applicant for employment on the basis of race, color, religion, creed, ancestry, national origin, sex, age, disability, marital status, citizenship status, sexual orientation, gender identity, military or veteran status or any other basis that is prohibited by applicable law. AB’s policies, as well as practices, seek to ensure that employment opportunities are available to all employees and applicants, based solely on job-related criteria. | Nashville, Tennessee",Nashville TN,Data Engineer
New York Life Insurance Co,/rc/clk?jk=0cbf850f61d43b5f&fccid=f0595ebb13247329&vjs=3,"A career at New York Life offers many opportunities. To be part of a growing and successful business. To reach your full potential, whatever your specialty. Above all, to make a difference in the world by helping people achieve financial security. It’s a career journey you can be proud of, and you’ll find plenty of support along the way. Our development programs range from skill-building to management training, and we value our diverse and inclusive workplace where all voices can be heard. Recognized as one of Fortune’s World’s Most Admired Companies, New York Life is committed to improving local communities through a culture of employee giving and service, supported by our Foundation. It all adds up to a rewarding career at a company where doing right by our customers is part of who we are, as a mutual company without outside shareholders. We invite you to bring your talents to New York Life, so we can continue to help families and businesses “Be Good At Life.” To learn more, please visit LinkedIn, our Newsroom and the Careers page of www.NewYorkLife.com. |  | Role Location: New York City (Primary), Clinton, NJ, Dallas, TX, Jersey City, NJ, White Plains, NY also an option | The Enterprise Data Management organization is tasked with creating and delivering a technology strategy for New York Life's Data &amp; Analytics. The continued evolution of this data strategy requires a Data Platform Engineer with AWS expertise to help drive the next phase of the journey! | Serving as a Data Platform Engineer, this provides a unique opportunity to help shape and influence one of the most strategic initiatives at New York Life. The position will be responsible for building enterprise data management platforms both on-premise and in the cloud. It is expected that this engineer will architect and build out the core data platforms for the enterprise. This role will require an advanced skillset across a variety of technologies. This individual will often have to learn on their own and remain on the cusp of new technologies in the Big Data, Analytics, and Cloud space. |  | Primary Responsibilities | Accountable for designing and delivering against New York Life’s data technology strategy | Work with a team of engineers and developers to deliver against the overall technology data strategy | Ensure enterprise data platforms are standardized, optimized, available, reliable, consistent, accessible and secure to support business and technology needs | Understand data related initiatives within New York Life and engineer optimal designs and solutions | Drive knowledge management practices for key enterprise data platforms and collaborate with team members | Collaborate with peers across Enterprise Data Management, to deliver on the overarching strategy | Develop framework, metrics and reporting to ensure progress can be measured, evaluated and continually improved | Stay current and informed on emerging technologies and new techniques to refine and improve overall delivery |  | Desired Qualifications: | 8+ years in a variety of technology – especially AWS, Big Data, Linux, Web, and Databases | Experience with Cloudera's Data Platform (CDP), Hortonworks Data Platform (HDP), and the AWS stack is highly preferred | Deep expertise in data related tools including latest data solutions (i.e. Big Data, Cloud, In Memory Analytics, etc.) | Hands-on experience with Hadoop, NoSQL DBs, Big Data warehouses, and insights on when to recommend a particular solution | Solid experience in standing up enterprise practices for Big Data, Analytics, Self-Service | Proven track record for identifying, architecting and building new technology solutions to solve complex business problems | Capable of working with open source software, debugging issues and working with vendors toward effective resolution |  | Competencies: | Thinks strategically - sets overall direction for solution design and delivery for enterprise platforms aligned to the data &amp; analytics strategy | Results Driven - sets aggressive goals and is accountable for continuously driving improved outcomes, leading change and ensuring high standards | Excellent communication skills, both written and verbal in conveying technical design and approach for delivering technical solution | Pragmatic in his/her approach, delivering incrementally and demonstrating value | Ability to help train/develop junior resources on the team | Other competencies: critical thinker, adaptable, self-starter, demonstrates sound judgment |  | Required Skills: | Proven expertise with AWS technologies (EC2, S3, Glue, Athena, RedShift, CloudFormation, LakeFormation, Terraform, etc.). AWS certifications highly preferred | Solid understanding of Hadoop technologies (YARN, Hive, MR, Tez, Spark, etc.) | Proficient with Unix/Linux (building/assembling packages, shell scripts, configuration management and OS tuning) | Knowledge of configuration management/automation tooling (Puppet/Chef/Salt/Terraform) | Experience with Java, Python and API's | Knowledge of enabling Kerberos and best practices for securing data a plus | Experience working with Vendors/Open Source in the Data and Cloud ecosystem | Knowledge of the open source community (opening issues, tracking issues and identifying problematic issues ahead of time by tracking open JIRA issues in the community) | Understanding of Networking (tracing, packet capture, etc.) |  | Education: | Minimum Bachelor’s Degree in relevant field; Master’s Degree a plus |  | SF: LI-CC1 | SF: LI-PC1 | EOE M/F/D/V |  | If you have difficulty using or interacting with any portions of this Web site due to incompatibility with an Assistive Technology, if you need the information in an alternative format, or if you have suggestions on how we can make this site more accessible, please contact us at: (212) 576-5811. | Job Requisition ID: 82914",New York NY,Data Platform Engineer
AllianceBernstein,/rc/clk?jk=11e7b2e6bdaed5d1&fccid=7c3a1f1f98dde031&vjs=3,"The Role | We are seeking a Data Engineer to join our Data Warehouse team and work in a fast-paced environment. Ideal candidates must be enthused about writing SQL and Python, solving data problems, as well as building data pipelines. They must be a quick learner, self-starting, and have the ability to maintain and build within a horizontal and vertical scaling data warehouse. The role will require extensive hands-on experience with handling data, demonstrated understanding and experience with troubleshooting and fixing data issues, as well as provisioning data pipelines and reports. | You’ll need to demonstrate first-rate attention to detail, and the ability to work quickly and accurately under pressure. You’ll need to be comfortable in developing ideas to solve problems, then executing on those ideas though deployment. In addition to the needed technical skills, the engineer will need strong verbal and written communication skills to interface with many areas of the business. | At SoFi, you’ll become part of a new kind of finance company whose ambition is to help our members achieve financial independence and reach their goals. We aim to be at the center of our members’ financial lives, and to help every member get their money right. Whether our members or potential members want to borrow, spend, save, invest, or protect; Sofi is the best place to go to get it right. SoFi has achieved significant growth, with ambitious plans ahead; but to continue this growth we need great talent. And that starts with you. | Responsibilities | Data modeling | Build and maintain data structures and ETL/ELT data pipelines | Provision, optimize and maintain data feeds to external systems | Write code to validate data quality and clean existing data | Help analytics team, upstream engineering teams, as well as non-technical business users in understanding the Data Warehouse | Be part of an on call support rotation to support the Data Warehouse and it’s automated processes | Creating technical documentation | What You’ll Need: | 3+ years working experience working with automated scripting, data modeling, and data architecture | Proficient in writing and optimizing SQL scripts | Understands database architecture | Working experience in the Python language with an emphasis on data | Working knowledge of some AWS data technologies | Understanding of the software development lifecycle process | Skills and experience in finding, investigating, and resolving data quality issues | Ability to work in a fast-paced environment, meet deadlines, and prioritize a workload | Ability to bring new ideas and promote process improvement | Strong business communication skills that can break down technical problems into business language for non-technical personnel | Nice to Have: | Experience using business intelligence reporting tools (Tableau, Looker, etc.) | Experience writing SQL against several different database platforms | Experience creating data pipelines using Python scripting | Experience using cloud data technologies such as Redshift, Snowflake, or GCP | Experience using AWS data technologies such as (S3, Glue, Kinesis, Lambda, etc.) | Experience in docker | Experience using kafka | Experience in building data feeds and business reports | Why You’ll Love Working Here: | Competitive salary packages and bonuses | Comprehensive medical, dental, vision and life insurance benefits | Generous vacation and holidays | Paid parental leave for eligible employees | 401(k) and education on retirement planning | Tuition reimbursement on approved programs | Monthly contribution up to $200 to help you pay off your student loans | Great health &amp; well-being benefits including: telehealth parental support, subsidized gym program | Employer paid lunch program (except for remote employees) | Fully stocked kitchen with snacks and drinks (When we’re back in the office of course) | SoFi provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion (including religious dress and grooming practices), sex (including pregnancy, childbirth and related medical conditions, breastfeeding, and conditions related to breastfeeding), gender, gender identity, gender expression, national origin, ancestry, age (40 or over), physical or medical disability, medical condition, marital status, registered domestic partner status, sexual orientation, genetic information, military and/or veteran status, or any other basis prohibited by applicable state or federal law. | Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. SoFi does not seek salary history information from job applicants. | #LI-IB1",Helena MT 59626,Data Engineer Data Warehouse
Dascena,/company/PC-Services-inc/jobs/Junior-Data-Engineer-edd07a0f53859485?fccid=958d05dfddcf74dd&vjs=3,"Job detailsSalary$69,205 - $169,581 a yearJob TypeContractNumber of hires for this role2 to 4Full Job DescriptionJob Title: Data Analyst - JrLocation: Plano 7: 2nd Floor (31067-0200)Quantity: 12+ years of experienceFocusing on SQL, Tableau and PythonHeavy data analytics and Tableau DevelopmentMerging data analyst/data engineeringExecuting things in python- jobs are automatedPython- writing scriptsCan be a Beginner in pythonHeavy on SQL and tableau-Job Type: ContractPay: $69,205.00 - $169,581.00 per yearSchedule:8 hour shiftWork Remotely:Temporarily due to COVID-19",Dallas TX 75240,Jr Data Engineer: W2 only : USC/ GC/ GC-EAD
Cardinal Group Companies,/rc/clk?jk=c42507d7fb0bb755&fccid=8e330101e6aee7e3&vjs=3,"Who We Are: | Narcity Media is a digital-only news &amp; travel network that reaches over 23M people monthly. We are a group of passionate and creative individuals who are devoted to creating the best local content for our readers. | We are on a mission to help our readers discover everything that’s new, now, and next where they live — because it’s the most interesting place in the world. |  | What We’re Looking For: | We're looking for a passionate Data Engineer to join our Product Team. In this role, you will implement leading-edge data methodologies and platforms in order to extract the narrative behind our data towards allowing our product, content and business teams to have precise alignment with our audience’s interests. |  | What We Stand For: | Before anything else, values come first. Without clear, shared values, we wander independently and contradict one another. | Everything's harder when we all believe different things about what's important to us, our Company and our Audience. | 1. to produce the highest quality content | 2. to have fun &amp; stay positive | 3. to remain authentic | 4. to always put our audience first | 5. to embrace technology |  | You Will Make an Impact By: |  | Being the voice for data within our organization. | Building the infrastructure, data architecture, pipelines and reporting dashboards that power our content and business decisions. | Building and advocating for innovative data solutions—including an ecosystem that is at the forefront of the first-party data advertising paradigm. | Working with other stakeholders to extract, standardize, aggregate, segment and transform data from a wide variety of data sources. | Implementing data governance in order to ensure integrity and adherence to security and privacy requirements. | Ensuring data hygiene through thorough cleansing of data sources and performing validation and regular audits of data pipelines. |  | Our Ideal Candidate |  | You have: |  | 2+ years experience with building data ecosystems from ideation to implementation. | Be fluent with data technology in addition to business and marketing concepts. | Strong problem-solving skills and critical thinking abilities. | The ability to discuss and gather data requirements from multiple categories of stakeholders and translate them into concise actionable tasks. | The ability to extract, query, transform, visualize, and interpret data from web and other digital platforms. | Understanding of common data formats, including CSV, XML, JSON, graph structures, etc… | Experience with data architecture. | Experience reliably integrating and handling data from multiple files, databases and APIs. | Experience working with Google Tag Manager, Data Studio, Big Query, and Analytics. (Preferred) | Experience working with data warehouse, CRM, DMP, and CDP (e.g. Segment) systems. (Preferred) | Familiarity with online advertising tools and technologies. (Preferred) | Familiarity with web development technologies. (Preferred) | Experience with SQL, Python and R. (Preferred) | Experience with BI tools. (Preferred) | A basic understanding of statistics and sampling. (Preferred) | Experience with AI and machine learning methodologies. (Preferred) |  | You are: |  | Excited about a high-octane, dynamic, start-up environment. | Curious and eager to learn about our audience. | Entrepreneurial. | Extremely detail-oriented, with excellent organizational skills and can maintain a high calibre of work while managing multiple projects simultaneously. | Adaptable and able to pivot quickly to align with changes in trends and market needs. | Comfortable with ambiguity and can make thoughtful decisions based on partial and probabilistic data sets. | Capable of speaking up for your opinions but also capable of seeing answers from differing perspectives. | Someone who believes in our mission of rethinking how local news and travel intersects, and connecting users to their city in a more efficient way. |  | Position Type |  | This is a permanent position. The position is based anywhere in Canada because we are a flexible first company. Compensation will be commensurate with experience. |  | We offer many perks such as a flexible first office, wellness fund, phone and internet allowance, an extensive benefits package, and a competitive salary. |  | More About Us |  | The name Narcity comes from the belief that we should all be narcissists for our city. Narcity is the spark that connects you with the awesome city where you live. Every single day, you can discover what's new, what's now, and what's next, and the tools you need to uncover the people, places, and passions that power your city. |  | Why do what we do everyday? Because your city matters. |  | If this resonates with who you are, take the first step and apply, we can’t wait to talk with you!",Remote,Data Engineer
Bloomberg,/rc/clk?jk=70829abdb4763d06&fccid=2450f074763f2a1a&vjs=3,"Want to work at a fast-growing and award-winning fintech company? In our first two years, we’ve nearly tripled headcount and added two new asset classes to our cutting-edge platform for financial advisors. As a cloud-based company, we transitioned seamlessly into a socially-distanced workforce in 2020 and, with a pace that hasn’t slowed, we’re on the lookout for smart, passionate, and collaborative talent to join our remotely connected team. |  | Job Description | We are looking for outstanding Data Engineers to join our New York-based team who can help build the infrastructure to make data a central part of SIMON. Data driven decisions are very critical for SIMON, as a result, we are looking for engineers that will not only be able to use the data but also understand it and help make actionable decisions. Data Engineers quickly grasp complex and fluid business problems and solve them with robust and creative analyses after having built highly-performant and highly scalable infrastructure to warehouse the data. If you love using the latest technologies, working on creative software projects, and/or thinking about innovative new business plans in your spare time, read on. |  | How You Will Fulfill Your Potential |  | Leverage open-source technologies and cloud solutions to build elegant features that SIMON platform users love | Develop and automate large scale, high-performance data platform infrastructure to drive SIMON business growth and enable data-driven organization | Design and develop reusable components and frameworks for ingestion, cleansing, and data quality | Streamline the ingestion of raw data from various sources into our Data Lake and Data Warehouse | Design data models for optimal storage and retrieval that represent the product entities and meet business requirements | Coordinate closely with sales and product development teams daily to push SIMON’s FinTech strategy and improve the overall profitability of our | What We’re Looking For |  | Bachelor’s Degree in Computer Science, Data Science, Mathematics, Statistics or other quantitative area or related field | 1–4 years of experience with open-source technologies or object-oriented/functional programming, strong ability to write easy-to-scale, high-quality code | Experienced in at least 1 numeric research framework (python/pandas, R/Splus, Octave/Matlab) | Familiarity with OLAP (Redshift, Snowflake) and OLTP (PostgreSQL, MongoDB) databases. | Familiarity with various database designs (Relational, Columnar, NoSQL) | Some background in probability/statistics | Detail-oriented, ability to multitask and work in a fast-paced environment | Ability to work independently while also being a strong team player | Excellent written and verbal communication | Passionate about programming and cutting-edge technologies | Preferred Qualifications |  | Master’s or Ph.D. Degree in Computer Science, Data Science or related field | Professional experience with Python and JVM based languages such as Scala, Java, and Kotlin | Experience building data-pipelines, data-lakes and data warehouses. | Good knowledge of financial markets and financial instruments | Experience with AWS solutions such as Lambda, S3, Kinesis, ElastiCache | Familiarity with AWS and infrastructure-as-code (terraform or cloud formation) | Full-time or internship experience as a data engineer in the financial technology industry is a plus | We offer a competitive salary and benefits, the chance to work with a curated team of top-notch, highly creative talent, and a fun and agile work environment with many perks in the heart of New York City’s Chelsea district, but are operating remotely through at least summer 2021. |  | Meet SIMON | SIMON Markets is a fast-growing and award-winning FinTech company, on the lookout for smart, collaborative talent to join our team. |  | Originally founded within Goldman Sachs, SIMON Markets spun-out in December 2018 under the shared ownership and direction of seven leading financial institutions. Our team works to build transparency around complex, risk-managed financial products by way of a single, modern platform. Our platform provides financial advisors with access to multi-issuer solutions, a depth of educational tools, and advanced post-trade analysis that hasn’t before been possible. |  | By combining state-of-the art technology with the leadership of niche industry experts and an agile start-up mentality, SIMON is well-positioned to reshape our industry. We are leading the way in a space we helped create, we are passionate about pushing boundaries, and we are growing like crazy. |  | No matter which part of the team you join, there is something interesting to work on. Our front-end team is building out our web and mobile presence using React, Redux, and Webpack along with some very sophisticated data visualizations. Our back-end team is using Scala, Akka, Postgres and other open-source technologies to build a micro-services architecture that can scale to handle our ambitious roadmap. Our quantitative engineering team is researching and building novel financial strategies to widen our competitive advantage. Our dev-ops team is creating a development and production environment with Docker and Kubernetes to keep us nimble. Product Management sits in the middle of it all to make it happen.",New York NY 10001,Data Engineer
Best Buy,/rc/clk?jk=ff52e8b565e748e1&fccid=a5b4499d9e91a5c6&vjs=3,"Note: By applying to this position your application is automatically submitted to the following locations: New York, NY, USA; Sunnyvale, TX 75182, USA | Minimum qualifications: |  | Bachelor's degree in Engineering or Architecture, or equivalent practical experience. | 8 years of experience involving mission critical commercial design, DFMA, and Product Development. |  | Preferred qualifications: |  | Data Center or other highly technical industry design or construction experience. | Architectural and Engineering coordination and collaboration experience. | Experience with Mission Critical infrastructure systems (CSA, MEP, FP, FA, Controls, SEC, Networking/Telecom). | Program management skills. | About the job | As a Technical Program Manager Portfolio Engineer, you will participate in the development and maintenance of product roadmaps and product development, leading complex design, manufacturing, and construction programs with cross-functional teams. In this position you may work within a rapidly changing environment as data evolves with technological advances and leadership guidance on adapting to technical trends. You'll gather and review business requirements from stakeholders, and compile data to meet those requirements, and design next generation Data Center Architecture. | Behind everything our users see online is the architecture built by the Technical Infrastructure team to keep it running. From developing and maintaining our data centers to building the next generation of Google platforms, we make Google's product portfolio possible. We're proud to be our engineers' engineers and love voiding warranties by taking things apart so we can rebuild them. We keep our networks up and running, ensuring our users have the best and fastest experience possible. | Responsibilities | Plan product roadmaps and ensure alignment across functional product areas. | Develop and run multiple large-scale design programs and project plans from inception through completion. | Ensure alignment of product and business goals by developing and driving project plans, managing cross-functional teams, and coordinating activities within the established project timelines and budgets. | Drive routine project meetings, manage project resources, budgets, schedules, contractual scopes of work, risk management, and communications plans. | Ensure validation and verification of products align to all key performance indicators and provide visibility to all partners and stakeholders on progress. | Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form.",New York NY,Technical Program Manager Portfolio Engineer Google Data Centers
Narcity Media,/rc/clk?jk=66aa435092a51abb&fccid=387574df7090a75c&vjs=3,"Junior Data Engineer | Location: Austin, Texas | Division: Services | Position: Full-Time | Supervisor: Department Director |  | Summary | This position will be responsible for the maintenance and optimization of our internal sales and marketing databases and proprietary software solutions. This individual must be comfortable with supporting the data needs across internal teams and systems. |  | Position Responsibilities | Maintain EBQ’s sales and marketing databases | Design and optimize analytics models for use with both EBQ and its customer’s sales and marketing databases | Design, optimize, and maintain EBQ’s proprietary Python software solutions | Consistently achieve or exceed monthly target(s) established by supervisor | Adhere to company policies and values | Work effectively in a collaborative work environment and professionally represent EBQ to clients | Support the EBQ Data queue for Data LITE clients, as needed | Perform other duties as assigned | Minimum Qualifications | Bachelor's degree in Software Development, Computer Information Systems, or Mathematics | Strong working knowledge of Python | Strong working knowledge of Excel and/or Google Sheets and the Internet | Strong analytical skills related to working with structured datasets | Experience with data cleansing, analysis, modeling, and visualization | Excellent attention to detail | Highly motivated self-starter with excellent oral and written communication skills | Ability to effectively utilize tools and resources | Process-oriented | Possess strong critical thinking and interpersonal communication skills and have the ability to work with minimal supervision in a team environment | Must be able to type a minimum of 40 wpm | Physical Demands | The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. |  | While performing the duties of this job, the employee is regularly required to talk or hear. The employee frequently is required to stand; walk; use hands to finger, handle or feel; and reach with hands and arms. Must be able to lift 15 pounds at times. |  |  | EBQ is an equal opportunity employer. We provide equal employment opportunities to all qualified employees and applicants for employment without regard to race, color, religion, sex, gender, gender identity, age, marital status, national origin, sexual orientation, citizenship status, disability or any other legally protected status. We prohibit discrimination in decisions concerning recruitment, hiring, compensation, benefits, training, termination, promotions, or any other condition of employment or career development. No visa sponsorship is available for this position. |  |  | If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact 512-637-9696 for assistance.",Austin TX 78744,Junior Data Engineer
Digital First Media,/rc/clk?jk=d4efabfd91a90e17&fccid=cdadf408d58109f2&vjs=3,"Job detailsSalary$105,000 - $125,000 a yearFull Job DescriptionPOSITION TITLE: Senior Data Engineer (Full Time, Exempt) | COMPENSATION: $105k-$125k DOE Exempt / Salaried | DIRECT REPORT: Director, Data &amp; Analytics |  SUMMARY | Cardinal is undergoing a strategic initiative to build a data insights platform leveraging Google Cloud Platform for the data warehouse layer and Domo for visualization. The data warehouse will ingest critical operational, financial, and external data to provide valuable insights to the organization. | RESPONSIBILITIES (Included but not limited to) | Engage with business partners to understand data needs being adept at translating business requirements into technical specifications | Collaborate and work closely with the data and analytics director, who will provide design guidance, best practices, and technical escalation support | Take lead of analysis, data profiling, and design and development efforts, providing additional guidance to all team members as needed and oversee the software development lifecycle | Drive the overall data maturity progress based on leadership guidance and strategy, to include shaping and implementing predictive analytics | Develop analytic solutions, integrations, and microservices with serverless computing, virtual machines, and fully managed databases. | Curate data using dimensional modeling techniques ingesting from source system and utilizing staging layers, integration layers, reporting layers, and data flows for Domo consumption | Utilize Domo to present data in dashboards and visualizations in a meaningful manner | Participate in Cardinal U training as required and champion the Cardinal Culture. | QUALIFICATIONS | 7+ years of relevant experience, including experience in a senior technical role leading and mentoring team members | Experience leading development efforts, mentoring developers, advocating and implementing scrum and development best practices | Experience prioritizing development work for yourself and for the whole team following scrum practices to deliver the highest value features and deliverables to the business customer. | Experience working with users and stakeholders, translating business needs into technical requirements and solution design | Ability to communicate and present to other data engineers, leadership, and describe concepts to non-technical audiences | Experience in data modeling, design, and developing in a comparable data warehouse environment | Experience working with cloud infrastructure, | Understanding of the importance of best practices and supportability requirements | Ability to manage own daily tasks against established priorities to meet commitments | Strong attention to detail in ensuring data quality | Strong drive to continue to learn and build skills | Strong communication (oral and written) and interpersonal skills required to interact with colleagues and clients | Ability to develop productive business relationships with internal team members through cooperation, courtesy, and professionalism | Excellent at troubleshooting issues | Ability to embody the Cardinal Culture and Cardinal’s Core Values every day | TECHNICAL EXPERIENCE | Proficient experience in Python Development and all aspects of SDLC with Source Code Control, GitHub, and Code Reviews | Proficient in PostgreSQL, MySQL, or similar database engines | Experience with Domo, PowerBI, Tableau, or similar data visualization tools | Proficient in developing and understating complex SQL queries, Stored Procedures, and Functions, particularly in data warehouses or data lakes | Proficient with data structures such as JSON, Parquet, CSV, etc. | Strong understanding of Web API integrations for data consumption of distribution | Google Cloud Platform or another comparable cloud platform (e.g., AWS, Azure), with working knowledge of compute services, storage, and databases on the cloud | Experience with statistical programming languages or libraries like R, SciPy, or NumPy for advanced analytics | Experience with building cloud solutions for data pipelines with tools like Dataflow, Airflow/Composer, Apache Spark, or Apache Storm | Experience with Big Data platform like BigQuery, RedShift, HIVE, Snowflake is a plus | WORK ENVIRONMENT | The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. | Incumbents primarily work in an office environment but may also have frequent exposure to outside of apartment buildings and in all areas of the property including amenities and have frequent exposure to outside elements where temperature, weather, odors, and/or landscape may be unpleasant and/or hazardous. | PHYSICAL DEMANDS | The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is regularly required to use hands to finger, type, handle, or feel and talk or hear. The employee regularly required to stand; walk; reach with hands and arms, and climb, stoop, or squat. The employee is often required to sit. Incumbents must be able to physically access all exterior and interior parts of the property and amenities and must be able to work inside and outside in all weather conditions including, but not limited to rain, snow, heat, hail, wind and sleet. The employee must be able to push, pull, lift, carry, or maneuver office products and supplies of up to twenty (20) pounds. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception and ability to adjust focus. Routine local travel is required to attend training classes, client visits, or other situations necessary for the accomplishment of some or all of the daily responsibilities of this position. Overnight travel required to attend company functions, training, property visits, and other situations necessary for the accomplishment of special projects that may be assigned from time to time. | The above job description is not intended to be an all-inclusive list of duties and standards of the position. Incumbents will follow any other instructions, and perform any other related duties, as assigned by their supervisor. | Experience | Required | 7 year(s): 7+ years of software and database development experience | Education | Required | Bachelors or better in Information Technology or related field",Denver CO,Sr. Data Engineer
Annalect,/rc/clk?jk=159d1a31bb0c687b&fccid=dfc44f3b8c44a6db&vjs=3,"What You'll Do | Provide Planning, Designing, Implementation, Operation and Optimization services to Cisco Professional Services Customers around Cisco’s DC products and solutions. Participate in the preparation of proposals and Statements of Work. Establish trusted advisor relationship with customer management and teams on Data Center Infrastructure areas and future technology trends | Participates in or leads complex DC/ Cloud infrastructure (network, storage and compute) design/implementation and migration, performance engineering or problem resolution projects. Solves complex, multi-functional issues that cross many technologies in network, server, storage, O/S, virtualization, consolidation, middleware, security, applications, operations, content switching, caching and/or content delivery networks, and security. Applies good analytic process to help bring any issue to resolution. | Provides technical support for customers during software and hardware upgrades and Proof of Concepts. Delivers migration plans, with a thorough understanding on networked applications and related dependencies. Participates in weekly status meetings and other ad-hoc interactions with customers leading specific asks from the customer and leading customer expectations from Cloud Consumption engagements | Support the PM in resolution of project issues. Track work against the engagement schedule and track financial, resource and material requirements for the engagements. Ensure smooth closure of engagements using relationship with the customer to guide them to the next step i.e. Sampler service to Assessment Service, Assessment Service to Optimization Service. | Guide other consultants and engineers, resolving challenges and ensuring customer success. A high degree of raw analytic horse-power, logical reasoning ability, and curiosity. Be creative with a practical approach and have an intuitive feel for financial consequences of recommendations. | You will be passionate about their area of expertise and be interested in a specialist track as a long-term career objective. Who You'll Work With | CX is a team of outstanding technical guides whose #1 focus is to deliver best-in-class customer experience. We help tackle the toughest business challenges with network-centric solutions that accelerate customer and partner success and dedication. Our success is measured through outstanding financial results, growing customer happiness metrics, industry recognition, and employee happiness scores. | The CX Center provides flexible and innovative ways to help meet customer needs while improving profitability for Professional Services (PS) delivery teams. CXC’s highly technical, global resources support customer engagements in all theaters acting as a precise delivery partner to PS practice and theatre teams. Who You Are |  | Minimum Qualifications: | Requires 3+ years of related experience in IT industry. | Good technical knowledge of the Data Center and its components: Applications, Storage, Network, Server and projects surrounding their planning, design, implementation, operation, and optimization. | Cisco’s Data Center Product Line: ACI, Nexus Products (N7K, N5k, N2k, N1kV), Catalyst products (95xx, 65xx, 49xx switches) and UCS/Virtualization | Knowledge of virtualization tools and concepts. Architectural knowledge of Three-Tiered application environments. Experienced familiarity with cloud computing concepts including virtualization, web services API's, elastic infrastructure, distributed data storage (database, block, object), multi-tenancy, and metered usage patterns. | Storage technologies: file (NAS), block (SAN), object storage, transport (FCoE, iSCSI, etc), replication, backup, and performance. Programmatic control of infrastructure elements (data model, API), integration frameworks and technologies (SP-style). | OSS / BSS, including: Network Management Systems, Service Delivery, Service Fulfillment (including the network inventory, activation and provisioning), Service Assurance, Customer Care Desired Skills: | BS/BE/BTech degree or equivalent | CCIE Preferred, VCP Certification a plus | Passionate, persuasive, charismatic inspiring leader capable of establishing executive interest in how to architect, deploy and operate the virtualized data center of the future. Understands and can articulate the value and business advantage of data center virtualization techniques and cloud architectures. | Strong customer-facing sales and architectural skills. Core expertise to include design concepts, IT analysis/analytical thinking, innovation management, enterprise perspective and process knowledge. Analyzes opportunities with a strategic view; integrates business and technology requirements to achieve cross-domain solutions that work across the enterprise; applies methodologies that are appropriate for multiple users / technology platforms. | Good understanding of the industry landscape (competitors and partners). High-level, out-of-the-box thinking, analytical reasoning, and creative problem-solving skills. Ability to shift from high-level thinking to realistic and pragmatic execution is important. | Solid grasp of the end-to-end IT process, including architecture, design &amp; engineering, implementation, and operations; Excellent written and verbal communications skills: ability to communicate technology strategy and architecture approach to engineers, executives, and Cisco customers. Willingness and ability to work with teams, ability to build relationships using an open and respectful communication and collaboration style Why Cisco | We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation. | #WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference powering an inclusive future for all. | We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (36 years strong) and only about hardware, but we’re also a software company. And a security company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box! | But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.) | Day to day, we focus on the give and take. We give our best, give our egos a break, and give of ourselves (because giving back is built into our DNA.) We take accountability, bold steps, and take difference to heart. Because without diversity of thought and a dedication to equality for all, there is no moving forward. | So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us!",Research Triangle Park NC,Consulting Engineer - Data Center Nexus Switching
Centene Corporation,/rc/clk?jk=569828abb7328ee4&fccid=cbfe4b98d5d8dd69&vjs=3,"**Remote must be located in the United States of America** |  | Company Description: |  | Quartet is a platform that makes it easier for people to get the best mental health care for them. Our technology and services reach people who need care, connect them to the right care, track the quality of their care, and support their care journeys. Backed by $160.5MM in venture funding from top investors like Oak HC/FT, GV (formerly Google Ventures), F-Prime Capital Partners, Polaris Partners, Deerfield Management, Centene Corporation, and Echo Health Ventures, Quartet partners with health insurance plans and health systems in 32 states across the country to help people get the care they need. |  | About the Team and Opportunity |  | We are looking for a Data Engineer that will work on ingesting, transforming, and organizing disparate sources of healthcare data from our partners and internal teams into canonical data models for application and data science insights. Our data platform team works to not only clean, standardize, and load external data sources into our platform but also generates key base features from that data to be utilized by downstream systems and internal customers — from the backend API to data science teams. This role will also work to vend tooling and automation to support our growing data needs and allow data integration team members working with customers to self-service data prior to it being ingested and distributed to the rest of Quartet. You are passionate about data and will collaborate with and learn from other engineers, data scientists, and clinicians. |  |  | Accountabilities |  | Develop and maintain tools and systems to ingest, transform, and distribute numerous application streams as canonical data models and features | Work with subject matter experts and product managers in architecting models to improve reliability and interpretability of data for analytical and business needs | Understanding Quartet business objectives and design services that couple business logic with reusable components for future expansion | Build a world-class, scalable health data ingestion pipeline | Drive down the time to insight by improving the accessibility and organization of data |  | Minimum Qualifications |  | Experience with integration of data from multiple heterogeneous data sources | Experience developing in Python and Spark (Scala and Java a plus); Experience working in distributed systems | Experience with SQL and data modeling |  | Preferred Qualifications |  | Ideal candidate has 4 + years of engineering experience | Experience with healthcare claims data, HL7 and other healthcare data exchange standards - EMR / EHR - a big plus | Experience with AWS Experience with Apache Airflow |  | Employee Benefits for Quartet include: Unlimited vacation, volunteer opportunities, catered lunches, snacks, team events and outings, mental healthcare coverage of 15 free therapy sessions + unlimited copay reimbursements, medical, dental + vision coverage, generous parental leave, commuter benefits, 401K, stock option grants, gym benefits. |  | Employee Benefits for Quartet include: Unlimited vacation, volunteer opportunities, team events, mental healthcare coverage of 15 free therapy sessions + unlimited copay reimbursements, medical, dental + vision coverage, generous parental and military leave, commuter benefits, 401K, and stock option grants. |  | Want to know what Quartet life is like? Click here to meet our team. |  | Quartet is committed to building a diverse team and fostering an inclusive culture, and is proud to be an equal opportunity employer. We embrace and encourage our employees' differences in race, religion, color, national origin, gender, family status, sexual orientation, gender identity, gender expression, age, veteran status, disability, pregnancy, medical conditions, and other characteristics. Headhunters and recruitment agencies may not submit resumes/CVs through this Web site or directly to managers. Quartet does not accept unsolicited headhunter and agency resumes. Quartet will not pay fees to any third-party agency or company that does not have a signed agreement with Quartet. |  | Please note: Quartet interview requests and job offers only originate from quartethealth.com email addresses (e.g. jsmith@quartethealth.com). Quartet will also never ask for bank information (e.g. account and routing number), social security numbers, passwords, or other sensitive information to be delivered via email. If you receive a scam email or wish to report a security issue involving Quartet, please notify us at: security@quartethealth.com. |  | Have someone to refer? Email talent@quartethealth.com to submit their details to us.",New York NY,Data Platform Engineer
Cisco Systems,/rc/clk?jk=d6ef5eb5f1e2a100&fccid=2c1a2db4e678196f&vjs=3,"Overview: |  | About Technica: |  | At Technica Corporation, our goal is to provide exceptional professional services and innovative technology solutions that meet or exceed our customer’s expectations. We specialize in a wide range of advanced information technology solutions from Systems Engineering to Information Assurance, and from Software Development to Product Solutions. From our locations across the United States, we provide technological subject matter expertise, program management and business process knowledge as a trusted advisor in support of our Department of Defense and other Federal Agency customers. | Responsibilities: |  | Technica Corporation is seeking to fill a Researcher (Data Science) position with our IR&amp;D Team, here at HQ in Dulles, VA. This is an ideal opportunity for big thinkers, researchers, coders and data scientists who are interested in working in a small, innovative team. The selected candidate will be working within a team operating in an agile environment, where the focus is on teamwork and project ownership. |  | Responsibilities include: | Support a team of Developers and Data Scientists working on a variety of research and development projects as well as customer projects | Research and analyze cutting edge algorithms and technologies with a focus on Natural Language Processing and data visualization techniques | Effectively communicate results of research and analysis with teammates and senior management in the form of essays, whitepapers, and PowerPoint presentations | Design, Develop and Deploy: | Automated analytic software, techniques, and algorithms | Data-driven analytics; event-driven analytics | Requirements: |  | Bachelor’s in Computer Science, Mathematics, or relevant technical field | Experience with web frameworks (React, Flask, NodeJS) | Experience with Python | Able to obtain clearance | Experience using Linux as a development operating system | Other duties as assigned | Certifications: | Experience with Docker and Singularity container platforms | Experience with Machine Learning toolkits such as Tensorflow, Pytorch | Experience with data visualization | EEO: |  | EQUAL EMPLOYMENT OPPORTUNITY | It is Technica's policy to affirmatively support Equal Employment Opportunity (EEO) for all qualified individuals without regard to color, gender, religion, creed, national origin, age, race, disability, gender identity, genetic information, sexual orientation, marital status, veteran status or any other characteristic protected by law. This policy covers all aspects of the employment relationship including recruiting, hiring, compensation, assignment, promotion, transfer, training, working conditions, employment longevity, retirement, employee benefits and termination. | Technica's EEO philosophy promotes equal employment opportunity throughout the organization. Any form of unlawful employee harassment based on the above mentioned characteristics is prohibited. Equal Opportunity Employer Minorities/Women/Vets/Disabled/Gender Identity/Sexual Orientation.",Dulles VA 20166,Junior Research Engineer/Data Scientist
Google,/rc/clk?jk=a5fd194ccba37599&fccid=f770da67b3b51c62&vjs=3,"Apply Now |  | The Market Data Capacity and Latency (MDCL) team is in charge of monitoring and analyzing latency of the market data pipeline at Bloomberg. We are building a robust monitoring platform, to identify and report end-end latency for real-time enterprise market data products. | Our systems collect, store, and analyze billions of data points every day. They are used to generate analytics for historical latency analysis and ad-hoc troubleshooting of incidents. |  | We also proactively alert and respond to latency events globally, coordinating with component development and SRE teams to fix and prevent future issues. Our work is critical in making Bloomberg more competitive in the real time market data Enterprise space, and we push every component team to perform better and more reliably in order to improve latency across the entire market data pipeline. |  | Our services are written primarily in Python, using BAS, Flask, and Celery. There’s also some C++ thrown into the mix for good measure. If you’re interested in these technologies and excited to join a fast-paced team working on complex technical challenges then we’d love to hear from you! |  | We’ll Trust You to: | Design, build, and deploy resilient and scalable services and applications | Contribute to and support our high standards for code quality, reliability, testing, and automation | Collaborate with other engineers within our team and across Bloomberg to continuously improve and expand our systems | You’ll Need to Have: | 5+ years of professional work experience as a software engineer | Experience working in one of Python/C++ | Strong communication and collaboration skills | Desire to be a part of a strong and inclusive team culture | We’d Love to See: | Experience with Celery or other distributed job queues | Knowledge of Bloomberg’s market data systems |  | At Bloomberg we are extremely proud of our diverse, open, and inclusive culture. We value diversity of thought and perspective in every form. We're looking for engineers with a real passion for writing reusable, efficient solutions to complex problems, who can adapt to an ever-changing market landscape, and who can collaborate and work effectively on small teams to develop software that impacts thousands of financial institutions and decision makers around the world. |  | If this sounds like you, please apply! | Bloomberg is an equal opportunities employer, and we value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. |  | Apply Now",New York NY,Senior Software Engineer - Market Data Capacity and Latency
Quartet Health,/rc/clk?jk=37735ed2011a3e3f&fccid=bbf67f64a44af561&vjs=3,"Description: |  | Surgical Notes continues to expand and is looking for a full-time, salaried Business Intelligence Developer / Data Engineer to be located anywhere in the continental United States. In this role, you will work in collaboration with stakeholders from various departments within the organization to provide data-driven technical solutions to help guide decisions. | The ideal candidate has excellent organizational skills, communication skills (verbal and written), critical thinking skills, and the ability to plan and develop data solutions across the complete data stack. | Our organization prides itself on being built upon a set of strong core values. We are looking for candidates who will actively exhibit Service Excellence, Transparency, Teamwork, Accountability, Hard Work, and a Positive Attitude in their job. | . Requirements: |  | Primary Responsibilities: | Compile, review, analyze, and evaluate various types of data from financial data to revenue cycle management and billing data | Manipulate, format, and load data from various sources to include text files, spreadsheets, Microsoft Access, SQL Server databases, and Practice Management Systems (ETL experience) | Create concise and meaningful reports and dashboards in multiple formats to include trending graphs, tables, and Key Performance Indicators (BI development experience) | Streamline reporting as much as possible by automating queries and distribution of reports | Conduct routine data validation activities to ensure accuracy of information collected and reported | Distribute work products to various stakeholders within the company to include operations, accounting, sales, marketing, and executive team members | Interpret data and analyze results using statistical techniques and segmentation techniques | Maintain documentation of all reporting requests, data parameters and reports in an accurate and accessible manner (reporting catalog) | Interpret raw data to recommend changes to business processes, participate in business meetings to present progress | Perform other related duties as required/assigned | Required Knowledge, Skills, Abilities &amp; Education: | Must be in the continental United States (W2-only; no sponsorships) | High School Diploma plus 4 to 6 years’ experience in an IT support role -or- a bachelor’s degree in Math, Computer Science, Software Engineering, or Management Information Systems | Proven working experience as a data analyst or reporting data analyst | 3+ years utilizing database software to manipulate data, generate reports and analyze data | 3+ years’ experience in SQL programming | Experience with business intelligence, dashboarding, and data discovery tools such as Power BI or Tableau | Direct experience with SQL Server, SSRS, PowerBI, and DAX | Advanced spreadsheet and graphing knowledge (e.g., PivotTables, sparklines, lookups, slicers, filters, Power Query, etc.) | Proven ability to build cross functional relationships and partner with leaders to deliver solutions that are aligned to strategy and support business objectives | Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with intense attention to detail and accuracy | Skills to anticipate additional reporting parameters and confirm requirements | Organizational skills and experience self-managing multiple projects | Relentless problem-solver with excellent listening skills | Ability to work independently and as part of a team | Flexibility to assume new tasks or assignments as needed | Preferred Knowledge, Skills, Abilities &amp; Education: | PostgreSQL experience | Azure Analysis Services experience | Azure Application Insights experience | Experience with revenue cycle management or healthcare billing data to include ICD, CPT, and other healthcare codes | Experience with the following practice management systems: AdvantX, SIS, Vision, Amkai, and HSTPathways | Familiarity with Python | XML and JavaScript experience | HL7 and/or EDI interface experience (bonus)",Remote,Business Intelligence Developer / Data Engineer
MassMutual,/rc/clk?jk=b56dbc1c073211d9&fccid=430cffbf1c607717&vjs=3,"Since 1851, MassMutualâ€™s commitment has always been to help people protect their families, support their communities, and help one another. This is why we want to inspire people to Live Mutual.Â We are people helping people. | A career with us means you will work alongside exceptional people and be empowered to reach your professional and personal goals. Â Our employees are the foundation of what makes MassMutual a strong, stable and ethical business. Â We seek and value unique and varied perspectives and experiences because we believe we are stronger when all voices are heard.Â We invite you to bring your bright, innovative ideas to MassMutual as we continue to help millions of Americans rely on each other. | Together, we are stronger |  | What great looks like in this role |  | Our ideal Advanced Data Engineer is a collaborative leader skilled in data analytics, data modeling, and database design. Youâ€™re also committed to data integrity, are highly analytical, and can work on multiple projects at once. Youâ€™ll use your skills to develop, monitor, and manage data systems across our platform. Additionally, you will act as a mentor to junior team members and coach them on best practices and engineering standards.Â The team culture of working collaboratively, cross-functionally, using new technologies combined with the work/life balance providedÂ by MassMutual are core reasons people enjoy working on the Data Analytics team at MassMutual.Â |  | Objectives of the role |  | Design, build, and measure complex ELT jobs to process disparate data sources and form a high integrity, high quality, clean data asset. | Working on a range of projects including batch pipelines, data modeling, and data mart solutions youâ€™ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business need. | Daily and Monthly Responsibilities |  | Design, build, and measure complex ELT jobs to process disparate data sources and form a high integrity, high quality, clean data asset. | Executes and provides feedback for data modeling policies, procedure, processes, and standards. | Assists with capturing and documenting system flow and other pertinent technical information about data, database design, and systems. | Develop data quality standards and tools for ensuring accuracy. | Work across departments to understand new data patterns. | Translate high-level business requirements into technical specs. | Basic Qualifications |  | Bachelorâ€™s degree in computer science or engineering. | 5+ years of experience with data analytics, data modeling, and database design. | 3+ years of coding and scripting (Python, Java, Scala)Â and design experience. | 3+ years of experience with Spark framework. | Experience with ELT methodologies and tools. | Expertise in tuning and troubleshooting SQL. | Strong data integrity, analytical and multitasking skills. | Excellent communication, problem solving, organizational and analytical skills. | Able to work independently. | Preferred Qualifications |  | Masterâ€™s degree in computer science or engineering. | Familiar with agile project delivery process. | Knowledge of SQL and use in data access and analysis. | Ability to manage diverse projects impacting multiple roles and processes.Â | Able to troubleshoot problem areas and identify data gaps and issues. | Ability to adapt to fast changing environment. | ExperienceÂ with Python. | Basic knowledge of database technologies (Vertica, Redshift, etc.). | Experience designing and implementing automated ETL processes.",Boston MA,Data Engineer - Advanced
Technica Corporation,/rc/clk?jk=a34c3cc029af523b&fccid=2b9b6e64f8a58c92&vjs=3,"Overview: |  | Shift4 Payments (NYSE: FOUR) is a leading provider of integrated payment processing and technology solutions, delivering a complete omnichannel ecosystem that extends beyond payments to include a wide range of value-added services. The company’s technologies help power over 350 software providers in numerous industries, including hospitality, retail, F&amp;B, e-commerce, lodging, gaming, and many more. With over 7,000 sales partners, the company securely processed more than 3.5 billion transactions annually for over 200,000 businesses in 2019. For more information, visit shift4.com. | Responsibilities: |  | We are looking for a Data Engineer to join our Information Technology Group. This position will engage in an Agile-based SDLC to complete data layer requirements (includes but not limited to table creation, stored procedure creation and updates, ETL processes) as well as acquire a deep understanding of business processes and flows to assist with reporting / business intelligence tasks. The Data Engineer will work in a “Rapid Response” team comprised of front / backend developers, business analysts, QA engineers, and will follow the Project Manager’s lead to provide quick and effective support for production systems. | Helping design, develop, debug and optimize stored procedures and views producing data suitable for reporting purposes | Ensuring code standards are followed on all code | Refactoring existing code based on existing DBs and modifying it for newer models | Validating data, performing cross reference checking between source data and outputs | Working with stakeholders and Technology Group to fine tune outputs to ensure all requirements are met for reports | Working closely on related issues with internal business units | Qualifications: | Bachelor's degree in Computer Science or equivalent work experience | 3 - 5 years of experience with SQL Server. | Experience with SSIS and SSRS essential. SSAS preferred but not required. | Thorough understanding of SDLC and how it pertains to the database. | Experience with Jira and Confluence | Excellent written communication skills, particularly in the realm of technical documentation | Basic VB &amp; C# experience a plus. | Experience in a fast-paced development environment a plus. | Experience with an Agile SDLC a plus. | Ability to communicate technical issues to all levels |  | Shift4 Payments is an equal opportunity employer and considers for employment all qualified applicants without regard to race, color, sex, sexual orientation, gender identity, disability, protected veteran status, religion, national origin, age or any other protected characteristic.",United States,Data Engineer
Shift4 Payments,/rc/clk?jk=7288d39c439828cb&fccid=d7c744248a9e6859&vjs=3,"Are you someone who enjoys working with data? Are you a self-motivated thinker who wants to make an impact in the fast-growing healthcare data industry? |  | We are looking for an entry-level Data Engineer who highly values research, wants to work with multifaceted datasets, and craves new challenges in programming. As a Data Engineer, you will have the opportunity to gain first-hand experience integrating and structuring the healthcare data that shapes policy on many key topics, such as the Affordable Care Act, Medicare, and Medicaid. You will also have the chance to work closely with seasoned programmers, developing the skills to work with data management tools and various programming languages. In addition, you will work alongside smart, vibrant people with a passion for the exciting future of healthcare research. |  | The Data Engineer will: |  | Extract, transform, and load (ETL) big data. | Develop complex data processing algorithms that combine multiple data sources, while optimizing run-time efficiency. | Develop data structures, databases, and querying programs which facilitate efficient data access. | Develop data structures from claims and enrollment data which support research and analytic activities of in-house analysts as well as congressional and federal agencies. | Ensure data inventory is complete and accurate through application design, including fault analysis and detection, quality control, and the development of tracking systems. | Collaborate with other Data Engineers and in-house researchers to maintain systems, produce documentation, and educate internal and external users about company resources. | Perform validation checks across multiple sources to verify data integrity as needed. | Perform other duties and responsibilities as assigned. |  | Qualifications Required |  | A Bachelor’s in Computer Science, Statistics, Mathematics, Operations Research, Economics, Public Health, or related field with quantitative emphasis | Strong organizational, planning, and problem solving skills | Team player with strong interpersonal skills | Excellent written and oral communication skills | Familiarity with one or more computer programming languages |  | Qualifications Desired |  | Master’s in Information Management Systems, Statistics, Mathematics, Operations Research, Economics, Public Health, a related field with quantitative emphasis, or 2+ years of work experience in a field with quantitative emphasis | Interest in big data | Interest in making an impact in the field of healthcare policy research | Previous experience in a Data Analyst/Data Engineer position | 1+ years of experience working with programming languages such as SAS, SQL, Python, or R | 1+ years of experience working with databases or data pipelining tools |  | Please submit a cover letter and resume to be considered for this position. |  | Due to the sensitive nature of much of our work, all Acumen employees must undergo a background check. Your employment will be contingent upon your completing, and Acumen reviewing to its satisfaction, a mandatory background check. Employees who work with particularly sensitive information may be asked to undergo an additional background check after starting work. | Required Skills |  | Required Experience",Burlingame CA 94010,Data Engineer I
Acumen LLC,/rc/clk?jk=dbfbfc2d3f93deaf&fccid=9dd30dd046d9ac7a&vjs=3,"Data Center Engineer | Engineering &amp; Tech Ops | Chicago, Illinois |  | Our Agreement with Employees: | DocuSign is committed to building trust and making the world more agree-able for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what's right, every day. At DocuSign, everything is equal. We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better. And for that, you'll be loved by us, our customers, and the world in which we live. |  | The Team: | Our Engineering &amp; Tech Operations team builds and operates complex solutions for global business challenges that cross cultures, legal jurisdictions, and impacts millions of people and businesses every day. We hire people with a broad set of skills and people who want to work on creating never-been-done-before solutions at scale while ensuring world-class reliability and security. Our Agreement Cloud is a revolutionary solution that changes the way people live, work, and come to agreement. |  | This Position: | DocuSign is seeking a Data Center Engineer to support our best-in-class, carrier grade service infrastructure. This Datacenter Engineer will partner across teams from engineering, quality assurance, build, technical operations and more to troubleshoot and resolve infrastructure and network issues and ensure we are meeting our availability SLAs. |  | This position is an Individual Contributor and reports to the Manager of Global Operations. |  |  | Responsibilities: | Coordinate logistics, capacity planning, changes, monitoring, testing, and physical security of data center infrastructurePhysically rack, cable, and manage assets within data centers | Manage resolution of hardware issues through spares replacement or working with the pertinent vendor | Manage and coordinate shipping and receiving of packages and equipment for the data center | Ensure data centers are maintained and documented to standard in order to meet our rigorous compliance requirements | Provide first line troubleshooting and information gathering | Coordinate efforts among internal teams and the DocuSign Operations Center (DOC), ensuring issues are resolved, and facilitate communication across teams | Collaborate with engineering to drive improvements in processes and tools | Provide high quality support following standard operating procedures. Identify missing processes and proactively author SOPs. | Identify recurring issues and escalate appropriately for permanent resolution. |  | Basic Qualifications: | 2+ years' experience in data center environments | Able to quickly assess and address problems, escalating as necessary | Due to government contract requirements, candidate must be a U.S. citizenDue to government contract requirements, candidate must be able to clear a government agency-specific background investigation and may be required to undergo additional background checks, including fingerprinting, drug testing, and /or other government-related investigations to obtain security clearance(s). |  | Preferred Qualifications: |  | Excellent verbal, written, and interpersonal skillsStrong troubleshooting and analytical reasoning skillsDetail oriented and capable of working on multiple problems at onceProject Management experience related to interacting with vendors is desirable |  | About Us: | DocuSign® helps organizations connect and automate how they prepare, sign, act on, and manage agreements. As part of the DocuSign Agreement Cloud, DocuSign offers eSignature: the world's #1 way to sign electronically on practically any device, from almost anywhere, at any time. Today, hundreds of thousands of customers and hundreds of millions of users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people's lives. Plus, we save more trees together! And that's a good thing. |  | DocuSign is an Equal Opportunity Employer. DocuSign is committed to building a diverse team of talented individuals who bring different perspectives to the business and who feel a sense of inclusion and belonging when they join our team. Individuals seeking employment at DocuSign are considered without regards to race, ethnicity, color, age, sex, religion, national origin, ancestry, pregnancy, sexual orientation, gender identity, gender expression, genetic information, physical or mental disability, registered domestic partner status, caregiver status, marital status, veteran or military status, citizenship status, or any other legally protected category. |  | #LI-DS1",Chicago IL,Data Center Engineer
Community Medical Centers,/rc/clk?jk=522db4b97d83bad0&fccid=bab9e00ab9b5be26&vjs=3,"Opportunity |  | Pager is looking to hire a Senior Data Engineer to build and expand the company's data engineering and analytics infrastructure. You'll be working in a fast-paced collaborative environment and contribute to our team's mission: Turn the complex world of healthcare into a simple and beautiful experience for millions of users. |  | Responsibilities |  | Work with internal and external stakeholders to determine their data needs and formulate the technical solutions to solve them | Collaborate with other engineers to implement these solutions in a scalable fashion using the latest open-source technologies | Maintain and expand existing data engineering and analytics infrastructure to ensure their efficiency and stability | Educate and mentor other engineers on data engineering best practices |  | You'll be a good fit if |  | You know data science: | Have a strong hands-on experience with Data Engineering projects, especially with regards to ETL Management and Business Intelligence platforms | Hands-on experience with machine learning tools and libraries including (not limited to) Numpy, Pandas, and Seaborn | Are familiar with data cleaning, sanitization and adequate handling of sensitive information | You know computer science and engineering: | Have a strong CS background to choose the right algorithms, systems approaches and patterns to solve problems: you won't reinvent the wheel | Have a proven track record designing and implementing data driven products | Write production quality code and tests | You can hit the ground running: | M.Sc. or Ph.D. in computer science, engineering, statistics, computational linguistics, or other quantitative field, or equivalent professional experience | 2+ years of experience in a production data science environment | Experience working with AWS and Docker | You have a sense of ownership: | Take responsibility for your projects and pride in your work | You come up with novel solutions to a diverse set of problems: | Ask hard questions and challenge assumptions to ensure that we're solving the right problems | Have flexibility to work on the team's most pressing problems |  | Nice to haves |  | Experience dealing with health system partners data (e.g. EHRs, HIEs, ADT feeds, claims/pre-auth feeds, …) | Experience dealing with FHIR data format and APIs | Experience with storage models coming from the SQL (PostgreSQL, MySQL) and NoSQL (MongoDB, Elasticsearch, Redis, Graph based storage) | RabbitMQ and NodeJS experience | Experience using Docker containers and configuration management systems",New York State,Senior Data Engineer (Remote)
Eventbrite,/company/Professional-Technicians/jobs/Data-Platform-Engineer-abc2934b73e069db?fccid=130ec67f1fba05e0&vjs=3,"Job detailsSalary$154,000 - $190,214 a yearJob TypeFull-timeNumber of hires for this role1QualificationsBachelor's (Preferred)ETL: 4 years (Preferred)Ruby: 4 years (Preferred)Python: 4 years (Preferred)Elasticsearch: 4 years (Preferred)Data warehouse: 4 years (Preferred)SaaS: 4 years (Preferred)Full Job DescriptionResponsibilities IncludeDesign and develop data pipelines, ETL, storage solutions, and workflows that are optimized for speed, fault-tolerance, and scalabilityWork with Application, Machine Learning, and Site Reliability/DevOps engineers to create systems that support their varied data needs while allowing for independent manipulation and iteration of dataDefine robust data schemas for the rapid intake and processing of customer data with diverse structuresSupport product-focused engineering teams with data infrastructure, APIs, and scalable deploymentsArchitect and author internal libraries for use by fellow engineersHelp create data analytics tools for software telemetry and business intelligence purposesCultivate a better understanding of data handling best practices across engineering teamsCollaborate on security efforts for customer dataQualificationsStrong skills with SaaS languages: Should have at least 4 years of experience writing code in one of: Ruby, Go, Python, Scala, Elixir, Java. Should be interested in learning new technologies as well.Strong understanding of relational and non-relational databases such as PostgreSQL, ElasticSearch, and RedisAbility to organize and model data to support many use casesExperience creating and deploying container-based softwareFamiliarity with asynchronous data processing patterns with an added focus on monitoring and loggingPrior experience working with AWS or a similar cloud providerA BS/MS in computer science or related field of study, or equivalent experienceAbility to communicate ideas to technical and non-technical colleaguesJob Type: Full-timePay: $154,000.00 - $190,214.00 per yearSchedule:8 hour shiftSupplemental Pay:Bonus payEducation:Bachelor's (Preferred)Experience:ETL: 4 years (Preferred)Ruby: 4 years (Preferred)Python: 4 years (Preferred)Elasticsearch: 4 years (Preferred)Data warehouse: 4 years (Preferred)SaaS: 4 years (Preferred)Work Location:Fully RemoteVisa Sponsorship Potentially Available:Yes: H-1B work authorizationYes: Other non-immigrant work authorization (e.g. L-1, TN, E-3, O-1, etc.)Yes: Immigrant visa sponsorship (e.g., green card sponsorship)COVID-19 Precaution(s):Remote interview processVirtual meetings",Remote,Data Platform Engineer - remote US Only
Professional Technicians,/rc/clk?jk=cbe5be2c7a6e8a48&fccid=cd16d7546cac8264&vjs=3,"Software Data Engineer - Data |  | Location: Ideally Austin, TX, Denver, CO or Redwood City, CA |  |  We will, however, also look at 100% remote talent based elsewhere in the USA |  | What we do: | We are a cloud-native SaaS machine data analytics platform, solving complex monitoring problems for DevOps, SecOps and ITOps teams. Customers love our product because it allows them to easily monitor and optimize their mission critical, large scale applications. |  | Massive Scale: | Our microservices architecture in AWS ingests hundreds of terabytes daily across many geographic regions. Millions of queries a day analyze hundreds of petabytes of data. |  | At Sumo Logic, we are building a data platform designed to power the analytics and investigations that are common in the Security Operation Centers of large enterprises. It is designed to accept hundreds of billions of events from security-relevant data sources (detection products, network sensors, log shippers, inventory systems) per day. It is cloud-native, with no plan to support an on-premise deployment. It is multi-tenant, and is designed to simultaneously process events from thousands of our customers. And, it is security-focused — it is designed to perform the kinds of stateful analyses that security analysts demand. |  | We are building this platform the way startups should — with ruthless prioritization, and with a live and demanding customer base. By joining as a Data Engineer, you have the opportunity to make our vision a reality, one feature at a time. |  |  |  What You Will be Doing |  | Building, improving, maintaining, and scaling stream processing services. | Writing code. Reviewing code. Revising code. | Giving feedback on our standards. Holding your team mates to them. | Collaborating with teammates on major feature designs. Sometimes, you will own features, sometimes others will. | Helping our team grow organically. We value referrals. We value your feedback on candidates. |  |  | Who You Are |  | You are a software engineer. (We treat our data systems as software systems, and engineer them accordingly.) | You love working with data. (Small data. Big data. All the data.) | You are excited to optimize for events per second (not requests per second). | You have deep experience working with the technologies we use: Kafka, RocksDB, ElasticSearch, JanusGraph, Postgres, Spark, HBase. | You know (or want to write software in) Scala. | You love collecting data about your software as much as writing software that collects data. We measure everything. We make data-driven decisions. | You are collaborative. Nothing this hard can be accomplished by working alone. We work as a team. | Minimum Bachelors in Computer Science or similar technical education. Masters or PhD preferred! |  |  ___________________________________________________ |  | About Us |  | Mission: |  | Democratize machine data analytics through the Sumo Logic platform, bringing real-time data insights securely through the cloud. |  |  | #LI-Remote",Austin TX,Software Engineer - Data
eClinical Solutions LLC,/rc/clk?jk=5bbf454de66f1ed9&fccid=2e7fb2f475ab63dd&vjs=3,"General information | Agency: Kinesso | Job Function: Technology | Location: Conway, United States | Job Ref#: 1587 | Description &amp; Requirements | Position Summary | In the ever-evolving world of ad tech and marketing, Kinesso operates at the forefront of meeting the needs of many of the world’s most compelling brands. Kinesso is building an agile team of exceptional engineers and data scientists based out of Conway, Arkansas. If you are interested in pushing the limits of existing technologies, developing best-in-class data products, and scaling applications globally, we would love to hear from you. We are seeking a Data Engineer to join our identity engineering team. |  | Job Responsibilities | Discuss technical solutions with the development team, communicate goals and facilitate execution. | Work with product team to identify and build scalable data pipelines, for consumption by downstream clients. | Develop and maintain data pipelines for processing large data flows. | Create monitoring and KPIs for data pipelines. | Participate in coding and contribute to the codebase. | Have results driven personality, high enthusiasm, energy, and confidence. | Coordinate with QA team members and work together to achieve automated testing in support of continuous delivery capabilities. | Perform code reviews and provide feedback to developers. | Have results driven personality, high enthusiasm and can work well with others. | Desired Skills &amp; Experience |  Progressive software engineering experience |  Experience with advanced SQL query construction and performance tuning. Experience with building ETL data pipelines using any toolset. Experience with Spark, Hadoop, map reduce and distributed computing highly desired. Experience with Java, Python or Scala in the construction of data pipelines highly desired. Experience with using Snowflake, Redshift and other large scale data warehousing tools is a plus Experience with EMR, glue on AWS cloud infrastructure a plus Experience with AirFlow a plus Able to stay ahead of the curve with new and emerging technologies | Experience in ad tech a strong plus | Experience with agile methodologies highly desirable | Excellent communication and collaboration skills | Employment Transparency | It is the policy of Kinesso, division of the Interpublic group, to provide equal employment opportunities to all employees and applicants for employment without regard to race, color, ethnicity, gender, age, religion, creed, national origin, sexual orientation, gender identity, marital status, citizenship, genetic information, veteran status, disability, or any other basis prohibited by applicable federal, state, or local law. | Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice. | The employer will make reasonable accommodations in compliance with the American with Disabilities Act of 1990. The job description will be reviewed periodically as duties and responsibilities change with business necessity. Essential and other job functions are subject to modification. Reasonable accommodations may be provided to enable individuals with disabilities to perform the essential functions. | For applicants to jobs in the United States: In compliance with the current Americans with Disabilities Act and state and local laws, if you have a disability and would like to request an accommodation to apply for a position with Kinesso, please email accommodation@mbww.com | About Us | Kinesso brings together the best minds in data, technology and media activation to deliver seamless data-driven marketing solutions. As a technology-enabler, Kinesso powers marketing systems through a proprietary suite of applications that integrate ad tech and martech. Kinesso’s application framework spans audience, planning, and activation, delivering insights and business outcomes for leading global brands.",Conway AR,Data Engineer DTI (Open to Remote)
DocuSign,/rc/clk?jk=13f42ee7caff0dc1&fccid=c55f4ad42cee2cd3&vjs=3,"Posted Date: Feb 26, 2021 | Position Title: TECH, DEVELOPER SR I | Pay GRADE 15 | Reports To: | At E15, we are the spark that ignites. Our team delivers next-generation insights based on data, not hunches, to drive business in healthcare, campus, corporate, sports, entertainment, hospitality, and retail industries to help companies make forward-looking decisions to benefit their business and their guests. For more information on what we are about as a company, check us out by following the link below: www.e15group.com | Job Summary | JOB DESCRIPTION | We're looking for a hands-on, collaborative Data Engineer to join our Data Engineering team. When you get out of bed in the morning, you look forward to building solutions, you love working with data, and you enjoy working as a team. In this role, you'll have the opportunity to make code decisions and build cloud infrastructure and pipelines that will deliver data solutions to our Food, Beverage and Sports industry clients. You will help bring best practices to the team while working with a team of Data Engineers. We’re a Python and AWS focused team, using a tech stack of: Python, Spark, Docker, Airflow, and a range of AWS services like SQS, SNS, S3, Redshift, Glue. If you have a git repository, we'd be excited to see it! | CORE RESPONSIBILITIESCollaborate with our reporting, analytics, and data science teams to understand data sources and business requirementsGather, clean, enrich, and transform data to feed internal and external client needsDefine, build, test, and implement data pipelines, batch and streamingWork within a collaborative team, adhering to Agile best practices, documentation, and knowledge sharingManaging personal work output via Jira/Confluence | REQUIRED SKILLS2+ years of hands on Python backend or Data Engineering ExperienceExcellent written and verbal communicatorAttention to detailData validation and testing | PREFFERED SKILLSExperience in DevOps and CI/CD technologies and methodologies | ABOUT E15 | At E15, we are the spark that ignites. Our team delivers next-generation insights based on data, not hunches, to drive business in sports, entertainment, healthcare, campus, corporate, hospitality, and retail industries to help companies make forward-looking decisions to benefit their business and their guests. www.e15group.com | Apply to E15 today! | E15 is a member of Compass Group USA | Click here to Learn More about the Compass Story | Compass Group is an equal opportunity employer. At Compass, we are committed to treating all Applicants and Associates fairly based on their abilities, achievements, and experience without regard to race, national origin, sex, age, disability, veteran status, sexual orientation, gender identity, or any other classification protected by law. | Req ID: 456708 | E15 | HUNTER VANDUSEN | SALARIED EXEMPT",Chicago IL 60651,E15 DATA ENGINEER--REMOTE
Sumo Logic,/rc/clk?jk=cb3ea12890546864&fccid=2472edbe03e34796&vjs=3,"We are witnessing a massive shift of consumer presence from offline to online. With it, there is a need for technologies that enable online businesses to thrive. Bolt is at the center of this universe leading the next generation of e-commerce, having created a best-in-class buying experience from checkout to fraud detection, payments, and more. With our help, retailers can successfully compete with the online retail giants that have the means to invest in technology. | Bolt is looking for founding engineers for the data team, working closely with all teams and cross-functional partners (including product, engineering, and data analysts) to build a foundational data stack powering business analytics. You will be responsible for creating a strategy for data ownership at Bolt and help define the data architecture, data model, and pipelines to drive understanding of Bolt’s business. | Ideal profile: | 4+ years of managing data infrastructure / distributed systems | Strong knowledge of SQL and at least one programming language | Must be willing to both architect solutions &amp; get deep into the weeds of delivering solutions | Experience with big data technologies such as Redshift, EMR + Spark, S3, Glue, Kinesis Firehose, Lambda, etc. | Comfortable thinking about infrastructure as code | Experience with Terraform or similar tools | Nice to haves: NoSQL tech like DynamoDb, Kafka, Kinesis Data Streams. Familiarity with BI tools Eg: Quicksight, Metabase, etc. | What you will do: | We are at the intersection of e-commerce and payments. We collect a lot of data that is key to decision-making at Bolt. |  | In addition to general problem-solving you will: | Participate in creating and executing a roadmap for all things data infrastructure. | Build scalable systems that effectively store and crunch tons of data. | Work with cross-functional partners (product, infra, security, analysts) to power data-driven products. | Create fault-tolerant, timely, and optimized pipelines for data ingestion powering the company’s business analytics. Evangelize best practices with the engineering, infra, analyst teams for building data models, pipelines, and materialized views. Standardize access to data across teams, and build tooling to reuse queries. | Partner with our ML team to make it easier to do feature engineering and build a reliable data stack to power Bolt’s ML products | Own data reliability and help assess and determine which warehousing technologies to use, when to move data between data stores, and help productionize models. | Our stack: | AWS Lambda for data ingestion and Kinesis Firehose to transport them into S3 | Step functions and Lambdas to coordinate workflows | AWS Redshift as the data warehouse + Spark on EMR for doing ETL | Terraform for maintaining infra | Jenkins &amp; CircleCI for build pipelines | Postgres RDS is our application database | Our code base is primarily in Golang &amp; Typescript. However, our data bits are mostly in Python. | Perks: | Competitive compensation | Flexible Paid Time Off | Comprehensive health coverage: Medical, Dental and Vision | Retirement plans | Commuter Benefits + Safe rides programs | Gym and wellness subsidy | Cell phone reimbursement | Monthly team events | Paid parental leave | Awesome teammates! | Bolt is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. If you have a disability or special need that requires accommodation, please let us know.",San Francisco CA,Data Infrastructure Engineer
Bolt,/rc/clk?jk=1cb13299e7bee78a&fccid=9f02eee31b134261&vjs=3,"Job Details | Maxar is seeking a GIS Data Engineer to join our team in Orlando FL. The GIS Data Engineer will work directly with Maxar’s One World Terrain (OWT) Engineering and subcontractor teams by providing data development services. | What you will do day-to-day: | The Data Engineer will be primarily responsible for the collection and management of all source data, development of processes for maps, development of the conflation process, development of the Data Model, and development of the airport production process. The ideal candidate will have prior experience working with mid and senior level engineers in a distributed, DoD development environment. | Minimum Requirements: | U.S Citizen with a current/active DoD Secret clearance. | Bachelors' degree in Software Engineering or Bachelor's degree in Geographic Science(s). | 8+ years of direct GIS toolset use data development experience applied to geographic datasets used Live Virtual and Constructive training systems. | Prior experience working in an Integrated Product Team environment, hands-on experience with SEI software life Cycle processes and controls applied to GIS data generation. | Preferred Qualifications: | Extensive knowledge in a variety of GIS systems including ArcPro and QGIS. | Hands-on experience developing 3D Visual runtime databases for image generator systems and or modern gaming engines such as Unity or Unreal. | Prior experience generating JOG and Topographics Line Maps (TLM). | Prior experience in generating geospatial datasets for U.S. Army Mission Command systems. | 3+ years prior experience working on U.S. Army PEOSTRI managed programs. | Maxar Technologies values diversity in the workplace and is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law.",Orlando FL,GIS Data Engineer
Disys India Private Limited,/rc/clk?jk=8e89c8fb91e2d11f&fccid=520cfaabb27a92ab&vjs=3,"Company Description | When you join Renaissance®, you join a global leader in pre-K–12 education technology! | Renaissance’s solutions help educators analyze, customize, and plan personalized learning paths for students, allowing time for what matters—creating energizing learning experiences in the classroom. Our fiercely passionate employees and educational partners have helped drive phenomenal student growth, with Renaissance solutions being used in over one-third of US schools and in more than 100 countries worldwide. | Every day, we are connected to our mission by exemplifying our values: trust each other, win together, strive for the best, own our actions, and grow and evolve. | For more information about Renaissance, visit https://www.renaissance.com/about-us/, and Schoolzilla, visit https://schoolzilla.com/about/. |  | Job Description | Empower People to Use Data to Increase Student Success | Schoolzilla is a product group within Renaissance whose mission is to change millions of students’ lives by enabling people to use data to run great schools. Teachers and school leaders need lots of data to make good decisions for their students, but most of them can’t get the data they need in any kind of useful, actionable format. Schoolzilla's team of developers, data visualizers, seasoned educators and K-12 experts have done just that: we've made data easy to find, understand, and act on for school districts everywhere. We've already helped thousands of schools make better, faster decisions with our platform. | Every day we live our founding values: Driven by Mission, Better Together, Teammates Matter, Equity At The Center, and Intellectual Humilarity (humility + hilarity). | How You Can Help | Schoolzilla’s Engineering team is responsible for empowering school systems across the country to take data-driven action by delivering timely, accurate, and actionable data. As a member of this team, you'll be at the center of product innovation. You'll collaborate with product managers, designers, and your teammates to take ideas from concept to reality, utilizing your full range of technical skills and a healthy dose of user empathy. | What You’ll Do: | Design and implement fault-tolerant data pipelines and distributed systems that can scale by millions of students | Ensure that timely, accurate data is delivered to the variety of tools and systems that depend on it | Partner with your peer engineers and product owners to bring new features and products to market | Design and build connections to proprietary data systems, allowing us to bring new customers onboard easily and cost-effectively | Define and calculate new key performance indicators for a deeper understanding of student performance at a school or district | Develop and improve internal services, scripts, and tools that will be leveraged throughout the company | Establish new patterns and frameworks for quickly aggregating millions of raw data records into actionable metrics | Craft sample data sets for use in demos and testing | For this role you must have: | Experience designing, architecting, implementing, and supporting modern enterprise-scale ETL / ELT pipelines that are both efficient and intuitive. | Experience architecting data warehouses and data lakes that are organized, performant, and easy to use | Proven fluency in one or more major programming languages such as Python and expertise in SQL | Proficiency with major relational databases, particularly Microsoft SQL Server or Postgres | Experience using cloud-based data warehouses, particularly Snowflake or BigQuery | Knowledge of modern, cloud-based data pipeline best practices | Experience working in an Agile software development organization | Mission-Driven: You feel a deep sense of ownership for your work, and a relentless desire to deliver better results. You are passionate about solving problems for our users. | A Lifelong Learner: You love learning new things. You're curious and ask good questions. You solicit feedback from others, accept it with grace, and act on it. | Flexible: You are comfortable with technical ambiguity and welcome rapid iteration |  | Qualifications | For this role you must have: | Experience designing, architecting, implementing, and supporting modern enterprise-scale ETL / ELT pipelines that are both efficient and intuitive. | Experience architecting data warehouses and data lakes that are organized, performant, and easy to use | Proven fluency in one or more major programming languages such as Python and expertise in SQL | Proficiency with major relational databases, particularly Microsoft SQL Server or Postgres | Experience using cloud-based data warehouses, particularly Snowflake or BigQuery | Knowledge of modern, cloud-based data pipeline best practices | Experience working in an Agile software development organization | Mission-Driven: You feel a deep sense of ownership for your work, and a relentless desire to deliver better results. You are passionate about solving problems for our users. | A Lifelong Learner: You love learning new things. You're curious and ask good questions. You solicit feedback from others, accept it with grace, and act on it. | Flexible: You are comfortable with technical ambiguity and welcome rapid iteration | Bonus Points For: | Understanding of how data is used in K-12 education | Experience with the tools we use, like AWS (including Glue and Lambda), Apache Airflow, and Docker - plus experience with tools we don’t use, but should, and the wisdom to know when to recommend them | Experience with ETL management tools such as Apache Airflow or dbt | Experience working remotely or as part of a geographically distributed team |  | Experience working in an an organization the utilizes DevOps practices | Additional Information | All your information will be kept confidential according to EEO guidelines. | Note: This job can be performed anywhere in the US except for Colorado. | Medical, Dental &amp; Vision | 401K | Generous Tuition &amp; Professional Development Reimbursement | 10+ paid holidays | 2 Volunteer Days off yearly | 14 weeks fully paid family leave | At Renaissance our mission is: “To accelerate learning for all children and adults of all ability levels and ethnic and social backgrounds, worldwide.” Many of us choose to work at Renaissance because we are driven by this mission. | Inherent in a mission that strives to serve “all children and adults” who represent “all ability levels...and backgrounds,” is the need to recognize the importance of Diversity, Equity, and Inclusion (DEI) in our culture, in our work, and in our products. | Frequently cited statistics show that women and underrepresented minorities will only apply to roles if they feel they meet 100% of the qualifications. At Renaissance, we encourage you to break through that statistic. Roles evolve over time, especially with innovation, and you may be just the person we need into the future. NO ONE ever meets 100% of the qualifications. We hope you're open to learning new skills in order to grow with us. Make our team, your team!",New York NY 10003,Principal Software Engineer Data (Schoolzilla by Renaisssance) - Remote US
Compass Group,/rc/clk?jk=fa22764cc95195dc&fccid=313b746e8d8ef297&vjs=3,"Overview: | This is the second of a four level job series within the CMC Corporate Information Systems, Clinical Systems Group. This position will help design, develop and maintain the data/information solutions to meet particular business/clinical needs. Working with various teams internal and external to the Corporate Information Systems department. A level two Data Developer/Engineer will periodically lead and participate in the gathering of the data/information specifications. Utilizing technical and analytical skills, extract the data from the various supporting systems and presents the data in the appropriate tool and format to the customer. Data Developer/Engineer will also change and/or update existing solutions. In addition, a Data Developer/Engineer will help support the analytical and database systems that house and distribute the data. This position is part of a 11 person team that serves the entire network. |  | We know that life goes on while you’re at the office or out visiting patients, so we’re here to support you with competitive pay and excellent benefits. We know that our ability to provide the highest level of care begins with our incredible staff. Because of this, we provide unparalleled resources, benefits and a variety of incentives - including referral bonuses, enhanced visits rates, special compensation packages, and generous relocation bonuses. Responsibilities: | Your Career at Community | Opportunity. Challenge. Growth. |  | You will participate and may coordinate in the implementation of assigned tasks and projects related to Data Services. Demonstrates a proficient level understanding of data and application knowledge consistent with business/clinical needs. This role operates with guidance from leadership and more senior Data Developer/Engineer. Demonstrates the ability to participate and sometimes lead projects. You are knowledgeable in data structures, data models, systems and data/information extracting, aggregation and use of data Qualifications: |  | Experience and education minimum requirements: | MINIMUM REQUIRED: | Bachelors’ degree in a Technology, Clinical or Business related field. | More than two years’ experience implementing and supporting data/information systems. | Equivalent combination of education and experience may be substituted. |  | PREFERRED: | Bachelor’s Degree in Technology, Clinical or Business Related field. | More than three years’ experience implementing and supporting data/information systems. | More than two years of Project Management experience. | Position can be remote.",Fresno CA 93721,Data Developer Engineer
SoFi,/rc/clk?jk=257e2ba564facda7&fccid=7c3a1f1f98dde031&vjs=3,"Who we are | SoFi is a digital personal finance company whose mission is to help its members achieve financial independence to realize their ambitions, whether that be to buy a house one day, start a family on their own terms or be debt free. We aim to be at the center of our members’ financial lives, and to help every member Get Their Money Right®. By joining SoFi, you’ll become part of a forward-thinking company that is transforming financial services by embracing technology to build innovative loan products, investment tools, and more. One of the fastest growing fintech companies, we’ve grown from 250 employees in 2015 to over 1,500 employees today, with over 1 million members. With offices across the US, we offer the excitement of a rapidly growing startup with the stability of a seasoned management team and some of the best talent around. As an employer, we strive to hire employees who are committed to both our company’s mission and our desire to build the best culture in the world. If you are driven, passionate about what you do, and excited about the SoFi mission, we would love to hear from you. | The role | SoFi runs on data! In this role you will be contributing to the long-term success of SoFi’s big data vision of establishing a democratized data platform that enables teams to ingest, model, and consume data with confidence. Join the Big Data Infrastructure team as it refines this vision and establishes industry-leading standards for data lifecycle management ushering in best-in-class architectural components and processes in extracting value from disparate data sources. The success of this team is central to the success of the company and your contributions will have very visible and lasting impact. | As an engineer on the big data platform at SoFi, you'll be tasked with building critical components and features. You will implement battle-tested patterns and interfaces, squash bugs, refactor code and continually grow as an engineer. The ideal candidate has a strong software engineering background and problem-solving ability. Additionally, you will demonstrate SoFi’s core values by honing your skills as an effective communicator, showing personal responsibility, and setting ambitious goals. If you like working on problems with tangible and lasting impact, we would love to have you in our team! | What you’ll do: | Directly contribute to high-performance batch and stream data processing systems | Manage and evolve cloud-based data warehousing services | Work with amazing product and business managers to deliver compelling features | Partner with team members to implement and design interfaces and abstractions | Sharpen your skills as a developer and build technical domain knowledge on data infrastructure modernization | What you’ll need: | Bachelor’s degree, ideally in a technical field | 2+ years experience as a software Engineer | Intellectual curiosity and aptitude to pick up new technical skills | Skilled at reading and understanding technical documentation | Ability to focus on tasks and drive work to completion | A passion and instinct for data quality | Ability to influence outcomes and discuss technical challenges with team members | Strong fundamentals of data structures, algorithms, and design patterns | Software development experience in Java, C/C++, or C# | Experience building solutions using public clouds (Azure, AWS, GCP) | Proficiency with SQL and strong Python development skills | Familiarity with big data platforms and tooling (AWS, Snowflake, Kafka, Luigi, Hadoop, Hive, Spark, Cassandra, Airflow, etc). | Nice to have: | Prior experience with CI/CD (gradle, git, automated testing and deployments) | Data exploration and analysis experience using SQL/Python/R/Tableau. Experience surfacing insights using math/statistics/ML techniques | Passion and curiosity for FinTech | Why you’ll love working here | Competitive salary packages and bonuses | Comprehensive medical, dental, vision and life insurance benefits | Generous vacation and holidays | Paid parental leave for eligible employees | 401(k) and education on retirement planning | Tuition reimbursement on approved programs | Monthly contribution up to $200 to help you pay off your student loans | Great health &amp; well-being benefits including: telehealth parental support, subsidized gym program | Employer paid lunch program (except for remote employees) | Fully stocked kitchen (snacks and drinks) | SoFi provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion (including religious dress and grooming practices), sex (including pregnancy, childbirth and related medical conditions, breastfeeding, and conditions related to breastfeeding), gender, gender identity, gender expression, national origin, ancestry, age (40 or over), physical or medical disability, medical condition, marital status, registered domestic partner status, sexual orientation, genetic information, military and/or veteran status, or any other basis prohibited by applicable state or federal law. | Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. | SoFi does not seek salary history information from job applicants. | #LI-IB1",San Francisco CA 94129,Data Engineer
Pager,/rc/clk?jk=0db51d2b58bc7076&fccid=2ac1f406582acd9b&vjs=3,"Job detailsSalary$70 - $80 an hourJob TypeContractFull Job DescriptionJob Description : |  | Great Data Engineer opportunity at DISYS. Would you like to be a member of an Advanced Analytics team and work on interesting projects to help clients make data-driven solutions? Would you like to be a Big Data Engineer who works closely with the client and data scientists to understand their business needs and design a scalable data processing infrastructure? If you like what you are seeing, please read further. |  | What Will You Be Doing: |  | You will work closely with your team including analysts, TPMs, and data scientists to define tasks, provide estimates, and work together to deliver a world class solution. The ideal candidate will have the balance of technical skills and business acumen to help the client better understand their core needs while understanding technical limitations. Your solutions will enable machine learning and data-driven decisions to improve safety, customer experience, and efficiency. | Experience partnering &amp; communicating with executive management team to understand business needs and pain points | Ability to communicate engineering concepts to business stakeholders | Passion for building large scale machine learning pipelines | Adept at developing and iterating solutions rapidly |  | Requirements: | Bachelor's degree in Business or Technology required | 5+ years of previous experience in Data Engineering, | 3+ years of experience in PySpark | Advanced engineering skills with Python | Previous experience with Cloud platform, AWS preferred | Data engineering implementation experience in Python, Spark and PySpark, or SparkSQL | Previous experience with very large data | Demonstrated ability to identify business and technical impacts of user requirements and incorporate them into the project schedule | Strong communication and interpersonal skills | Ability to work both independently and as part of a team | Work across team to solve technical roadblocks for our customers. |  | It is plus… | Experience building data and computational systems that support machine learning | Knowledge of AWS services | Knowledge of modern software delivery practices, including source control, testing, continuous delivery | Previous Agile experience | Data streaming experience in Spark |  | Job Details: | Type: 6-12 month W2 contract | Start Date: 02/22/2021 | Contract End Date: 08/27/2021 with a possibility of extension | Time Zone: If you a remote candidate, you need to be able to work during PST timezone. | Location: Seattle, WA | Remote Work: They are currently 100% remote. | Pay: W2 $70-$80/hourly | Benefits: Health, dental, vision, 401k, FSA and HSA plans, and disability and health insurances from DISYS | Work Authorization: You must be authorized to work in the United States. | This person must be authorized to work in the United States. We are not open to corp-to-corp candidates at this time. |  | For immediate consideration please contact Mika Paeaekkoenen at . |  | 5.00-8.00 Years",United States,PySpark Data Engineer
DISH Network,/rc/clk?jk=73610cc1e338e35f&fccid=ef46103379dd7975&vjs=3,"Job detailsJob TypeContractFull Job DescriptionContract W2, Contract Corp-To-Corp, 12 Months | Depends on Experience | Job Description | ontract | San Francisco, CA | The ideal candidate will be responsible for developing high-quality applications. They will also be responsible for designing and implementing testable and scalable code. | Responsibilities | Develop quality software and web applications | Experience with Big Data | Design highly scalable, testable code | Discover and fix programming bugs | Qualifications | Bachelor's degree or equivalent experience in Computer Science or related field | Development experience with programming languages | SQL database or relational database skills | Strong exp with Scala and Spark | Please send resumes to Manisha at mmehta (AT) sprucetech.com / | Company Information | Founded in 2006, Spruce Technology is a leading provider of IT services for the public, private, and federal sectors, specializing in Strategic Staffing Services, Solutions &amp; Professional Services, and Advisory Services. With clients in 30+ states, Spruce is a certified MBE and MS Gold Partner. | Dice Id : 10215935 | Position Id : DE_MM_S",San Francisco CA,Data Engineer (Spark and Scala)
Spruce Technology,/company/eProSoft-America-Inc/jobs/Senior-Data-Engineer-dfeb5bee9d915ef9?fccid=5fa67da353b58aa6&vjs=3,"Job detailsSalaryUp to $65 an hourJob TypeFull-timeContractQualificationsAZURE: 7 years (Required)Scala: 7 years (Required)Bachelor's (Preferred)Full Job DescriptionLooking for a very strong Data Engineer for a long term contract opportunity.Exp.: 10+ YearsMandatory Skills: Spark, Scala, AzureSkills Required: Automate the creation of data marts using ETLsStrong experience in Spark, Scala &amp; AzureExperience developing Spark jobs/processesUnderstanding of Data modeling and schema designKnowledge of AWS or Azure Functions (serverless compute) * * Agile (Scrum) development – ability to accurately size user storiesStrong SQL skillsJob Types: Full-time, ContractPay: Up to $65.00 per hourSchedule:8 hour shiftEducation:Bachelor's (Preferred)Experience:Data Engineer: 10 years (Required)AZURE: 7 years (Required)Scala: 7 years (Required)SPARK: 7 years (Required)Contract Renewal:LikelyWork Location:One locationCompany's website:www.eprosoft.comWork Remotely:Temporarily due to COVID-19",Las Vegas NV 89119,Sr Data Engineer
Maxar Technologies,/rc/clk?jk=138c906404fff23e&fccid=a46683ec51ead49e&vjs=3,"Data Engineer- Erlanger, KY | This is an exempt level position. |  | Position Summary: |  | The Data Engineer will work on designing, building, and implementing data foundations to support strategic advanced analytics projects that drive strategic growth for ADM (go-to-market strategies, pricing intelligence, economic and trade data modeling, and many others). This involves liaising with business teams that build or acquire the underlying data, application owners to understand raw or source data structures, and data scientists and end users to accurately architect strategic data marts. Finally, architecting and building the data pipelines, constructing and modifying data marts, preparing (cleaning, transforming) data, and collaborating with digital and analytics team members to design impactful business solutions. The Senior Data Engineer should be able to coach and lead other junior team members as well as drive critical solutions independently. | Additional responsibilities of this role will include the following: |  | Creating custom queries, scripts, and job runs for ad hoc data processing and/or data investigation in Python | Understanding and navigating a wide array of source data systems (enterprise data warehouses, relational databases, IT systems, in house and COTS applications, documents, APIs, unstructured data, big data, NoSQL databases, etc.) | Understanding of software development principles such as project architecture, version control (Git), test-driven development, etc. | Automating data flows with resilient, production-grade code | Monitor and troubleshoot data issues in solution pipelines | Developing prototypes and proof of concepts visualizations/dashboards for the selected solutions | Establishing and maintaining information security standards | Mapping data ecosystems and creating data model diagrams | Extending traditional ETL solutions (Informatica, Alteryx, SQL, etc.) | Analyzing data and developing insights (e.g., via data visualization tools like Excel/Power BI/Tableau |  | Essential job functions: |  | Work simultaneously on multiple projects without sacrificing delivery | Work with data warehouse and data integration teams to ensure successful delivery of enterprise data warehouse solutions | Have natural curiosity and real passion for data investigation, talent for strategic leadership, interest in digital product development, and experience in people management | Lead development of strategy in an environment rich in complex biological, environmental, operational, global economic, and business data | Establish and manage collaborations engaging business units to develop novel data analytic approaches and coordinated decision science solutions | Self-starter who is organized , communicative, quick learner, and team-oriented | Prepare and present ideas and recommendations to colleagues and management | Document solutions through high/low level design documentation | Learn new groundbreaking data engineering and analytic tools as needed | Job Requirements: |  | 5+ years of hands-on experience crafting and implementing data and analytics solutions | 1-3 years of work experience or equivalent academic background in data processing using Python | Fluency in SQL | Experience delivering Cloud based Data Solutions in Azure | Experience working with Big Data technologies (Spark, Kafka, DataBricks, Hive, or equivalent) | Knowledge in programming scripting languages like Java, Scala and Python | Experience in delivering solutions using iterative development methodologies like Agile, KanBan, DevOps, etc. | Working knowledge of BI architecture, data warehousing concepts, and data integration standard methodologies | Experience with metadata management discipline and practices | High accountability with a demonstrated ability to deliver | Strong written communication skills including functional design documentation | Strong collaboration skills working with design and development teams | Motivated, demonstrates initiative and leadership | Able to critically think and be solution-driven with strong interpersonal skills and able to work easily with team members | Great organizational, time management and problem-solving skills | Possesses a professional attitude | Required education: |  | 4-year Bachelor’s degree or equivalent in IT, Computer Science, science, engineering, statistics, programming or mathematical field | About ADM: | At ADM, we unlock the power of nature to provide access to nutrition worldwide. With industry-advancing innovations, a complete portfolio of ingredients and solutions to meet any taste, and a commitment to sustainability, we give customers an edge in solving the nutritional challenges of today and tomorrow. We’re a global leader in human and animal nutrition and the world’s premier agricultural origination and processing company. Our breadth, depth, insights, facilities and logistical expertise give us unparalleled capabilities to meet needs for food, beverages, health and wellness, and more. From the seed of the idea to the outcome of the solution, we enrich the quality of life the world over. Learn more at www.adm.com. | EEO | ADM is an equal opportunity employer and makes employment decisions without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability and veterans status. | Ref ID | #LI-41182201_NA1",Erlanger KY,Data Engineer
Archer Daniels Midland Company,/rc/clk?jk=c8e85003f086d429&fccid=47a15cde3c2472ae&vjs=3,"Job Locations: US-VA-Smithfield Your Opportunity: |  | Headquartered in Smithfield, Va., since 1936, Smithfield Foods, Inc. is an American food company with agricultural roots and a global reach. Our 40,000 U.S. employees are dedicated to producing ""Good food. Responsibly.®"" and have made us one of the world's leading vertically integrated protein companies. We have pioneered sustainability standards for more than two decades, including many industry firsts, such as our ambitious commitment to cut our carbon impact by 25 percent by 2025. We believe in the power of protein to end food insecurity and have donated hundreds of millions of food servings to our neighbors in need. Smithfield boasts a portfolio of high-quality iconic brands, such as Smithfield®, Eckrich®, and Nathan's Famous®, among many others. For more information, visit www.smithfieldfoods.com, and connect with us on Facebook, Twitter, LinkedIn, and Instagram. |  | As a Data Engineer in the Finance Data Analytics department at Smithfield Foods you will be responsible for expanding and optimizing our data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. You will be responsible for supporting the data pipeline needs of the Finance Data Analytics team, ensuring data is delivered accurately and automated. You will work to support the Data Analytics team as they work on projects from various functional areas of the business. You will implement process improvements to streamline the efficiency and accuracy of the reporting process. You will build data pipelines to support the delivery of daily/weekly/monthly reports. You will also be involved in ad hoc analysis of large datasets to quickly provide insight into the business. | Core Responsibilities: | Create and maintain optimal data pipeline architecture | Function as a support service for the Data Analytics team to facilitate the development of standard reports, curated data pools, and ad hoc queries | Participate in the development of reports, metrics, models, scorecards, and dashboards by gathering data and transforming it into stories/insights that drive recommendations and decision-making. | Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery | Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Python, Altreyx, Power BI, etc. | Build automated Processes using scripting tools | Hold Training sessions on how to use the various tools and reports created by the department | Communicate effectively with report stake holders to ensure strong understanding of the methodology behind all calculations | Perform ad hoc analysis using data from various systems and explain results of analysis to all levels of management | Create SOPs and documentation for processes | Demonstrate continuous efforts to improve processes, increase data integrity, and provide quality customer service to our business partners. | Provide comprehensive analytical and fact-based reporting to support and/or identify opportunities to impact operational results, make changes to processes, and support strategic initiatives. | The above statements are intended to describe the general nature and level of work being performed by people assigned to this job. They are not intended to be an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. May perform other duties as assigned. | Qualifications: |  | To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals to perform the essential functions. | Bachelor's Degree from a regionally accredited four-year college or university in Accounting/Finance, Computer Science/Data Analytics or related field and 2+ years of relevant work experience in a Manufacturing/Logistics business environment, or equivalent combination of education and/or experience, required. | Ability to build models using advanced features of Excel. | Strong experience with Power BI / Tableau, Power Pivot, SAP, SAP Analysis for Office, BPC, Bex Query Designer. | Strong Experience in analyzing data within relational database systems such as MS Access, MS SQL Server, and MySQL. | Experience with ETL Tools such as Alteryx, Paxata, DataGuru or equivalent technologies, preferred. | Understanding of Windows environment command line tools, .BAT files, and Task Scheduler, preferred. | Experience with HTML/CSS/Javascript and a backend technology like python-flask, preferred. | Experience with Data analysis in Python using Pandas/Numpy/Jupyter Notebooks, preferred. | Ability to quickly learn new technology and business logic. | Superior analytical and problem-solving skills with superb attention to detail. | Strong written and verbal communication skills. | Ability to work independently in a fast-paced and rapidly changing and ambiguous environment. | Ability to work well with others in fast paced, dynamic environment. | Ability to be respectful, approachable and team oriented while building strong working relationships and a positive work environment. | Up to 10% travel required. | Careers and Benefits: |  | To learn more about Smithfield’s benefits, visit smithfieldfoods.com/careerbenefits. | About Smithfield Foods: |  | Headquartered in Smithfield, Va. since 1936, Smithfield Foods, Inc. is an American food company with agricultural roots and a global reach. Our 40,000 U.S. and 15,000 European employees are dedicated to producing ""Good food. Responsibly.®"" and have made us one of the world's leading vertically integrated protein companies. We have pioneered sustainability standards for more than two decades, including many industry firsts, such as our ambitious commitment to cut our carbon impact by 25 percent by 2025. We believe in the power of protein to end food insecurity and have donated hundreds of millions of food servings to our neighbors in need. Smithfield boasts a portfolio of high-quality iconic brands, such as Smithfield®, Eckrich® and Nathan's Famous®, among many others. For more information, visit www.smithfieldfoods.com, and connect with us on Facebook, Twitter, LinkedIn and Instagram. | EEO/AA Information: |  | Smithfield, is an equal opportunity employer committed to workplace diversity. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, age, gender identity, protected veterans status, status as a disabled individual or any other protected group status or non-job characteristic as directed by law.",Smithfield VA 23430,Data Engineer
Smithfield Foods,/rc/clk?jk=b09528c0d724455d&fccid=3c371b712ace44cf&vjs=3,"Department Summary: | DISH Wireless is building a next-gen 5G network to disrupt the wireless industry and fuel innovation in transportation, health care, education, sustainability, city management, and agriculture. | We’re driven by curiosity, pride, adventure, and a desire to win – and we’re looking for people with boundless energy, intelligence, and an overwhelming need to achieve. Join us as we embark on our greatest adventure of all. | Opportunity is here. We are DISH Wireless. |  | Job Duties and Responsibilities: |  | DISH Wireless is seeking a Transport Data Traffic Engineer with a deep understanding of how the Internet works on a global and local basis. This position requires a solid understanding and experience with DNS, IP addressing, BGP, etc. to support efficient, low latency processing of all DISH mobile data traffic. Traffic is expected to grow as the new 5G national network is built and candidate should have the ability to define and specified a range of connectivity options based on changing traffic. | Candidate will help write requirements for an RFP to screen ISPs for their ability to interconnect Dish data traffic and later to manage the peering and other interconnect partners. Candidate should have familiarity to network operators and other service provider networks and processes including Local Exchange Carriers (LEC), Tier 1 ISPs and specialized carriers. | The Transport Data Traffic Engineer will be responsible for evaluating and performing detailed engineering activities supporting the design, development, and optimization of the data interconnection in a wireless network. This person may also be responsible for specifying equipment, APIs, virtual resources, etc. to create world class data services for Dish customers. | This role is a key, strategic part of our business and presents an opportunity to implement and optimize the latest networking solutions. | Responsibilities fall into the following categories: | Experience working with Internet Exchange Points (IPX) and Tier 1 telco hotels/data centers. | Define data interconnection requirements including APIs, transport interfaces and other interconnection. | Develop peering, IP transit and other interconnection requirements for the 5G network. | Manage data traffic flows and recommend network changes as traffic profiles change. | Define and later support the implementation of peering agreements and interconnection. | Understanding of traffic engineering, Internet access services and Ethernet services. | System architecture including reliability, security and redundancy requirements. | Working knowledge of various applicable standards and frameworks: IETF, MEF, etc. | Actively contribute to improvements and participate in innovation, lessons learned, and knowledge sharing activities. | Ability to define and manage performance with SLAs and KPIs. | Ensure that work meets all applicable engineering, financial, planning, and operational standards. | #LI-SL3 | Skills, Experience and Requirements: |  | The successful Transport Data Traffic Engineer will have: | BS in a technical field or an equivalent combination of education and experience | 8 + years Internet / Telecommunications network experience | Working knowledge of DNS and BGP | Working knowledge of virtualization and software defined networking | Strong understanding of O-RAN and other 5G architectures | Familiarity with optical networking and interfaces | Extensive work with data centers and familiarity with virtualized environments such as Amazon Web Services (AWS) | Analyzing new network requirements, deployments, augmentations, bandwidth upgrades and software upgrades | Ability to write detailed technical specifications | Ability to manage multiple commitments simultaneously | Excellent communication and follow-up skills | Proven ability to work independently and escalate issues appropriately | Ability to deliver in fast-paced, high volume environment supporting aggressive targets",Littleton CO 80120,Data Traffic Engineer
Etsy,/rc/clk?jk=590e87e3271b5328&fccid=93665162b6da8dac&vjs=3,"The Data Engineer role |  | COLLABORATE | OPTIMIZE | EVOLVE |  | As a Data Engineer at A Cloud Guru, you will ensure the data platform infrastructure and architecture supports the evolving requirements of the Data Engineering and Data Analytics teams as well as other parts of our business! You will work closely with the Director of Data Engineering to develop a strategy for our long term Data Platform architecture to identify gaps in the data processes and drive improvements while mentoring and coaching other team members. Thanks to your contributions, our data platform will continue to optimize and revolutionize. This role reports to the Director, Data Engineering. |  | Hello, we're A Cloud Guru |  | Our friends call us ACG. |  | A Cloud Guru was built by engineers for everyone, everywhere. Here, you’ll have the freedom to follow your curiosity. We’re not afraid to just try, because when you’re working with cutting edge technologies, experimentation and trying out new ideas have to be encouraged and celebrated. Our engineers are building the world’s largest (and most awesome) cloud learning platform. Why? Our mission is to teach the world to cloud. Our fun, practical courses have helped over 2 million people learn to cloud, and we’re just getting started. |  | There aren't many company cultures like A Cloud Guru's in the world. This year, we were awarded the #1 Place to work in Austin, as well as Best Company Culture and Best Companies for Diversity. |  | What makes the Engineering team awesome... |  | We’re not a training company that just decided to sell training courses. We grew up out of the cloud ecosystem. We were a bunch of cloud engineers who pulled people together to create a training platform. That’s why we’re genuinely passionate about what we create. And we are known for practicing what we preach. We’ve built a product using cloud-first Serverless Architecture with tools like Lambda, API Gateway, GraphQL and ReactJS. All that aside, we're a friendly, down-to-earth, and collaborative group. There are no high-performing jerks and no heroes. Just great teams. |  | You'll do well at ACG if you're open to learning and trying new things, and you like to be surrounded by other friendly, passionate and driven people. –Natasja, Makeup Guru (and Software Developer) |  | As a Data Engineer at ACG, you’ll get to: | Be an essential part of designing and building ACG’s new data platform, as we evolve the existing databases into a cutting-edge solution to meet the needs of our 2021 data plans and beyond | Explore and contribute to discussions around technologies under consideration, such as Snowflake, Kappa/Lambda architecture, Delta Lakes and Data Vault | Develop, test and maintain existing architecture, including databases, data pipelines and large-scale processing systems | Collaborate with the Analytics team on transformation processes to populate data models | Recommend ways to improve data reliability, efficiency and quality of the data platform and optimise for performance, scalability and cost | Discover opportunities for data acquisition and explore new ways of using existing data | Identify gaps in data processes and drive improvements | Coach and mentor other team members | What you bring to the table |  | We focus on hiring values-aligned people, because we believe the right person can learn all the things to be successful in their role. Self-belief plays a big part in what you apply for. We encourage all job applicants to apply even if they are nervous to do so. Uni degrees aren't required for any roles, and career gaps or switches are totally welcome. |  | 2+ years of Data Engineering, Data Warehousing, or related experience | 2+ years of development experience with Python or similar scripting language | 2+ years of SQL experience, including experience with schema design and dimensional data modelling | Experience working with AWS services such as DynamoDB, Glue, Lambda, Step Functions, S3, CloudFormation or Redshift | Experience with ETL development, metadata management, and data quality | Knowledge of software engineering best practices with experience with implementing CI/CD, monitoring &amp; alerting for production systems | Experience with complex data structures and No-SQL databases | Experience with open source orchestration platforms (e.g. Airflow) |  | We want the people who care about doing a good job. The ones who have the humility and hunger to learn. - Sam Kroonenburg, Co-Founder and CEO |  | More than a job |  | Where you work isn’t just a career decision — it’s a life decision. Everyone has family, friends and interests outside of their careers, so we offer perks and benefits to make work, work better for you. |  | 4 weeks PTO, plus 10 sick days, and holidays. Because even when your office is your living room, we all need time to unplug. | Remotely awesome. Get $500 to level up your home office, monthly snack boxes, free Headspace access, weekly lunch funds, and $50 monthly for internet. | Human connection. Get to know the Gurus with good times and get-togethers inspired by our values, virtual happy hours, lunchtime trivia, or a socially distanced drive-in movie. | Gender-neutral paid parental leave. Expanding your family? We offer 12 weeks of gender-neutral paid parental leave, and reimburse up to $10,000 for eligible adoption expenses. | $1,000 continuing education budget. All Gurus get $250 a quarter to spend on personal development, and 2 hours each week reserved for learning something new. | What’s the interview process like at ACG? |  | Applying for a job can feel intimidating and like a full-time job of its own. You shouldn’t have to burn through a week of sick time or all your best out-of-office excuses just to put feelers out for a new career opportunity. We want to be as transparent about the process as possible to help ease your mind. It’s our goal to provide you a fair, efficient interviewing experience that respects you and your time — and to do it all with a sidecar of delight. |  | Once you submit an application, we’ll review it. If you’re a good fit, you’ll have an initial chat with a recruiter over the phone. A phone interview with a manager typically follows. Depending on your role, you might then be asked to do a little homework (but nothing too time consuming). Then we’ll schedule a Zoom call to meet other members of the team, answer any questions you have, and give you a feel for what it’s really like to work at ACG. If you're on the fence, just give it a try. |  | Keep being awesome, Cloud Gurus.",Austin TX,Data Engineer
Fulton Financial Corporation,/rc/clk?jk=cc61b170f602ac72&fccid=32dee392619784a4&vjs=3,"About Limelight: | Limelight is building a hiring platform to bring more opportunities to skilled workers by valuing skills over credentials. With initial attention to manufacturing, construction, and skilled trade, Limelight's platform performs skills assessments optimized for specific jobs and responsibilities. Our approach uses data to produce more successful hiring outcomes while eliminating the subjectivity, bias and discrimination embedded in credential-oriented hiring. | To learn more about skills-based hiring and the post-COVID job market, consider this insight from our CEO, Ben Pfeifer: https://medium.com/@mylimelight/skills-based-hiring-and-the-post-covid-job-market-79a507484a46 | What you'll do | Take the lead | As Limelight's first data engineer, you'll architect the foundation for how we build an industry leading data set. | No people management experience required – just the willingness to jump in to a new project and build it from 0 to 1. | Be Bold | We're reconsidering every aspect of the hiring process. You'll bring a new perspective to how we bring in new data sources, creating new opportunities for your team. | Learn | Every engineer at Limelight is deeply committed to both employers and talent. You'll learn about diverse skilled trades, the needs of people working in those careers, and the needs of the people staffing those positions. | Ship, continuously | You'll deliver code iteratively, using best practices to avoid long-lived branches, keep scope tight, and get features in the hands of your users. | Test | You'll use best practices in testing to help increase your development velocity and your confidence in the correctness of your code. | Level up | You'll contribute to improvements to best practices, leveling up the entire team. | What you bring | Experience | At least five years building robust ETL pipelines | In-depth knowledge of distributed systems and data flows | Has led the design of: | A large scale data pipeline from scratch | Information extraction from semi-structured as well as unstructured text data | A data warehouse | Expertise with data modeling, data access, and data storage techniques | Proficient in SQL and Python | Implemented data pipelines in the public cloud, especially AWS | Expertise in NLP or Text Mining is a big plus | Communication | You can effectively communicate your process to the rest of the team. | Testing | You understand best practices for unit testing and integration testing. | Extracting Information | Develop versatile software components to extract useful information from various unstructured or semi-structured text data. | Work closely with data science to develop, test and iterate data models and algorithms. | You effectively collaborate with your application development partners to get your algorithms into production. | Tools | You know how to use your development environment to improve your productivity. You leverage debugging, autocomplete, documentation, static analysis, version control navigation, and more integrated in your environment. | Continuous Delivery | You know best practices in continuous delivery.",New York NY,Senior Data Engineer
Babylon Health,/company/Ursi-Technologies-Inc/jobs/Cloud-Data-Engineer-344f369a5b13a028?fccid=4080d24e411d5a46&vjs=3,"Job detailsSalary$76,185 - $154,901 a yearJob TypePart-timeContractNumber of hires for this role5 to 10QualificationsBachelor's (Preferred)SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)Full Job DescriptionHello,Hope you are doing great,We have an urgent requirement for Cloud Data Engineer based in Austin, TX  for 1+ years contract so please let me know if you are interested.Looking for candidates who can work on W2, ( Green Card, US Citizens, H4-EAD, H1B Transfers).__**__**_ Required skills: (Note : Azure ALDS Gen 2, Azure Data Factory, Databricks) _Job Description: 5-10 years of experience in a Data Engineer role. Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative fieldAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with MS Azure, Snowflake and Qlik Replicate (Attunity)Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.Strong project management and organizational skills.Experience supporting and working with cross-functional teams in a dynamic environment.They should also have experience using the following software/tools:Azure ALDS Gen 2, Azure Data Factory, DatabricksJob Types: Part-time, ContractPay: $76,185.00 - $154,901.00 per yearSchedule:8 hour shiftEducation:Bachelor's (Preferred)Experience:SQL: 1 year (Preferred)Data Warehouse: 1 year (Preferred)",Austin TX,Cloud Data Engineer
NYC Health + Hospitals,/rc/clk?jk=7c44e351b3a095b7&fccid=55e29249bd82ae44&vjs=3,"Tasks | We are hiring a Data Engineer for a great opportunity to work with an iconic car brand. This position will be responsible for migrating our local analytics platform/EDW to our new Snowflake database, as well as the ongoing maintenance and development Snowflake and AWS. This position will require a strong background in MPP databases, analytics, cloud architecture, and the tools to implement these solutions. Additionally, this position will be expected to become an expert on Porsche business processes as it relates to serving customers, AWS, and Snowflake | Responsibilities: | Design, develop and maintain reliable automated data solutions based on the identification, collection and evaluation of business requirements. Including but not limited to data models, database objects, stored procedures and views | Developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality ) components | Conduct system monitoring across cloud environments | Analyzes and profiles large and complex data sets to discover and/or validate data quality issues and business rules | Resolve technical and user issues | Automate installation, configuration, backup, monitoring and alerting processes in Snowflake | Drive collaboration across a global, multicultural, multi-company team | Implementation of RESTful API’s supporting system integrations | Actively researches philosophical and technological changes in data management and pursue an understanding of the organization's business plans and strategies. Evaluates the value of these trends to the organization and presents the information to management and other team members. | Facilitates the development of data-related policies, processes, procedures and standards | Qualifications | Education: Bachelor's Degree in Computer Science, Engineering, Data Analytics or comparable experience | Skills: | 5+ years professional data engineering experience focused on batch and real time data pipelines using Spark, Python, SQL, Java | 8+ Years of hands-on design and development experience in data space : data processing / data transformation using ETL tools, data warehouse (data modeling, programming), RDBMS | Experience with creating API’s. | Fundamental experience with leveraging AWS for Analytics | Exposure in Microsoft SSIS and SQL Server | Working knowledge of MPP systems or Snowflake a plus | Experience using JIRA and Agile Project Management software | Experience with code repository solutions | Able to work in a global, multicultural environment | Knowledge of best practices and IT operations in an always-up, always-available services | Critical thinking/problem solving | Experience in the Automotive and/or Financial Industries is a plus | German language capability a plus | Exclusive look behind the scenes | Porsche is an equal opportunity employer and we take pride in our diversity. In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Porsche will be based on merit, qualifications and abilities. Porsche does not discriminate in employment opportunities or practices on the basis of race, color, religion, sex, pregnancy, status as a parent, national origin, age, disability, family medical history, ancestry, medical condition, genetic information, sexual orientation, gender, gender identity, gender expression, marital status, familial status, registered domestic partner status, family and medical leave status, military status, criminal conviction history, or any other characteristic protected by federal, state or local law.",Atlanta GA,Data Engineer
Fortive Corporate,/company/SHGT/jobs/Data-Engineer-e1436d9262ce435e?fccid=f1ad4b2a562822c9&vjs=3,"Job detailsSalary$60 - $70 an hourJob TypePart-timeContractNumber of hires for this role1QualificationsExperience:AWS, 2 years (Preferred)Big Data, 3 years (Preferred)Full Job DescriptionHands-on experience in Big Data technologies like Apache Spark / Flink / Hadoop / Hive is a must.Experience with data pipeline building, backend microservice development, and REST API using Python, Java or comparable language.Experience with various offerings from AWS, including S3, EMR, Redshift, Athena; other cloud providers is a plus.Demonstrate a passion for developing well architected, elegant applications &amp; services.2+ years DevOps experience including configuration, optimization, backup, high reliability, monitoring and systems version control.Contract length: 12 monthsJob Types: Part-time, ContractSalary: $60.00 - $70.00 per hourSchedule:8 hour shiftExperience:AWS: 2 years (Preferred)Big Data: 3 years (Preferred)Work Remotely:Temporarily due to COVID-19",Mountain View CA,Data Engineer
eClinical Solutions LLC,/rc/clk?jk=d47b0f6135ba5cbb&fccid=387b89c11ffe51f9&vjs=3,"eClinical Solutions helps companies regain control of their data through software and data services. Our platform, the elluminate® Clinical Data Hub, centralizes and stores all clinical research data. Through its easy to use data aggregation, standardization, and analytics applications, client teams can improve data quality, oversight, and access. We also provide tech enabled data services, from data acquisition through submissions. These are delivered by an experienced team of data management, clinical programming, and biostats professionals. |  |  | OVERVIEW OF THE POSITION | The Senior Data Engineer will work closely with clients and provide technical consulting services and oversight for specific projects that include ETL and custom analytics development. The Senior Data Engineer will provide direction on database aspects to the team of existing ETL developers on various database related activities. | KEY TASKS &amp; RESPONSIBILITIES |  | Design, develop, test and deploy highly efficient SQL code and data mapping code according to specifications | Design and develop ETL code in support of analytic software applications and related analysis projects | Work with Analytics developers, other team members and clients to review the business requirements and translate them into database objects | Research and utilize new technologies | Collaborate with the Quality Assurance team to test the applications functionality | Provide technical guidance, training and support to other team members | Ensure compliance with eClinical Solutions/industry quality standards, regulations, guidelines and procedures | Manage multiple timelines and deliverable (for single or multiple clients) and managing client communications as assigned | Provide programming solutions and support using elluminate Clinical Data Platform | Configuration, migration and support of the elluminate platform | Knowledge of clinical trial data is a plus - CDISC SDTM, or ADAM standards | Other duties as assigned |  | CANDIDATE’S PROFILE | Education/Language: |  | Basic Science/Bachelor of Science or Master of Science degree in Computer Science and/or equivalent work experience | Excellent knowledge of English |  |  |  Professional Skills &amp; Experience |  | Minimum of 7 years in database design and development experience | Thorough experience in data warehouse architecture, design and development. | Thorough understanding of database design principles and best practices | Excellent experience in designing scalable, modular SQL code and ETL procedures using MS-SQL Server | Strong Software Development Lifecycle experience (Agile methodology experience is a plus) | Excellent understanding of relational database concepts, data modeling and design | Strong technical project management experience and team leadership skills including scope management, work planning and work delegation | Strong troubleshooting skills and use of defect/feature management systems | Proven ability to work independently and with technical team members (Startup environment experience is preferred | Experience in the Biotechnology, Pharmaceutical, or Life Sciences industries ( Clinical Research Organization - CRO or Clinical Trial regulated environment preferred) | Excellent verbal and written communication skills |  Technical Skills &amp; Experience |  | 7+ years with SQL Server, ETL and Data Warehousing | 5+ years with ETL architecture and design | 5+ years with Data modeling (physical &amp; logical) | Experience with Performance tuning and management of SQL Server | Experience with Dimensional modeling | Experience with any BI tools(e.g. Qlik, MicroStrategy, etc) | Knowledge of clinical trial data is a plus",Mansfield MA 02048,Senior Clinical Data Engineer
Lehigh Valley Health Network,/rc/clk?jk=1092cd759716e669&fccid=aaf3b433897ea465&vjs=3,"As an experienced member of our Software Engineering Group we look first and foremost for people who are passionate around solving business problems through innovation &amp; engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You'll work in a collaborative, trusting, thought-provoking environment-one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally. | J.P. Morgan Chase is a global institution that prides itself in the power of scale - offering first class financial products and services to its clients (and its clients' clients) across the spectrum of consumer, commercial and institutional needs. For the Corporate &amp; Investment Bank (CIB) specifically, clients range from hedge funds, governments, institutional investors and corporations around the world - each made up of individuals who interact with our employees in large volumes by phone, email and digital chats every day. | The Client Intelligence team's mission is to leverage those large datasets of communications to power cutting-edge applications and analytical capabilities within the CIB. As a Senior Python Data Engineer, you will evolve at the intersection of business analytics, data warehousing and software engineering, and will be responsible for building the foundational data layer critical to the Client Intelligence platform success. You'll take the lead on relevant projects, supported by an organization that provides the support and mentorship you need to learn and grow. | Responsibilities |  | Design and architect the next generation data pipelines, lake and warehouse for the Client Intelligence team. Build and communicate a technical vision to the team and the stakeholdersBuild large-scale batch, ETL and real-time data pipelines using cloud and on-premises data technologies, such as Redshift, Athena, DBT, Python, Apache Airflow and Apache KafkaDesign best practices for data processing, data modeling and warehouse development throughout our team and groupDevelop the vision and map strategy to provide proactive solutions and enable stakeholders to extract insights and value from dataUnderstand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions.Coach and mentor team members as applicable | Qualifications |  | Preferred qualifications | Expertise in data warehouse / data lake architectures like Redshift, Snowflake, Big Query, Impala, Presto, AthenaExperience with workflow orchestration tools such as Apache AirflowKnowledge of data transformation and collection tools such as DBT or FivetranHands-on experience with stream processing platforms such as Kafka, Kinesis, Flink, Beam, DataflowAdvanced knowledge of data columnar and serialization formats such as JSON, XML, Arrow, Parquet, Protobuf, Thrift, AvroStrong experience with container technologies such as Docker and KubernetesExperience writing infrastructure as code with TerraformExperience with CI/CD systems e.g. Jenkins and automation / DevOps best practicesAdvanced knowledge of AWS ecosystem, including S3, Glue, Redshift, Athena, Kinesis, MSK, IAM, Batch, ECS, EKS etc | Minimum requirements | BS/BA degree or equivalent experience in computer science or engineeringSignificant hands-on experience in building a data warehouse / data lake and data pipelinesExpert level skills in SQL, data integration, data modeling and data architectureExpert level skills in Python, its standard library and its package ecosystem - Pytest, Tox, Pandas, Requests, Pylint, Boto3, Jinja... | Soft skills | Leadership and ability to influence the team's directionMentoring: help your junior teammates achieve their goals and growCuriosity, creativity, resourcefulness and a collaborative spiritClear and effective verbal and written communication skillsDemonstrated ability to work on multi-disciplinary teams with diverse backgroundsInterest in problems related to the financial services domain (specific past experience in the domain is not required)JPMorgan Chase &amp; Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management. | We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs. | Equal Opportunity Employer/Disability/Veterans",Jersey City NJ,Senior Python Data Engineer
IntegriChain,/rc/clk?jk=261dfa1c0720bfc4&fccid=698630bfb24a65a2&vjs=3,"Mission: | Deliver modern data pipelines and create custom data extracts that meet the needs of both internal and external customers. |  | Duties: | Develop, support, and refine new data pipelines, data models, business logic, data schemas as code, and analytics to product specifications. | Prototype and optimize data type checks to ensure data uniformity prior to load. | Develop, and refine both streaming and batch processing data pipeline frameworks. | Maintain, improve, and develop expertise in existing production data, models, and algorithms. | Learn and utilize business data domain knowledge and its correlation to underlying data sources. | Define, document, and maintain a data dictionary including: data definitions, data sources, business meaning and usage of information. | Identify and validate opportunities to reuse existing data and algorithms. | Works with stakeholders to gather requirements on merging, de-duplicating, standardizing data. | Collaborate on design and implementation of data standardization procedures. | Share team responsibilities; such as contributing to development of data warehouses and productizing algorithms created by Data Science team members. |  | Qualifications and Competencies: | Bachelor's Degree in technical background or equivalent work experience. | 2 - 3+ years of experience building data pipelines and using ETL tools. Prefer python programming experience. | 3+ years of experience in at least one basic relational database platform (sql server, oracle, postgres, mysql) and languages (PL/SQL, SQL). | 1+ years experience developing modern, industry standard big data frameworks with AWS or other cloud services. | Experience with common GitHub developer practices and paradigms. | Experience working with agile methodologies and cross-functional teams. | Knowledge of redshift or any other columnar database is prefered. | Knowledge of aws services and airflow is a plus. | Experience in building AWS data pipelines using python, S3 data lake is a plus. | Knowledge of speciality pharmaceutical and retail pharmacy is a plus.",Remote,Data Engineer
Toyota,/rc/clk?jk=d6e216f8a3d25cac&fccid=e14545884595e2ac&vjs=3,"Big Data Engineer – NY – 12 Months | Role- Big Data Engineer |  | Location- NYC, NY |  | Duration- 12 Months |  |  | Job Description: |  | A formal background and proven experience in engineering, mathematics and computer science, particularly within the financial services sector | Hands on Programming / Scripting Experience (Python, Java, Scala, Bash) | Hands on with Ansible playbooks and create Hadoop builds and deployments | DevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins) | Linux / Windows (Command line). An understanding of Unix/Linux including system administration and shell scripting | Proficiency with Hadoop v2, MapReduce, HDFS, Spark | Management of Hadoop cluster, with all included services | Good knowledge of Big Data querying tools, such as Pig, Hive, Impala and Spark | Data Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management) | The ability to function within a multidisciplinary, global team. Be a self-starter with a strong curiosity for extracting knowledge from data and the ability to elicit technical requirements from a non-technical audience | Collaboration with team members, business stakeholders and data SMEs to elicit, translate, and prescribe requirements. Cultivate sustained innovation to deliver exceptional products to customers | Experience with integration of data from multiple data sources | Strong communication skills and the ability to present deep technical findings to a business audience",New York NY,Big Data Engineer
Porsche Cars North America,/rc/clk?jk=af4dd7bba11b6a14&fccid=90f0cbc4a30f8dba&vjs=3,"Who we are | Collaborative. Respectful. A place to dream and do. These are just a few words that describe what life is like at Toyota. As one of the world’s most admired brands, Toyota is growing and leading the future of mobility through innovative, high-quality solutions designed to enhance lives and delight those we serve. We’re looking for diverse, talented team members who want to Dream. Do. Grow. with us. |  |  |  |  | Who we’re looking for | Toyota’s Platform Enablement Department is looking for a passionate and highly-motivated Data Engineer. | The primary responsibility of this role is to build and support data integration services for Toyota Financial Services' (TFS) internal and external systems which supply data to critical business functions, in a meta-data driven quality enabled pipeline. | Reporting to the Digital Factory Owner, the person in this role will support the Platform Enablement’s objective to integrate Toyota Financial Services’ disparate internal and external systems via the data supply chain. | What you’ll be doing |  | Develop, enhance, operate and maintain an Enterprise Data Platform comprised of, but not limited to, features such as Data Ingestion, Metadata driven development, Data Preparation/Aggregation, and polyglot data delivery in a containerized Cloud-based Kubernetes cluster with data services capability that can be leveraged for enterprise data consumption |  | Develop and support services and integration patterns such as API, Events, and Batch between enterprise systems comprised of, but not limited to, Mainframe, Salesforce, SAP, Informatica in a cloud infrastructure |  | Program in Java Spring boot framework and microservices-based data processing |  |  | Develop data processing framework using NiFi, Kafka Streaming, deployment using Jenkins, any NoSQL Document database, AWS S3 object store, and codebase management using Git in a containerized Kubernetes platform |  | Drive opportunities for increased efficiencies of the Enterprise Data Platform to be fully operational, understanding means of automation, tuning and, uplift of the data applications |  | What you bring | Bachelor’s degree (or higher) in Computer Science, Information Systems or equivalent professional work experience | Experience as a Data Engineer and in Data Integration | SPARK computing framework using Java/Python | Experience developing time tracking solutions, data extracts and reporting dashboards | Broad-based IT experience participating in projects and playing a key role toward successful implementation of the project | Added bonus if you have | Expertise in DevOps toolchain leveraging Jenkins | Prior experience in ICP or equivalent cloud platform | Certification in AWS or any other Cloud platform | Certification in any Kubernetes based development | What we’ll bring | During your interview process, our team can fill you in on all the details of our industry-leading benefits and career development opportunities. A few highlights include: | A work environment built on teamwork, flexibility and respect | Professional growth and development programs to help advance your career, as well as tuition reimbursement | Vehicle purchase &amp; lease programs | Comprehensive health care and wellness plans for your entire family | Flextime and virtual work options (if applicable) | Toyota 401(k) Savings Plan featuring a company match, as well as an annual retirement contribution from Toyota regardless of whether you contribute | Paid holidays and paid time off | Referral services related to prenatal services, adoption, child care, schools and more | Flexible spending accounts | Relocation assistance (if applicable) | What you should know | Our success begins and ends with our people. We embrace diverse perspectives and value unique human experiences. We are proud to be an equal opportunity employer that celebrates the diversity of the communities where we live and do business. Applicants for our positions are considered without regard to race, ethnicity, national origin, sex, sexual orientation, gender identity or expression, age, disability, religion, military or veteran status, or any other characteristics protected by law. | Have a question or need assistance with your application? Check out the How to Apply section of our careers page on Toyota.com | To save time applying, Toyota does not offer sponsorship of job applicants for employment-based visas or any other work authorization for this position at this time | #LI-TMNA",Plano TX 75023,Data Engineer
JPMorgan Chase Bank N.A.,/rc/clk?jk=db833e47bcb0bf1e&fccid=8ba13cf21fd7c2de&vjs=3,"Job detailsJob TypeFull-timeTemporaryFull Job DescriptionAbout NYC Health + Hospitals |  | Empower Every New Yorker — Without Exception — to Live the Healthiest Life Possible | NYC Health + Hospitals is the largest public health care system in the United States. We provide essential outpatient, inpatient and home-based services to more than one million New Yorkers every year across the city’s five boroughs. Our large health system consists of ambulatory centers, acute care centers, post-acute care/long-term care, rehabilitation programs, Home Care, and Correctional Health Services. Our diverse workforce is uniquely focused on empowering New Yorkers, without exception, to live the healthiest life possible. |  | Job Description |  | The Test and Trace Corps is looking for a Data Engineer to join the Data, Analytics and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative. | Summary of Duties and Responsibilities: | Reporting to the Senior Data Engineer within the Data and Analytics Unit, the Data Engineer designs, evaluates and tests data structures and will be responsible for: | Cleaning large datasets, with responsibility for accuracy and complex analyses through programming and performing statistical analyses on large data sets to ensure integrity of data for analyses and use across the Data, Analytics and Product Development Team to support planning, reporting and development initiatives under the Test and Trace Corps | Completing complex network, statistical and programming analyses to clean and prepare data, ensuring accuracy | Serving as the internal engineering consultant to solve structural data issues within the Quality Control (QC) workflow | Regulating the cleanliness of data produced from Data Engineers, Data Scientists, and Data Managers across the Data, Analytics and Product Development Team | Collaborating with data staff and other departments to identify and mitigate potential data issues | Auditing and reporting on the QC of data migration and integrations throughout the Test and Trace Corps organization | Checking and preparing data to provide to the Data Scientist and other analysts, as needed | Minimum Qualifications |  | 1. A Baccalaureate Degree from an accredited college or university with a major in Computer Science, Systems Engineering, applied Mathematics, Business Administration, Economics/Statistics, Telecommunications, Data Communications, or a related field of study; and | 2. Five (5) years of progressive, responsible experience in the field of data processing, computer systems and applications. | Operations Specialty requires supervisory experience (5 years). | Network Services requires a telecommunications background and experience. | 3. Broad knowledge and expertise in the characteristics of computers, peripheral devices, communications systems and hardware capabilities, programming languages, E.D.P. applications, systems analysis methodology, data management and retrieval techniques; or | 4. A satisfactory equivalent combination of training, education and experience |  | Department Preferences |  | Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise | Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams | Strong quality control abilities and exceptional attention to detail | General knowledge of SQL, R, Python, Excel and related data analytics toolsets | 3+ years of experience in a data training, analytics, management, or QC role | NYC residency | How To Apply |  | If you wish to apply for this position, please apply online by clicking the ""Apply Now"" button. | If applying online, please include your cover letter in the same file attachment with your uploaded resume.",New York NY,Data Engineer (Senior Consultant MIS - Level A) Test & Trace Corps *Temporary/Grant Funded*
PayPal,/rc/clk?jk=f664facac334861c&fccid=978d9fd9799d55a8&vjs=3,"Who we are: Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 375 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies. | Job Description Summary: A strong data engineer with prior hands-on knowledge of Payments industry with a passion to deliver quality deliverables in a fast-paced environment with an undivided customer focus | Job Description: | Job Responsibilities: | Build scalable systems, lead technical discussions, deliver good quality results, participate in code reviews and guide the team in engineering best practices | Provide technical leadership and contribute to the definition, development, integration, test, documentation and support across multiple platforms (Python, Hadoop, SAP, Teradata, Machine Learning) | Establish a consistent project management framework and develop processes to deliver high quality software, in rapid iterations, for business partners in multiple geographies | Participate in a team that designs, develops, troubleshoots and debugs software programs for databases, applications, tools etc. | Experienced in balancing production platform stability, feature delivery and reduction of technical debt across a broad landscape of technologies | Establish a productive and collaborative team environment to ensure there is a strong customer focus in all activities of the function at all times | Experience required: | Undergraduate degree in Engineering or equivalent from a leading university | 10+ years experience of Data Analytics platform implementation | Good team player that can collaborate with both internal and external teams | Good understanding about data processing concepts and data warehousing knowledge | Master SQL language on top of RDBMS (Oracle/MySql)/ HiveQL/ SparkSQL/ Big Query | Understanding of Apache Spark/Hadoop ecosystem and the Data Analytics ecosystem | Strong conceptual and creative problem-solving skills | Relentlessly resourceful and scrappy | A great communicator and strong project management skills | Skills &amp; Abilities required: | Must be results focused and highly energetic to drive defined team and organizational goals | Ability to work with considerable ambiguity | Ability to learn new and complex concepts quickly | Given the growth nature of this business, individual must be pragmatic and capable of working in a fast moving and entrepreneurial environment | Analytical with superior problem-solving skills over near and long-term planning horizons | Highly detailed with a systematic approach, sense of responsibility and strong, positive customer focus | Financial literacy to manage P&amp;L and efficiently communicate with stakeholders | Cloud experience is a plus | Behaviors: | Deliver Stand out Results – Delivers outcomes that make a significant business impact | Execute Well – Creates clear focus, plans and priorities | Innovate – Identifies and owns significant business improvements | Evangelize Technology - Absorbs and propagates new technology | We're a purpose-driven company whose beliefs are the foundation for how we conduct business every day. We hold ourselves to our One Team Behaviors which demand that we hold the highest ethical standards, to empower an open and diverse workplace, and strive to treat everyone who is touched by our business with dignity and respect. Our employees challenge the status quo, ask questions, and find solutions. We want to break down barriers to financial empowerment. Join us as we change the way the world defines financial freedom. | PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities.",Chicago IL 60654,MTS 1 Data Engineer
Apple,/rc/clk?jk=90117ebc8937aaaf&fccid=e290923a98afee94&vjs=3,"Job Description | ICF is hiring for two new Data Engineer professionals in the Washington DC Metro Area! While we are under restrictions due to the pandemic, this role will be remote. While we hope to offer long term remote options to successful candidates for this role, there will be a need for onsite client meetings and onsite presence in the DC Metro area after restrictions are lifted. | What you’ll be doing: | Extract, transform, and load (ETL) processing routines and data feeds to transmit data to and from clients and subcontractors; create necessary data structures or data models to support data at all stages; and design and implement custom data analytic and BI/reporting products. | Perform extensive data profiling and analysis based on the client’s data | Work with UI teams and/or client to define BI and reporting requirements | Developer custom reports and data visualization products | Support project delivery on Data Warehouse/BI projects for external and internal clients, including partnering with ICF subject matter experts on project execution | What you must have: | Bachelor’s degree (e.g., Computer Science, Engineering, or related discipline) | 2-5 years’ experience developing business intelligence applications such as Talend, Informatica, SAS | 2-5 years’ experience in SQL and procedural programming | 1-2 years of experience working with databases and BI tools such as Tableau, PowerBI, Proficient with data warehouse design and development and big data systems | 1+ years’ experience with services AWS Glue, Lambda, Microsoft Azure Data Factory, Google Cloud Data Flow | US Citizen or Permanent Lawful Resident (Green Card Holder) preferred. Employment must be compliant with eligibility for Public Trust Clearance due to Government Contract. | What we’d like you to know: | Demonstrated experience showing strong critical thinking and problem-solving skills paired with a desire to take initiative | Proficient with one or more programming languages such as Java or Python | Knowledge of Big Data integration tools such as Storm, and Spark, AWS Kinesis, Kafka a plus | Experience with DevOps tools like Jenkins/Git to assist development process | Experience with agile development process | Technologies you’ll use in this role: | SQL, BI, Talend, Informatica, SAS, Tableau, PowerBI | Spark, AWS Kinesis, Storm, Kafka | Jenkins/Git | Agile | AWS, Azure, Google Cloud Platform | Why you’ll love working here: | Comprehensive health benefits | Generous vacation and retirement plans | Employee support program | Participation in charity initiatives | Working at ICF | Working at ICF means applying a passion for meaningful work with intellectual rigor to help solve the leading issues of our day. Smart, compassionate, innovative, committed, ICF employees tackle unprecedented challenges to benefit people, businesses, and governments around the globe. We believe in collaboration, mutual respect, open communication, and opportunity for growth. If you’re seeking to make a difference in the world, visit www.icf.com/careers to find your next career. ICF—together for tomorrow. | ICF is an equal opportunity employer that values diversity at all levels. (EOE – Minorities/Females/ Protected Veterans Status/Disability Status/Sexual Orientation/Gender Identity). For more information, please read our EEO &amp; AA policy . | Reasonable Accommodations are available for disabled veterans and applicants with disabilities in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: EEO is the law and Pay Transparency Statement . | DC Remote Office (DC99)",Washington DC 20006,Data Engineer
McKinsey & Company,/rc/clk?jk=c1c3124840f81768&fccid=1544766d4c2915b0&vjs=3,"Staff – Financial Services – Data and Analytics – Data Architecture and Integration | EY is the only professional services organization with a separate business unit (FSO) that is dedicated to the financial services marketplace. Our FSO teams have been at the forefront of every event that has reshaped and redefined the financial services industry. If you have a passion for rallying together to address the most complex challenges in the financial services industry, come join our dynamic FSO team! | Data has yet to be utilized to its fullest potential. Financial institutions are looking to build smarter and more efficient ways to operate their business, through new opportunities uncovered by their data. Our clients look to us as trusted advisors to provide insights on how to ingest, interpret and manage their data. We focus on business intelligence, advanced analytics, and information management. | The opportunity | We provide constant room for growth through client engagements and personal development. Our practitioners develop and lead innovative data techniques and methods, supporting both business and technology leaders. This role provides the unique opportunity to build a professional network, tackle complex data issues, and drive cutting-edge approaches across industry-leading clients and domains. The experiences you gain here will be more valuable than anywhere else. | Your key responsibilities | You will be part of diverse teams of professionals across different geographies to deliver a wide range of data and analytics services. You will be addressing complex issues and driving growth across financial services. | Skills and attributes for success | Fostering an innovative and inclusive team-oriented work environment | Leading and coaching diverse teams of professionals with different backgrounds | Demonstrating in-depth technical capabilities and professional knowledge | Establishing strong relationships with the clients | Working in an entrepreneurial environment to pave your own career path |  | To qualify for the role you must have | A bachelor’s or master’s degree | Around one or more years of related work experience in banking, capital markets, insurance or asset management | Experience assisting clients with strategic Data Architecture/Integration or Big Data initiatives, preferably focused in financial services | Strong experience in data architecture and integration technologies (ETL, data ingestion and distribution patterns, SQL, large data processing on distributed platforms, data warehousing) | Hands-on experience with at least one major Hadoop platform (Cloudera, Hortonworks, MapR) | Hands-on experience in developing or architecting Big Data applications using Hadoop and related technologies such as Spark, MapReduce, YARN, HDFS, Hive, Impala, Oozie, NoSQL, HBase, Elasticsearch, Cassandra, Flink, Flume, Splunk, etc. | Hands-on experience working with at least one programming language and related technologies such as Java, Scala, Python, .NET and APIs | Hands-on experience with streaming platforms and applications such as Spark Streaming, Kafka, Storm, etc. | Hands-on experience with cloud-related technologies (such as AWS, Azure, Redshift, EMR, etc.) | Strong written and verbal communication skills | Strong analytical and problem-solving skills | Ability to assimilate new knowledge | Ability to understand business challenges and translate them into value add analytics solutions | Excellent leadership and teaming skills | A willingness and ability to travel to meet client needs; travel is estimated at 80%-90% | Valid driver’s license in the US | Valid passport |  | Ideally, you’ll also have | A degree in computer science, engineering, accounting, finance or a related field; MBA or MS degree preferred | Consulting experience |  | What we look for |  | We’re interested in self-motivated and passionate leaders with a strong vision and desire to stay on the cutting edge of data technology and science. We are looking for individuals who aspire to build collaborative team environments and possess effective management skills while demonstrating in-depth technical capabilities and industry knowledge. |  | If you have a genuine drive for helping consumers achieve the full potential of their data while working toward your own development, this role is for you! |  | What working at EY offers |  | We offer a competitive compensation package where you’ll be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, pension and 401(k) plans, a minimum of three weeks of vacation plus ten observed holidays and three paid personal days, and a range of programs and benefits designed to support your physical, financial and social well-being. Plus, we offer: | Support and coaching from some of the most engaging colleagues in the industry | Opportunities to develop new skills and progress your career | The freedom and flexibility to handle your role in a way that’s right for you |  | About EY |  | As a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime. |  | If you can confidently demonstrate that you meet the criteria above, please contact us as soon as possible. |  | Make your mark. Apply today. | EY provides equal employment opportunities to applicants and employees without regard to race, color, religion, age, sex, sexual orientation, gender identity/expression, national origin, protected veteran status, disability status, or any other legally protected basis, in accordance with applicable law.",United States,Staff 2 - Data and Analytics - Bid Data Engineer - Banking
Lehigh Valley Health Network,/rc/clk?jk=a605900a34562cfe&fccid=dfecc8bbe451f8c2&vjs=3,"HI, | Kindly let me know if you have a suitable fit for the following position | Thanks |  | Data Engineer | Location: Phoenix, AZ | Duration: 12+Months |  | Initially Remote |  | ONLY W2 |  | Please send the resume to charan@ivytechsol.us or 847- 350-1008 |  | The data engineer is a critical role that will provide data engineering design, ETL development and technical expertise to an ETL scrum team. They will work on a team with other ETL developers, QA engineers and analysts to support data streams for payment, claim and customer service operations. The team is responsible for a large catalog of jobs that use a mixture of batch ETL architecture and real-time data streaming with API-integrated data services. The data engineer will spend their time doing hands on development, designing future data processes, conducting data analysis, consulting with other teams and interacting throughout the Agile process in a stable scrum team environment. |  | Essential Functions / Principal Responsibilities |  | Develops data pipelines in both batch ETL and real-time streaming architectures. | Develops data models to define new or modify existing data structures in support of data integration initiatives. | Provides expert technical knowledge of data solutions for business projects. | Provides source system analysis, data discovery, complex transformation assessment and target system exploration to understand information data requirements and anticipate user needs. | Contributes to data pipeline design, coding, and technical / functional reviews while collaborating with source system developers, data engineers and functional subject matter experts. | Develops effective data pipeline solutions to deliver business features. | Adheres to best practices for data movement, data quality, data profiling, data cleansing and other data pipeline related activities. | Applies tuning and optimization for continuous improvement. | Presents technical information in easily understood terms (written, verbal and visual). | Communicates effectively within the Agile team and to external stakeholders and management. | Follows Agile best practices and adheres to internal IT processes like change management and problem management. |  | Skills that will Ensure Success: |  | Specialist in ETL development with a demonstrated understanding of transactional data processing, streaming data and data pipeline best practices. | Experience in build, unit test, and deployment of Informatica ETL processes. | Knowledgeable in making REST API calls within data processes. | Familiar with real-time data pipeline platforms, preferably StreamSets, AWS Glue or similar platform. | Hands on experience with data streaming in Apache Kafka. | Able to interpret business needs and turn them into a technical plan of attack with pros and cons of various approaches to the data processing options. | Demonstrates a solid understanding of technical standards and processes related to batch and real-time data pipeline development. | Excellent team player, able to work with product owners, technical developers, DBAs, system administrators, BI professional services, data warehouse operations and functional experts. | Expertise in SQL query transactions and optimization, especially T-SQL. | Understand nulls, cardinality, joins, data types to develop technical ETL specifications and technical metadata. | Ability to integrate an application solution into the broader business and IT ecosystem in which it will operate. | Firm understanding of quality assurance activities and automation in data pipeline and ETL processing. | Desire experience working with financial and/or claims data requiring compliance, balancing and integrity checks, especially payment-related data, PCI compliant data and banking industry formats such as NACHA. | Desire a firm understanding of cloud data processing and data streaming architectures, especially in AWS. |  | Warm Regards, | Charan Kumar | IVY Tech Sols Inc. | 3403 N Kennecott Avenue, Suite B&amp;C Arlington Heights, IL 60004 | ( Direct: (847) 350-1008 |  charan@ivytechsol.us|Gtalk : charan.ivytech| |  | Powered by JazzHR | 19EMA7VJlz",Remote,Data Engineer at Phoenix AZ
Tanium,/company/Rayn-Solutions-LLC/jobs/Data-Integration-Engineer-045b8b6f9719d288?fccid=4bfd272c478ab81f&vjs=3,"Job detailsSalary$100 an hourJob TypeFull-timeContractNumber of hires for this role1QualificationsAzure: 4 years (Required)C#/.Net: 4 years (Required)SQL: 4 years (Required)Full Job Description*Looking for a talented and experienced consultant for long term C2C contract. No recruitment agencies. Must be US Permanent Resident or US Citizen*You will work with the Data Architecture team and be tasked to build re-usable, scalable APIs used to integrate on-premise and cloud-based systems including Azure and Salesforce CRM. Will help design and build data pipelines, data streams, data service APIs, data generators and other end-user information portals. Will be a critical part of the data supply chain, ensuring that stakeholders can access and manipulate data for routine and ad hoc analysis to drive business outcomes.ResponsibilitiesDelivery of architecture for transformation and modernization of enterprise data solutions using Azure cloud and MuleSoft technologies.Translate business requirements to technical solutions leveraging strong business acumen.Design, develop and deploy System, Process and Experience layer APIs and integrations based on technical design documents and business requirements.Design and adhere to best practices, standards and procedures as they pertain to the Azure and MuleSoft platform.Maintain a deep understanding of the Azure platform including Azure Functions Architecture, Azure Messaging services and RESTful Azure API management and development.Maintain a good understanding of the MuleSoft platform including Mule Runtime, available connectors, Design Center and RESTful MuleSoft API management and development.Maintain clear and concise documentation on all APIs being developed to promote for re-use.Test, debug and resolve issues as they arise during all parts of the development lifecycle.Develop and maintain data store schematics, layouts, architectures and relational/non-relational databases for data access.Implement effective metrics and monitoring processes.SkillsExcellent oral and verbal communication skills.Demonstrated experience in turning business use cases and requirements into technical solutions.A solid understanding of all aspects of the application development process as they pertain to the Microsoft Azure and MuleSoft platform.Experience in integrating with technology built in the Microsoft .NET, SQL, NoSQL, Azure and MuleSoft ecosystem.Experience in cloud computing, API integration, cloud messaging and scalable distributed systems.Ability to multi-task and be able to handle projects throughout the entire project life cycle.Ability to work independently and as a member of a team.Knowledge of Azure Data Factory, Azure Functions, Azure Messaging Services, Azure SQL, Azure App Services, Azure Storage/Data Lake and Azure Cosmos DB.Knowledge of MuleSoft Anypoint Platform and SOA.Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows.Experience coding with SQL, C#, Java and JavaScript.Experience coding with Gremlin and Python a plus.Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals.Experience with Git/TFS/Azure DevOps.Job Types: Full-time, ContractPay: $100.00 per hourSchedule:Monday to FridayExperience:Azure: 4 years (Required)C#/.Net: 4 years (Required)MuleSoft: 2 years (Required)SQL: 4 years (Required)Full Time Opportunity:NoWork Location:Fully RemoteVisa Sponsorship Potentially Available:No: Not providing sponsorship for this jobCompany's website:https://raynsolutions.comCOVID-19 Precaution(s):Remote interview processVirtual meetings",Remote,Data Integration Engineer / Architect
Clinical Ink,/rc/clk?jk=6a7f16bd8491b9b4&fccid=3a67644f730619de&vjs=3,"Job Summary: The Associate Data Engineer in Populytics contributes to the development and optimization of `big data' data pipelines, architectures, and data sets for Populytics, as required for population health management, provider profiling, clinical initiatives, medical expense budget tracking, and other applications. Helps maintain complex technology infrastructure and collaborates with other data engineers, clinical &amp; business analysts, and web developers to implement new features and plan for future projects. Associate Data Engineers must learn and use proven design principles, design patterns, and automated testing while helping to build and maintain the data pipeline. They may participate in group meetings with other departments to clarify processing requirements and designs, and they must become knowledgeable on relevant technologies and new industry trends to help sustain a strong technical direction for Populytics. | The Associate Data Engineer contributes to the development and maintenance of an optimal data pipeline architecture, using SQL and HDP big data' technologies, as required for optimal extraction, transformation, and loading of data from a wide variety of data sources, including but not limited to medical and pharmacy claims, HR, lab, EMR, Provider, and Payer systems. Responsibilities include using and supporting appropriate processes and tools for secure, efficient, and reliable data exchange, for data gap analysis and transformation, for data profiling and auditing, for data integration across time periods or data sources, for generating input files and consuming output files from advanced analytics tools, and for feeding the outputs to reporting data marts in a manner that satisfies resource and performance constraints. | Also responsible for contributing to the development and maintenance of audits and monitoring tools that utilize the data pipeline to provide actionable insights into data accuracy, operational efficiency, volume or cost fluctuations, and other key business performance metrics. The Associate Data Engineer contributes to the development of internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, extensibility, and maintainability. Works collaboratively with business partners, including lead and senior data engineers, clinical and business analysts, web developers, and data scientists to assist with data-related technical issues and support their data infrastructure needs. Routinely assists with root cause analyses on internal and external data and pipeline processes to answer specific business questions and identify opportunities for improvement. |  | Minimum Requirements: Work requires the level of knowledge normally attained through completion of a Bachelor's degree in Software Engineering, Computer Science, or Computer &amp; Information Science. |  | Minimum Experience: Must have completed a Bachelor?s degree in Software Engineering, Computer Science, or Computer &amp; Information Science and possess working knowledge of programming with object-oriented/object function scripting languages such as Java, Python, C#, and Scala. |  | Must possess the initiative to identify and carry-out responsibilities to their completion. Strong problem solving abilities and analytical skills. Requires a high degree of professional judgement and inter-personal skills at all levels. Requires the individual to work independently, so as to require minimal supervision. Requires the ability to function under pressure and deadlines. |  |  | Preferred Qualifications: Preferred Experience: Strong preference for an individual with working knowledge of SQL and query authoring. Prefer someone who has some familiarity with manipulating, processing and extracting value from large disconnected datasets. Familiarity with relational SQL and NoSQL databases, such as MySQL, SQL Server, Postgres, and HBase is also preferred. Familiarity with data pipeline and workflow management tools such as Oozie, Azkaban, SSIS, and Pentaho, and familiarity with big data tools such as Hadoop, Spark, Hive, Hive LLAP, and Map Reduce Programming are big pluses. |  |  | Licensure and Certifications: Hortonworks Certified Associate |  | #LI-ML1 | ""Populytics"" |  |  | Location: Lehigh Valley Health Network · Health Care Analytics | Schedule: Full Time 40 hours, Day Shift, 7:30a-4:30p",Allentown PA 18102,Data Engineer Assoc
ZEISS Group,/company/United-Business-Solutions/jobs/Senior-Data-Engineer-ef54ee91499710e7?fccid=16a3b20928e3683f&vjs=3,"Job detailsSalary$120,000 - $130,000 a yearJob TypeFull-timeNumber of hires for this role2 to 4QualificationsKafka,: 3 years (Required)Stream Analytics: 3 years (Required)ETL/ELT: 2 years (Required)MPP databases: 1 year (Required)Bachelor's (Preferred)Full Job DescriptionRole: Senior Data EngineerLocation: Remote (prefers MN) Can be anywhereDuration: Full Time Perm(Benefits include, free membership, healthcare, 401k etc.)Must speak clear English, have a personality with ability to articulate their experience.Full telecommute is an option – we can look nationallyRole is a mix of Data Engineer and Data Architect role. In addition, would be looking for someone to take on some leadership tasks and establishing best practices. This person will be 60% hands-on and 40% leadership and strategy.Wants someone to come in and challenge the status quo. Looking for Engineering “mind-sight” – really missing that with current team. Needs someone with independent thoughts. RJ anticipates he will be mentoring someone in the leadership and best practices so if we come across a hands-on engineer that would like to get more into leadership and strategy we should submit.Environment: Most of LTF data will remain structured. What is changing is how they are bringing their data in. Most of their data is pulled in through nightly batch jobs. Primary targets are cloud based destinations, including their vendor products. Primarily in Azure, some of the vendor environments are in AWS. Currently they do a lot of flat file processing, data extraction and ETL is done out of SQL – loading into an Azure Data Warehouse. Do not have best practices established; this will be a big part of the role. They have a data strategy and reference architecture in place. Now they need to take it from concept to execution.Critical skills: Streaming – Kafka, Stream Analytics or something similar – the company has some application teams doing it but they do not have a consistent pattern on how they do it so this person will be part of establishing best practices for this pieceUnderstanding of MPP platform is criticalExperience outside of Azure Data Warehouse would be good to havePosition SummaryThe Senior Data Engineer within the Enterprise BI and Data team at Life Time Fitness is responsible for delivering the technical design and development of data engineering solutions for assigned projects. This role will work closely with data architecture, business partners and engineering teams to provide solutions leveraging both structured and unstructured data following modern data architecture principles.Job Duties and ResponsibilitiesCollaborates closely with business and technical leaders to deliver data solutions to support operational reporting, financial analysis, member personalization and advanced analytics.Influences the Modern Data Architecture (MDA) strategy, and best practices, by partnering with lead architectsCollaborates on, and implement, domain data modeling to support self-service business intelligence capabilitiesLeads the implementation, and development, of solution designs for medium/large projectsDesigns and deliver distributed data solutions that enables data analysis and self-service reporting across the organizationDevelops complex data pipelines to ingest and transform data using modern data engineering practicesPerforms design, code and performance tuning reviewsProvides advice, technical guidance and mentoring to other team membersPosition RequirementsBachelors degree, or equivalent work experience, in Computer Science (or related field of study)7+ years experience designing and developing data warehouse solutions5+ years experience working with relational databases (preferably SQL Server, Postgres)5+ years experience developing ETL/ELT solutions using tools such as SSIS, Azure Data Factory, Informatica, DataStage, Databricks or Python2+ years experience working with real-time streaming data (Kafka, Azure Event Hub or Azure Stream Analytics)2+ years experience working with MPP databases such as HIVE, Azure SQL Data Warehouse, SnowflakePreferred RequirementsExperience working as part of an Agile teamExperience implementing solution leveraging Azure or AWS SaaS offerings is a plusExperience developing solutions leveraging modern database technologies such as Mongo, Azure Cosmos or other NoSQL technologiesExperience with CI/CD technologies such as Jenkins, GitHub or Azure DevOps Pipelines is a plusFamiliarity with Data Lake concepts and technologiesTeam structure: RJ is responsible for the Data and Data Operations team. This person will be working with just the Data Engineers – that is a team of 6Interview Process: 1st round interview – 2 hour block – meet with RJ and a couple of leads and then a 60 minute technical interview***Angie will need to meet with them for 15 minutes – will be an open house type interview set-upMay need to schedule a subsequent 30-minute interview if there are pending questions.Final conversation with HRIdeally they want someone who has had longer tenure, who’s done mostly architecture and wants to get into a lead role. The manager wants this person to be his right hand man/women. Lead the data strategy, mentor/grow the team and challenge the status quo. The team consists of 12 team members, 4 BI engineers and 8 data engineers. They’d rather have someone who’s more of a jack of all trades than pillared into one data strategy. This person will have an opportunity to move into the manager role within a year.share resume at paul(at)ubsols.comJob Type: Full-timePay: $120,000.00 - $130,000.00 per yearBenefits:401(k)Health savings accountSchedule:8 hour shiftMonday to FridaySupplemental Pay:Bonus payEducation:Bachelor's (Preferred)Experience:Data Engineer: 8 years (Required)Data Architect: 10 years (Preferred)Kafka,: 3 years (Required)Stream Analytics: 3 years (Required)ETL/ELT: 2 years (Required)MPP databases: 1 year (Required)Work Location:Fully RemoteCOVID-19 Precaution(s):Remote interview process",Remote,Senior Data Engineer/remote/Full Time
Inspirato,/company/Caliber-Technologies-LLC/jobs/Data-Analytic-Engineer-f2e51e0bd39a4cf8?fccid=36127dd79be74ba5&vjs=3,"Job detailsSalaryFrom $75,000 a yearJob TypeFull-timeNumber of hires for this role1QualificationsBachelor's (Preferred)Python: 3 years (Preferred)Machine learning: 3 years (Preferred)Full Job DescriptionAs a Data Analytics Engineer, you will be working with a team of talented engineers and data scientists, who are responsible for the implementation and maintenance of a data ingestion, storage, and visualization platform. This is an exciting opportunity for the right candidate to work on some of the exciting products.Responsibilities: - Design, implement, and test internal libraries and cloud-based infrastructure- Responsible for the full life cycle of development, from requirements gathering through ETL coding and report/dashboard design and creation.- Triage incoming issues and resolve defects in a timely manner- Work with project managers to gather and document functional requirementsRequired skills: - 3+ years’ experience of Python- 3+ years’ experience in machine learning, deep learning, or natural language processing- 3+ years’ experience working with large data setsQualifications: - Bachelor's degree in IT or related major.- 3+ years related experience.- Excellent knowledge of data design, SQL, and data warehousing.About Us: Caliber Technologies based in motor city Detroit supports automotive industry in their mission of developing advanced technologies and provide expertise-based resources. We will exceed your expectations and meet project needs as we strive to become ‘extension’ of your engineering team.Through our expertise and customer focused vision we aim to help OEMs, Tier-1’s or Start-ups achieve faster time to market with safety and quality in mind. We bring CAN DO attitude. If you are excited about working on variety of challenging engineering projects, then come join us.Job Type: Full-timePay: From $75,000.00 per yearBenefits:Dental insuranceHealth insuranceVision insuranceSchedule:Monday to FridayEducation:Bachelor's (Preferred)Experience:Python: 3 years (Preferred)Machine learning: 3 years (Preferred)",Detroit MI,Data Analytics Engineer
Atlantic Health System,/rc/clk?jk=6386cdd10ade93e6&fccid=82c5e27e77990fa2&vjs=3,"Job detailsSalary$84,000 - $104,000 a yearFull Job DescriptionPowered by endlessly curious people with an unwavering mission focus, Ball Aerospace pioneers discoveries that enable our customers to perform beyond expectation and protect what matters most. |  | We create innovative space solutions, enable more accurate weather forecasts, drive insightful observations of our planet, deliver actionable data and intelligence, and ensure those who defend our freedom go forward bravely and return home safely. |  | The Engineering Strategic Support Unit comprises the organizational talent and technical leadership that enables the successful delivery of high-impact discriminating technologies for our customers’ missions. Our collaborative, cross-functional teams are committed to innovation, integrity, continual learning and strong execution. | Engineer I – Software Data |  | Ball Aerospace is looking for a data engineer to join the Mission and Process Analytics (MPA) team. | What You’ll Do: |  | Be a member of a dynamic data analytics team; a team tasked with being a champion for data-driven decisions throughout the enterprise while advancing Ball Aerospace’s data analytics and data governance strategies and goals. | Collaborate with team members and business stakeholders on projects that answer key questions that enable informed decision-making. | Follow established procedures and leadership guidance to discover, blend, transform and interpret data from disparate sources. | Work with a variety of systems to integrate data and build out a data model/semantic layer for use in projects. Be open to guidance and technical feedback from leadership regarding results. | Design, implement, and publish visualizations/dashboards and other reporting tools for consumption by the business community, contributing as part of a team to fulfill the team’s goals in relation to MPA’s objectives. | Assist in the operationalization and quality assessments of software algorithms and re-usable components to mature the MPA toolkit. | Work closely with the Information Technology organization in the development of data solutions. | Partner with internal business stakeholders to gather requirements and develop solutions that provide access to information, when appropriate and in accordance with leadership guidance. | Bring new and innovative ideas to the data analytics team and Ball Aerospace. | Present technical and programmatic aspects of a project to the MPA team and to the stakeholder, when appropriate and in accordance with leadership guidance. | Maintain a regular and predictable work schedule. | Establish and maintain effective working relationships within the department, the Strategic Business Units, Strategic Support Units and the Company. Interact appropriately with others in order to maintain a positive and productive work environment. | Perform other duties as necessary. |  | What You’ll Need: |  | BS degree or higher in Engineering or a related technical field is required plus 2 or more years related experience. | Each higher-level degree, i.e., Master’s Degree or Ph.D., may substitute for two years of experience. Related technical experience may be considered in lieu of education. Degree must be from a university, college, or school which is accredited by an agency recognized by the US Secretary of Education, US Department of Education. | Degree in Computer Science is preferred. | A passion for solving problems and finding answers to business questions of moderate scope and complexity. | Attention to detail, self-motivation and go-getter attitude is required. | Working knowledge and understanding of data integration tools, such as SQL or VQL. | Understanding and ability to apply visualization tools and the development of dashboards and reporting solutions; tools like Tableau, Power BI, Business Objects Web Intelligence. | Familiar with software development fundamentals – data structures, methodologies, and writing code in R, Python, JavaScript, or C#. | Understanding of data warehousing concepts. | Strong collaboration skills. |  | Working Conditions: |  | Work is performed in an office, laboratory, production floor, or cleanroom, outdoors or remote environment. | May occasionally work in production work centers where use of protective equipment and gear is required. | May access other facilities in various weather conditions. | Travel and local commute between Ball campuses and other possible non-Ball locations may be required. | #LI-LO1 | Relocation for this position is not available. |  | Compensation &amp; Benefits: |  | HIRING SALARY RANGE: $84,000 - $104,000 (Salary to be determined by the education, experience, knowledge, skills, and abilities of the applicant, internal equity, and alignment with market data.) | For RFT/RPT Jobs: This position includes a competitive benefits package. Click here to learn more. |  | US CITIZENSHIP MAY BE REQUIRED |  | Ball Aerospace is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.",Westminster CO 80021,Engineer I - Software Data
Slalom Consulting,/rc/clk?jk=c6ea3cb4f00d2f77&fccid=8a97d8070e7b31f8&vjs=3,"Frequently cited statistics show that women and underrepresented groups apply to jobs only if they think they meet 100% of the qualifications on a job description. IMO wants you to apply, even if you don’t think you meet 100% of the qualifications listed. We look forward to receiving your application! | Work that is meaningful. A job that has impact. Colleagues that inspire. That’s what you’ll find at Intelligent Medical Objects (IMO), a growing health IT company creating clinical terminology and insights solutions that are used by more than 500,000 US physicians and 4,500 US hospitals to power better patient care and support meaningful analytics. | At IMO, a core team of clinicians, software developers, and data scientists combine computer science and medical expertise to help patients and healthcare professionals access high quality health information quickly and easily to improve total patient health. We are currently in need of a Data Engineer to join this team! | The Data Engineer will support our software developers, database architects, data analysts, and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. | Join our growing Software Engineering department as a Data Engineer to help design, create, and support high quality solutions that support 80% of US clinicians and build the application of Data Engineering within IMO! | Responsibilities | Maintain optimal data pipeline architecture. | Assemble large, complex data sets that meet functional / non-functional business requirements. | Implement internal process improvements in automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. | Manage existing data pipelines to implement source ingestion, gathering and logging access data. | Develop proficiency of some or all of tech stack for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies. | Maintain analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. | Requirements | A relevant technical BA/ BS Degree and one year of experience, or three years of relevant professional experience implementing well-architected data pipelines that are dynamically scalable, highly available, fault-tolerant, and reliable for analytics and platform solutions. | Working SQL knowledge and experience working with relational databases as well as working familiarity with a variety of databases. | Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. | Experience with relational SQL and NoSQL databases, such as PostgreSQL, DynamoDB, MongoDB and Elasticsearch. | Experience with ETL and BI Dashboard tools, such as Talend, Informatica, Tableau, Looker, etc. | Experience with AWS cloud services, such as EC2, EMR, RDS, Redshift. | Experience with object-oriented/functional scripting languages, such as Python, Java, C++, Scala, etc. | At IMO, we celebrate diversity and are committed to creating an inclusive environment for all employees. IMO is proud to be an equal opportunity workplace and is an affirmative action employer.",Rosemont IL,Data Engineer
Adtaxi,/company/srsconsultancy/jobs/Azure-Data-Engineer-4414e345885e148d?fccid=328678c13bfd2e06&vjs=3,"Job detailsSalary$75 an hourJob TypeContractNumber of hires for this role1Full Job DescriptionRole:  Azure Data EngineerLocation:  Pleasanton, CADuration:  08 MonthsWe are not Sponsoring at this time, we are looking for someone who can work for any employerEssential Skills· Azure Data Factory, Azure Synapse, Sql Server, SSIS, ETLRole Description· Works with Architects, Technical Leads and Business Teams· Contributes to the development of technical design· Develops, constructs, tests and maintains flows/pipelines· Designs and develops ETL Processes· Develops data warehouse and data marts· Develops code using programming language and tools· Performs code review· Coordinates and develops process automation/job scheduling; Perform testing; Deploys code to Production; Performs defect analysis.· Ensures compliance with industry best practices· Follows SAFe Agile framework and processesContract length: 9 monthsPart-time hours: 48 per weekJob Type: ContractSalary: $75.00 per hourSchedule:8 hour shiftVisa Sponsorship Potentially Available:No: Not providing sponsorship for this jobSpeak with the employer+91 510-257-2524",Fremont CA 94538,Azure Data Engineer